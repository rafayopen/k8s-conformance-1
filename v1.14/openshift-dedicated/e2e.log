Nov  7 00:31:04.756: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1107 00:31:04.756518    5393 e2e.go:242] Starting e2e run "e191d10d-00f5-11ea-aaa6-525400524259" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1573086663 - Will randomize all specs
Will run 204 of 3586 specs

Nov  7 00:31:04.820: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 00:31:04.823: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Nov  7 00:31:05.475: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov  7 00:31:05.764: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov  7 00:31:05.764: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Nov  7 00:31:05.764: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov  7 00:31:05.860: INFO: e2e test version: v1.14.9-beta.0.18+ac756284b8908e
Nov  7 00:31:05.952: INFO: kube-apiserver version: v1.14.6+868bc38
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:31:05.953: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
Nov  7 00:31:06.328: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e308d80f-00f5-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 00:31:06.618: INFO: Waiting up to 5m0s for pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259" in namespace "secrets-894" to be "success or failure"
Nov  7 00:31:06.711: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.708409ms
Nov  7 00:31:08.806: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188308942s
Nov  7 00:31:10.900: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282081934s
Nov  7 00:31:12.994: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376521454s
Nov  7 00:31:15.088: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470252894s
Nov  7 00:31:17.181: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.563844003s
Nov  7 00:31:19.277: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.659015073s
STEP: Saw pod success
Nov  7 00:31:19.277: INFO: Pod "pod-secrets-e3174789-00f5-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:31:19.370: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-e3174789-00f5-11ea-aaa6-525400524259 container secret-env-test: <nil>
STEP: delete the pod
Nov  7 00:31:19.570: INFO: Waiting for pod pod-secrets-e3174789-00f5-11ea-aaa6-525400524259 to disappear
Nov  7 00:31:19.663: INFO: Pod pod-secrets-e3174789-00f5-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:31:19.663: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-894" for this suite.
Nov  7 00:31:26.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:31:34.896: INFO: namespace secrets-894 deletion completed in 14.951436496s

• [SLOW TEST:28.944 seconds]
[sig-api-machinery] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:31:34.899: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Nov  7 00:31:35.373: INFO: Waiting up to 5m0s for pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259" in namespace "emptydir-3539" to be "success or failure"
Nov  7 00:31:35.466: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.394519ms
Nov  7 00:31:37.562: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189012479s
Nov  7 00:31:39.656: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282695993s
Nov  7 00:31:41.752: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.379399963s
Nov  7 00:31:43.848: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.475043683s
Nov  7 00:31:45.945: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.571782208s
Nov  7 00:31:48.044: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.671473253s
STEP: Saw pod success
Nov  7 00:31:48.045: INFO: Pod "pod-f43b1c93-00f5-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:31:48.138: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-f43b1c93-00f5-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 00:31:48.335: INFO: Waiting for pod pod-f43b1c93-00f5-11ea-aaa6-525400524259 to disappear
Nov  7 00:31:48.430: INFO: Pod pod-f43b1c93-00f5-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:31:48.431: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-3539" for this suite.
Nov  7 00:31:54.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:32:03.685: INFO: namespace emptydir-3539 deletion completed in 14.977604674s

• [SLOW TEST:28.787 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:32:03.686: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Nov  7 00:32:04.065: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-7455'
Nov  7 00:32:05.635: INFO: stderr: ""
Nov  7 00:32:05.635: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Nov  7 00:32:06.730: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:06.730: INFO: Found 0 / 1
Nov  7 00:32:07.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:07.729: INFO: Found 0 / 1
Nov  7 00:32:08.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:08.729: INFO: Found 0 / 1
Nov  7 00:32:09.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:09.729: INFO: Found 0 / 1
Nov  7 00:32:10.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:10.729: INFO: Found 0 / 1
Nov  7 00:32:11.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:11.729: INFO: Found 0 / 1
Nov  7 00:32:12.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:12.729: INFO: Found 0 / 1
Nov  7 00:32:13.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:13.729: INFO: Found 0 / 1
Nov  7 00:32:14.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:14.729: INFO: Found 0 / 1
Nov  7 00:32:15.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:15.729: INFO: Found 0 / 1
Nov  7 00:32:16.729: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:16.729: INFO: Found 1 / 1
Nov  7 00:32:16.729: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov  7 00:32:16.822: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 00:32:16.822: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Nov  7 00:32:16.822: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig logs redis-master-29nsm redis-master --namespace=kubectl-7455'
Nov  7 00:32:17.364: INFO: stderr: ""
Nov  7 00:32:17.364: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 07 Nov 00:31:40.420 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 07 Nov 00:31:40.420 # Server started, Redis version 3.2.12\n1:M 07 Nov 00:31:40.420 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 07 Nov 00:31:40.420 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Nov  7 00:32:17.364: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig log redis-master-29nsm redis-master --namespace=kubectl-7455 --tail=1'
Nov  7 00:32:17.896: INFO: stderr: ""
Nov  7 00:32:17.896: INFO: stdout: "1:M 07 Nov 00:31:40.420 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Nov  7 00:32:17.896: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig log redis-master-29nsm redis-master --namespace=kubectl-7455 --limit-bytes=1'
Nov  7 00:32:18.421: INFO: stderr: ""
Nov  7 00:32:18.421: INFO: stdout: " "
STEP: exposing timestamps
Nov  7 00:32:18.421: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig log redis-master-29nsm redis-master --namespace=kubectl-7455 --tail=1 --timestamps'
Nov  7 00:32:18.984: INFO: stderr: ""
Nov  7 00:32:18.984: INFO: stdout: "2019-11-07T00:31:40.420388979Z 1:M 07 Nov 00:31:40.420 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Nov  7 00:32:21.485: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig log redis-master-29nsm redis-master --namespace=kubectl-7455 --since=1s'
Nov  7 00:32:22.037: INFO: stderr: ""
Nov  7 00:32:22.037: INFO: stdout: ""
Nov  7 00:32:22.037: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig log redis-master-29nsm redis-master --namespace=kubectl-7455 --since=24h'
Nov  7 00:32:22.561: INFO: stderr: ""
Nov  7 00:32:22.561: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 07 Nov 00:31:40.420 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 07 Nov 00:31:40.420 # Server started, Redis version 3.2.12\n1:M 07 Nov 00:31:40.420 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 07 Nov 00:31:40.420 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Nov  7 00:32:22.561: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-7455'
Nov  7 00:32:23.080: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 00:32:23.080: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Nov  7 00:32:23.080: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=kubectl-7455'
Nov  7 00:32:23.608: INFO: stderr: "No resources found.\n"
Nov  7 00:32:23.608: INFO: stdout: ""
Nov  7 00:32:23.608: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -l name=nginx --namespace=kubectl-7455 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  7 00:32:24.054: INFO: stderr: ""
Nov  7 00:32:24.054: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:32:24.054: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-7455" for this suite.
Nov  7 00:32:36.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:32:45.290: INFO: namespace kubectl-7455 deletion completed in 20.960318551s

• [SLOW TEST:41.604 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:32:45.292: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 00:32:45.855: INFO: (0) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 95.68975ms)
Nov  7 00:32:45.949: INFO: (1) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.009973ms)
Nov  7 00:32:46.043: INFO: (2) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.997218ms)
Nov  7 00:32:46.137: INFO: (3) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.793313ms)
Nov  7 00:32:46.231: INFO: (4) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.66452ms)
Nov  7 00:32:46.325: INFO: (5) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.767301ms)
Nov  7 00:32:46.419: INFO: (6) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.002939ms)
Nov  7 00:32:46.513: INFO: (7) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.03556ms)
Nov  7 00:32:46.606: INFO: (8) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.651308ms)
Nov  7 00:32:46.700: INFO: (9) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.930204ms)
Nov  7 00:32:46.794: INFO: (10) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.705785ms)
Nov  7 00:32:46.888: INFO: (11) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.680304ms)
Nov  7 00:32:46.982: INFO: (12) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.006333ms)
Nov  7 00:32:47.076: INFO: (13) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.660965ms)
Nov  7 00:32:47.169: INFO: (14) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.604751ms)
Nov  7 00:32:47.263: INFO: (15) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.974453ms)
Nov  7 00:32:47.357: INFO: (16) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.69738ms)
Nov  7 00:32:47.451: INFO: (17) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.131852ms)
Nov  7 00:32:47.545: INFO: (18) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.029171ms)
Nov  7 00:32:47.639: INFO: (19) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.045223ms)
[AfterEach] version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:32:47.639: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-8256" for this suite.
Nov  7 00:32:54.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:33:01.914: INFO: namespace proxy-8256 deletion completed in 14.18089313s

• [SLOW TEST:16.623 seconds]
[sig-network] Proxy
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:33:01.915: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 00:33:02.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259" in namespace "projected-1819" to be "success or failure"
Nov  7 00:33:02.483: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.103073ms
Nov  7 00:33:04.577: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186926986s
Nov  7 00:33:06.671: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281095812s
Nov  7 00:33:08.765: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.37514424s
Nov  7 00:33:10.860: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470072803s
Nov  7 00:33:12.957: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.567140082s
STEP: Saw pod success
Nov  7 00:33:12.957: INFO: Pod "downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:33:13.052: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 00:33:13.256: INFO: Waiting for pod downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259 to disappear
Nov  7 00:33:13.350: INFO: Pod downwardapi-volume-2818d15b-00f6-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:33:13.350: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1819" for this suite.
Nov  7 00:33:19.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:33:28.573: INFO: namespace projected-1819 deletion completed in 14.947220224s

• [SLOW TEST:26.658 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:33:28.574: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:34:29.144: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-9181" for this suite.
Nov  7 00:34:41.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:34:50.378: INFO: namespace container-probe-9181 deletion completed in 20.957906319s

• [SLOW TEST:81.804 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:34:50.379: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 00:34:50.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259" in namespace "projected-6804" to be "success or failure"
Nov  7 00:34:50.951: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.311336ms
Nov  7 00:34:53.045: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187133805s
Nov  7 00:34:55.140: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282440236s
Nov  7 00:34:57.234: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.377036649s
Nov  7 00:34:59.328: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470764914s
Nov  7 00:35:01.422: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.564750231s
Nov  7 00:35:03.517: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.659419143s
STEP: Saw pod success
Nov  7 00:35:03.517: INFO: Pod "downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:35:03.610: INFO: Trying to get logs from node ip-10-0-136-94.us-west-2.compute.internal pod downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 00:35:03.810: INFO: Waiting for pod downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259 to disappear
Nov  7 00:35:03.903: INFO: Pod downwardapi-volume-68bf8014-00f6-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:35:03.903: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6804" for this suite.
Nov  7 00:35:10.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:35:19.134: INFO: namespace projected-6804 deletion completed in 14.955618709s

• [SLOW TEST:28.755 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:35:19.135: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov  7 00:35:31.984: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-79e3062b-00f6-11ea-aaa6-525400524259,GenerateName:,Namespace:events-681,SelfLink:/api/v1/namespaces/events-681/pods/send-events-79e3062b-00f6-11ea-aaa6-525400524259,UID:64e615c4-00f6-11ea-98c4-0655ae4e8e56,ResourceVersion:60902,Generation:0,CreationTimestamp:2019-11-07 00:34:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 509768713,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.32"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9b2jt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9b2jt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-9b2jt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c26,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-wcmhm}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00261a4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00261a510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 00:34:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 00:34:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 00:34:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 00:34:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:10.130.2.32,StartTime:2019-11-07 00:34:44 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-11-07 00:34:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 cri-o://135d5147007e7816dd8e1050a28178a63ec584c37e321a8632dae296c3f1761e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Nov  7 00:35:34.079: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov  7 00:35:36.173: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:35:36.269: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-681" for this suite.
Nov  7 00:36:20.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:36:29.540: INFO: namespace events-681 deletion completed in 52.99540081s

• [SLOW TEST:70.406 seconds]
[k8s.io] [sig-node] Events
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:36:29.543: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov  7 00:36:30.018: INFO: Waiting up to 5m0s for pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259" in namespace "emptydir-4455" to be "success or failure"
Nov  7 00:36:30.111: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.503512ms
Nov  7 00:36:32.205: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187399021s
Nov  7 00:36:34.299: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280921034s
Nov  7 00:36:36.394: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375697325s
Nov  7 00:36:38.487: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469383042s
Nov  7 00:36:40.581: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.563493937s
STEP: Saw pod success
Nov  7 00:36:40.582: INFO: Pod "pod-a3da4c86-00f6-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:36:40.675: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-a3da4c86-00f6-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 00:36:40.872: INFO: Waiting for pod pod-a3da4c86-00f6-11ea-aaa6-525400524259 to disappear
Nov  7 00:36:40.965: INFO: Pod pod-a3da4c86-00f6-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:36:40.965: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4455" for this suite.
Nov  7 00:36:47.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:36:56.189: INFO: namespace emptydir-4455 deletion completed in 14.947609394s

• [SLOW TEST:26.647 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:36:56.190: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov  7 00:37:21.520: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:21.614: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:23.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:23.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:25.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:25.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:27.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:27.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:29.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:29.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:31.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:31.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:33.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:33.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:35.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:35.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:37.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:37.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:39.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:39.708: INFO: Pod pod-with-poststart-exec-hook still exists
Nov  7 00:37:41.614: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov  7 00:37:41.708: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:37:41.708: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4607" for this suite.
Nov  7 00:37:54.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:38:02.969: INFO: namespace container-lifecycle-hook-4607 deletion completed in 20.985266589s

• [SLOW TEST:66.779 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:38:02.969: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-8xtmt in namespace proxy-1332
I1107 00:38:03.540127    5393 runners.go:184] Created replication controller with name: proxy-service-8xtmt, namespace: proxy-1332, replica count: 1
I1107 00:38:04.640752    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:05.641150    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:06.641551    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:07.641834    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:08.643094    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:09.644183    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:10.644438    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:11.644730    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:12.645207    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:13.645590    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:14.645886    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:38:15.646151    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:16.646433    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:17.646672    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:18.646898    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:19.647105    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:20.647402    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:21.647579    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:22.647737    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1107 00:38:23.647949    5393 runners.go:184] proxy-service-8xtmt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  7 00:38:23.742: INFO: setup took 20.395712284s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov  7 00:38:23.840: INFO: (0) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.932436ms)
Nov  7 00:38:23.840: INFO: (0) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.089442ms)
Nov  7 00:38:23.840: INFO: (0) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.259499ms)
Nov  7 00:38:23.840: INFO: (0) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 97.286917ms)
Nov  7 00:38:23.840: INFO: (0) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.322503ms)
Nov  7 00:38:23.844: INFO: (0) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 101.09958ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 188.143523ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 186.967814ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 187.108889ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 186.985998ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 187.021218ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 187.10526ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 188.308072ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 187.032469ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 187.040758ms)
Nov  7 00:38:23.930: INFO: (0) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 187.100802ms)
Nov  7 00:38:24.026: INFO: (1) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.367318ms)
Nov  7 00:38:24.027: INFO: (1) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.529962ms)
Nov  7 00:38:24.027: INFO: (1) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 96.963281ms)
Nov  7 00:38:24.027: INFO: (1) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 96.925704ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.203086ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.249492ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.290349ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.433718ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.528392ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.763482ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.7027ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.832185ms)
Nov  7 00:38:24.028: INFO: (1) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 97.885611ms)
Nov  7 00:38:24.116: INFO: (1) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 186.134405ms)
Nov  7 00:38:24.116: INFO: (1) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 186.20572ms)
Nov  7 00:38:24.117: INFO: (1) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 186.189434ms)
Nov  7 00:38:24.214: INFO: (2) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 96.76327ms)
Nov  7 00:38:24.214: INFO: (2) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 96.93556ms)
Nov  7 00:38:24.214: INFO: (2) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 96.868322ms)
Nov  7 00:38:24.214: INFO: (2) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.496068ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.331391ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.670898ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 98.228788ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.83373ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 98.007876ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 98.555631ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.283786ms)
Nov  7 00:38:24.215: INFO: (2) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.284302ms)
Nov  7 00:38:24.216: INFO: (2) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 99.6513ms)
Nov  7 00:38:24.216: INFO: (2) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 98.997519ms)
Nov  7 00:38:24.216: INFO: (2) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 99.428874ms)
Nov  7 00:38:24.216: INFO: (2) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 99.323316ms)
Nov  7 00:38:24.312: INFO: (3) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.5779ms)
Nov  7 00:38:24.312: INFO: (3) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.635133ms)
Nov  7 00:38:24.312: INFO: (3) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 95.733121ms)
Nov  7 00:38:24.312: INFO: (3) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 95.623831ms)
Nov  7 00:38:24.313: INFO: (3) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.031712ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.596384ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.498237ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.666214ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.680252ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.827226ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.747339ms)
Nov  7 00:38:24.314: INFO: (3) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 98.066137ms)
Nov  7 00:38:24.315: INFO: (3) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 98.29255ms)
Nov  7 00:38:24.315: INFO: (3) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 98.370189ms)
Nov  7 00:38:24.315: INFO: (3) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 98.259076ms)
Nov  7 00:38:24.315: INFO: (3) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.498026ms)
Nov  7 00:38:24.411: INFO: (4) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 95.649635ms)
Nov  7 00:38:24.411: INFO: (4) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 95.497024ms)
Nov  7 00:38:24.413: INFO: (4) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.579841ms)
Nov  7 00:38:24.413: INFO: (4) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.388189ms)
Nov  7 00:38:24.413: INFO: (4) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.947283ms)
Nov  7 00:38:24.413: INFO: (4) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.047148ms)
Nov  7 00:38:24.414: INFO: (4) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 98.083408ms)
Nov  7 00:38:24.414: INFO: (4) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 98.66691ms)
Nov  7 00:38:24.414: INFO: (4) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 99.258814ms)
Nov  7 00:38:24.414: INFO: (4) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 99.102721ms)
Nov  7 00:38:24.415: INFO: (4) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 99.181353ms)
Nov  7 00:38:24.415: INFO: (4) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 99.388199ms)
Nov  7 00:38:24.415: INFO: (4) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 99.209464ms)
Nov  7 00:38:24.415: INFO: (4) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 99.820997ms)
Nov  7 00:38:24.415: INFO: (4) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 99.878882ms)
Nov  7 00:38:24.415: INFO: (4) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 99.931651ms)
Nov  7 00:38:24.510: INFO: (5) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 94.218285ms)
Nov  7 00:38:24.510: INFO: (5) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 94.436076ms)
Nov  7 00:38:24.510: INFO: (5) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 94.641371ms)
Nov  7 00:38:24.512: INFO: (5) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.013677ms)
Nov  7 00:38:24.512: INFO: (5) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 96.516661ms)
Nov  7 00:38:24.512: INFO: (5) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.322735ms)
Nov  7 00:38:24.512: INFO: (5) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 96.499644ms)
Nov  7 00:38:24.512: INFO: (5) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 96.840806ms)
Nov  7 00:38:24.513: INFO: (5) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.865558ms)
Nov  7 00:38:24.513: INFO: (5) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.036688ms)
Nov  7 00:38:24.513: INFO: (5) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 96.888023ms)
Nov  7 00:38:24.513: INFO: (5) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.758497ms)
Nov  7 00:38:24.514: INFO: (5) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 98.36626ms)
Nov  7 00:38:24.514: INFO: (5) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.100529ms)
Nov  7 00:38:24.514: INFO: (5) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 98.217777ms)
Nov  7 00:38:24.514: INFO: (5) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 97.905069ms)
Nov  7 00:38:24.609: INFO: (6) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 94.809163ms)
Nov  7 00:38:24.610: INFO: (6) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 95.635787ms)
Nov  7 00:38:24.610: INFO: (6) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.820184ms)
Nov  7 00:38:24.610: INFO: (6) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 95.803435ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.418698ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.193086ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 96.947751ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 96.593586ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.824994ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.317383ms)
Nov  7 00:38:24.611: INFO: (6) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 97.318104ms)
Nov  7 00:38:24.612: INFO: (6) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.301304ms)
Nov  7 00:38:24.612: INFO: (6) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.227407ms)
Nov  7 00:38:24.612: INFO: (6) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.246206ms)
Nov  7 00:38:24.612: INFO: (6) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.429402ms)
Nov  7 00:38:24.612: INFO: (6) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 97.562555ms)
Nov  7 00:38:24.710: INFO: (7) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.413629ms)
Nov  7 00:38:24.711: INFO: (7) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 98.392361ms)
Nov  7 00:38:24.711: INFO: (7) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 98.992142ms)
Nov  7 00:38:24.711: INFO: (7) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 99.080023ms)
Nov  7 00:38:24.712: INFO: (7) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 99.338348ms)
Nov  7 00:38:24.712: INFO: (7) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 100.343688ms)
Nov  7 00:38:24.713: INFO: (7) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 100.245337ms)
Nov  7 00:38:24.713: INFO: (7) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 100.701362ms)
Nov  7 00:38:24.714: INFO: (7) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 100.955807ms)
Nov  7 00:38:24.714: INFO: (7) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 101.707287ms)
Nov  7 00:38:24.714: INFO: (7) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 101.672496ms)
Nov  7 00:38:24.715: INFO: (7) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 102.368081ms)
Nov  7 00:38:24.715: INFO: (7) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 102.489239ms)
Nov  7 00:38:24.715: INFO: (7) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 102.644099ms)
Nov  7 00:38:24.715: INFO: (7) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 102.587165ms)
Nov  7 00:38:24.717: INFO: (7) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 104.70334ms)
Nov  7 00:38:24.813: INFO: (8) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.779252ms)
Nov  7 00:38:24.814: INFO: (8) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.928615ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.047689ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.430813ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.436662ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.534483ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.742549ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.750205ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.709046ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.850594ms)
Nov  7 00:38:24.815: INFO: (8) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.867031ms)
Nov  7 00:38:24.816: INFO: (8) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.878677ms)
Nov  7 00:38:24.816: INFO: (8) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 98.38288ms)
Nov  7 00:38:24.816: INFO: (8) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 98.470645ms)
Nov  7 00:38:24.816: INFO: (8) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.479122ms)
Nov  7 00:38:24.816: INFO: (8) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 98.468231ms)
Nov  7 00:38:24.911: INFO: (9) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 94.928203ms)
Nov  7 00:38:24.911: INFO: (9) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 94.671554ms)
Nov  7 00:38:24.912: INFO: (9) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.370998ms)
Nov  7 00:38:24.912: INFO: (9) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 95.452242ms)
Nov  7 00:38:24.912: INFO: (9) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 95.383843ms)
Nov  7 00:38:24.912: INFO: (9) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 95.612761ms)
Nov  7 00:38:24.912: INFO: (9) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.023572ms)
Nov  7 00:38:24.913: INFO: (9) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 96.308171ms)
Nov  7 00:38:24.913: INFO: (9) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 96.917598ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.291521ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.288759ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.353166ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.339083ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.635601ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.589362ms)
Nov  7 00:38:24.914: INFO: (9) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 97.737366ms)
Nov  7 00:38:25.011: INFO: (10) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 97.211546ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.460602ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.608937ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.640814ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.868693ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.746246ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.811444ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.9009ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.967914ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 98.13076ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 98.140907ms)
Nov  7 00:38:25.012: INFO: (10) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 98.417634ms)
Nov  7 00:38:25.013: INFO: (10) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 98.471621ms)
Nov  7 00:38:25.013: INFO: (10) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.56818ms)
Nov  7 00:38:25.013: INFO: (10) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.759858ms)
Nov  7 00:38:25.014: INFO: (10) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 99.59917ms)
Nov  7 00:38:25.110: INFO: (11) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.822293ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.02977ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.093628ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.285231ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.297525ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.358674ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.386905ms)
Nov  7 00:38:25.111: INFO: (11) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.812114ms)
Nov  7 00:38:25.112: INFO: (11) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.931188ms)
Nov  7 00:38:25.112: INFO: (11) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.972716ms)
Nov  7 00:38:25.112: INFO: (11) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.956657ms)
Nov  7 00:38:25.112: INFO: (11) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 98.250185ms)
Nov  7 00:38:25.113: INFO: (11) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 98.962496ms)
Nov  7 00:38:25.113: INFO: (11) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.974323ms)
Nov  7 00:38:25.113: INFO: (11) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 99.159209ms)
Nov  7 00:38:25.113: INFO: (11) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 99.142803ms)
Nov  7 00:38:25.209: INFO: (12) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 95.587322ms)
Nov  7 00:38:25.209: INFO: (12) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 96.356256ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 96.486065ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 96.713728ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 96.707287ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.882373ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.057326ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.12722ms)
Nov  7 00:38:25.210: INFO: (12) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.119373ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.46515ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 97.489943ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.735685ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.704184ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.950315ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.0932ms)
Nov  7 00:38:25.211: INFO: (12) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.010672ms)
Nov  7 00:38:25.307: INFO: (13) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 95.778096ms)
Nov  7 00:38:25.307: INFO: (13) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 95.502911ms)
Nov  7 00:38:25.307: INFO: (13) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 95.761173ms)
Nov  7 00:38:25.307: INFO: (13) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 95.781838ms)
Nov  7 00:38:25.307: INFO: (13) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.201234ms)
Nov  7 00:38:25.308: INFO: (13) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 96.140937ms)
Nov  7 00:38:25.308: INFO: (13) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 96.864954ms)
Nov  7 00:38:25.308: INFO: (13) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 96.885395ms)
Nov  7 00:38:25.309: INFO: (13) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.024233ms)
Nov  7 00:38:25.309: INFO: (13) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 97.305647ms)
Nov  7 00:38:25.309: INFO: (13) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.260631ms)
Nov  7 00:38:25.309: INFO: (13) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.998641ms)
Nov  7 00:38:25.309: INFO: (13) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.636541ms)
Nov  7 00:38:25.310: INFO: (13) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.052627ms)
Nov  7 00:38:25.310: INFO: (13) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 98.259502ms)
Nov  7 00:38:25.310: INFO: (13) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 98.288451ms)
Nov  7 00:38:25.405: INFO: (14) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 94.904307ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.889573ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.581453ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.549473ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 97.799999ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.936844ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.641906ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.749861ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.895818ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.797084ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.9721ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.113285ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 98.37341ms)
Nov  7 00:38:25.408: INFO: (14) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 98.299573ms)
Nov  7 00:38:25.409: INFO: (14) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.294058ms)
Nov  7 00:38:25.409: INFO: (14) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 98.400021ms)
Nov  7 00:38:25.505: INFO: (15) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 96.548643ms)
Nov  7 00:38:25.506: INFO: (15) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.210609ms)
Nov  7 00:38:25.506: INFO: (15) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.388779ms)
Nov  7 00:38:25.506: INFO: (15) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.558592ms)
Nov  7 00:38:25.506: INFO: (15) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 97.661035ms)
Nov  7 00:38:25.506: INFO: (15) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.739471ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 98.123326ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 98.311889ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.332817ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 98.41066ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 98.470028ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 98.618278ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 98.700452ms)
Nov  7 00:38:25.507: INFO: (15) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.710221ms)
Nov  7 00:38:25.508: INFO: (15) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 99.448727ms)
Nov  7 00:38:25.508: INFO: (15) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 99.560808ms)
Nov  7 00:38:25.603: INFO: (16) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 94.853598ms)
Nov  7 00:38:25.604: INFO: (16) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.947633ms)
Nov  7 00:38:25.605: INFO: (16) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.6839ms)
Nov  7 00:38:25.605: INFO: (16) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 96.658234ms)
Nov  7 00:38:25.605: INFO: (16) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 96.857223ms)
Nov  7 00:38:25.606: INFO: (16) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.853907ms)
Nov  7 00:38:25.606: INFO: (16) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.980601ms)
Nov  7 00:38:25.606: INFO: (16) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.890449ms)
Nov  7 00:38:25.606: INFO: (16) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 98.01202ms)
Nov  7 00:38:25.606: INFO: (16) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.891311ms)
Nov  7 00:38:25.607: INFO: (16) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.641485ms)
Nov  7 00:38:25.607: INFO: (16) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 99.058858ms)
Nov  7 00:38:25.608: INFO: (16) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 99.173927ms)
Nov  7 00:38:25.608: INFO: (16) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 99.183318ms)
Nov  7 00:38:25.608: INFO: (16) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 99.214514ms)
Nov  7 00:38:25.608: INFO: (16) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 99.436306ms)
Nov  7 00:38:25.705: INFO: (17) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.686236ms)
Nov  7 00:38:25.706: INFO: (17) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.755835ms)
Nov  7 00:38:25.706: INFO: (17) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.040792ms)
Nov  7 00:38:25.706: INFO: (17) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 97.95004ms)
Nov  7 00:38:25.706: INFO: (17) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 98.086718ms)
Nov  7 00:38:25.706: INFO: (17) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 98.462257ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 99.553544ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 99.504373ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 99.451937ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 99.489646ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 99.505585ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 99.445899ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 99.462684ms)
Nov  7 00:38:25.707: INFO: (17) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 99.58956ms)
Nov  7 00:38:25.708: INFO: (17) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 99.724343ms)
Nov  7 00:38:25.708: INFO: (17) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 99.763336ms)
Nov  7 00:38:25.803: INFO: (18) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 95.384435ms)
Nov  7 00:38:25.803: INFO: (18) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 95.641258ms)
Nov  7 00:38:25.804: INFO: (18) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 95.85214ms)
Nov  7 00:38:25.804: INFO: (18) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 96.001151ms)
Nov  7 00:38:25.804: INFO: (18) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 95.882879ms)
Nov  7 00:38:25.805: INFO: (18) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 97.327706ms)
Nov  7 00:38:25.805: INFO: (18) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.532475ms)
Nov  7 00:38:25.805: INFO: (18) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 97.627117ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 97.704064ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 97.683577ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.79276ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.825634ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.93078ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.001392ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 98.201755ms)
Nov  7 00:38:25.806: INFO: (18) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 98.332215ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh/proxy/rewriteme">test</a> (200; 97.321026ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:443/proxy/tlsrewritem... (200; 97.393537ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.392906ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:160/proxy/: foo (200; 97.421406ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:460/proxy/: tls baz (200; 97.536625ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname1/proxy/: tls baz (200; 97.816777ms)
Nov  7 00:38:25.904: INFO: (19) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 97.806776ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname1/proxy/: foo (200; 98.273815ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">test<... (200; 98.294363ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:1080/proxy/rewriteme">... (200; 98.413764ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/services/http:proxy-service-8xtmt:portname2/proxy/: bar (200; 98.597974ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname1/proxy/: foo (200; 98.704696ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/services/https:proxy-service-8xtmt:tlsportname2/proxy/: tls qux (200; 98.874795ms)
Nov  7 00:38:25.905: INFO: (19) /api/v1/namespaces/proxy-1332/services/proxy-service-8xtmt:portname2/proxy/: bar (200; 98.932123ms)
Nov  7 00:38:25.906: INFO: (19) /api/v1/namespaces/proxy-1332/pods/http:proxy-service-8xtmt-pnhlh:162/proxy/: bar (200; 99.293209ms)
Nov  7 00:38:25.906: INFO: (19) /api/v1/namespaces/proxy-1332/pods/https:proxy-service-8xtmt-pnhlh:462/proxy/: tls qux (200; 99.406535ms)
STEP: deleting ReplicationController proxy-service-8xtmt in namespace proxy-1332, will wait for the garbage collector to delete the pods
Nov  7 00:38:26.195: INFO: Deleting ReplicationController proxy-service-8xtmt took: 96.181615ms
Nov  7 00:38:26.295: INFO: Terminating ReplicationController proxy-service-8xtmt pods took: 100.238336ms
[AfterEach] version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:38:31.196: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-1332" for this suite.
Nov  7 00:38:37.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:38:45.664: INFO: namespace proxy-1332 deletion completed in 14.191244365s

• [SLOW TEST:42.694 seconds]
[sig-network] Proxy
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:38:45.664: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 00:38:46.042: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5384'
Nov  7 00:38:50.384: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov  7 00:38:50.384: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Nov  7 00:38:50.753: INFO: scanned /home/jeder for discovery docs: <nil>
Nov  7 00:38:50.753: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5384'
Nov  7 00:39:16.238: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov  7 00:39:16.238: INFO: stdout: "Created e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6\nScaling up e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Nov  7 00:39:16.238: INFO: stdout: "Created e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6\nScaling up e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Nov  7 00:39:16.238: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5384'
Nov  7 00:39:16.677: INFO: stderr: ""
Nov  7 00:39:16.677: INFO: stdout: "e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6-98fr4 "
Nov  7 00:39:16.677: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6-98fr4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5384'
Nov  7 00:39:17.133: INFO: stderr: ""
Nov  7 00:39:17.133: INFO: stdout: "true"
Nov  7 00:39:17.133: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6-98fr4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5384'
Nov  7 00:39:17.564: INFO: stderr: ""
Nov  7 00:39:17.564: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Nov  7 00:39:17.564: INFO: e2e-test-nginx-rc-a7b225ae937ef0775a2d0659d6f3f5b6-98fr4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Nov  7 00:39:17.564: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete rc e2e-test-nginx-rc --namespace=kubectl-5384'
Nov  7 00:39:18.120: INFO: stderr: ""
Nov  7 00:39:18.120: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:39:18.120: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5384" for this suite.
Nov  7 00:39:24.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:39:33.350: INFO: namespace kubectl-5384 deletion completed in 14.954353661s

• [SLOW TEST:47.687 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:39:33.352: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:40:18.671: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-6686" for this suite.
Nov  7 00:40:25.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:40:33.866: INFO: namespace container-runtime-6686 deletion completed in 14.919563557s

• [SLOW TEST:60.514 seconds]
[k8s.io] Container Runtime
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:40:33.868: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4486.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4486.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4486.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4486.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4486.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4486.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  7 00:40:55.386: INFO: DNS probes using dns-4486/dns-test-357bdfaa-00f7-11ea-aaa6-525400524259 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:40:55.489: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-4486" for this suite.
Nov  7 00:41:02.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:41:10.702: INFO: namespace dns-4486 deletion completed in 14.934721944s

• [SLOW TEST:36.834 seconds]
[sig-network] DNS
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:41:10.705: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov  7 00:41:11.179: INFO: Waiting up to 5m0s for pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259" in namespace "emptydir-3653" to be "success or failure"
Nov  7 00:41:11.272: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.207334ms
Nov  7 00:41:13.366: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187107571s
Nov  7 00:41:15.460: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280611235s
Nov  7 00:41:17.553: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374446829s
Nov  7 00:41:19.647: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.467470988s
Nov  7 00:41:21.741: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.562364465s
STEP: Saw pod success
Nov  7 00:41:21.742: INFO: Pod "pod-4b701c6e-00f7-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:41:21.834: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-4b701c6e-00f7-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 00:41:22.029: INFO: Waiting for pod pod-4b701c6e-00f7-11ea-aaa6-525400524259 to disappear
Nov  7 00:41:22.121: INFO: Pod pod-4b701c6e-00f7-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:41:22.121: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-3653" for this suite.
Nov  7 00:41:28.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:41:37.328: INFO: namespace emptydir-3653 deletion completed in 14.931985694s

• [SLOW TEST:26.623 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:41:37.329: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov  7 00:41:37.803: INFO: Waiting up to 5m0s for pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259" in namespace "emptydir-4806" to be "success or failure"
Nov  7 00:41:37.897: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.98192ms
Nov  7 00:41:39.990: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186529483s
Nov  7 00:41:42.084: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28005759s
Nov  7 00:41:44.179: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375573056s
Nov  7 00:41:46.272: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.468788032s
Nov  7 00:41:48.368: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.564227836s
STEP: Saw pod success
Nov  7 00:41:48.368: INFO: Pod "pod-5b4ec539-00f7-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:41:48.462: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-5b4ec539-00f7-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 00:41:48.658: INFO: Waiting for pod pod-5b4ec539-00f7-11ea-aaa6-525400524259 to disappear
Nov  7 00:41:48.751: INFO: Pod pod-5b4ec539-00f7-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:41:48.751: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4806" for this suite.
Nov  7 00:41:55.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:42:03.961: INFO: namespace emptydir-4806 deletion completed in 14.935591378s

• [SLOW TEST:26.632 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:42:03.963: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-6b2f25ec-00f7-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 00:42:04.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259" in namespace "configmap-9757" to be "success or failure"
Nov  7 00:42:04.632: INFO: Pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 94.172333ms
Nov  7 00:42:06.726: INFO: Pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188185687s
Nov  7 00:42:08.818: INFO: Pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281032716s
Nov  7 00:42:10.914: INFO: Pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376581368s
Nov  7 00:42:13.007: INFO: Pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.469944582s
STEP: Saw pod success
Nov  7 00:42:13.007: INFO: Pod "pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:42:13.101: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 00:42:13.295: INFO: Waiting for pod pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259 to disappear
Nov  7 00:42:13.388: INFO: Pod pod-configmaps-6b3ddb89-00f7-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:42:13.388: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9757" for this suite.
Nov  7 00:42:19.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:42:28.588: INFO: namespace configmap-9757 deletion completed in 14.925472444s

• [SLOW TEST:24.626 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:42:28.592: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Nov  7 00:42:39.470: INFO: Pod pod-hostip-79dd81df-00f7-11ea-aaa6-525400524259 has hostIP: 10.0.132.48
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:42:39.471: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1350" for this suite.
Nov  7 00:43:08.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:43:16.677: INFO: namespace pods-1350 deletion completed in 36.929413705s

• [SLOW TEST:48.086 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:43:16.680: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-9686d0a5-00f7-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 00:43:17.253: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259" in namespace "projected-5335" to be "success or failure"
Nov  7 00:43:17.346: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.023792ms
Nov  7 00:43:19.440: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186621228s
Nov  7 00:43:21.534: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280851922s
Nov  7 00:43:23.628: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374522474s
Nov  7 00:43:25.721: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.468014854s
Nov  7 00:43:27.816: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.563361516s
STEP: Saw pod success
Nov  7 00:43:27.817: INFO: Pod "pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:43:27.910: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 00:43:28.106: INFO: Waiting for pod pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259 to disappear
Nov  7 00:43:28.199: INFO: Pod pod-projected-configmaps-969550c4-00f7-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:43:28.199: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5335" for this suite.
Nov  7 00:43:34.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:43:43.396: INFO: namespace projected-5335 deletion completed in 14.922834134s

• [SLOW TEST:26.716 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:43:43.399: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 00:43:43.773: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9221'
Nov  7 00:43:44.260: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov  7 00:43:44.260: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Nov  7 00:43:46.447: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=kubectl-9221'
Nov  7 00:43:47.015: INFO: stderr: ""
Nov  7 00:43:47.015: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:43:47.016: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9221" for this suite.
Nov  7 00:43:53.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:44:02.202: INFO: namespace kubectl-9221 deletion completed in 14.912163676s

• [SLOW TEST:18.803 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:44:02.202: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b1a9874b-00f7-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 00:44:02.778: INFO: Waiting up to 5m0s for pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259" in namespace "secrets-8623" to be "success or failure"
Nov  7 00:44:02.871: INFO: Pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.759675ms
Nov  7 00:44:04.964: INFO: Pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186500844s
Nov  7 00:44:07.058: INFO: Pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280320341s
Nov  7 00:44:09.152: INFO: Pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3736156s
Nov  7 00:44:11.245: INFO: Pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.467200268s
STEP: Saw pod success
Nov  7 00:44:11.245: INFO: Pod "pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:44:11.338: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 00:44:11.532: INFO: Waiting for pod pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259 to disappear
Nov  7 00:44:11.625: INFO: Pod pod-secrets-b1b7dd46-00f7-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:44:11.625: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-8623" for this suite.
Nov  7 00:44:18.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:44:26.932: INFO: namespace secrets-8623 deletion completed in 15.032895298s

• [SLOW TEST:24.731 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:44:26.933: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3239
I1107 00:44:27.399444    5393 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3239, replica count: 1
I1107 00:44:28.500048    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:29.500364    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:30.500651    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:31.500927    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:32.501212    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:33.501567    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:34.501787    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:35.502050    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1107 00:44:36.502280    5393 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov  7 00:44:36.700: INFO: Created: latency-svc-rbfpz
Nov  7 00:44:36.711: INFO: Got endpoints: latency-svc-rbfpz [108.629019ms]
Nov  7 00:44:36.810: INFO: Created: latency-svc-d56sv
Nov  7 00:44:36.814: INFO: Got endpoints: latency-svc-d56sv [102.97226ms]
Nov  7 00:44:36.815: INFO: Created: latency-svc-96chc
Nov  7 00:44:36.819: INFO: Got endpoints: latency-svc-96chc [107.956158ms]
Nov  7 00:44:36.892: INFO: Created: latency-svc-s9fxh
Nov  7 00:44:36.892: INFO: Got endpoints: latency-svc-s9fxh [181.506525ms]
Nov  7 00:44:36.904: INFO: Created: latency-svc-5h965
Nov  7 00:44:36.908: INFO: Created: latency-svc-qrgd8
Nov  7 00:44:36.911: INFO: Got endpoints: latency-svc-5h965 [199.794731ms]
Nov  7 00:44:36.912: INFO: Got endpoints: latency-svc-qrgd8 [201.353296ms]
Nov  7 00:44:36.916: INFO: Created: latency-svc-6bspm
Nov  7 00:44:36.925: INFO: Created: latency-svc-4mmmg
Nov  7 00:44:36.925: INFO: Got endpoints: latency-svc-6bspm [214.284823ms]
Nov  7 00:44:36.953: INFO: Got endpoints: latency-svc-4mmmg [242.342094ms]
Nov  7 00:44:36.954: INFO: Created: latency-svc-w8jd8
Nov  7 00:44:36.954: INFO: Created: latency-svc-9k5sm
Nov  7 00:44:36.954: INFO: Got endpoints: latency-svc-9k5sm [242.533941ms]
Nov  7 00:44:36.954: INFO: Created: latency-svc-8m92w
Nov  7 00:44:36.954: INFO: Got endpoints: latency-svc-8m92w [242.777306ms]
Nov  7 00:44:36.983: INFO: Created: latency-svc-bz7r5
Nov  7 00:44:36.983: INFO: Got endpoints: latency-svc-bz7r5 [271.576106ms]
Nov  7 00:44:36.983: INFO: Created: latency-svc-xwsqm
Nov  7 00:44:36.983: INFO: Created: latency-svc-zp7jp
Nov  7 00:44:36.983: INFO: Got endpoints: latency-svc-zp7jp [271.65945ms]
Nov  7 00:44:36.983: INFO: Got endpoints: latency-svc-w8jd8 [271.734507ms]
Nov  7 00:44:36.985: INFO: Created: latency-svc-rns4h
Nov  7 00:44:36.985: INFO: Created: latency-svc-q9rhx
Nov  7 00:44:36.985: INFO: Got endpoints: latency-svc-q9rhx [273.537484ms]
Nov  7 00:44:36.985: INFO: Got endpoints: latency-svc-xwsqm [273.717895ms]
Nov  7 00:44:36.987: INFO: Got endpoints: latency-svc-rns4h [275.639886ms]
Nov  7 00:44:36.988: INFO: Created: latency-svc-7gs27
Nov  7 00:44:36.995: INFO: Created: latency-svc-6jwxd
Nov  7 00:44:36.995: INFO: Got endpoints: latency-svc-7gs27 [181.289027ms]
Nov  7 00:44:37.000: INFO: Got endpoints: latency-svc-6jwxd [180.445085ms]
Nov  7 00:44:37.002: INFO: Created: latency-svc-4pd7f
Nov  7 00:44:37.007: INFO: Got endpoints: latency-svc-4pd7f [114.903497ms]
Nov  7 00:44:37.010: INFO: Created: latency-svc-g98xp
Nov  7 00:44:37.015: INFO: Got endpoints: latency-svc-g98xp [103.923472ms]
Nov  7 00:44:37.017: INFO: Created: latency-svc-sz5v4
Nov  7 00:44:37.026: INFO: Got endpoints: latency-svc-sz5v4 [113.300126ms]
Nov  7 00:44:37.028: INFO: Created: latency-svc-6ndww
Nov  7 00:44:37.034: INFO: Got endpoints: latency-svc-6ndww [108.771077ms]
Nov  7 00:44:37.053: INFO: Created: latency-svc-mknwb
Nov  7 00:44:37.056: INFO: Created: latency-svc-lfggb
Nov  7 00:44:37.059: INFO: Got endpoints: latency-svc-mknwb [105.205794ms]
Nov  7 00:44:37.063: INFO: Got endpoints: latency-svc-lfggb [109.338435ms]
Nov  7 00:44:37.079: INFO: Created: latency-svc-kwbxb
Nov  7 00:44:37.083: INFO: Created: latency-svc-465hs
Nov  7 00:44:37.083: INFO: Got endpoints: latency-svc-kwbxb [129.707843ms]
Nov  7 00:44:37.090: INFO: Created: latency-svc-bzhw6
Nov  7 00:44:37.093: INFO: Got endpoints: latency-svc-465hs [110.303948ms]
Nov  7 00:44:37.096: INFO: Created: latency-svc-9fgtg
Nov  7 00:44:37.098: INFO: Got endpoints: latency-svc-bzhw6 [115.014764ms]
Nov  7 00:44:37.101: INFO: Got endpoints: latency-svc-9fgtg [117.9038ms]
Nov  7 00:44:37.105: INFO: Created: latency-svc-w29ss
Nov  7 00:44:37.116: INFO: Created: latency-svc-5hrxf
Nov  7 00:44:37.116: INFO: Got endpoints: latency-svc-5hrxf [131.41432ms]
Nov  7 00:44:37.116: INFO: Got endpoints: latency-svc-w29ss [131.336777ms]
Nov  7 00:44:37.118: INFO: Created: latency-svc-59jq6
Nov  7 00:44:37.125: INFO: Got endpoints: latency-svc-59jq6 [137.80215ms]
Nov  7 00:44:37.125: INFO: Created: latency-svc-g8zmq
Nov  7 00:44:37.135: INFO: Created: latency-svc-whp4d
Nov  7 00:44:37.135: INFO: Got endpoints: latency-svc-whp4d [135.369137ms]
Nov  7 00:44:37.135: INFO: Got endpoints: latency-svc-g8zmq [139.762993ms]
Nov  7 00:44:37.135: INFO: Created: latency-svc-fdptq
Nov  7 00:44:37.135: INFO: Got endpoints: latency-svc-fdptq [127.952776ms]
Nov  7 00:44:37.143: INFO: Created: latency-svc-ldskg
Nov  7 00:44:37.143: INFO: Got endpoints: latency-svc-ldskg [128.449468ms]
Nov  7 00:44:37.147: INFO: Created: latency-svc-59tvt
Nov  7 00:44:37.150: INFO: Got endpoints: latency-svc-59tvt [124.31532ms]
Nov  7 00:44:37.164: INFO: Created: latency-svc-4rlq7
Nov  7 00:44:37.164: INFO: Created: latency-svc-xcftd
Nov  7 00:44:37.164: INFO: Got endpoints: latency-svc-4rlq7 [130.18014ms]
Nov  7 00:44:37.164: INFO: Created: latency-svc-b5m9g
Nov  7 00:44:37.165: INFO: Got endpoints: latency-svc-b5m9g [106.508077ms]
Nov  7 00:44:37.168: INFO: Got endpoints: latency-svc-xcftd [105.207361ms]
Nov  7 00:44:37.181: INFO: Created: latency-svc-t5n6b
Nov  7 00:44:37.186: INFO: Got endpoints: latency-svc-t5n6b [102.291197ms]
Nov  7 00:44:37.190: INFO: Created: latency-svc-k879m
Nov  7 00:44:37.195: INFO: Got endpoints: latency-svc-k879m [102.303277ms]
Nov  7 00:44:37.198: INFO: Created: latency-svc-zxnbx
Nov  7 00:44:37.205: INFO: Created: latency-svc-t97q7
Nov  7 00:44:37.208: INFO: Got endpoints: latency-svc-zxnbx [110.492525ms]
Nov  7 00:44:37.210: INFO: Got endpoints: latency-svc-t97q7 [109.431623ms]
Nov  7 00:44:37.215: INFO: Created: latency-svc-pplhv
Nov  7 00:44:37.220: INFO: Created: latency-svc-5q77j
Nov  7 00:44:37.222: INFO: Got endpoints: latency-svc-pplhv [106.457525ms]
Nov  7 00:44:37.224: INFO: Got endpoints: latency-svc-5q77j [107.911401ms]
Nov  7 00:44:37.225: INFO: Created: latency-svc-kjslg
Nov  7 00:44:37.231: INFO: Got endpoints: latency-svc-kjslg [106.689624ms]
Nov  7 00:44:37.236: INFO: Created: latency-svc-wgpl5
Nov  7 00:44:37.246: INFO: Created: latency-svc-9pp2w
Nov  7 00:44:37.246: INFO: Got endpoints: latency-svc-wgpl5 [111.196913ms]
Nov  7 00:44:37.254: INFO: Created: latency-svc-rpz7c
Nov  7 00:44:37.255: INFO: Got endpoints: latency-svc-9pp2w [119.584398ms]
Nov  7 00:44:37.256: INFO: Got endpoints: latency-svc-rpz7c [119.971082ms]
Nov  7 00:44:37.259: INFO: Created: latency-svc-2vqdx
Nov  7 00:44:37.270: INFO: Created: latency-svc-2tl9w
Nov  7 00:44:37.270: INFO: Created: latency-svc-p9mth
Nov  7 00:44:37.270: INFO: Got endpoints: latency-svc-p9mth [106.042826ms]
Nov  7 00:44:37.270: INFO: Got endpoints: latency-svc-2tl9w [120.322374ms]
Nov  7 00:44:37.270: INFO: Got endpoints: latency-svc-2vqdx [127.064071ms]
Nov  7 00:44:37.274: INFO: Created: latency-svc-cfd2q
Nov  7 00:44:37.276: INFO: Got endpoints: latency-svc-cfd2q [111.109042ms]
Nov  7 00:44:37.281: INFO: Created: latency-svc-ndc8x
Nov  7 00:44:37.283: INFO: Got endpoints: latency-svc-ndc8x [114.71576ms]
Nov  7 00:44:37.286: INFO: Created: latency-svc-9bp7h
Nov  7 00:44:37.290: INFO: Got endpoints: latency-svc-9bp7h [103.988454ms]
Nov  7 00:44:37.293: INFO: Created: latency-svc-t7h8x
Nov  7 00:44:37.296: INFO: Got endpoints: latency-svc-t7h8x [100.232449ms]
Nov  7 00:44:37.306: INFO: Created: latency-svc-swd5h
Nov  7 00:44:37.309: INFO: Created: latency-svc-9k2vh
Nov  7 00:44:37.311: INFO: Got endpoints: latency-svc-swd5h [102.579751ms]
Nov  7 00:44:37.316: INFO: Got endpoints: latency-svc-9k2vh [105.985616ms]
Nov  7 00:44:37.323: INFO: Created: latency-svc-m9kps
Nov  7 00:44:37.327: INFO: Created: latency-svc-8qnwj
Nov  7 00:44:37.328: INFO: Got endpoints: latency-svc-m9kps [105.567525ms]
Nov  7 00:44:37.330: INFO: Created: latency-svc-bxqk5
Nov  7 00:44:37.333: INFO: Got endpoints: latency-svc-8qnwj [109.025296ms]
Nov  7 00:44:37.334: INFO: Got endpoints: latency-svc-bxqk5 [103.016391ms]
Nov  7 00:44:37.343: INFO: Created: latency-svc-skqg4
Nov  7 00:44:37.347: INFO: Got endpoints: latency-svc-skqg4 [101.134009ms]
Nov  7 00:44:37.356: INFO: Created: latency-svc-w7wsb
Nov  7 00:44:37.361: INFO: Got endpoints: latency-svc-w7wsb [106.015623ms]
Nov  7 00:44:37.361: INFO: Created: latency-svc-vd8bh
Nov  7 00:44:37.362: INFO: Got endpoints: latency-svc-vd8bh [106.079555ms]
Nov  7 00:44:37.367: INFO: Created: latency-svc-dgg5g
Nov  7 00:44:37.374: INFO: Got endpoints: latency-svc-dgg5g [103.16797ms]
Nov  7 00:44:37.374: INFO: Created: latency-svc-lz8q7
Nov  7 00:44:37.376: INFO: Created: latency-svc-hdjzq
Nov  7 00:44:37.378: INFO: Got endpoints: latency-svc-lz8q7 [107.231268ms]
Nov  7 00:44:37.380: INFO: Got endpoints: latency-svc-hdjzq [109.654534ms]
Nov  7 00:44:37.383: INFO: Created: latency-svc-7f9vx
Nov  7 00:44:37.386: INFO: Got endpoints: latency-svc-7f9vx [109.292152ms]
Nov  7 00:44:37.392: INFO: Created: latency-svc-f6t4q
Nov  7 00:44:37.397: INFO: Created: latency-svc-66v9w
Nov  7 00:44:37.397: INFO: Got endpoints: latency-svc-f6t4q [113.782634ms]
Nov  7 00:44:37.398: INFO: Got endpoints: latency-svc-66v9w [107.78646ms]
Nov  7 00:44:37.400: INFO: Created: latency-svc-mmz7d
Nov  7 00:44:37.406: INFO: Got endpoints: latency-svc-mmz7d [110.588433ms]
Nov  7 00:44:37.413: INFO: Created: latency-svc-nbmrl
Nov  7 00:44:37.416: INFO: Got endpoints: latency-svc-nbmrl [104.83174ms]
Nov  7 00:44:37.417: INFO: Created: latency-svc-r28pd
Nov  7 00:44:37.424: INFO: Got endpoints: latency-svc-r28pd [107.688379ms]
Nov  7 00:44:37.426: INFO: Created: latency-svc-b8f7f
Nov  7 00:44:37.433: INFO: Created: latency-svc-r7v86
Nov  7 00:44:37.433: INFO: Got endpoints: latency-svc-b8f7f [104.87366ms]
Nov  7 00:44:37.436: INFO: Created: latency-svc-zw5n4
Nov  7 00:44:37.438: INFO: Got endpoints: latency-svc-r7v86 [104.84144ms]
Nov  7 00:44:37.439: INFO: Got endpoints: latency-svc-zw5n4 [105.059759ms]
Nov  7 00:44:37.444: INFO: Created: latency-svc-zhn6w
Nov  7 00:44:37.448: INFO: Got endpoints: latency-svc-zhn6w [100.622021ms]
Nov  7 00:44:37.458: INFO: Created: latency-svc-bm6lq
Nov  7 00:44:37.460: INFO: Created: latency-svc-zc54r
Nov  7 00:44:37.462: INFO: Got endpoints: latency-svc-bm6lq [101.304899ms]
Nov  7 00:44:37.465: INFO: Got endpoints: latency-svc-zc54r [103.603693ms]
Nov  7 00:44:37.472: INFO: Created: latency-svc-v7f8t
Nov  7 00:44:37.476: INFO: Created: latency-svc-5bl7m
Nov  7 00:44:37.482: INFO: Got endpoints: latency-svc-5bl7m [104.240748ms]
Nov  7 00:44:37.482: INFO: Got endpoints: latency-svc-v7f8t [108.400238ms]
Nov  7 00:44:37.487: INFO: Created: latency-svc-2zj6j
Nov  7 00:44:37.488: INFO: Got endpoints: latency-svc-2zj6j [108.071375ms]
Nov  7 00:44:37.496: INFO: Created: latency-svc-9czvk
Nov  7 00:44:37.496: INFO: Created: latency-svc-sv5kd
Nov  7 00:44:37.503: INFO: Got endpoints: latency-svc-9czvk [106.679264ms]
Nov  7 00:44:37.503: INFO: Got endpoints: latency-svc-sv5kd [117.556672ms]
Nov  7 00:44:37.506: INFO: Created: latency-svc-qmz7g
Nov  7 00:44:37.511: INFO: Created: latency-svc-hx7fj
Nov  7 00:44:37.511: INFO: Got endpoints: latency-svc-qmz7g [113.114382ms]
Nov  7 00:44:37.514: INFO: Got endpoints: latency-svc-hx7fj [108.193746ms]
Nov  7 00:44:37.517: INFO: Created: latency-svc-vtdnp
Nov  7 00:44:37.521: INFO: Got endpoints: latency-svc-vtdnp [105.698797ms]
Nov  7 00:44:37.523: INFO: Created: latency-svc-x4q7s
Nov  7 00:44:37.529: INFO: Got endpoints: latency-svc-x4q7s [104.672894ms]
Nov  7 00:44:37.531: INFO: Created: latency-svc-tlzqk
Nov  7 00:44:37.537: INFO: Created: latency-svc-jcfl8
Nov  7 00:44:37.540: INFO: Got endpoints: latency-svc-tlzqk [106.684245ms]
Nov  7 00:44:37.543: INFO: Created: latency-svc-ttrj8
Nov  7 00:44:37.545: INFO: Got endpoints: latency-svc-jcfl8 [106.721228ms]
Nov  7 00:44:37.548: INFO: Got endpoints: latency-svc-ttrj8 [108.576547ms]
Nov  7 00:44:37.550: INFO: Created: latency-svc-6lzdd
Nov  7 00:44:37.552: INFO: Got endpoints: latency-svc-6lzdd [104.08822ms]
Nov  7 00:44:37.562: INFO: Created: latency-svc-z5b5k
Nov  7 00:44:37.563: INFO: Got endpoints: latency-svc-z5b5k [100.884553ms]
Nov  7 00:44:37.565: INFO: Created: latency-svc-m4jwb
Nov  7 00:44:37.568: INFO: Got endpoints: latency-svc-m4jwb [102.835677ms]
Nov  7 00:44:37.580: INFO: Created: latency-svc-mdcjr
Nov  7 00:44:37.587: INFO: Got endpoints: latency-svc-mdcjr [104.413385ms]
Nov  7 00:44:37.588: INFO: Created: latency-svc-ftmwr
Nov  7 00:44:37.590: INFO: Got endpoints: latency-svc-ftmwr [108.440643ms]
Nov  7 00:44:37.591: INFO: Created: latency-svc-z6ncg
Nov  7 00:44:37.596: INFO: Got endpoints: latency-svc-z6ncg [107.717316ms]
Nov  7 00:44:37.601: INFO: Created: latency-svc-ls522
Nov  7 00:44:37.607: INFO: Got endpoints: latency-svc-ls522 [103.808268ms]
Nov  7 00:44:37.607: INFO: Created: latency-svc-pd6zl
Nov  7 00:44:37.612: INFO: Created: latency-svc-vvfdd
Nov  7 00:44:37.613: INFO: Got endpoints: latency-svc-pd6zl [109.371108ms]
Nov  7 00:44:37.615: INFO: Got endpoints: latency-svc-vvfdd [103.97676ms]
Nov  7 00:44:37.620: INFO: Created: latency-svc-2sfdj
Nov  7 00:44:37.625: INFO: Created: latency-svc-thsn8
Nov  7 00:44:37.644: INFO: Got endpoints: latency-svc-2sfdj [129.69572ms]
Nov  7 00:44:37.644: INFO: Created: latency-svc-jbrq4
Nov  7 00:44:37.649: INFO: Got endpoints: latency-svc-thsn8 [127.528549ms]
Nov  7 00:44:37.660: INFO: Created: latency-svc-z5kqm
Nov  7 00:44:37.661: INFO: Got endpoints: latency-svc-jbrq4 [132.33616ms]
Nov  7 00:44:37.669: INFO: Got endpoints: latency-svc-z5kqm [129.296133ms]
Nov  7 00:44:37.676: INFO: Created: latency-svc-srx28
Nov  7 00:44:37.684: INFO: Got endpoints: latency-svc-srx28 [121.54502ms]
Nov  7 00:44:37.685: INFO: Created: latency-svc-mnwqw
Nov  7 00:44:37.687: INFO: Created: latency-svc-hdcf7
Nov  7 00:44:37.688: INFO: Got endpoints: latency-svc-mnwqw [143.279367ms]
Nov  7 00:44:37.696: INFO: Got endpoints: latency-svc-hdcf7 [148.218267ms]
Nov  7 00:44:37.697: INFO: Created: latency-svc-4vzvf
Nov  7 00:44:37.700: INFO: Created: latency-svc-pgcgl
Nov  7 00:44:37.707: INFO: Created: latency-svc-s2c6j
Nov  7 00:44:37.715: INFO: Got endpoints: latency-svc-pgcgl [146.469902ms]
Nov  7 00:44:37.715: INFO: Got endpoints: latency-svc-4vzvf [162.785185ms]
Nov  7 00:44:37.716: INFO: Created: latency-svc-xmwvm
Nov  7 00:44:37.719: INFO: Got endpoints: latency-svc-s2c6j [132.539055ms]
Nov  7 00:44:37.719: INFO: Created: latency-svc-b9dmf
Nov  7 00:44:37.728: INFO: Got endpoints: latency-svc-b9dmf [132.184266ms]
Nov  7 00:44:37.728: INFO: Got endpoints: latency-svc-xmwvm [137.737968ms]
Nov  7 00:44:37.732: INFO: Created: latency-svc-zvpkv
Nov  7 00:44:37.737: INFO: Created: latency-svc-5r7tv
Nov  7 00:44:37.741: INFO: Got endpoints: latency-svc-zvpkv [133.550583ms]
Nov  7 00:44:37.746: INFO: Got endpoints: latency-svc-5r7tv [133.209395ms]
Nov  7 00:44:37.746: INFO: Created: latency-svc-2ctnd
Nov  7 00:44:37.748: INFO: Got endpoints: latency-svc-2ctnd [133.263153ms]
Nov  7 00:44:37.754: INFO: Created: latency-svc-bjsxq
Nov  7 00:44:37.759: INFO: Got endpoints: latency-svc-bjsxq [115.043165ms]
Nov  7 00:44:37.764: INFO: Created: latency-svc-4h2q8
Nov  7 00:44:37.767: INFO: Created: latency-svc-h5bn9
Nov  7 00:44:37.777: INFO: Got endpoints: latency-svc-4h2q8 [127.666004ms]
Nov  7 00:44:37.779: INFO: Created: latency-svc-tz6hs
Nov  7 00:44:37.782: INFO: Got endpoints: latency-svc-h5bn9 [121.106084ms]
Nov  7 00:44:37.786: INFO: Got endpoints: latency-svc-tz6hs [116.638409ms]
Nov  7 00:44:37.792: INFO: Created: latency-svc-829gt
Nov  7 00:44:37.797: INFO: Created: latency-svc-rv6v7
Nov  7 00:44:37.799: INFO: Got endpoints: latency-svc-829gt [114.04991ms]
Nov  7 00:44:37.803: INFO: Created: latency-svc-6bs5k
Nov  7 00:44:37.811: INFO: Got endpoints: latency-svc-rv6v7 [123.203388ms]
Nov  7 00:44:37.813: INFO: Got endpoints: latency-svc-6bs5k [116.638093ms]
Nov  7 00:44:37.817: INFO: Created: latency-svc-jbcgm
Nov  7 00:44:37.823: INFO: Created: latency-svc-z8mw8
Nov  7 00:44:37.826: INFO: Got endpoints: latency-svc-jbcgm [111.163989ms]
Nov  7 00:44:37.832: INFO: Got endpoints: latency-svc-z8mw8 [117.718063ms]
Nov  7 00:44:37.835: INFO: Created: latency-svc-ts5rv
Nov  7 00:44:37.841: INFO: Created: latency-svc-b7j5j
Nov  7 00:44:37.846: INFO: Got endpoints: latency-svc-ts5rv [126.718996ms]
Nov  7 00:44:37.849: INFO: Created: latency-svc-bx6xf
Nov  7 00:44:37.850: INFO: Got endpoints: latency-svc-b7j5j [121.431597ms]
Nov  7 00:44:37.852: INFO: Got endpoints: latency-svc-bx6xf [123.786943ms]
Nov  7 00:44:37.855: INFO: Created: latency-svc-fwzxn
Nov  7 00:44:37.859: INFO: Created: latency-svc-lplvm
Nov  7 00:44:37.861: INFO: Got endpoints: latency-svc-fwzxn [120.107865ms]
Nov  7 00:44:37.867: INFO: Created: latency-svc-srb48
Nov  7 00:44:37.867: INFO: Got endpoints: latency-svc-lplvm [121.154813ms]
Nov  7 00:44:37.873: INFO: Got endpoints: latency-svc-srb48 [124.527363ms]
Nov  7 00:44:37.874: INFO: Created: latency-svc-gtl68
Nov  7 00:44:37.882: INFO: Created: latency-svc-hx4bq
Nov  7 00:44:37.890: INFO: Created: latency-svc-zlwkw
Nov  7 00:44:37.893: INFO: Got endpoints: latency-svc-hx4bq [116.294129ms]
Nov  7 00:44:37.897: INFO: Created: latency-svc-ptwnl
Nov  7 00:44:37.906: INFO: Created: latency-svc-csg7v
Nov  7 00:44:37.907: INFO: Got endpoints: latency-svc-zlwkw [125.029229ms]
Nov  7 00:44:37.910: INFO: Got endpoints: latency-svc-gtl68 [150.626408ms]
Nov  7 00:44:37.913: INFO: Got endpoints: latency-svc-ptwnl [127.464399ms]
Nov  7 00:44:37.916: INFO: Got endpoints: latency-svc-csg7v [117.379792ms]
Nov  7 00:44:37.916: INFO: Created: latency-svc-vbx69
Nov  7 00:44:37.922: INFO: Created: latency-svc-cp28z
Nov  7 00:44:37.924: INFO: Got endpoints: latency-svc-vbx69 [112.906255ms]
Nov  7 00:44:37.928: INFO: Created: latency-svc-65g27
Nov  7 00:44:37.933: INFO: Got endpoints: latency-svc-cp28z [120.31307ms]
Nov  7 00:44:37.937: INFO: Created: latency-svc-2gmpc
Nov  7 00:44:37.938: INFO: Got endpoints: latency-svc-65g27 [111.416354ms]
Nov  7 00:44:37.947: INFO: Created: latency-svc-mwv47
Nov  7 00:44:37.947: INFO: Got endpoints: latency-svc-2gmpc [114.290948ms]
Nov  7 00:44:37.951: INFO: Created: latency-svc-tdl4q
Nov  7 00:44:37.954: INFO: Got endpoints: latency-svc-mwv47 [108.110274ms]
Nov  7 00:44:37.957: INFO: Created: latency-svc-92km5
Nov  7 00:44:37.963: INFO: Got endpoints: latency-svc-tdl4q [113.453147ms]
Nov  7 00:44:37.965: INFO: Created: latency-svc-smnpf
Nov  7 00:44:37.973: INFO: Got endpoints: latency-svc-92km5 [121.198118ms]
Nov  7 00:44:37.975: INFO: Created: latency-svc-7nls4
Nov  7 00:44:37.984: INFO: Got endpoints: latency-svc-smnpf [123.508119ms]
Nov  7 00:44:37.985: INFO: Got endpoints: latency-svc-7nls4 [117.529777ms]
Nov  7 00:44:37.988: INFO: Created: latency-svc-6cp8m
Nov  7 00:44:37.994: INFO: Created: latency-svc-d2mc9
Nov  7 00:44:37.995: INFO: Got endpoints: latency-svc-6cp8m [122.128565ms]
Nov  7 00:44:37.996: INFO: Got endpoints: latency-svc-d2mc9 [103.003287ms]
Nov  7 00:44:38.004: INFO: Created: latency-svc-4schg
Nov  7 00:44:38.012: INFO: Created: latency-svc-5m9xp
Nov  7 00:44:38.012: INFO: Got endpoints: latency-svc-4schg [104.454321ms]
Nov  7 00:44:38.014: INFO: Created: latency-svc-qt8sk
Nov  7 00:44:38.020: INFO: Created: latency-svc-bcdvg
Nov  7 00:44:38.024: INFO: Got endpoints: latency-svc-qt8sk [111.178526ms]
Nov  7 00:44:38.024: INFO: Got endpoints: latency-svc-5m9xp [114.525116ms]
Nov  7 00:44:38.028: INFO: Got endpoints: latency-svc-bcdvg [111.528507ms]
Nov  7 00:44:38.028: INFO: Created: latency-svc-t4p2h
Nov  7 00:44:38.035: INFO: Got endpoints: latency-svc-t4p2h [110.957495ms]
Nov  7 00:44:38.037: INFO: Created: latency-svc-gthxd
Nov  7 00:44:38.045: INFO: Created: latency-svc-mxwv8
Nov  7 00:44:38.049: INFO: Got endpoints: latency-svc-gthxd [115.525189ms]
Nov  7 00:44:38.049: INFO: Created: latency-svc-d6tcr
Nov  7 00:44:38.054: INFO: Got endpoints: latency-svc-d6tcr [107.550285ms]
Nov  7 00:44:38.054: INFO: Got endpoints: latency-svc-mxwv8 [116.762317ms]
Nov  7 00:44:38.058: INFO: Created: latency-svc-rmrsl
Nov  7 00:44:38.061: INFO: Got endpoints: latency-svc-rmrsl [106.622795ms]
Nov  7 00:44:38.064: INFO: Created: latency-svc-hqphn
Nov  7 00:44:38.066: INFO: Got endpoints: latency-svc-hqphn [103.018912ms]
Nov  7 00:44:38.075: INFO: Created: latency-svc-6m42j
Nov  7 00:44:38.078: INFO: Got endpoints: latency-svc-6m42j [104.358879ms]
Nov  7 00:44:38.082: INFO: Created: latency-svc-rrfrd
Nov  7 00:44:38.089: INFO: Created: latency-svc-cc448
Nov  7 00:44:38.089: INFO: Got endpoints: latency-svc-rrfrd [104.587544ms]
Nov  7 00:44:38.092: INFO: Got endpoints: latency-svc-cc448 [106.867924ms]
Nov  7 00:44:38.094: INFO: Created: latency-svc-pvp4l
Nov  7 00:44:38.098: INFO: Created: latency-svc-gx4vv
Nov  7 00:44:38.099: INFO: Got endpoints: latency-svc-pvp4l [103.866794ms]
Nov  7 00:44:38.104: INFO: Got endpoints: latency-svc-gx4vv [107.949228ms]
Nov  7 00:44:38.111: INFO: Created: latency-svc-j9hpk
Nov  7 00:44:38.115: INFO: Got endpoints: latency-svc-j9hpk [103.459499ms]
Nov  7 00:44:38.130: INFO: Created: latency-svc-wzpv9
Nov  7 00:44:38.136: INFO: Got endpoints: latency-svc-wzpv9 [111.497765ms]
Nov  7 00:44:38.139: INFO: Created: latency-svc-ng6dw
Nov  7 00:44:38.143: INFO: Got endpoints: latency-svc-ng6dw [118.34066ms]
Nov  7 00:44:38.145: INFO: Created: latency-svc-cb4x4
Nov  7 00:44:38.149: INFO: Created: latency-svc-gl4qm
Nov  7 00:44:38.151: INFO: Got endpoints: latency-svc-cb4x4 [123.640551ms]
Nov  7 00:44:38.159: INFO: Got endpoints: latency-svc-gl4qm [110.275461ms]
Nov  7 00:44:38.160: INFO: Created: latency-svc-fm5wj
Nov  7 00:44:38.163: INFO: Created: latency-svc-n64t2
Nov  7 00:44:38.164: INFO: Got endpoints: latency-svc-fm5wj [129.043673ms]
Nov  7 00:44:38.169: INFO: Created: latency-svc-jk9k9
Nov  7 00:44:38.170: INFO: Got endpoints: latency-svc-n64t2 [115.502338ms]
Nov  7 00:44:38.174: INFO: Created: latency-svc-x9npk
Nov  7 00:44:38.176: INFO: Got endpoints: latency-svc-jk9k9 [121.903946ms]
Nov  7 00:44:38.178: INFO: Created: latency-svc-b6k6z
Nov  7 00:44:38.179: INFO: Got endpoints: latency-svc-x9npk [118.570813ms]
Nov  7 00:44:38.183: INFO: Created: latency-svc-9pz2l
Nov  7 00:44:38.183: INFO: Got endpoints: latency-svc-b6k6z [117.121321ms]
Nov  7 00:44:38.189: INFO: Got endpoints: latency-svc-9pz2l [111.529054ms]
Nov  7 00:44:38.190: INFO: Created: latency-svc-6kcnj
Nov  7 00:44:38.195: INFO: Created: latency-svc-jc587
Nov  7 00:44:38.196: INFO: Got endpoints: latency-svc-6kcnj [106.861131ms]
Nov  7 00:44:38.201: INFO: Got endpoints: latency-svc-jc587 [109.484234ms]
Nov  7 00:44:38.205: INFO: Created: latency-svc-kkw86
Nov  7 00:44:38.206: INFO: Got endpoints: latency-svc-kkw86 [107.246556ms]
Nov  7 00:44:38.217: INFO: Created: latency-svc-xfm8v
Nov  7 00:44:38.217: INFO: Created: latency-svc-vdkcf
Nov  7 00:44:38.217: INFO: Got endpoints: latency-svc-xfm8v [102.14805ms]
Nov  7 00:44:38.217: INFO: Got endpoints: latency-svc-vdkcf [113.26129ms]
Nov  7 00:44:38.233: INFO: Created: latency-svc-8q8d4
Nov  7 00:44:38.239: INFO: Got endpoints: latency-svc-8q8d4 [103.49426ms]
Nov  7 00:44:38.241: INFO: Created: latency-svc-kdnw5
Nov  7 00:44:38.247: INFO: Got endpoints: latency-svc-kdnw5 [104.109212ms]
Nov  7 00:44:38.249: INFO: Created: latency-svc-rlmlw
Nov  7 00:44:38.254: INFO: Got endpoints: latency-svc-rlmlw [102.738768ms]
Nov  7 00:44:38.258: INFO: Created: latency-svc-hv9ck
Nov  7 00:44:38.262: INFO: Got endpoints: latency-svc-hv9ck [102.741526ms]
Nov  7 00:44:38.262: INFO: Created: latency-svc-nj7dt
Nov  7 00:44:38.267: INFO: Got endpoints: latency-svc-nj7dt [102.786793ms]
Nov  7 00:44:38.269: INFO: Created: latency-svc-ttx2t
Nov  7 00:44:38.274: INFO: Got endpoints: latency-svc-ttx2t [103.640573ms]
Nov  7 00:44:38.276: INFO: Created: latency-svc-znzk7
Nov  7 00:44:38.281: INFO: Created: latency-svc-lmj9s
Nov  7 00:44:38.281: INFO: Got endpoints: latency-svc-znzk7 [104.298372ms]
Nov  7 00:44:38.286: INFO: Created: latency-svc-b5h7k
Nov  7 00:44:38.291: INFO: Got endpoints: latency-svc-lmj9s [112.018905ms]
Nov  7 00:44:38.292: INFO: Created: latency-svc-pnnz4
Nov  7 00:44:38.296: INFO: Got endpoints: latency-svc-pnnz4 [106.680576ms]
Nov  7 00:44:38.296: INFO: Got endpoints: latency-svc-b5h7k [112.6068ms]
Nov  7 00:44:38.298: INFO: Created: latency-svc-kkphb
Nov  7 00:44:38.303: INFO: Got endpoints: latency-svc-kkphb [107.182596ms]
Nov  7 00:44:38.304: INFO: Created: latency-svc-mc7r6
Nov  7 00:44:38.311: INFO: Created: latency-svc-d44xb
Nov  7 00:44:38.311: INFO: Got endpoints: latency-svc-mc7r6 [110.187066ms]
Nov  7 00:44:38.314: INFO: Got endpoints: latency-svc-d44xb [108.29015ms]
Nov  7 00:44:38.323: INFO: Created: latency-svc-ggjhl
Nov  7 00:44:38.323: INFO: Created: latency-svc-qqxfd
Nov  7 00:44:38.323: INFO: Got endpoints: latency-svc-qqxfd [105.486997ms]
Nov  7 00:44:38.325: INFO: Got endpoints: latency-svc-ggjhl [107.586696ms]
Nov  7 00:44:38.336: INFO: Created: latency-svc-49fpq
Nov  7 00:44:38.339: INFO: Got endpoints: latency-svc-49fpq [99.842858ms]
Nov  7 00:44:38.345: INFO: Created: latency-svc-m4282
Nov  7 00:44:38.349: INFO: Got endpoints: latency-svc-m4282 [102.3477ms]
Nov  7 00:44:38.353: INFO: Created: latency-svc-6qq4n
Nov  7 00:44:38.358: INFO: Got endpoints: latency-svc-6qq4n [103.966268ms]
Nov  7 00:44:38.361: INFO: Created: latency-svc-ts575
Nov  7 00:44:38.367: INFO: Got endpoints: latency-svc-ts575 [104.906094ms]
Nov  7 00:44:38.368: INFO: Created: latency-svc-xpdxv
Nov  7 00:44:38.373: INFO: Got endpoints: latency-svc-xpdxv [106.089044ms]
Nov  7 00:44:38.374: INFO: Created: latency-svc-jfgc7
Nov  7 00:44:38.380: INFO: Got endpoints: latency-svc-jfgc7 [105.920002ms]
Nov  7 00:44:38.381: INFO: Created: latency-svc-9khs2
Nov  7 00:44:38.386: INFO: Got endpoints: latency-svc-9khs2 [105.672184ms]
Nov  7 00:44:38.390: INFO: Created: latency-svc-hg924
Nov  7 00:44:38.394: INFO: Got endpoints: latency-svc-hg924 [102.299477ms]
Nov  7 00:44:38.396: INFO: Created: latency-svc-hpr9j
Nov  7 00:44:38.399: INFO: Got endpoints: latency-svc-hpr9j [103.025154ms]
Nov  7 00:44:38.399: INFO: Latencies: [99.842858ms 100.232449ms 100.622021ms 100.884553ms 101.134009ms 101.304899ms 102.14805ms 102.291197ms 102.299477ms 102.303277ms 102.3477ms 102.579751ms 102.738768ms 102.741526ms 102.786793ms 102.835677ms 102.97226ms 103.003287ms 103.016391ms 103.018912ms 103.025154ms 103.16797ms 103.459499ms 103.49426ms 103.603693ms 103.640573ms 103.808268ms 103.866794ms 103.923472ms 103.966268ms 103.97676ms 103.988454ms 104.08822ms 104.109212ms 104.240748ms 104.298372ms 104.358879ms 104.413385ms 104.454321ms 104.587544ms 104.672894ms 104.83174ms 104.84144ms 104.87366ms 104.906094ms 105.059759ms 105.205794ms 105.207361ms 105.486997ms 105.567525ms 105.672184ms 105.698797ms 105.920002ms 105.985616ms 106.015623ms 106.042826ms 106.079555ms 106.089044ms 106.457525ms 106.508077ms 106.622795ms 106.679264ms 106.680576ms 106.684245ms 106.689624ms 106.721228ms 106.861131ms 106.867924ms 107.182596ms 107.231268ms 107.246556ms 107.550285ms 107.586696ms 107.688379ms 107.717316ms 107.78646ms 107.911401ms 107.949228ms 107.956158ms 108.071375ms 108.110274ms 108.193746ms 108.29015ms 108.400238ms 108.440643ms 108.576547ms 108.771077ms 109.025296ms 109.292152ms 109.338435ms 109.371108ms 109.431623ms 109.484234ms 109.654534ms 110.187066ms 110.275461ms 110.303948ms 110.492525ms 110.588433ms 110.957495ms 111.109042ms 111.163989ms 111.178526ms 111.196913ms 111.416354ms 111.497765ms 111.528507ms 111.529054ms 112.018905ms 112.6068ms 112.906255ms 113.114382ms 113.26129ms 113.300126ms 113.453147ms 113.782634ms 114.04991ms 114.290948ms 114.525116ms 114.71576ms 114.903497ms 115.014764ms 115.043165ms 115.502338ms 115.525189ms 116.294129ms 116.638093ms 116.638409ms 116.762317ms 117.121321ms 117.379792ms 117.529777ms 117.556672ms 117.718063ms 117.9038ms 118.34066ms 118.570813ms 119.584398ms 119.971082ms 120.107865ms 120.31307ms 120.322374ms 121.106084ms 121.154813ms 121.198118ms 121.431597ms 121.54502ms 121.903946ms 122.128565ms 123.203388ms 123.508119ms 123.640551ms 123.786943ms 124.31532ms 124.527363ms 125.029229ms 126.718996ms 127.064071ms 127.464399ms 127.528549ms 127.666004ms 127.952776ms 128.449468ms 129.043673ms 129.296133ms 129.69572ms 129.707843ms 130.18014ms 131.336777ms 131.41432ms 132.184266ms 132.33616ms 132.539055ms 133.209395ms 133.263153ms 133.550583ms 135.369137ms 137.737968ms 137.80215ms 139.762993ms 143.279367ms 146.469902ms 148.218267ms 150.626408ms 162.785185ms 180.445085ms 181.289027ms 181.506525ms 199.794731ms 201.353296ms 214.284823ms 242.342094ms 242.533941ms 242.777306ms 271.576106ms 271.65945ms 271.734507ms 273.537484ms 273.717895ms 275.639886ms]
Nov  7 00:44:38.399: INFO: 50 %ile: 111.109042ms
Nov  7 00:44:38.399: INFO: 90 %ile: 143.279367ms
Nov  7 00:44:38.399: INFO: 99 %ile: 273.717895ms
Nov  7 00:44:38.399: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:44:38.399: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svc-latency-3239" for this suite.
Nov  7 00:44:52.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:45:01.413: INFO: namespace svc-latency-3239 deletion completed in 22.920156998s

• [SLOW TEST:34.481 seconds]
[sig-network] Service endpoints latency
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:45:01.414: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259
Nov  7 00:45:01.974: INFO: Pod name my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259: Found 1 pods out of 1
Nov  7 00:45:01.974: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259" are running
Nov  7 00:45:10.162: INFO: Pod "my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259-m8rv9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 00:44:26 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 00:44:26 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 00:44:26 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 00:44:26 +0000 UTC Reason: Message:}])
Nov  7 00:45:10.162: INFO: Trying to dial the pod
Nov  7 00:45:15.440: INFO: Controller my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259: Got expected result from replica 1 [my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259-m8rv9]: "my-hostname-basic-d4f36a1a-00f7-11ea-aaa6-525400524259-m8rv9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:45:15.441: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-1973" for this suite.
Nov  7 00:45:21.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:45:30.525: INFO: namespace replication-controller-1973 deletion completed in 14.813394947s

• [SLOW TEST:29.111 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:45:30.525: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov  7 00:45:39.543: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:45:39.825: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-3327" for this suite.
Nov  7 00:45:52.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:46:00.899: INFO: namespace replicaset-3327 deletion completed in 20.80405654s

• [SLOW TEST:30.374 seconds]
[sig-apps] ReplicaSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:46:00.899: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov  7 00:46:06.200: INFO: Pod name wrapped-volume-race-fb2d5760-00f7-11ea-aaa6-525400524259: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fb2d5760-00f7-11ea-aaa6-525400524259 in namespace emptydir-wrapper-9923, will wait for the garbage collector to delete the pods
Nov  7 00:46:29.141: INFO: Deleting ReplicationController wrapped-volume-race-fb2d5760-00f7-11ea-aaa6-525400524259 took: 95.302456ms
Nov  7 00:46:29.341: INFO: Terminating ReplicationController wrapped-volume-race-fb2d5760-00f7-11ea-aaa6-525400524259 pods took: 200.351707ms
STEP: Creating RC which spawns configmap-volume pods
Nov  7 00:47:14.413: INFO: Pod name wrapped-volume-race-23c803d2-00f8-11ea-aaa6-525400524259: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-23c803d2-00f8-11ea-aaa6-525400524259 in namespace emptydir-wrapper-9923, will wait for the garbage collector to delete the pods
Nov  7 00:47:57.450: INFO: Deleting ReplicationController wrapped-volume-race-23c803d2-00f8-11ea-aaa6-525400524259 took: 94.875191ms
Nov  7 00:47:57.550: INFO: Terminating ReplicationController wrapped-volume-race-23c803d2-00f8-11ea-aaa6-525400524259 pods took: 100.456949ms
STEP: Creating RC which spawns configmap-volume pods
Nov  7 00:48:33.623: INFO: Pod name wrapped-volume-race-52fe4fcb-00f8-11ea-aaa6-525400524259: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-52fe4fcb-00f8-11ea-aaa6-525400524259 in namespace emptydir-wrapper-9923, will wait for the garbage collector to delete the pods
Nov  7 00:49:08.566: INFO: Deleting ReplicationController wrapped-volume-race-52fe4fcb-00f8-11ea-aaa6-525400524259 took: 95.095412ms
Nov  7 00:49:08.666: INFO: Terminating ReplicationController wrapped-volume-race-52fe4fcb-00f8-11ea-aaa6-525400524259 pods took: 100.151423ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:49:48.786: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9923" for this suite.
Nov  7 00:49:55.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:50:03.779: INFO: namespace emptydir-wrapper-9923 deletion completed in 14.811544947s

• [SLOW TEST:242.881 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:50:03.782: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W1107 00:50:04.797148    5393 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov  7 00:50:04.797: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:50:04.797: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-5470" for this suite.
Nov  7 00:50:11.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:50:19.688: INFO: namespace gc-5470 deletion completed in 14.797301771s

• [SLOW TEST:15.906 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:50:19.689: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 00:50:20.058: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1569'
Nov  7 00:50:24.515: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov  7 00:50:24.515: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Nov  7 00:50:24.607: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=kubectl-1569'
Nov  7 00:50:25.162: INFO: stderr: ""
Nov  7 00:50:25.162: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:50:25.162: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1569" for this suite.
Nov  7 00:50:37.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:50:46.240: INFO: namespace kubectl-1569 deletion completed in 20.807184576s

• [SLOW TEST:26.551 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:50:46.241: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7201
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7201 to expose endpoints map[]
Nov  7 00:50:46.801: INFO: successfully validated that service multi-endpoint-test in namespace services-7201 exposes endpoints map[] (92.108313ms elapsed)
STEP: Creating pod pod1 in namespace services-7201
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7201 to expose endpoints map[pod1:[100]]
Nov  7 00:50:51.821: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.918829517s elapsed, will retry)
Nov  7 00:50:55.372: INFO: successfully validated that service multi-endpoint-test in namespace services-7201 exposes endpoints map[pod1:[100]] (8.469737658s elapsed)
STEP: Creating pod pod2 in namespace services-7201
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7201 to expose endpoints map[pod1:[100] pod2:[101]]
Nov  7 00:51:00.850: INFO: Unexpected endpoints: found map[8d9b6cb0-00f8-11ea-98c4-0655ae4e8e56:[100]], expected map[pod1:[100] pod2:[101]] (5.379068221s elapsed, will retry)
Nov  7 00:51:07.226: INFO: successfully validated that service multi-endpoint-test in namespace services-7201 exposes endpoints map[pod1:[100] pod2:[101]] (11.754667949s elapsed)
STEP: Deleting pod pod1 in namespace services-7201
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7201 to expose endpoints map[pod2:[101]]
Nov  7 00:51:07.506: INFO: successfully validated that service multi-endpoint-test in namespace services-7201 exposes endpoints map[pod2:[101]] (183.32909ms elapsed)
STEP: Deleting pod pod2 in namespace services-7201
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7201 to expose endpoints map[]
Nov  7 00:51:07.692: INFO: successfully validated that service multi-endpoint-test in namespace services-7201 exposes endpoints map[] (91.298854ms elapsed)
[AfterEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:51:07.797: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-7201" for this suite.
Nov  7 00:51:14.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:51:22.881: INFO: namespace services-7201 deletion completed in 14.813067736s
[AfterEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:36.640 seconds]
[sig-network] Services
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:51:22.882: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1107 00:51:33.999275    5393 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov  7 00:51:33.999: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:51:33.999: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-4386" for this suite.
Nov  7 00:51:40.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:51:48.924: INFO: namespace gc-4386 deletion completed in 14.831991638s

• [SLOW TEST:26.043 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:51:48.925: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov  7 00:51:49.850: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6994,SelfLink:/api/v1/namespaces/watch-6994/configmaps/e2e-watch-test-label-changed,UID:b2e8c1f8-00f8-11ea-98c4-0655ae4e8e56,ResourceVersion:69473,Generation:0,CreationTimestamp:2019-11-07 00:51:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov  7 00:51:49.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6994,SelfLink:/api/v1/namespaces/watch-6994/configmaps/e2e-watch-test-label-changed,UID:b2e8c1f8-00f8-11ea-98c4-0655ae4e8e56,ResourceVersion:69474,Generation:0,CreationTimestamp:2019-11-07 00:51:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov  7 00:51:49.851: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6994,SelfLink:/api/v1/namespaces/watch-6994/configmaps/e2e-watch-test-label-changed,UID:b2e8c1f8-00f8-11ea-98c4-0655ae4e8e56,ResourceVersion:69476,Generation:0,CreationTimestamp:2019-11-07 00:51:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov  7 00:52:00.498: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6994,SelfLink:/api/v1/namespaces/watch-6994/configmaps/e2e-watch-test-label-changed,UID:b2e8c1f8-00f8-11ea-98c4-0655ae4e8e56,ResourceVersion:69520,Generation:0,CreationTimestamp:2019-11-07 00:51:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov  7 00:52:00.498: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6994,SelfLink:/api/v1/namespaces/watch-6994/configmaps/e2e-watch-test-label-changed,UID:b2e8c1f8-00f8-11ea-98c4-0655ae4e8e56,ResourceVersion:69521,Generation:0,CreationTimestamp:2019-11-07 00:51:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Nov  7 00:52:00.498: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6994,SelfLink:/api/v1/namespaces/watch-6994/configmaps/e2e-watch-test-label-changed,UID:b2e8c1f8-00f8-11ea-98c4-0655ae4e8e56,ResourceVersion:69523,Generation:0,CreationTimestamp:2019-11-07 00:51:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:52:00.498: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-6994" for this suite.
Nov  7 00:52:07.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:52:15.576: INFO: namespace watch-6994 deletion completed in 14.806110594s

• [SLOW TEST:26.651 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:52:15.578: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:52:24.325: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-898" for this suite.
Nov  7 00:52:30.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:52:39.505: INFO: namespace kubelet-test-898 deletion completed in 14.909391601s

• [SLOW TEST:23.928 seconds]
[k8s.io] Kubelet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:52:39.507: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov  7 00:52:40.728: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:40.728: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:40.728: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:40.821: INFO: Number of nodes with available pods: 0
Nov  7 00:52:40.821: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:42.094: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:42.094: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:42.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:42.187: INFO: Number of nodes with available pods: 0
Nov  7 00:52:42.187: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:43.095: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:43.095: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:43.095: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:43.188: INFO: Number of nodes with available pods: 0
Nov  7 00:52:43.188: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:44.095: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:44.095: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:44.095: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:44.189: INFO: Number of nodes with available pods: 0
Nov  7 00:52:44.189: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:45.095: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:45.095: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:45.095: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:45.188: INFO: Number of nodes with available pods: 0
Nov  7 00:52:45.188: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:46.095: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:46.095: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:46.095: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:46.188: INFO: Number of nodes with available pods: 0
Nov  7 00:52:46.188: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:47.096: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:47.096: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:47.096: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:47.189: INFO: Number of nodes with available pods: 0
Nov  7 00:52:47.189: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:48.094: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:48.094: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:48.094: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:48.187: INFO: Number of nodes with available pods: 0
Nov  7 00:52:48.187: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:49.094: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:49.094: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:49.095: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:49.188: INFO: Number of nodes with available pods: 1
Nov  7 00:52:49.188: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:50.095: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:50.095: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:50.095: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:50.188: INFO: Number of nodes with available pods: 2
Nov  7 00:52:50.188: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:51.095: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:51.096: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:51.096: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:51.189: INFO: Number of nodes with available pods: 2
Nov  7 00:52:51.189: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:52.097: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:52.097: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:52.097: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:52.190: INFO: Number of nodes with available pods: 2
Nov  7 00:52:52.190: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:53.092: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:53.092: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:53.092: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:53.183: INFO: Number of nodes with available pods: 2
Nov  7 00:52:53.183: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:54.092: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:54.092: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:54.092: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:54.183: INFO: Number of nodes with available pods: 3
Nov  7 00:52:54.183: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:55.092: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:55.092: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:55.092: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:55.184: INFO: Number of nodes with available pods: 4
Nov  7 00:52:55.184: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov  7 00:52:55.556: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:55.556: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:55.556: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:55.648: INFO: Number of nodes with available pods: 3
Nov  7 00:52:55.648: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:56.921: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:56.921: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:56.921: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:57.013: INFO: Number of nodes with available pods: 3
Nov  7 00:52:57.013: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:57.920: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:57.920: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:57.920: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:58.012: INFO: Number of nodes with available pods: 3
Nov  7 00:52:58.012: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:58.920: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:58.920: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:58.920: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:52:59.012: INFO: Number of nodes with available pods: 3
Nov  7 00:52:59.012: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:52:59.919: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:52:59.919: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:52:59.919: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:53:00.011: INFO: Number of nodes with available pods: 3
Nov  7 00:53:00.011: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:53:00.919: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:53:00.919: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:53:00.919: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:53:01.011: INFO: Number of nodes with available pods: 3
Nov  7 00:53:01.011: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:53:01.939: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:53:01.940: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:53:01.940: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:53:02.032: INFO: Number of nodes with available pods: 3
Nov  7 00:53:02.032: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:53:02.920: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:53:02.920: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:53:02.920: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:53:03.012: INFO: Number of nodes with available pods: 3
Nov  7 00:53:03.012: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:53:03.920: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:53:03.920: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:53:03.920: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:53:04.013: INFO: Number of nodes with available pods: 4
Nov  7 00:53:04.013: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7761, will wait for the garbage collector to delete the pods
Nov  7 00:53:04.481: INFO: Deleting DaemonSet.extensions daemon-set took: 94.102994ms
Nov  7 00:53:04.582: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.265878ms
Nov  7 00:53:12.473: INFO: Number of nodes with available pods: 0
Nov  7 00:53:12.474: INFO: Number of running nodes: 0, number of available pods: 0
Nov  7 00:53:12.568: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7761/daemonsets","resourceVersion":"70094"},"items":null}

Nov  7 00:53:12.660: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7761/pods","resourceVersion":"70094"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:53:13.208: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-7761" for this suite.
Nov  7 00:53:19.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:53:28.232: INFO: namespace daemonsets-7761 deletion completed in 14.841752602s

• [SLOW TEST:48.725 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:53:28.233: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 00:53:28.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259" in namespace "downward-api-4175" to be "success or failure"
Nov  7 00:53:28.801: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.765684ms
Nov  7 00:53:30.897: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187410344s
Nov  7 00:53:32.991: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28125008s
Nov  7 00:53:35.085: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375623377s
Nov  7 00:53:37.178: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.468836006s
Nov  7 00:53:39.271: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.561440506s
STEP: Saw pod success
Nov  7 00:53:39.272: INFO: Pod "downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:53:39.364: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 00:53:39.558: INFO: Waiting for pod downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259 to disappear
Nov  7 00:53:39.650: INFO: Pod downwardapi-volume-030a3134-00f9-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:53:39.651: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4175" for this suite.
Nov  7 00:53:46.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:53:54.764: INFO: namespace downward-api-4175 deletion completed in 14.841391023s

• [SLOW TEST:26.531 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:53:54.765: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-12da0bf1-00f9-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 00:53:55.330: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259" in namespace "projected-676" to be "success or failure"
Nov  7 00:53:55.422: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.674399ms
Nov  7 00:53:57.514: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183838016s
Nov  7 00:53:59.606: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276139011s
Nov  7 00:54:01.699: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368747981s
Nov  7 00:54:03.792: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.461219022s
Nov  7 00:54:05.884: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.55327122s
STEP: Saw pod success
Nov  7 00:54:05.884: INFO: Pod "pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:54:05.975: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 00:54:06.168: INFO: Waiting for pod pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259 to disappear
Nov  7 00:54:06.260: INFO: Pod pod-projected-configmaps-12e850bf-00f9-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:54:06.260: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-676" for this suite.
Nov  7 00:54:12.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:54:21.383: INFO: namespace projected-676 deletion completed in 14.85207378s

• [SLOW TEST:26.617 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:54:21.383: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Nov  7 00:54:33.019: INFO: Successfully updated pod "annotationupdate22b7c34b-00f9-11ea-aaa6-525400524259"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:54:35.207: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4606" for this suite.
Nov  7 00:55:03.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:55:12.328: INFO: namespace projected-4606 deletion completed in 36.849292429s

• [SLOW TEST:50.946 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:55:12.330: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 00:55:12.699: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:55:23.483: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-4206" for this suite.
Nov  7 00:56:08.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:56:16.597: INFO: namespace pods-4206 deletion completed in 52.842203899s

• [SLOW TEST:64.267 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:56:16.599: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:56:27.535: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-8705" for this suite.
Nov  7 00:56:40.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:56:48.686: INFO: namespace replication-controller-8705 deletion completed in 20.876243923s

• [SLOW TEST:32.088 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:56:48.688: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov  7 00:56:58.225: INFO: Successfully updated pod "pod-update-7a851606-00f9-11ea-aaa6-525400524259"
STEP: verifying the updated pod is in kubernetes
Nov  7 00:56:58.409: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:56:58.409: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1538" for this suite.
Nov  7 00:57:26.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:57:35.544: INFO: namespace pods-1538 deletion completed in 36.86335484s

• [SLOW TEST:46.857 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:57:35.545: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-1306/secret-test-9672835c-00f9-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 00:57:36.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259" in namespace "secrets-1306" to be "success or failure"
Nov  7 00:57:36.204: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.719148ms
Nov  7 00:57:38.296: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186210649s
Nov  7 00:57:40.390: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279936253s
Nov  7 00:57:42.482: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372104759s
Nov  7 00:57:44.575: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.46520828s
Nov  7 00:57:46.668: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.557718736s
STEP: Saw pod success
Nov  7 00:57:46.668: INFO: Pod "pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:57:46.760: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259 container env-test: <nil>
STEP: delete the pod
Nov  7 00:57:46.955: INFO: Waiting for pod pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259 to disappear
Nov  7 00:57:47.047: INFO: Pod pod-configmaps-9680c3ef-00f9-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:57:47.047: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1306" for this suite.
Nov  7 00:57:53.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:58:02.192: INFO: namespace secrets-1306 deletion completed in 14.873609217s

• [SLOW TEST:26.648 seconds]
[sig-api-machinery] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:58:02.194: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Nov  7 00:58:02.663: INFO: Waiting up to 5m0s for pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259" in namespace "containers-3709" to be "success or failure"
Nov  7 00:58:02.756: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.174997ms
Nov  7 00:58:04.848: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184662142s
Nov  7 00:58:06.942: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278145619s
Nov  7 00:58:09.034: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370404247s
Nov  7 00:58:11.126: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463068013s
Nov  7 00:58:13.219: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.555775127s
Nov  7 00:58:15.312: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.648398692s
STEP: Saw pod success
Nov  7 00:58:15.312: INFO: Pod "client-containers-a6549fbf-00f9-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 00:58:15.404: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod client-containers-a6549fbf-00f9-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 00:58:15.597: INFO: Waiting for pod client-containers-a6549fbf-00f9-11ea-aaa6-525400524259 to disappear
Nov  7 00:58:15.689: INFO: Pod client-containers-a6549fbf-00f9-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:58:15.689: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-3709" for this suite.
Nov  7 00:58:22.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:58:30.808: INFO: namespace containers-3709 deletion completed in 14.846357807s

• [SLOW TEST:28.614 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:58:30.808: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov  7 00:58:32.012: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:32.012: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:32.012: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:32.105: INFO: Number of nodes with available pods: 0
Nov  7 00:58:32.105: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:33.377: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:33.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:33.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:33.470: INFO: Number of nodes with available pods: 0
Nov  7 00:58:33.470: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:34.380: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:34.380: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:34.381: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:34.473: INFO: Number of nodes with available pods: 0
Nov  7 00:58:34.473: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:35.378: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:35.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:35.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:35.470: INFO: Number of nodes with available pods: 0
Nov  7 00:58:35.470: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:36.378: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:36.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:36.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:36.470: INFO: Number of nodes with available pods: 0
Nov  7 00:58:36.470: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:37.378: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:37.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:37.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:37.470: INFO: Number of nodes with available pods: 0
Nov  7 00:58:37.470: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:38.379: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:38.379: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:38.379: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:38.471: INFO: Number of nodes with available pods: 0
Nov  7 00:58:38.471: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:39.377: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:39.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:39.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:39.470: INFO: Number of nodes with available pods: 0
Nov  7 00:58:39.470: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:40.378: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:40.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:40.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:40.470: INFO: Number of nodes with available pods: 0
Nov  7 00:58:40.470: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:41.378: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:41.378: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:41.378: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:41.470: INFO: Number of nodes with available pods: 4
Nov  7 00:58:41.470: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov  7 00:58:41.846: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:41.846: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:41.846: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:41.938: INFO: Number of nodes with available pods: 3
Nov  7 00:58:41.938: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:43.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:43.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:43.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:43.304: INFO: Number of nodes with available pods: 3
Nov  7 00:58:43.304: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:44.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:44.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:44.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:44.303: INFO: Number of nodes with available pods: 3
Nov  7 00:58:44.303: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:45.212: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:45.212: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:45.212: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:45.305: INFO: Number of nodes with available pods: 3
Nov  7 00:58:45.305: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:46.213: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:46.213: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:46.213: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:46.306: INFO: Number of nodes with available pods: 3
Nov  7 00:58:46.306: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:47.212: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:47.212: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:47.212: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:47.304: INFO: Number of nodes with available pods: 3
Nov  7 00:58:47.304: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:48.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:48.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:48.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:48.303: INFO: Number of nodes with available pods: 3
Nov  7 00:58:48.303: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:49.212: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:49.212: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:49.213: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:49.305: INFO: Number of nodes with available pods: 3
Nov  7 00:58:49.305: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:50.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:50.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:50.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:50.303: INFO: Number of nodes with available pods: 3
Nov  7 00:58:50.303: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:51.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:51.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:51.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:51.304: INFO: Number of nodes with available pods: 3
Nov  7 00:58:51.304: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:52.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:52.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:52.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:52.303: INFO: Number of nodes with available pods: 3
Nov  7 00:58:52.303: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:53.210: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:53.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:53.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:53.303: INFO: Number of nodes with available pods: 3
Nov  7 00:58:53.303: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:54.211: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:54.211: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:54.211: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:54.304: INFO: Number of nodes with available pods: 3
Nov  7 00:58:54.304: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:55.212: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:55.212: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:55.212: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:55.305: INFO: Number of nodes with available pods: 3
Nov  7 00:58:55.305: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:56.212: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:56.212: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:56.212: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:56.304: INFO: Number of nodes with available pods: 3
Nov  7 00:58:56.304: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 00:58:57.212: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 00:58:57.212: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 00:58:57.212: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 00:58:57.305: INFO: Number of nodes with available pods: 4
Nov  7 00:58:57.305: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6376, will wait for the garbage collector to delete the pods
Nov  7 00:58:57.685: INFO: Deleting DaemonSet.extensions daemon-set took: 95.203982ms
Nov  7 00:58:57.786: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.315421ms
Nov  7 00:59:07.778: INFO: Number of nodes with available pods: 0
Nov  7 00:59:07.778: INFO: Number of running nodes: 0, number of available pods: 0
Nov  7 00:59:07.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6376/daemonsets","resourceVersion":"72643"},"items":null}

Nov  7 00:59:07.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6376/pods","resourceVersion":"72643"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 00:59:08.520: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-6376" for this suite.
Nov  7 00:59:14.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 00:59:23.555: INFO: namespace daemonsets-6376 deletion completed in 14.851987622s

• [SLOW TEST:52.747 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 00:59:23.555: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Nov  7 00:59:23.927: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-2395'
Nov  7 00:59:25.544: INFO: stderr: ""
Nov  7 00:59:25.544: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  7 00:59:25.544: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2395'
Nov  7 00:59:25.999: INFO: stderr: ""
Nov  7 00:59:25.999: INFO: stdout: "update-demo-nautilus-gndlw update-demo-nautilus-hbrfd "
Nov  7 00:59:26.000: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-gndlw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 00:59:26.427: INFO: stderr: ""
Nov  7 00:59:26.427: INFO: stdout: ""
Nov  7 00:59:26.427: INFO: update-demo-nautilus-gndlw is created but not running
Nov  7 00:59:31.427: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2395'
Nov  7 00:59:31.881: INFO: stderr: ""
Nov  7 00:59:31.881: INFO: stdout: "update-demo-nautilus-gndlw update-demo-nautilus-hbrfd "
Nov  7 00:59:31.881: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-gndlw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 00:59:32.311: INFO: stderr: ""
Nov  7 00:59:32.311: INFO: stdout: ""
Nov  7 00:59:32.311: INFO: update-demo-nautilus-gndlw is created but not running
Nov  7 00:59:37.312: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2395'
Nov  7 00:59:37.744: INFO: stderr: ""
Nov  7 00:59:37.744: INFO: stdout: "update-demo-nautilus-gndlw update-demo-nautilus-hbrfd "
Nov  7 00:59:37.744: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-gndlw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 00:59:38.178: INFO: stderr: ""
Nov  7 00:59:38.178: INFO: stdout: "true"
Nov  7 00:59:38.178: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-gndlw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 00:59:38.631: INFO: stderr: ""
Nov  7 00:59:38.631: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 00:59:38.631: INFO: validating pod update-demo-nautilus-gndlw
Nov  7 00:59:38.725: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 00:59:38.725: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 00:59:38.725: INFO: update-demo-nautilus-gndlw is verified up and running
Nov  7 00:59:38.725: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-hbrfd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 00:59:39.161: INFO: stderr: ""
Nov  7 00:59:39.161: INFO: stdout: "true"
Nov  7 00:59:39.161: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-hbrfd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 00:59:39.588: INFO: stderr: ""
Nov  7 00:59:39.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 00:59:39.588: INFO: validating pod update-demo-nautilus-hbrfd
Nov  7 00:59:39.682: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 00:59:39.682: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 00:59:39.682: INFO: update-demo-nautilus-hbrfd is verified up and running
STEP: rolling-update to new replication controller
Nov  7 00:59:39.859: INFO: scanned /home/jeder for discovery docs: <nil>
Nov  7 00:59:39.860: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2395'
Nov  7 01:00:13.234: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov  7 01:00:13.234: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  7 01:00:13.234: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2395'
Nov  7 01:00:13.785: INFO: stderr: ""
Nov  7 01:00:13.785: INFO: stdout: "update-demo-kitten-5cx2h update-demo-kitten-lgz75 update-demo-nautilus-hbrfd "
STEP: Replicas for name=update-demo: expected=2 actual=3
Nov  7 01:00:18.786: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2395'
Nov  7 01:00:19.245: INFO: stderr: ""
Nov  7 01:00:19.245: INFO: stdout: "update-demo-kitten-5cx2h update-demo-kitten-lgz75 "
Nov  7 01:00:19.245: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-kitten-5cx2h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 01:00:19.692: INFO: stderr: ""
Nov  7 01:00:19.692: INFO: stdout: "true"
Nov  7 01:00:19.692: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-kitten-5cx2h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 01:00:20.145: INFO: stderr: ""
Nov  7 01:00:20.145: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov  7 01:00:20.145: INFO: validating pod update-demo-kitten-5cx2h
Nov  7 01:00:20.240: INFO: got data: {
  "image": "kitten.jpg"
}

Nov  7 01:00:20.240: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov  7 01:00:20.240: INFO: update-demo-kitten-5cx2h is verified up and running
Nov  7 01:00:20.240: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-kitten-lgz75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 01:00:20.687: INFO: stderr: ""
Nov  7 01:00:20.687: INFO: stdout: "true"
Nov  7 01:00:20.687: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-kitten-lgz75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2395'
Nov  7 01:00:21.141: INFO: stderr: ""
Nov  7 01:00:21.141: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov  7 01:00:21.141: INFO: validating pod update-demo-kitten-lgz75
Nov  7 01:00:21.235: INFO: got data: {
  "image": "kitten.jpg"
}

Nov  7 01:00:21.235: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov  7 01:00:21.235: INFO: update-demo-kitten-lgz75 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:00:21.235: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2395" for this suite.
Nov  7 01:00:39.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:00:48.412: INFO: namespace kubectl-2395 deletion completed in 26.903625967s

• [SLOW TEST:84.857 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:00:48.414: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-0968094f-00fa-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 01:00:48.980: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259" in namespace "projected-7285" to be "success or failure"
Nov  7 01:00:49.073: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.79001ms
Nov  7 01:00:51.165: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185480248s
Nov  7 01:00:53.259: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279069602s
Nov  7 01:00:55.351: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371085177s
Nov  7 01:00:57.444: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464317394s
Nov  7 01:00:59.537: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.557471236s
Nov  7 01:01:01.631: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.650781273s
STEP: Saw pod success
Nov  7 01:01:01.631: INFO: Pod "pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:01:01.723: INFO: Trying to get logs from node ip-10-0-139-168.us-west-2.compute.internal pod pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  7 01:01:01.922: INFO: Waiting for pod pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:01:02.013: INFO: Pod pod-projected-secrets-09764957-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:01:02.014: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7285" for this suite.
Nov  7 01:01:08.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:01:17.128: INFO: namespace projected-7285 deletion completed in 14.840620835s

• [SLOW TEST:28.714 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:01:17.128: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Nov  7 01:01:17.601: INFO: Waiting up to 5m0s for pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259" in namespace "downward-api-5999" to be "success or failure"
Nov  7 01:01:17.694: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.008327ms
Nov  7 01:01:19.787: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185652253s
Nov  7 01:01:21.882: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280147103s
Nov  7 01:01:23.975: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373271899s
Nov  7 01:01:26.067: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.46566456s
Nov  7 01:01:28.161: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.559372481s
STEP: Saw pod success
Nov  7 01:01:28.161: INFO: Pod "downward-api-1a85c17f-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:01:28.253: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downward-api-1a85c17f-00fa-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 01:01:28.447: INFO: Waiting for pod downward-api-1a85c17f-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:01:28.540: INFO: Pod downward-api-1a85c17f-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:01:28.540: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5999" for this suite.
Nov  7 01:01:35.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:01:43.675: INFO: namespace downward-api-5999 deletion completed in 14.862976578s

• [SLOW TEST:26.546 seconds]
[sig-node] Downward API
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:01:43.676: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6740
[It] Should recreate evicted statefulset [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6740
STEP: Creating statefulset with conflicting port in namespace statefulset-6740
STEP: Waiting until pod test-pod will start running in namespace statefulset-6740
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6740
Nov  7 01:01:54.720: INFO: Observed stateful pod in namespace: statefulset-6740, name: ss-0, uid: 1ad511fd-00fa-11ea-98c4-0655ae4e8e56, status phase: Pending. Waiting for statefulset controller to delete.
Nov  7 01:01:55.119: INFO: Observed stateful pod in namespace: statefulset-6740, name: ss-0, uid: 1ad511fd-00fa-11ea-98c4-0655ae4e8e56, status phase: Failed. Waiting for statefulset controller to delete.
Nov  7 01:01:55.124: INFO: Observed stateful pod in namespace: statefulset-6740, name: ss-0, uid: 1ad511fd-00fa-11ea-98c4-0655ae4e8e56, status phase: Failed. Waiting for statefulset controller to delete.
Nov  7 01:01:55.127: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6740
STEP: Removing pod with conflicting port in namespace statefulset-6740
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6740 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Nov  7 01:02:05.790: INFO: Deleting all statefulset in ns statefulset-6740
Nov  7 01:02:05.882: INFO: Scaling statefulset ss to 0
Nov  7 01:02:16.256: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 01:02:16.348: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:02:16.628: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-6740" for this suite.
Nov  7 01:02:23.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:02:31.656: INFO: namespace statefulset-6740 deletion completed in 14.844211599s

• [SLOW TEST:47.980 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:02:31.658: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov  7 01:02:32.128: INFO: Waiting up to 5m0s for pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259" in namespace "emptydir-5986" to be "success or failure"
Nov  7 01:02:32.220: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.825849ms
Nov  7 01:02:34.312: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183946351s
Nov  7 01:02:36.406: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277654013s
Nov  7 01:02:38.499: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370562472s
Nov  7 01:02:40.592: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463912201s
Nov  7 01:02:42.685: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.556791963s
STEP: Saw pod success
Nov  7 01:02:42.685: INFO: Pod "pod-46f1ad5d-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:02:42.777: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-46f1ad5d-00fa-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 01:02:42.979: INFO: Waiting for pod pod-46f1ad5d-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:02:43.071: INFO: Pod pod-46f1ad5d-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:02:43.071: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5986" for this suite.
Nov  7 01:02:49.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:02:58.179: INFO: namespace emptydir-5986 deletion completed in 14.837152282s

• [SLOW TEST:26.521 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:02:58.180: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:02:58.552: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:03:09.398: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-5518" for this suite.
Nov  7 01:03:53.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:04:02.544: INFO: namespace pods-5518 deletion completed in 52.873685134s

• [SLOW TEST:64.364 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:04:02.545: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 01:04:03.015: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259" in namespace "downward-api-5755" to be "success or failure"
Nov  7 01:04:03.107: INFO: Pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.913395ms
Nov  7 01:04:05.201: INFO: Pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18512905s
Nov  7 01:04:07.293: INFO: Pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277360473s
Nov  7 01:04:09.385: INFO: Pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369431447s
Nov  7 01:04:11.478: INFO: Pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.462013191s
STEP: Saw pod success
Nov  7 01:04:11.478: INFO: Pod "downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:04:11.570: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 01:04:11.764: INFO: Waiting for pod downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:04:11.855: INFO: Pod downwardapi-volume-7d1ddffb-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:04:11.855: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5755" for this suite.
Nov  7 01:04:18.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:04:27.055: INFO: namespace downward-api-5755 deletion completed in 14.9282948s

• [SLOW TEST:24.510 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:04:27.056: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov  7 01:04:27.584: INFO: Waiting up to 5m0s for pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259" in namespace "emptydir-4325" to be "success or failure"
Nov  7 01:04:27.676: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.027598ms
Nov  7 01:04:29.768: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184104275s
Nov  7 01:04:31.861: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276938832s
Nov  7 01:04:33.953: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369063256s
Nov  7 01:04:36.046: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.461468983s
Nov  7 01:04:38.138: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.553702725s
STEP: Saw pod success
Nov  7 01:04:38.138: INFO: Pod "pod-8bbdff27-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:04:38.230: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-8bbdff27-00fa-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 01:04:38.421: INFO: Waiting for pod pod-8bbdff27-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:04:38.513: INFO: Pod pod-8bbdff27-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:04:38.513: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4325" for this suite.
Nov  7 01:04:45.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:04:53.624: INFO: namespace emptydir-4325 deletion completed in 14.839975411s

• [SLOW TEST:26.568 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:04:53.625: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov  7 01:04:54.093: INFO: Waiting up to 5m0s for pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259" in namespace "emptydir-5108" to be "success or failure"
Nov  7 01:04:54.185: INFO: Pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.830252ms
Nov  7 01:04:56.281: INFO: Pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187782841s
Nov  7 01:04:58.374: INFO: Pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280986322s
Nov  7 01:05:00.467: INFO: Pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373284938s
Nov  7 01:05:02.559: INFO: Pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.465388656s
STEP: Saw pod success
Nov  7 01:05:02.559: INFO: Pod "pod-9b8fd5d8-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:05:02.650: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-9b8fd5d8-00fa-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 01:05:02.844: INFO: Waiting for pod pod-9b8fd5d8-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:05:02.936: INFO: Pod pod-9b8fd5d8-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:05:02.936: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5108" for this suite.
Nov  7 01:05:09.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:05:18.177: INFO: namespace emptydir-5108 deletion completed in 14.969536905s

• [SLOW TEST:24.552 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:05:18.177: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Nov  7 01:05:18.656: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7499" to be "success or failure"
Nov  7 01:05:18.749: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 93.71785ms
Nov  7 01:05:20.843: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187394921s
Nov  7 01:05:22.940: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.284083462s
Nov  7 01:05:25.033: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.377121338s
Nov  7 01:05:27.125: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469471937s
Nov  7 01:05:29.218: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.56255852s
STEP: Saw pod success
Nov  7 01:05:29.219: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Nov  7 01:05:29.311: INFO: Trying to get logs from node ip-10-0-136-94.us-west-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov  7 01:05:29.507: INFO: Waiting for pod pod-host-path-test to disappear
Nov  7 01:05:29.599: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:05:29.599: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "hostpath-7499" for this suite.
Nov  7 01:05:36.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:05:44.740: INFO: namespace hostpath-7499 deletion completed in 14.868523415s

• [SLOW TEST:26.564 seconds]
[sig-storage] HostPath
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:05:44.741: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ba078b99-00fa-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 01:05:45.303: INFO: Waiting up to 5m0s for pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259" in namespace "secrets-2144" to be "success or failure"
Nov  7 01:05:45.395: INFO: Pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.620054ms
Nov  7 01:05:47.487: INFO: Pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183757668s
Nov  7 01:05:49.580: INFO: Pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276625543s
Nov  7 01:05:51.672: INFO: Pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368734109s
Nov  7 01:05:53.764: INFO: Pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.460984829s
STEP: Saw pod success
Nov  7 01:05:53.764: INFO: Pod "pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:05:53.856: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 01:05:54.049: INFO: Waiting for pod pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:05:54.141: INFO: Pod pod-secrets-ba15cdc5-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:05:54.141: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2144" for this suite.
Nov  7 01:06:00.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:06:09.240: INFO: namespace secrets-2144 deletion completed in 14.827257718s

• [SLOW TEST:24.498 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:06:09.240: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-n4bn
STEP: Creating a pod to test atomic-volume-subpath
Nov  7 01:06:09.898: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-n4bn" in namespace "subpath-2943" to be "success or failure"
Nov  7 01:06:09.990: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Pending", Reason="", readiness=false. Elapsed: 91.941549ms
Nov  7 01:06:12.082: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184485354s
Nov  7 01:06:14.175: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276900658s
Nov  7 01:06:16.267: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369372313s
Nov  7 01:06:18.360: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.461836631s
Nov  7 01:06:20.452: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 10.554220576s
Nov  7 01:06:22.545: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 12.646583036s
Nov  7 01:06:24.638: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 14.739683762s
Nov  7 01:06:26.730: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 16.832054149s
Nov  7 01:06:28.822: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 18.924173314s
Nov  7 01:06:30.914: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 21.015977264s
Nov  7 01:06:33.006: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 23.108139217s
Nov  7 01:06:35.098: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 25.200173778s
Nov  7 01:06:37.191: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Running", Reason="", readiness=true. Elapsed: 27.293385397s
Nov  7 01:06:39.284: INFO: Pod "pod-subpath-test-configmap-n4bn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.385645293s
STEP: Saw pod success
Nov  7 01:06:39.284: INFO: Pod "pod-subpath-test-configmap-n4bn" satisfied condition "success or failure"
Nov  7 01:06:39.376: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-subpath-test-configmap-n4bn container test-container-subpath-configmap-n4bn: <nil>
STEP: delete the pod
Nov  7 01:06:39.569: INFO: Waiting for pod pod-subpath-test-configmap-n4bn to disappear
Nov  7 01:06:39.661: INFO: Pod pod-subpath-test-configmap-n4bn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-n4bn
Nov  7 01:06:39.661: INFO: Deleting pod "pod-subpath-test-configmap-n4bn" in namespace "subpath-2943"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:06:39.753: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-2943" for this suite.
Nov  7 01:06:46.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:06:54.861: INFO: namespace subpath-2943 deletion completed in 14.836547596s

• [SLOW TEST:45.621 seconds]
[sig-storage] Subpath
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:06:54.862: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Nov  7 01:07:06.395: INFO: Successfully updated pod "labelsupdatee3d32fe2-00fa-11ea-aaa6-525400524259"
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:07:08.584: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4719" for this suite.
Nov  7 01:07:21.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:07:29.713: INFO: namespace downward-api-4719 deletion completed in 20.857516163s

• [SLOW TEST:34.852 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:07:29.714: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f89f772d-00fa-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 01:07:30.316: INFO: Waiting up to 5m0s for pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259" in namespace "secrets-4461" to be "success or failure"
Nov  7 01:07:30.408: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.537402ms
Nov  7 01:07:32.500: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183773326s
Nov  7 01:07:34.594: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277217876s
Nov  7 01:07:36.687: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370882801s
Nov  7 01:07:38.781: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.465008073s
Nov  7 01:07:40.875: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.558864228s
STEP: Saw pod success
Nov  7 01:07:40.875: INFO: Pod "pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:07:40.969: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 01:07:41.164: INFO: Waiting for pod pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259 to disappear
Nov  7 01:07:41.257: INFO: Pod pod-secrets-f8ad9944-00fa-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:07:41.257: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-4461" for this suite.
Nov  7 01:07:47.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:07:56.483: INFO: namespace secrets-4461 deletion completed in 14.95088908s

• [SLOW TEST:26.769 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:07:56.485: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:07:56.859: INFO: Creating deployment "test-recreate-deployment"
Nov  7 01:07:56.955: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov  7 01:07:57.141: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov  7 01:07:57.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:07:59.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:08:01.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:08:03.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708685641, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:08:05.333: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov  7 01:08:05.522: INFO: Updating deployment test-recreate-deployment
Nov  7 01:08:05.522: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Nov  7 01:08:05.712: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4690,SelfLink:/apis/apps/v1/namespaces/deployment-4690/deployments/test-recreate-deployment,UID:f390f8f7-00fa-11ea-98c4-0655ae4e8e56,ResourceVersion:76944,Generation:2,CreationTimestamp:2019-11-07 01:07:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-11-07 01:07:30 +0000 UTC 2019-11-07 01:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-11-07 01:07:30 +0000 UTC 2019-11-07 01:07:21 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Nov  7 01:08:05.806: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-4690,SelfLink:/apis/apps/v1/namespaces/deployment-4690/replicasets/test-recreate-deployment-745fb9c84c,UID:f8b066e6-00fa-11ea-98c4-0655ae4e8e56,ResourceVersion:76943,Generation:1,CreationTimestamp:2019-11-07 01:07:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f390f8f7-00fa-11ea-98c4-0655ae4e8e56 0xc0026501f7 0xc0026501f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov  7 01:08:05.806: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov  7 01:08:05.806: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-4690,SelfLink:/apis/apps/v1/namespaces/deployment-4690/replicasets/test-recreate-deployment-6566d46b4b,UID:f391bdd8-00fa-11ea-98c4-0655ae4e8e56,ResourceVersion:76933,Generation:2,CreationTimestamp:2019-11-07 01:07:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f390f8f7-00fa-11ea-98c4-0655ae4e8e56 0xc002650127 0xc002650128}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov  7 01:08:05.899: INFO: Pod "test-recreate-deployment-745fb9c84c-ztr6f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-ztr6f,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-4690,SelfLink:/api/v1/namespaces/deployment-4690/pods/test-recreate-deployment-745fb9c84c-ztr6f,UID:f8b141f2-00fa-11ea-98c4-0655ae4e8e56,ResourceVersion:76945,Generation:0,CreationTimestamp:2019-11-07 01:07:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c f8b066e6-00fa-11ea-98c4-0655ae4e8e56 0xc002650ab7 0xc002650ab8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f9fd7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f9fd7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f9fd7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-55pb4}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc002650b20} {node.kubernetes.io/not-ready Exists  NoExecute 0xc002650b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:07:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:07:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:07:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:07:30 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:07:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:08:05.899: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-4690" for this suite.
Nov  7 01:08:12.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:08:21.128: INFO: namespace deployment-4690 deletion completed in 14.95211376s

• [SLOW TEST:24.643 seconds]
[sig-apps] Deployment
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:08:21.128: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:08:21.506: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig version'
Nov  7 01:08:22.143: INFO: stderr: ""
Nov  7 01:08:22.143: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14+\", GitVersion:\"v1.14.9-beta.0.18+ac756284b8908e\", GitCommit:\"ac756284b8908eb438f6db1847fcc790a338605c\", GitTreeState:\"clean\", BuildDate:\"2019-11-07T00:29:40Z\", GoVersion:\"go1.12.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14+\", GitVersion:\"v1.14.6+868bc38\", GitCommit:\"868bc38\", GitTreeState:\"clean\", BuildDate:\"2019-10-22T23:38:21Z\", GoVersion:\"go1.12.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:08:22.143: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8375" for this suite.
Nov  7 01:08:28.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:08:37.378: INFO: namespace kubectl-8375 deletion completed in 14.959755524s

• [SLOW TEST:16.250 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:08:37.379: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:08:46.223: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-7508" for this suite.
Nov  7 01:09:32.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:09:41.357: INFO: namespace kubelet-test-7508 deletion completed in 54.858791531s

• [SLOW TEST:63.979 seconds]
[k8s.io] Kubelet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:09:41.358: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-47111087-00fb-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 01:09:41.926: INFO: Waiting up to 5m0s for pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259" in namespace "configmap-671" to be "success or failure"
Nov  7 01:09:42.018: INFO: Pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.170545ms
Nov  7 01:09:44.111: INFO: Pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184907297s
Nov  7 01:09:46.204: INFO: Pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277537569s
Nov  7 01:09:48.296: INFO: Pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370326538s
Nov  7 01:09:50.389: INFO: Pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.462723444s
STEP: Saw pod success
Nov  7 01:09:50.389: INFO: Pod "pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:09:50.481: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 01:09:50.675: INFO: Waiting for pod pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259 to disappear
Nov  7 01:09:50.767: INFO: Pod pod-configmaps-471f814a-00fb-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:09:50.767: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-671" for this suite.
Nov  7 01:09:57.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:10:05.896: INFO: namespace configmap-671 deletion completed in 14.856840214s

• [SLOW TEST:24.539 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:10:05.898: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Nov  7 01:10:06.380: INFO: Waiting up to 5m0s for pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259" in namespace "containers-9172" to be "success or failure"
Nov  7 01:10:06.473: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.00653ms
Nov  7 01:10:08.565: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184438541s
Nov  7 01:10:10.658: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277535149s
Nov  7 01:10:12.751: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370745137s
Nov  7 01:10:14.845: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464045058s
Nov  7 01:10:16.937: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.55652427s
STEP: Saw pod success
Nov  7 01:10:16.937: INFO: Pod "client-containers-55b12a0b-00fb-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:10:17.029: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod client-containers-55b12a0b-00fb-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 01:10:17.223: INFO: Waiting for pod client-containers-55b12a0b-00fb-11ea-aaa6-525400524259 to disappear
Nov  7 01:10:17.315: INFO: Pod client-containers-55b12a0b-00fb-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:10:17.315: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-9172" for this suite.
Nov  7 01:10:23.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:10:32.461: INFO: namespace containers-9172 deletion completed in 14.873835123s

• [SLOW TEST:26.564 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:10:32.465: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 01:10:32.835: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5561'
Nov  7 01:10:37.211: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov  7 01:10:37.211: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Nov  7 01:10:37.304: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete jobs e2e-test-nginx-job --namespace=kubectl-5561'
Nov  7 01:10:37.867: INFO: stderr: ""
Nov  7 01:10:37.867: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:10:37.867: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5561" for this suite.
Nov  7 01:10:44.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:10:53.006: INFO: namespace kubectl-5561 deletion completed in 14.866879158s

• [SLOW TEST:20.542 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:10:53.008: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-71c58bd1-00fb-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 01:10:53.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259" in namespace "configmap-8839" to be "success or failure"
Nov  7 01:10:53.667: INFO: Pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.068376ms
Nov  7 01:10:55.760: INFO: Pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184909889s
Nov  7 01:10:57.853: INFO: Pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278411142s
Nov  7 01:10:59.946: INFO: Pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371247821s
Nov  7 01:11:02.042: INFO: Pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.46752046s
STEP: Saw pod success
Nov  7 01:11:02.042: INFO: Pod "pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:11:02.134: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 01:11:02.327: INFO: Waiting for pod pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259 to disappear
Nov  7 01:11:02.419: INFO: Pod pod-configmaps-71d41f39-00fb-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:11:02.419: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8839" for this suite.
Nov  7 01:11:08.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:11:17.549: INFO: namespace configmap-8839 deletion completed in 14.856906863s

• [SLOW TEST:24.541 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:11:17.552: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 01:11:17.924: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-554'
Nov  7 01:11:18.399: INFO: stderr: ""
Nov  7 01:11:18.399: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Nov  7 01:11:28.501: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pod e2e-test-nginx-pod --namespace=kubectl-554 -o json'
Nov  7 01:11:28.940: INFO: stderr: ""
Nov  7 01:11:28.940: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"openshift-sdn\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.130.2.99\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2019-11-07T01:10:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-554\",\n        \"resourceVersion\": \"78459\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-554/pods/e2e-test-nginx-pod\",\n        \"uid\": \"6ba1555c-00fb-11ea-9f32-06a26e5abcda\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-glt48\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-vf4kl\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-132-48.us-west-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c35,c5\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-glt48\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-glt48\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-07T01:10:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-07T01:10:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-07T01:10:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-07T01:10:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://32201b0e49101ecdd2e5db81fa5360c83a9eaf2343c81dfab55f17b61b64d5f7\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-11-07T01:10:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.132.48\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.130.2.99\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-11-07T01:10:43Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov  7 01:11:28.941: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig replace -f - --namespace=kubectl-554'
Nov  7 01:11:30.469: INFO: stderr: ""
Nov  7 01:11:30.469: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Nov  7 01:11:30.562: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete pods e2e-test-nginx-pod --namespace=kubectl-554'
Nov  7 01:11:33.793: INFO: stderr: ""
Nov  7 01:11:33.793: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:11:33.793: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-554" for this suite.
Nov  7 01:11:40.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:11:48.970: INFO: namespace kubectl-554 deletion completed in 14.903571609s

• [SLOW TEST:31.418 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:11:48.971: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-9m6s
STEP: Creating a pod to test atomic-volume-subpath
Nov  7 01:11:49.636: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9m6s" in namespace "subpath-8264" to be "success or failure"
Nov  7 01:11:49.730: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 93.905641ms
Nov  7 01:11:51.824: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187529199s
Nov  7 01:11:53.916: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280135405s
Nov  7 01:11:56.010: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373602727s
Nov  7 01:11:58.104: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.467508821s
Nov  7 01:12:00.198: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 10.56143281s
Nov  7 01:12:02.292: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 12.656019028s
Nov  7 01:12:04.385: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 14.748883927s
Nov  7 01:12:06.479: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 16.843277182s
Nov  7 01:12:08.572: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 18.935956106s
Nov  7 01:12:10.665: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 21.02865977s
Nov  7 01:12:12.760: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 23.123728389s
Nov  7 01:12:14.853: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 25.216517069s
Nov  7 01:12:16.946: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Running", Reason="", readiness=true. Elapsed: 27.309816886s
Nov  7 01:12:19.039: INFO: Pod "pod-subpath-test-projected-9m6s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.402700908s
STEP: Saw pod success
Nov  7 01:12:19.039: INFO: Pod "pod-subpath-test-projected-9m6s" satisfied condition "success or failure"
Nov  7 01:12:19.131: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-subpath-test-projected-9m6s container test-container-subpath-projected-9m6s: <nil>
STEP: delete the pod
Nov  7 01:12:19.326: INFO: Waiting for pod pod-subpath-test-projected-9m6s to disappear
Nov  7 01:12:19.417: INFO: Pod pod-subpath-test-projected-9m6s no longer exists
STEP: Deleting pod pod-subpath-test-projected-9m6s
Nov  7 01:12:19.417: INFO: Deleting pod "pod-subpath-test-projected-9m6s" in namespace "subpath-8264"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:12:19.510: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-8264" for this suite.
Nov  7 01:12:26.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:12:34.744: INFO: namespace subpath-8264 deletion completed in 14.960540043s

• [SLOW TEST:45.773 seconds]
[sig-storage] Subpath
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:12:34.746: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:12:35.310: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-5032" for this suite.
Nov  7 01:12:47.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:12:56.266: INFO: namespace pods-5032 deletion completed in 20.86323154s

• [SLOW TEST:21.521 seconds]
[k8s.io] [sig-node] Pods Extended
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:12:56.269: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Nov  7 01:13:07.900: INFO: Successfully updated pod "annotationupdatebb3e3f53-00fb-11ea-aaa6-525400524259"
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:13:10.088: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1610" for this suite.
Nov  7 01:13:38.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:13:47.238: INFO: namespace downward-api-1610 deletion completed in 36.877216381s

• [SLOW TEST:50.969 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:13:47.239: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:13:47.701: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-5290" for this suite.
Nov  7 01:13:54.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:14:02.662: INFO: namespace services-5290 deletion completed in 14.867744937s
[AfterEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:15.424 seconds]
[sig-network] Services
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:14:02.663: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:14:03.153: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-e2e2ff45-00fb-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:14:17.816: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9107" for this suite.
Nov  7 01:14:34.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:14:42.941: INFO: namespace configmap-9107 deletion completed in 24.852616426s

• [SLOW TEST:40.277 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:14:42.943: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Nov  7 01:14:43.411: INFO: Waiting up to 5m0s for pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259" in namespace "downward-api-2103" to be "success or failure"
Nov  7 01:14:43.504: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.471382ms
Nov  7 01:14:45.597: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185796604s
Nov  7 01:14:47.690: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279098295s
Nov  7 01:14:49.783: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371674953s
Nov  7 01:14:51.876: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464300364s
Nov  7 01:14:53.969: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.557687756s
Nov  7 01:14:56.062: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 12.650556683s
Nov  7 01:14:58.156: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.744588684s
STEP: Saw pod success
Nov  7 01:14:58.156: INFO: Pod "downward-api-fad28022-00fb-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:14:58.249: INFO: Trying to get logs from node ip-10-0-139-168.us-west-2.compute.internal pod downward-api-fad28022-00fb-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 01:14:58.443: INFO: Waiting for pod downward-api-fad28022-00fb-11ea-aaa6-525400524259 to disappear
Nov  7 01:14:58.535: INFO: Pod downward-api-fad28022-00fb-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:14:58.535: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2103" for this suite.
Nov  7 01:15:05.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:15:13.655: INFO: namespace downward-api-2103 deletion completed in 14.844971472s

• [SLOW TEST:30.713 seconds]
[sig-node] Downward API
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:15:13.656: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:15:24.527: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-5265" for this suite.
Nov  7 01:16:13.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:16:21.661: INFO: namespace kubelet-test-5265 deletion completed in 56.860632674s

• [SLOW TEST:68.005 seconds]
[k8s.io] Kubelet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:16:21.662: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-35aa3a28-00fc-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 01:16:22.227: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259" in namespace "projected-3801" to be "success or failure"
Nov  7 01:16:22.319: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.900181ms
Nov  7 01:16:24.412: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18468239s
Nov  7 01:16:26.505: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27752926s
Nov  7 01:16:28.597: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370328564s
Nov  7 01:16:30.691: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464290536s
Nov  7 01:16:32.786: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.558881731s
STEP: Saw pod success
Nov  7 01:16:32.786: INFO: Pod "pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:16:32.879: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 01:16:33.075: INFO: Waiting for pod pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259 to disappear
Nov  7 01:16:33.167: INFO: Pod pod-projected-configmaps-35b89c18-00fc-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:16:33.168: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3801" for this suite.
Nov  7 01:16:39.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:16:48.398: INFO: namespace projected-3801 deletion completed in 14.953760998s

• [SLOW TEST:26.737 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:16:48.400: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-459abd3e-00fc-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 01:16:48.971: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259" in namespace "projected-1562" to be "success or failure"
Nov  7 01:16:49.069: INFO: Pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 98.150677ms
Nov  7 01:16:51.163: INFO: Pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191548304s
Nov  7 01:16:53.256: INFO: Pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285340085s
Nov  7 01:16:55.350: INFO: Pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378633501s
Nov  7 01:16:57.444: INFO: Pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.473199585s
STEP: Saw pod success
Nov  7 01:16:57.445: INFO: Pod "pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:16:57.537: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  7 01:16:57.734: INFO: Waiting for pod pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259 to disappear
Nov  7 01:16:57.831: INFO: Pod pod-projected-secrets-45a91ce3-00fc-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:16:57.831: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1562" for this suite.
Nov  7 01:17:04.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:17:13.109: INFO: namespace projected-1562 deletion completed in 15.001627909s

• [SLOW TEST:24.710 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:17:13.111: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-5455607e-00fc-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 01:17:13.682: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259" in namespace "projected-9321" to be "success or failure"
Nov  7 01:17:13.775: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.838204ms
Nov  7 01:17:15.869: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18713s
Nov  7 01:17:17.963: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280506932s
Nov  7 01:17:20.058: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375208263s
Nov  7 01:17:22.151: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.468637842s
Nov  7 01:17:24.244: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.561978814s
STEP: Saw pod success
Nov  7 01:17:24.244: INFO: Pod "pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:17:24.338: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  7 01:17:24.534: INFO: Waiting for pod pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259 to disappear
Nov  7 01:17:24.626: INFO: Pod pod-projected-secrets-5463f253-00fc-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:17:24.627: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-9321" for this suite.
Nov  7 01:17:31.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:17:39.834: INFO: namespace projected-9321 deletion completed in 14.933325399s

• [SLOW TEST:26.724 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:17:39.835: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:17:40.210: INFO: Creating deployment "nginx-deployment"
Nov  7 01:17:40.305: INFO: Waiting for observed generation 1
Nov  7 01:17:40.398: INFO: Waiting for all required pods to come up
Nov  7 01:17:40.492: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov  7 01:17:50.775: INFO: Waiting for deployment "nginx-deployment" to complete
Nov  7 01:17:50.961: INFO: Updating deployment "nginx-deployment" with a non-existent image
Nov  7 01:17:51.162: INFO: Updating deployment nginx-deployment
Nov  7 01:17:51.162: INFO: Waiting for observed generation 2
Nov  7 01:17:51.256: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov  7 01:17:51.349: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov  7 01:17:51.442: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Nov  7 01:17:51.719: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov  7 01:17:51.719: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov  7 01:17:51.812: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Nov  7 01:17:51.997: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Nov  7 01:17:51.997: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Nov  7 01:17:52.185: INFO: Updating deployment nginx-deployment
Nov  7 01:17:52.185: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Nov  7 01:17:52.371: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov  7 01:17:52.463: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Nov  7 01:17:52.650: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9003,SelfLink:/apis/apps/v1/namespaces/deployment-9003/deployments/nginx-deployment,UID:4f450170-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81618,Generation:3,CreationTimestamp:2019-11-07 01:17:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-11-07 01:17:16 +0000 UTC 2019-11-07 01:17:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-11-07 01:17:17 +0000 UTC 2019-11-07 01:17:04 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Nov  7 01:17:52.744: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-9003,SelfLink:/apis/apps/v1/namespaces/deployment-9003/replicasets/nginx-deployment-b79c9d74d,UID:55be459b-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81617,Generation:3,CreationTimestamp:2019-11-07 01:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4f450170-00fc-11ea-98c4-0655ae4e8e56 0xc002284fc7 0xc002284fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov  7 01:17:52.744: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Nov  7 01:17:52.744: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-9003,SelfLink:/apis/apps/v1/namespaces/deployment-9003/replicasets/nginx-deployment-85db8c99c5,UID:4f45b3c7-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81573,Generation:3,CreationTimestamp:2019-11-07 01:17:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4f450170-00fc-11ea-98c4-0655ae4e8e56 0xc002284ef7 0xc002284ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Nov  7 01:17:52.929: INFO: Pod "nginx-deployment-85db8c99c5-2c7r4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-2c7r4,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-2c7r4,UID:4f4ae01d-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81443,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.131.0.26"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc002285957 0xc002285958}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0022859c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022859e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:10.131.0.26,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://67a5349fea486d64f2b5cf4baf74263828084855f4ad4bb8f795142498bf80d7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.929: INFO: Pod "nginx-deployment-85db8c99c5-4gr2k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-4gr2k,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-4gr2k,UID:4f4b2f0b-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81432,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.21"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc002285ae0 0xc002285ae1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-165.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002285b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002285b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.165,PodIP:10.128.2.21,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://6de52ef05f41bc4b16804ebc36396a221c37138eefd0634ff58a24b91ceb462d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.930: INFO: Pod "nginx-deployment-85db8c99c5-bd8t8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-bd8t8,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-bd8t8,UID:4f48d0f6-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81418,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.23"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc002285c60 0xc002285c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002285cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002285ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:10.129.2.23,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:12 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://3832530d7e16ea0beb656b3de9fb70a43672de5d27f246592c8f43264d03efff}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.930: INFO: Pod "nginx-deployment-85db8c99c5-c7c9t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-c7c9t,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-c7c9t,UID:4f4e7ba2-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81416,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.25"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc002285e00 0xc002285e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002285e60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002285ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:10.129.2.25,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://704c451d890ad96da6058a2ec8bdb892972babc4e87cf4fa1627c878b645b353}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.930: INFO: Pod "nginx-deployment-85db8c99c5-ddwfv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-ddwfv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-ddwfv,UID:4f4e88f3-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81390,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.128.2.22"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc002285f90 0xc002285f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-165.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.165,PodIP:10.128.2.22,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:12 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://2113c350ae8b386495f2b2c2ae7f4e8b1744c2ce802818ebd3002ce757fc6543}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.931: INFO: Pod "nginx-deployment-85db8c99c5-fgptt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-fgptt,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-fgptt,UID:565b9db3-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81524,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4110 0xc001ea4111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.931: INFO: Pod "nginx-deployment-85db8c99c5-ht29v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-ht29v,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-ht29v,UID:565dbfbf-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81533,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4277 0xc001ea4278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea42e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.931: INFO: Pod "nginx-deployment-85db8c99c5-jvmq2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-jvmq2,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-jvmq2,UID:56650cbd-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81571,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea43e7 0xc001ea43e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-165.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.165,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.932: INFO: Pod "nginx-deployment-85db8c99c5-lkpmk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-lkpmk,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-lkpmk,UID:4f4b27d0-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81422,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.24"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4557 0xc001ea4558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea45c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea45e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:10.129.2.24,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://3f8dab4bc99657df5c59df63e188032354f0c1760389731c92213b101de47aaf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.932: INFO: Pod "nginx-deployment-85db8c99c5-mvmtp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-mvmtp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-mvmtp,UID:56614279-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81574,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea46d0 0xc001ea46d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.932: INFO: Pod "nginx-deployment-85db8c99c5-q758j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-q758j,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-q758j,UID:56661024-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81587,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4837 0xc001ea4838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea48a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea48c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.932: INFO: Pod "nginx-deployment-85db8c99c5-qn4vz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-qn4vz,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-qn4vz,UID:4f4b2eec-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81400,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.108"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea49a7 0xc001ea49a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4a10} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:10.130.2.108,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://0824a99d7b23cf942da730446d5f364a15c00b8ea6fc7a2390de5f9f8ee14916}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.933: INFO: Pod "nginx-deployment-85db8c99c5-qvp88" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-qvp88,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-qvp88,UID:565dbb0a-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81531,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4b20 0xc001ea4b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.933: INFO: Pod "nginx-deployment-85db8c99c5-r7vfh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-r7vfh,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-r7vfh,UID:56605122-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81547,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4c80 0xc001ea4c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.933: INFO: Pod "nginx-deployment-85db8c99c5-rk7hm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-rk7hm,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-rk7hm,UID:56655fb3-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81577,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4de0 0xc001ea4de1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.933: INFO: Pod "nginx-deployment-85db8c99c5-twvw8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-twvw8,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-twvw8,UID:4f4e3ab8-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81436,Generation:0,CreationTimestamp:2019-11-07 01:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.107"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea4f40 0xc001ea4f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea4fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea4fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:05 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:10.130.2.107,StartTime:2019-11-07 01:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-07 01:17:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://c530d8d597c2318d2dc58adecf63e97d4b851c93e04205dc81eac34bc7f389a2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.933: INFO: Pod "nginx-deployment-85db8c99c5-vdvrl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-vdvrl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-vdvrl,UID:5661502b-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81564,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea50e0 0xc001ea50e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.933: INFO: Pod "nginx-deployment-85db8c99c5-vdxtg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-vdxtg,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-vdxtg,UID:5665f8b3-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81588,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5250 0xc001ea5251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea52e0} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.934: INFO: Pod "nginx-deployment-85db8c99c5-w5z86" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-w5z86,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-w5z86,UID:56611cfe-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81552,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea53e0 0xc001ea53e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.934: INFO: Pod "nginx-deployment-85db8c99c5-zxsg6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-zxsg6,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-85db8c99c5-zxsg6,UID:5665681f-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81578,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 4f45b3c7-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5540 0xc001ea5541}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea55a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea55c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.934: INFO: Pod "nginx-deployment-b79c9d74d-8fl2t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-8fl2t,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-8fl2t,UID:566f5bdb-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81623,Generation:0,CreationTimestamp:2019-11-07 01:17:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea56a0 0xc001ea56a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.934: INFO: Pod "nginx-deployment-b79c9d74d-9sjmz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-9sjmz,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-9sjmz,UID:55c6450d-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81488,Generation:0,CreationTimestamp:2019-11-07 01:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5820 0xc001ea5821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea58b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.934: INFO: Pod "nginx-deployment-b79c9d74d-c485n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-c485n,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-c485n,UID:55c6691e-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81490,Generation:0,CreationTimestamp:2019-11-07 01:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea59a0 0xc001ea59a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:,StartTime:2019-11-07 01:17:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.934: INFO: Pod "nginx-deployment-b79c9d74d-cd74l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-cd74l,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-cd74l,UID:55d06d11-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81504,Generation:0,CreationTimestamp:2019-11-07 01:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5b20 0xc001ea5b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:17:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.935: INFO: Pod "nginx-deployment-b79c9d74d-g95lt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-g95lt,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-g95lt,UID:565e1ad9-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81558,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5ca0 0xc001ea5ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.168,PodIP:,StartTime:2019-11-07 01:17:16 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.935: INFO: Pod "nginx-deployment-b79c9d74d-jslqx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-jslqx,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-jslqx,UID:55c0d204-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81478,Generation:0,CreationTimestamp:2019-11-07 01:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5e20 0xc001ea5e21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ea5e90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ea5eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:17:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.935: INFO: Pod "nginx-deployment-b79c9d74d-ktrl6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-ktrl6,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-ktrl6,UID:566ec6ad-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81609,Generation:0,CreationTimestamp:2019-11-07 01:17:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc001ea5fa0 0xc001ea5fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-165.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd8010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd8030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.165,PodIP:,StartTime:2019-11-07 01:17:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.935: INFO: Pod "nginx-deployment-b79c9d74d-lh6tz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-lh6tz,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-lh6tz,UID:566ed21e-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81607,Generation:0,CreationTimestamp:2019-11-07 01:17:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc002dd8120 0xc002dd8121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd8190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd81b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:,StartTime:2019-11-07 01:17:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.935: INFO: Pod "nginx-deployment-b79c9d74d-mmttn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-mmttn,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-mmttn,UID:5667b138-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81593,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc002dd82a0 0xc002dd82a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-165.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd8310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd8330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.165,PodIP:,StartTime:2019-11-07 01:17:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.936: INFO: Pod "nginx-deployment-b79c9d74d-mrz2d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-mrz2d,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-mrz2d,UID:5673dc0d-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81613,Generation:0,CreationTimestamp:2019-11-07 01:17:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc002dd8420 0xc002dd8421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-168.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd8490} {node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd84b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.936: INFO: Pod "nginx-deployment-b79c9d74d-w4jbq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-w4jbq,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-w4jbq,UID:55ccc73b-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81495,Generation:0,CreationTimestamp:2019-11-07 01:17:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc002dd8550 0xc002dd8551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-143-165.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd85c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd85e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:15 +0000 UTC  }],Message:,Reason:,HostIP:10.0.143.165,PodIP:,StartTime:2019-11-07 01:17:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.936: INFO: Pod "nginx-deployment-b79c9d74d-xzh9v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-xzh9v,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-xzh9v,UID:566f2669-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81602,Generation:0,CreationTimestamp:2019-11-07 01:17:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc002dd86d0 0xc002dd86d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd8740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd8760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov  7 01:17:52.936: INFO: Pod "nginx-deployment-b79c9d74d-z29x8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-z29x8,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-9003,SelfLink:/api/v1/namespaces/deployment-9003/pods/nginx-deployment-b79c9d74d-z29x8,UID:566795b0-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:81620,Generation:0,CreationTimestamp:2019-11-07 01:17:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 55be459b-00fc-11ea-98c4-0655ae4e8e56 0xc002dd8800 0xc002dd8801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-52l2j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-52l2j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-52l2j true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-pq5zt}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002dd8870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002dd8890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:17:16 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:,StartTime:2019-11-07 01:17:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:17:52.936: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-9003" for this suite.
Nov  7 01:17:59.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:18:07.966: INFO: namespace deployment-9003 deletion completed in 14.935568531s

• [SLOW TEST:28.131 seconds]
[sig-apps] Deployment
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:18:07.968: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Nov  7 01:18:08.342: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig api-versions'
Nov  7 01:18:09.252: INFO: stderr: ""
Nov  7 01:18:09.252: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhealthchecking.openshift.io/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmanaged.openshift.io/v1alpha1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:18:09.253: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3001" for this suite.
Nov  7 01:18:15.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:18:24.459: INFO: namespace kubectl-3001 deletion completed in 14.931547502s

• [SLOW TEST:16.491 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:18:24.459: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov  7 01:18:25.208: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-725,SelfLink:/api/v1/namespaces/watch-725/configmaps/e2e-watch-test-watch-closed,UID:69ebdb05-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:82065,Generation:0,CreationTimestamp:2019-11-07 01:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov  7 01:18:25.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-725,SelfLink:/api/v1/namespaces/watch-725/configmaps/e2e-watch-test-watch-closed,UID:69ebdb05-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:82066,Generation:0,CreationTimestamp:2019-11-07 01:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov  7 01:18:25.582: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-725,SelfLink:/api/v1/namespaces/watch-725/configmaps/e2e-watch-test-watch-closed,UID:69ebdb05-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:82068,Generation:0,CreationTimestamp:2019-11-07 01:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov  7 01:18:25.582: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-725,SelfLink:/api/v1/namespaces/watch-725/configmaps/e2e-watch-test-watch-closed,UID:69ebdb05-00fc-11ea-98c4-0655ae4e8e56,ResourceVersion:82071,Generation:0,CreationTimestamp:2019-11-07 01:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:18:25.583: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-725" for this suite.
Nov  7 01:18:31.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:18:40.604: INFO: namespace watch-725 deletion completed in 14.927570625s

• [SLOW TEST:16.145 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:18:40.606: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-887bc0cc-00fc-11ea-aaa6-525400524259
STEP: Creating secret with name secret-projected-all-test-volume-887bc0b5-00fc-11ea-aaa6-525400524259
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov  7 01:18:41.270: INFO: Waiting up to 5m0s for pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259" in namespace "projected-6772" to be "success or failure"
Nov  7 01:18:41.368: INFO: Pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 98.061399ms
Nov  7 01:18:43.463: INFO: Pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.192976799s
Nov  7 01:18:45.557: INFO: Pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2868797s
Nov  7 01:18:47.651: INFO: Pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.380525672s
Nov  7 01:18:49.744: INFO: Pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.474259105s
STEP: Saw pod success
Nov  7 01:18:49.745: INFO: Pod "projected-volume-887bc08b-00fc-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:18:49.838: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod projected-volume-887bc08b-00fc-11ea-aaa6-525400524259 container projected-all-volume-test: <nil>
STEP: delete the pod
Nov  7 01:18:50.031: INFO: Waiting for pod projected-volume-887bc08b-00fc-11ea-aaa6-525400524259 to disappear
Nov  7 01:18:50.123: INFO: Pod projected-volume-887bc08b-00fc-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected combined
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:18:50.123: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6772" for this suite.
Nov  7 01:18:56.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:19:05.340: INFO: namespace projected-6772 deletion completed in 14.942350378s

• [SLOW TEST:24.735 seconds]
[sig-storage] Projected combined
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:19:05.342: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5627
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  7 01:19:05.716: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Nov  7 01:19:48.008: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5627 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:19:48.008: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:19:49.721: INFO: Found all expected endpoints: [netserver-0]
Nov  7 01:19:49.814: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.130.2.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5627 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:19:49.814: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:19:51.528: INFO: Found all expected endpoints: [netserver-1]
Nov  7 01:19:51.622: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.24 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5627 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:19:51.622: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:19:53.324: INFO: Found all expected endpoints: [netserver-2]
Nov  7 01:19:53.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.28 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5627 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:19:53.417: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:19:55.111: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:19:55.111: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-5627" for this suite.
Nov  7 01:20:01.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:20:10.324: INFO: namespace pod-network-test-5627 deletion completed in 14.936303989s

• [SLOW TEST:64.983 seconds]
[sig-network] Networking
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:20:10.325: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Nov  7 01:20:10.800: INFO: Waiting up to 5m0s for pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259" in namespace "containers-7031" to be "success or failure"
Nov  7 01:20:10.893: INFO: Pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.685852ms
Nov  7 01:20:12.986: INFO: Pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186004425s
Nov  7 01:20:15.079: INFO: Pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279491634s
Nov  7 01:20:17.173: INFO: Pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373600516s
Nov  7 01:20:19.268: INFO: Pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.46786894s
STEP: Saw pod success
Nov  7 01:20:19.268: INFO: Pod "client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:20:19.361: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 01:20:19.555: INFO: Waiting for pod client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259 to disappear
Nov  7 01:20:19.648: INFO: Pod client-containers-bdf5ec00-00fc-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:20:19.648: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-7031" for this suite.
Nov  7 01:20:26.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:20:34.866: INFO: namespace containers-7031 deletion completed in 14.942249339s

• [SLOW TEST:24.541 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:20:34.867: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2905
Nov  7 01:20:45.532: INFO: Started pod liveness-exec in namespace container-probe-2905
STEP: checking the pod's current state and verifying that restartCount is present
Nov  7 01:20:45.626: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:24:46.542: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-2905" for this suite.
Nov  7 01:24:53.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:25:01.650: INFO: namespace container-probe-2905 deletion completed in 14.835060572s

• [SLOW TEST:266.783 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:25:01.653: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov  7 01:25:13.202: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6b9aa15f-00fd-11ea-aaa6-525400524259"
Nov  7 01:25:13.202: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6b9aa15f-00fd-11ea-aaa6-525400524259" in namespace "pods-6358" to be "terminated due to deadline exceeded"
Nov  7 01:25:13.293: INFO: Pod "pod-update-activedeadlineseconds-6b9aa15f-00fd-11ea-aaa6-525400524259": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 91.719809ms
Nov  7 01:25:13.293: INFO: Pod "pod-update-activedeadlineseconds-6b9aa15f-00fd-11ea-aaa6-525400524259" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:25:13.294: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-6358" for this suite.
Nov  7 01:25:19.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:25:28.435: INFO: namespace pods-6358 deletion completed in 14.870782093s

• [SLOW TEST:26.782 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:25:28.436: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:25:29.449: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov  7 01:25:29.637: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:29.637: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:29.637: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:29.729: INFO: Number of nodes with available pods: 0
Nov  7 01:25:29.729: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:31.002: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:31.002: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:31.002: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:31.094: INFO: Number of nodes with available pods: 0
Nov  7 01:25:31.094: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:32.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:32.001: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:32.001: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:32.093: INFO: Number of nodes with available pods: 0
Nov  7 01:25:32.093: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:33.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:33.001: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:33.001: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:33.094: INFO: Number of nodes with available pods: 0
Nov  7 01:25:33.094: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:34.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:34.001: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:34.001: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:34.093: INFO: Number of nodes with available pods: 0
Nov  7 01:25:34.093: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:35.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:35.001: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:35.001: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:35.093: INFO: Number of nodes with available pods: 0
Nov  7 01:25:35.093: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:36.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:36.001: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:36.001: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:36.094: INFO: Number of nodes with available pods: 0
Nov  7 01:25:36.094: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:37.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:37.002: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:37.002: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:37.094: INFO: Number of nodes with available pods: 0
Nov  7 01:25:37.094: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:38.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:38.002: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:38.002: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:38.096: INFO: Number of nodes with available pods: 2
Nov  7 01:25:38.096: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:25:39.001: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:39.001: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:39.001: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:39.093: INFO: Number of nodes with available pods: 4
Nov  7 01:25:39.093: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov  7 01:25:39.742: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:39.742: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:39.742: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:39.742: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:39.836: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:39.836: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:39.836: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:40.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:40.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:40.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:40.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:41.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:41.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:41.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:41.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:41.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:41.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:41.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:41.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:42.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:42.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:42.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:42.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:42.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:42.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:42.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:42.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:43.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:43.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:43.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:43.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:43.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:43.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:43.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:43.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:44.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:44.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:44.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:44.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:44.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:44.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:44.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:44.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:45.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:45.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:45.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:45.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:45.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:45.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:45.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:45.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:46.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:46.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:46.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:46.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:46.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:46.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:46.930: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:46.930: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:47.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:47.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:47.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:47.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:47.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:47.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:47.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:47.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:48.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:48.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:48.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:48.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:48.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:48.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:48.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:48.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:49.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:49.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:49.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:49.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:49.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:49.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:49.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:49.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:50.113: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:50.113: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:50.113: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:50.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:50.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:50.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:50.929: INFO: Wrong image for pod: daemon-set-zvkj5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:50.929: INFO: Pod daemon-set-zvkj5 is not available
Nov  7 01:25:51.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:51.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:51.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:51.930: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:51.930: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:51.930: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:51.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:52.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:52.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:52.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:52.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:52.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:52.929: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:52.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:53.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:53.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:53.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:53.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:53.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:53.929: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:53.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:54.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:54.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:54.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:54.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:54.930: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:54.930: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:54.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:55.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:55.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:55.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:55.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:55.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:55.929: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:55.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:56.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:56.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:56.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:56.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:56.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:56.929: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:56.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:57.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:57.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:57.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:57.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:57.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:57.929: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:57.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:58.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:58.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:58.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:58.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:58.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:58.929: INFO: Pod daemon-set-km77x is not available
Nov  7 01:25:58.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:59.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:25:59.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:25:59.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:25:59.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:59.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:25:59.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:00.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:00.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:00.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:00.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:00.929: INFO: Wrong image for pod: daemon-set-bdbfx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:00.929: INFO: Pod daemon-set-bdbfx is not available
Nov  7 01:26:00.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:01.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:01.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:01.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:01.930: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:01.930: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:01.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:02.114: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:02.114: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:02.114: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:02.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:02.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:02.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:03.114: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:03.115: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:03.115: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:03.930: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:03.930: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:03.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:04.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:04.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:04.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:04.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:04.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:04.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:05.110: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:05.110: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:05.110: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:05.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:05.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:05.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:06.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:06.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:06.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:06.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:06.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:06.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:07.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:07.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:07.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:07.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:07.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:07.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:08.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:08.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:08.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:08.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:08.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:08.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:09.110: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:09.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:09.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:09.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:09.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:09.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:10.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:10.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:10.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:10.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:10.929: INFO: Pod daemon-set-q2lgw is not available
Nov  7 01:26:10.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:11.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:11.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:11.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:11.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:11.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:12.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:12.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:12.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:12.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:12.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:12.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:13.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:13.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:13.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:13.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:13.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:13.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:14.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:14.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:14.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:14.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:14.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:14.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:15.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:15.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:15.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:15.930: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:15.930: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:15.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:16.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:16.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:16.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:16.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:16.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:16.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:17.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:17.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:17.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:17.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:17.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:17.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:18.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:18.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:18.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:18.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:18.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:18.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:19.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:19.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:19.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:19.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:19.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:19.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:20.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:20.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:20.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:20.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:20.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:20.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:21.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:21.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:21.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:21.929: INFO: Wrong image for pod: daemon-set-5xsps. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:21.929: INFO: Pod daemon-set-5xsps is not available
Nov  7 01:26:21.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:22.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:22.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:22.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:22.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:22.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:23.117: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:23.117: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:23.117: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:23.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:23.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:24.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:24.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:24.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:24.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:24.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:25.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:25.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:25.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:25.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:25.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:26.110: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:26.110: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:26.110: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:26.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:26.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:27.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:27.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:27.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:27.930: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:27.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:28.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:28.113: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:28.113: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:28.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:28.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:29.110: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:29.110: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:29.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:29.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:29.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:30.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:30.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:30.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:30.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:30.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:31.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:31.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:31.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:31.929: INFO: Pod daemon-set-sr8dq is not available
Nov  7 01:26:31.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:32.110: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:32.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:32.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:32.929: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:33.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:33.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:33.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:33.930: INFO: Wrong image for pod: daemon-set-xx4qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov  7 01:26:33.930: INFO: Pod daemon-set-xx4qt is not available
Nov  7 01:26:34.112: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:34.112: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:34.112: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:34.929: INFO: Pod daemon-set-bz4nd is not available
Nov  7 01:26:35.111: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:35.111: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:35.111: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Nov  7 01:26:35.204: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:35.204: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:35.204: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:35.297: INFO: Number of nodes with available pods: 3
Nov  7 01:26:35.297: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:36.568: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:36.568: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:36.568: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:36.660: INFO: Number of nodes with available pods: 3
Nov  7 01:26:36.660: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:37.568: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:37.568: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:37.568: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:37.660: INFO: Number of nodes with available pods: 3
Nov  7 01:26:37.660: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:38.568: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:38.568: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:38.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:38.660: INFO: Number of nodes with available pods: 3
Nov  7 01:26:38.660: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:39.568: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:39.569: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:39.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:39.661: INFO: Number of nodes with available pods: 3
Nov  7 01:26:39.661: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:40.569: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:40.569: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:40.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:40.661: INFO: Number of nodes with available pods: 3
Nov  7 01:26:40.661: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:41.568: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:41.568: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:41.568: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:41.660: INFO: Number of nodes with available pods: 3
Nov  7 01:26:41.660: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:42.568: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:42.569: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:42.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:42.661: INFO: Number of nodes with available pods: 3
Nov  7 01:26:42.661: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:43.569: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:43.569: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:43.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:43.661: INFO: Number of nodes with available pods: 3
Nov  7 01:26:43.661: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:44.569: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:44.569: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:44.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:44.661: INFO: Number of nodes with available pods: 3
Nov  7 01:26:44.661: INFO: Node ip-10-0-143-165.us-west-2.compute.internal is running more than one daemon pod
Nov  7 01:26:45.569: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 01:26:45.569: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 01:26:45.569: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 01:26:45.661: INFO: Number of nodes with available pods: 4
Nov  7 01:26:45.661: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3110, will wait for the garbage collector to delete the pods
Nov  7 01:26:46.409: INFO: Deleting DaemonSet.extensions daemon-set took: 94.81617ms
Nov  7 01:26:46.509: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.325251ms
Nov  7 01:26:57.801: INFO: Number of nodes with available pods: 0
Nov  7 01:26:57.801: INFO: Number of running nodes: 0, number of available pods: 0
Nov  7 01:26:57.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3110/daemonsets","resourceVersion":"85399"},"items":null}

Nov  7 01:26:57.984: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3110/pods","resourceVersion":"85400"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:26:58.533: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-3110" for this suite.
Nov  7 01:27:04.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:27:13.570: INFO: namespace daemonsets-3110 deletion completed in 14.855642153s

• [SLOW TEST:105.135 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:27:13.571: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov  7 01:27:14.591: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5164,SelfLink:/api/v1/namespaces/watch-5164/configmaps/e2e-watch-test-resource-version,UID:a53d4cc9-00fd-11ea-98c4-0655ae4e8e56,ResourceVersion:85605,Generation:0,CreationTimestamp:2019-11-07 01:26:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov  7 01:27:14.592: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5164,SelfLink:/api/v1/namespaces/watch-5164/configmaps/e2e-watch-test-resource-version,UID:a53d4cc9-00fd-11ea-98c4-0655ae4e8e56,ResourceVersion:85606,Generation:0,CreationTimestamp:2019-11-07 01:26:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:27:14.592: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-5164" for this suite.
Nov  7 01:27:20.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:27:29.537: INFO: namespace watch-5164 deletion completed in 14.852397488s

• [SLOW TEST:15.967 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:27:29.539: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov  7 01:27:52.760: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:27:52.851: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:27:54.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:27:54.944: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:27:56.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:27:56.943: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:27:58.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:27:58.944: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:00.853: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:00.945: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:02.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:02.944: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:04.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:04.944: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:06.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:06.943: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:08.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:08.944: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:10.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:10.944: INFO: Pod pod-with-prestop-exec-hook still exists
Nov  7 01:28:12.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov  7 01:28:12.943: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:28:13.039: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5972" for this suite.
Nov  7 01:28:41.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:28:50.172: INFO: namespace container-lifecycle-hook-5972 deletion completed in 36.861452234s

• [SLOW TEST:80.633 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:28:50.174: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7172
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7172
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7172
Nov  7 01:28:50.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 01:29:01.011: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov  7 01:29:01.103: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 01:29:02.306: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 01:29:02.306: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 01:29:02.306: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 01:29:02.398: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov  7 01:29:12.492: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 01:29:12.492: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 01:29:12.862: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999743s
Nov  7 01:29:13.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.907308264s
Nov  7 01:29:15.049: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.812843513s
Nov  7 01:29:16.142: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.720296101s
Nov  7 01:29:17.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.627042622s
Nov  7 01:29:18.330: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.53462652s
Nov  7 01:29:19.423: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.439668061s
Nov  7 01:29:20.516: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.34654488s
Nov  7 01:29:21.610: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.253319607s
Nov  7 01:29:22.702: INFO: Verifying statefulset ss doesn't scale past 1 for another 159.68106ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7172
Nov  7 01:29:23.795: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:29:24.948: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov  7 01:29:24.949: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 01:29:24.949: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 01:29:25.041: INFO: Found 1 stateful pods, waiting for 3
Nov  7 01:29:35.135: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:29:35.135: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:29:35.135: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 01:29:45.134: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:29:45.135: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:29:45.135: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov  7 01:29:45.318: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 01:29:46.470: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 01:29:46.470: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 01:29:46.470: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 01:29:46.470: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 01:29:47.645: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 01:29:47.645: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 01:29:47.645: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 01:29:47.645: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 01:29:48.835: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 01:29:48.835: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 01:29:48.835: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 01:29:48.835: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 01:29:48.927: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov  7 01:29:59.112: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 01:29:59.112: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 01:29:59.112: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 01:29:59.391: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999505s
Nov  7 01:30:00.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.907255633s
Nov  7 01:30:01.577: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.813930606s
Nov  7 01:30:02.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.721302192s
Nov  7 01:30:03.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.628256906s
Nov  7 01:30:04.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.535506122s
Nov  7 01:30:05.948: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.44331827s
Nov  7 01:30:07.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.349859562s
Nov  7 01:30:08.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.257519713s
Nov  7 01:30:09.227: INFO: Verifying statefulset ss doesn't scale past 3 for another 164.73548ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7172
Nov  7 01:30:10.319: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:11.465: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov  7 01:30:11.465: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 01:30:11.465: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 01:30:11.465: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:12.625: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov  7 01:30:12.625: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 01:30:12.625: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 01:30:12.626: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:13.503: INFO: rc: 1
Nov  7 01:30:13.503: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001faa270 exit status 1 <nil> <nil> true [0xc00281c000 0xc00281c018 0xc00281c030] [0xc00281c000 0xc00281c018 0xc00281c030] [0xc00281c010 0xc00281c028] [0xb91410 0xb91410] 0xc001c84540 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Nov  7 01:30:23.504: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:23.949: INFO: rc: 1
Nov  7 01:30:23.950: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a83ce0 exit status 1 <nil> <nil> true [0xc0005bd2c8 0xc0005bd348 0xc0005bd388] [0xc0005bd2c8 0xc0005bd348 0xc0005bd388] [0xc0005bd330 0xc0005bd370] [0xb91410 0xb91410] 0xc0012b65a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:30:33.950: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:34.402: INFO: rc: 1
Nov  7 01:30:34.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002479800 exit status 1 <nil> <nil> true [0xc000e7c2b0 0xc000e7c2c8 0xc000e7c2e0] [0xc000e7c2b0 0xc000e7c2c8 0xc000e7c2e0] [0xc000e7c2c0 0xc000e7c2d8] [0xb91410 0xb91410] 0xc002735620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:30:44.403: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:44.833: INFO: rc: 1
Nov  7 01:30:44.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002479a10 exit status 1 <nil> <nil> true [0xc000e7c2e8 0xc000e7c300 0xc000e7c318] [0xc000e7c2e8 0xc000e7c300 0xc000e7c318] [0xc000e7c2f8 0xc000e7c310] [0xb91410 0xb91410] 0xc002d30060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:30:54.833: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:30:55.262: INFO: rc: 1
Nov  7 01:30:55.262: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002479c50 exit status 1 <nil> <nil> true [0xc000e7c320 0xc000e7c338 0xc000e7c350] [0xc000e7c320 0xc000e7c338 0xc000e7c350] [0xc000e7c330 0xc000e7c348] [0xb91410 0xb91410] 0xc002d30480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:31:05.264: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:31:05.711: INFO: rc: 1
Nov  7 01:31:05.711: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001faa4b0 exit status 1 <nil> <nil> true [0xc00281c038 0xc00281c050 0xc00281c068] [0xc00281c038 0xc00281c050 0xc00281c068] [0xc00281c048 0xc00281c060] [0xb91410 0xb91410] 0xc001c85380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:31:15.713: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:31:16.176: INFO: rc: 1
Nov  7 01:31:16.176: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027e81b0 exit status 1 <nil> <nil> true [0xc00281c008 0xc00281c020 0xc00281c038] [0xc00281c008 0xc00281c020 0xc00281c038] [0xc00281c018 0xc00281c030] [0xb91410 0xb91410] 0xc002734900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:31:26.177: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:31:26.620: INFO: rc: 1
Nov  7 01:31:26.621: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8330 exit status 1 <nil> <nil> true [0xc000e7c000 0xc000e7c018 0xc000e7c030] [0xc000e7c000 0xc000e7c018 0xc000e7c030] [0xc000e7c010 0xc000e7c028] [0xb91410 0xb91410] 0xc00290ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:31:36.621: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:31:37.052: INFO: rc: 1
Nov  7 01:31:37.053: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027e83c0 exit status 1 <nil> <nil> true [0xc00281c040 0xc00281c058 0xc00281c070] [0xc00281c040 0xc00281c058 0xc00281c070] [0xc00281c050 0xc00281c068] [0xb91410 0xb91410] 0xc002734ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:31:47.055: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:31:47.526: INFO: rc: 1
Nov  7 01:31:47.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8510 exit status 1 <nil> <nil> true [0xc000e7c038 0xc000e7c050 0xc000e7c068] [0xc000e7c038 0xc000e7c050 0xc000e7c068] [0xc000e7c048 0xc000e7c060] [0xb91410 0xb91410] 0xc00290f080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:31:57.528: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:31:57.988: INFO: rc: 1
Nov  7 01:31:57.988: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa2d0 exit status 1 <nil> <nil> true [0xc001dd6000 0xc001dd6018 0xc001dd6030] [0xc001dd6000 0xc001dd6018 0xc001dd6030] [0xc001dd6010 0xc001dd6028] [0xb91410 0xb91410] 0xc00270d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:32:07.989: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:32:08.420: INFO: rc: 1
Nov  7 01:32:08.420: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa510 exit status 1 <nil> <nil> true [0xc001dd6038 0xc001dd6050 0xc001dd6068] [0xc001dd6038 0xc001dd6050 0xc001dd6068] [0xc001dd6048 0xc001dd6060] [0xb91410 0xb91410] 0xc0037c8060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:32:18.421: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:32:18.887: INFO: rc: 1
Nov  7 01:32:18.887: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e88a0 exit status 1 <nil> <nil> true [0xc000e7c070 0xc000e7c088 0xc000e7c0a0] [0xc000e7c070 0xc000e7c088 0xc000e7c0a0] [0xc000e7c080 0xc000e7c098] [0xb91410 0xb91410] 0xc00290f560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:32:28.887: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:32:29.307: INFO: rc: 1
Nov  7 01:32:29.307: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8ae0 exit status 1 <nil> <nil> true [0xc000e7c0a8 0xc000e7c0c0 0xc000e7c0d8] [0xc000e7c0a8 0xc000e7c0c0 0xc000e7c0d8] [0xc000e7c0b8 0xc000e7c0d0] [0xb91410 0xb91410] 0xc0038e0120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:32:39.307: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:32:39.733: INFO: rc: 1
Nov  7 01:32:39.733: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa750 exit status 1 <nil> <nil> true [0xc001dd6070 0xc001dd6088 0xc001dd60a0] [0xc001dd6070 0xc001dd6088 0xc001dd60a0] [0xc001dd6080 0xc001dd6098] [0xb91410 0xb91410] 0xc0037c8360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:32:49.734: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:32:50.182: INFO: rc: 1
Nov  7 01:32:50.183: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025863c0 exit status 1 <nil> <nil> true [0xc0005bc040 0xc0005bc0b8 0xc0005bc150] [0xc0005bc040 0xc0005bc0b8 0xc0005bc150] [0xc0005bc0b0 0xc0005bc0e8] [0xb91410 0xb91410] 0xc001c844e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:33:00.183: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:33:00.659: INFO: rc: 1
Nov  7 01:33:00.659: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa930 exit status 1 <nil> <nil> true [0xc001dd60a8 0xc001dd60c0 0xc001dd60d8] [0xc001dd60a8 0xc001dd60c0 0xc001dd60d8] [0xc001dd60b8 0xc001dd60d0] [0xb91410 0xb91410] 0xc0037c8660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:33:10.659: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:33:11.099: INFO: rc: 1
Nov  7 01:33:11.099: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025865d0 exit status 1 <nil> <nil> true [0xc0005bc168 0xc0005bc188 0xc0005bc218] [0xc0005bc168 0xc0005bc188 0xc0005bc218] [0xc0005bc180 0xc0005bc1a8] [0xb91410 0xb91410] 0xc001c852c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:33:21.100: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:33:21.559: INFO: rc: 1
Nov  7 01:33:21.560: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025863f0 exit status 1 <nil> <nil> true [0xc0005bc070 0xc0005bc0d0 0xc0005bc228] [0xc0005bc070 0xc0005bc0d0 0xc0005bc228] [0xc0005bc0b8 0xc0005bc150] [0xb91410 0xb91410] 0xc00270d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:33:31.560: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:33:32.006: INFO: rc: 1
Nov  7 01:33:32.006: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa300 exit status 1 <nil> <nil> true [0xc001dd6000 0xc001dd6018 0xc001dd6030] [0xc001dd6000 0xc001dd6018 0xc001dd6030] [0xc001dd6010 0xc001dd6028] [0xb91410 0xb91410] 0xc00290ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:33:42.006: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:33:42.430: INFO: rc: 1
Nov  7 01:33:42.430: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa540 exit status 1 <nil> <nil> true [0xc001dd6038 0xc001dd6050 0xc001dd6068] [0xc001dd6038 0xc001dd6050 0xc001dd6068] [0xc001dd6048 0xc001dd6060] [0xb91410 0xb91410] 0xc00290f080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:33:52.430: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:33:52.877: INFO: rc: 1
Nov  7 01:33:52.877: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8360 exit status 1 <nil> <nil> true [0xc00281c000 0xc00281c018 0xc00281c030] [0xc00281c000 0xc00281c018 0xc00281c030] [0xc00281c010 0xc00281c028] [0xb91410 0xb91410] 0xc001c84360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:34:02.877: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:34:03.320: INFO: rc: 1
Nov  7 01:34:03.320: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa720 exit status 1 <nil> <nil> true [0xc001dd6070 0xc001dd6088 0xc001dd60a0] [0xc001dd6070 0xc001dd6088 0xc001dd60a0] [0xc001dd6080 0xc001dd6098] [0xb91410 0xb91410] 0xc00290f560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:34:13.321: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:34:13.766: INFO: rc: 1
Nov  7 01:34:13.767: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8570 exit status 1 <nil> <nil> true [0xc00281c038 0xc00281c050 0xc00281c068] [0xc00281c038 0xc00281c050 0xc00281c068] [0xc00281c048 0xc00281c060] [0xb91410 0xb91410] 0xc001c85500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:34:23.768: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:34:24.223: INFO: rc: 1
Nov  7 01:34:24.223: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027e82a0 exit status 1 <nil> <nil> true [0xc000e7c000 0xc000e7c018 0xc000e7c030] [0xc000e7c000 0xc000e7c018 0xc000e7c030] [0xc000e7c010 0xc000e7c028] [0xb91410 0xb91410] 0xc0037c8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:34:34.223: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:34:34.666: INFO: rc: 1
Nov  7 01:34:34.666: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0027e84b0 exit status 1 <nil> <nil> true [0xc000e7c038 0xc000e7c050 0xc000e7c068] [0xc000e7c038 0xc000e7c050 0xc000e7c068] [0xc000e7c048 0xc000e7c060] [0xb91410 0xb91410] 0xc0037c8540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:34:44.667: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:34:45.113: INFO: rc: 1
Nov  7 01:34:45.113: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8930 exit status 1 <nil> <nil> true [0xc00281c070 0xc00281c088 0xc00281c0a0] [0xc00281c070 0xc00281c088 0xc00281c0a0] [0xc00281c080 0xc00281c098] [0xb91410 0xb91410] 0xc001c85800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:34:55.114: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:34:55.539: INFO: rc: 1
Nov  7 01:34:55.539: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038e8ba0 exit status 1 <nil> <nil> true [0xc00281c0a8 0xc00281c0c0 0xc00281c0d8] [0xc00281c0a8 0xc00281c0c0 0xc00281c0d8] [0xc00281c0b8 0xc00281c0d0] [0xb91410 0xb91410] 0xc0027344e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:35:05.539: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:35:05.967: INFO: rc: 1
Nov  7 01:35:05.967: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl [kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0035aa9c0 exit status 1 <nil> <nil> true [0xc001dd60a8 0xc001dd60c0 0xc001dd60d8] [0xc001dd60a8 0xc001dd60c0 0xc001dd60d8] [0xc001dd60b8 0xc001dd60d0] [0xb91410 0xb91410] 0xc0038e0120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Nov  7 01:35:15.967: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7172 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 01:35:16.410: INFO: rc: 1
Nov  7 01:35:16.410: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Nov  7 01:35:16.410: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Nov  7 01:35:16.691: INFO: Deleting all statefulset in ns statefulset-7172
Nov  7 01:35:16.783: INFO: Scaling statefulset ss to 0
Nov  7 01:35:17.060: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 01:35:17.152: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:35:17.431: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-7172" for this suite.
Nov  7 01:35:23.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:35:32.490: INFO: namespace statefulset-7172 deletion completed in 14.875959907s

• [SLOW TEST:402.317 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:35:32.492: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Nov  7 01:35:32.867: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:35:43.516: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-6380" for this suite.
Nov  7 01:35:50.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:35:58.643: INFO: namespace init-container-6380 deletion completed in 14.853973263s

• [SLOW TEST:26.152 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:35:58.645: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 01:35:59.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259" in namespace "projected-5088" to be "success or failure"
Nov  7 01:35:59.209: INFO: Pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.063245ms
Nov  7 01:36:01.302: INFO: Pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185664898s
Nov  7 01:36:03.395: INFO: Pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27859251s
Nov  7 01:36:05.488: INFO: Pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371496594s
Nov  7 01:36:07.581: INFO: Pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.464510255s
STEP: Saw pod success
Nov  7 01:36:07.581: INFO: Pod "downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:36:07.674: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 01:36:07.868: INFO: Waiting for pod downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259 to disappear
Nov  7 01:36:07.959: INFO: Pod downwardapi-volume-f3339412-00fe-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:36:07.960: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5088" for this suite.
Nov  7 01:36:14.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:36:23.083: INFO: namespace projected-5088 deletion completed in 14.850079045s

• [SLOW TEST:24.440 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:36:23.084: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Nov  7 01:36:23.459: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig --namespace=kubectl-999 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Nov  7 01:36:36.533: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Nov  7 01:36:36.533: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:36:38.717: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-999" for this suite.
Nov  7 01:36:45.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:36:53.810: INFO: namespace kubectl-999 deletion completed in 14.821991899s

• [SLOW TEST:30.726 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:36:53.812: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-5p8j
STEP: Creating a pod to test atomic-volume-subpath
Nov  7 01:36:54.470: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5p8j" in namespace "subpath-8880" to be "success or failure"
Nov  7 01:36:54.562: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Pending", Reason="", readiness=false. Elapsed: 91.873058ms
Nov  7 01:36:56.656: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184958507s
Nov  7 01:36:58.750: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279887326s
Nov  7 01:37:00.843: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372641939s
Nov  7 01:37:02.936: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.465050688s
Nov  7 01:37:05.028: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 10.557222133s
Nov  7 01:37:07.120: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 12.649316934s
Nov  7 01:37:09.212: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 14.741666973s
Nov  7 01:37:11.305: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 16.833964377s
Nov  7 01:37:13.397: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 18.926347316s
Nov  7 01:37:15.489: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 21.018004761s
Nov  7 01:37:17.581: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 23.110540409s
Nov  7 01:37:19.673: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 25.202820328s
Nov  7 01:37:21.765: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Running", Reason="", readiness=true. Elapsed: 27.294766429s
Nov  7 01:37:23.858: INFO: Pod "pod-subpath-test-configmap-5p8j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.387082472s
STEP: Saw pod success
Nov  7 01:37:23.858: INFO: Pod "pod-subpath-test-configmap-5p8j" satisfied condition "success or failure"
Nov  7 01:37:23.950: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-subpath-test-configmap-5p8j container test-container-subpath-configmap-5p8j: <nil>
STEP: delete the pod
Nov  7 01:37:24.142: INFO: Waiting for pod pod-subpath-test-configmap-5p8j to disappear
Nov  7 01:37:24.233: INFO: Pod pod-subpath-test-configmap-5p8j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5p8j
Nov  7 01:37:24.234: INFO: Deleting pod "pod-subpath-test-configmap-5p8j" in namespace "subpath-8880"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:37:24.325: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-8880" for this suite.
Nov  7 01:37:30.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:37:39.412: INFO: namespace subpath-8880 deletion completed in 14.814946214s

• [SLOW TEST:45.600 seconds]
[sig-storage] Subpath
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:37:39.412: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Nov  7 01:37:39.781: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-3051'
Nov  7 01:37:41.406: INFO: stderr: ""
Nov  7 01:37:41.407: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov  7 01:37:42.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:42.499: INFO: Found 0 / 1
Nov  7 01:37:43.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:43.499: INFO: Found 0 / 1
Nov  7 01:37:44.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:44.499: INFO: Found 0 / 1
Nov  7 01:37:45.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:45.499: INFO: Found 0 / 1
Nov  7 01:37:46.500: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:46.500: INFO: Found 0 / 1
Nov  7 01:37:47.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:47.499: INFO: Found 0 / 1
Nov  7 01:37:48.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:48.499: INFO: Found 0 / 1
Nov  7 01:37:49.499: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:49.499: INFO: Found 0 / 1
Nov  7 01:37:50.500: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:50.500: INFO: Found 1 / 1
Nov  7 01:37:50.500: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov  7 01:37:50.591: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:50.591: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov  7 01:37:50.591: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig patch pod redis-master-v8rv2 --namespace=kubectl-3051 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov  7 01:37:51.128: INFO: stderr: ""
Nov  7 01:37:51.128: INFO: stdout: "pod/redis-master-v8rv2 patched\n"
STEP: checking annotations
Nov  7 01:37:51.221: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 01:37:51.221: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:37:51.221: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3051" for this suite.
Nov  7 01:38:03.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:38:12.321: INFO: namespace kubectl-3051 deletion completed in 20.829203231s

• [SLOW TEST:32.909 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:38:12.322: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 01:38:12.792: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259" in namespace "downward-api-8990" to be "success or failure"
Nov  7 01:38:12.883: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.663106ms
Nov  7 01:38:14.976: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184181733s
Nov  7 01:38:17.069: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27715077s
Nov  7 01:38:19.161: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369478443s
Nov  7 01:38:21.254: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462579554s
Nov  7 01:38:23.347: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.555321424s
STEP: Saw pod success
Nov  7 01:38:23.347: INFO: Pod "downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:38:23.439: INFO: Trying to get logs from node ip-10-0-136-94.us-west-2.compute.internal pod downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 01:38:23.631: INFO: Waiting for pod downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259 to disappear
Nov  7 01:38:23.723: INFO: Pod downwardapi-volume-42e0f073-00ff-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:38:23.723: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8990" for this suite.
Nov  7 01:38:30.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:38:38.824: INFO: namespace downward-api-8990 deletion completed in 14.829800154s

• [SLOW TEST:26.502 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:38:38.826: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 01:38:39.193: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1328'
Nov  7 01:38:39.654: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov  7 01:38:39.654: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Nov  7 01:38:39.837: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-p7bgx]
Nov  7 01:38:39.838: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-p7bgx" in namespace "kubectl-1328" to be "running and ready"
Nov  7 01:38:39.931: INFO: Pod "e2e-test-nginx-rc-p7bgx": Phase="Pending", Reason="", readiness=false. Elapsed: 92.352228ms
Nov  7 01:38:42.022: INFO: Pod "e2e-test-nginx-rc-p7bgx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184285092s
Nov  7 01:38:44.115: INFO: Pod "e2e-test-nginx-rc-p7bgx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276456284s
Nov  7 01:38:46.207: INFO: Pod "e2e-test-nginx-rc-p7bgx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368530752s
Nov  7 01:38:48.299: INFO: Pod "e2e-test-nginx-rc-p7bgx": Phase="Running", Reason="", readiness=true. Elapsed: 8.460846506s
Nov  7 01:38:48.299: INFO: Pod "e2e-test-nginx-rc-p7bgx" satisfied condition "running and ready"
Nov  7 01:38:48.299: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-p7bgx]
Nov  7 01:38:48.300: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig logs rc/e2e-test-nginx-rc --namespace=kubectl-1328'
Nov  7 01:38:48.943: INFO: stderr: ""
Nov  7 01:38:48.943: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Nov  7 01:38:48.943: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete rc e2e-test-nginx-rc --namespace=kubectl-1328'
Nov  7 01:38:49.465: INFO: stderr: ""
Nov  7 01:38:49.465: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:38:49.465: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1328" for this suite.
Nov  7 01:38:56.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:39:04.574: INFO: namespace kubectl-1328 deletion completed in 14.838060505s

• [SLOW TEST:25.749 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:39:04.575: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-6205fa3d-00ff-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 01:39:05.140: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259" in namespace "projected-2080" to be "success or failure"
Nov  7 01:39:05.231: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.590932ms
Nov  7 01:39:07.324: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184102868s
Nov  7 01:39:09.416: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276178761s
Nov  7 01:39:11.508: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368534857s
Nov  7 01:39:13.603: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463056173s
Nov  7 01:39:15.704: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.56410481s
STEP: Saw pod success
Nov  7 01:39:15.704: INFO: Pod "pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:39:15.796: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 01:39:15.989: INFO: Waiting for pod pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259 to disappear
Nov  7 01:39:16.080: INFO: Pod pod-projected-configmaps-6214313a-00ff-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:39:16.081: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2080" for this suite.
Nov  7 01:39:22.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:39:31.184: INFO: namespace projected-2080 deletion completed in 14.831756726s

• [SLOW TEST:26.609 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:39:31.186: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-71e2a64d-00ff-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 01:39:31.750: INFO: Waiting up to 5m0s for pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259" in namespace "configmap-6912" to be "success or failure"
Nov  7 01:39:31.842: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.258038ms
Nov  7 01:39:33.935: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185293061s
Nov  7 01:39:36.029: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27890485s
Nov  7 01:39:38.121: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371007671s
Nov  7 01:39:40.215: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464908637s
Nov  7 01:39:42.307: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.556851561s
STEP: Saw pod success
Nov  7 01:39:42.307: INFO: Pod "pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:39:42.398: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 01:39:42.592: INFO: Waiting for pod pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259 to disappear
Nov  7 01:39:42.683: INFO: Pod pod-configmaps-71f0e016-00ff-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:39:42.683: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6912" for this suite.
Nov  7 01:39:49.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:39:57.832: INFO: namespace configmap-6912 deletion completed in 14.877539038s

• [SLOW TEST:26.646 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:39:57.834: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 01:39:58.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259" in namespace "downward-api-6344" to be "success or failure"
Nov  7 01:39:58.399: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.534498ms
Nov  7 01:40:00.492: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184301306s
Nov  7 01:40:02.583: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276076762s
Nov  7 01:40:04.676: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.36907099s
Nov  7 01:40:06.768: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.4607898s
Nov  7 01:40:08.861: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.553609038s
STEP: Saw pod success
Nov  7 01:40:08.861: INFO: Pod "downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:40:08.953: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 01:40:09.144: INFO: Waiting for pod downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259 to disappear
Nov  7 01:40:09.236: INFO: Pod downwardapi-volume-81c518f0-00ff-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:40:09.237: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6344" for this suite.
Nov  7 01:40:15.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:40:24.331: INFO: namespace downward-api-6344 deletion completed in 14.823977494s

• [SLOW TEST:26.498 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:40:24.333: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov  7 01:40:24.703: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5695'
Nov  7 01:40:25.197: INFO: stderr: ""
Nov  7 01:40:25.197: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Nov  7 01:40:25.289: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete pods e2e-test-nginx-pod --namespace=kubectl-5695'
Nov  7 01:40:31.138: INFO: stderr: ""
Nov  7 01:40:31.138: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:40:31.138: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5695" for this suite.
Nov  7 01:40:37.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:40:46.232: INFO: namespace kubectl-5695 deletion completed in 14.823478775s

• [SLOW TEST:21.899 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:40:46.234: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:40:46.698: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-9eac81ee-00ff-11ea-aaa6-525400524259
STEP: Creating configMap with name cm-test-opt-upd-9eac8254-00ff-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9eac81ee-00ff-11ea-aaa6-525400524259
STEP: Updating configmap cm-test-opt-upd-9eac8254-00ff-11ea-aaa6-525400524259
STEP: Creating configMap with name cm-test-opt-create-9eac8264-00ff-11ea-aaa6-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:42:30.297: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5428" for this suite.
Nov  7 01:42:44.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:42:53.300: INFO: namespace projected-5428 deletion completed in 22.820141268s

• [SLOW TEST:127.066 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:42:53.301: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:43:04.141: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-9236" for this suite.
Nov  7 01:43:48.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:43:57.230: INFO: namespace kubelet-test-9236 deletion completed in 52.817998456s

• [SLOW TEST:63.930 seconds]
[k8s.io] Kubelet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:43:57.231: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6981
Nov  7 01:44:07.914: INFO: Started pod liveness-http in namespace container-probe-6981
STEP: checking the pod's current state and verifying that restartCount is present
Nov  7 01:44:08.009: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:48:08.723: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6981" for this suite.
Nov  7 01:48:15.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:48:23.809: INFO: namespace container-probe-6981 deletion completed in 14.815181585s

• [SLOW TEST:266.579 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:48:23.812: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1154
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Nov  7 01:48:24.464: INFO: Found 1 stateful pods, waiting for 3
Nov  7 01:48:34.558: INFO: Found 2 stateful pods, waiting for 3
Nov  7 01:48:44.556: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:48:44.556: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:48:44.556: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 01:48:54.558: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:48:54.558: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:48:54.558: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Nov  7 01:48:55.028: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov  7 01:48:55.408: INFO: Updating stateful set ss2
Nov  7 01:48:55.592: INFO: Waiting for Pod statefulset-1154/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Nov  7 01:49:06.083: INFO: Found 2 stateful pods, waiting for 3
Nov  7 01:49:16.177: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:49:16.177: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:49:16.177: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 01:49:26.177: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:49:26.177: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:49:26.177: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 01:49:36.178: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:49:36.178: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 01:49:36.178: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov  7 01:49:36.558: INFO: Updating stateful set ss2
Nov  7 01:49:36.744: INFO: Waiting for Pod statefulset-1154/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 01:49:47.125: INFO: Updating stateful set ss2
Nov  7 01:49:47.309: INFO: Waiting for StatefulSet statefulset-1154/ss2 to complete update
Nov  7 01:49:47.309: INFO: Waiting for Pod statefulset-1154/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 01:49:57.496: INFO: Waiting for StatefulSet statefulset-1154/ss2 to complete update
Nov  7 01:49:57.496: INFO: Waiting for Pod statefulset-1154/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 01:50:07.495: INFO: Waiting for StatefulSet statefulset-1154/ss2 to complete update
Nov  7 01:50:07.495: INFO: Waiting for Pod statefulset-1154/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 01:50:17.496: INFO: Waiting for StatefulSet statefulset-1154/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Nov  7 01:50:27.496: INFO: Deleting all statefulset in ns statefulset-1154
Nov  7 01:50:27.588: INFO: Scaling statefulset ss2 to 0
Nov  7 01:50:47.961: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 01:50:48.053: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:50:48.334: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-1154" for this suite.
Nov  7 01:50:54.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:51:03.412: INFO: namespace statefulset-1154 deletion completed in 14.894320322s

• [SLOW TEST:159.601 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:51:03.416: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:51:03.788: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov  7 01:51:03.974: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov  7 01:51:14.160: INFO: Creating deployment "test-rolling-update-deployment"
Nov  7 01:51:14.255: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov  7 01:51:14.440: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov  7 01:51:14.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:51:16.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:51:18.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:51:20.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:51:22.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688238, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 01:51:24.625: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Nov  7 01:51:24.904: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1217,SelfLink:/apis/apps/v1/namespaces/deployment-1217/deployments/test-rolling-update-deployment,UID:ffad341d-0100-11ea-98c4-0655ae4e8e56,ResourceVersion:94678,Generation:1,CreationTimestamp:2019-11-07 01:50:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-07 01:50:38 +0000 UTC 2019-11-07 01:50:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-07 01:50:47 +0000 UTC 2019-11-07 01:50:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov  7 01:51:24.996: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-1217,SelfLink:/apis/apps/v1/namespaces/deployment-1217/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:ffaf04b2-0100-11ea-98c4-0655ae4e8e56,ResourceVersion:94668,Generation:1,CreationTimestamp:2019-11-07 01:50:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ffad341d-0100-11ea-98c4-0655ae4e8e56 0xc002d16657 0xc002d16658}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov  7 01:51:24.996: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov  7 01:51:24.997: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1217,SelfLink:/apis/apps/v1/namespaces/deployment-1217/replicasets/test-rolling-update-controller,UID:f97e608a-0100-11ea-98c4-0655ae4e8e56,ResourceVersion:94677,Generation:2,CreationTimestamp:2019-11-07 01:50:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ffad341d-0100-11ea-98c4-0655ae4e8e56 0xc002d16587 0xc002d16588}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov  7 01:51:25.089: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-wdbqw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-wdbqw,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-1217,SelfLink:/api/v1/namespaces/deployment-1217/pods/test-rolling-update-deployment-57b6b5bb54-wdbqw,UID:ffb024ee-0100-11ea-98c4-0655ae4e8e56,ResourceVersion:94667,Generation:0,CreationTimestamp:2019-11-07 01:50:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.37"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 ffaf04b2-0100-11ea-98c4-0655ae4e8e56 0xc002d8c417 0xc002d8c418}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5c5g6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5c5g6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5c5g6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-lx9v2}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d8c480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d8c4a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:50:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:50:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:50:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 01:50:38 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:10.129.2.37,StartTime:2019-11-07 01:50:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-07 01:50:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://026a9b36517f9ef7486b42e0614e36b6d2061e9918c16b0adef2120fb73c5837}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:51:25.090: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-1217" for this suite.
Nov  7 01:51:31.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:51:40.209: INFO: namespace deployment-1217 deletion completed in 14.847541339s

• [SLOW TEST:36.793 seconds]
[sig-apps] Deployment
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:51:40.212: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4956
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  7 01:51:40.583: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Nov  7 01:52:14.857: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.27:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4956 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:14.857: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:15.623: INFO: Found all expected endpoints: [netserver-0]
Nov  7 01:52:15.715: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.130.2.134:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4956 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:15.715: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:16.483: INFO: Found all expected endpoints: [netserver-1]
Nov  7 01:52:16.577: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4956 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:16.577: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:17.319: INFO: Found all expected endpoints: [netserver-2]
Nov  7 01:52:17.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.38:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4956 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:17.411: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:18.148: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:52:18.150: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-4956" for this suite.
Nov  7 01:52:24.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:52:33.324: INFO: namespace pod-network-test-4956 deletion completed in 14.901956142s

• [SLOW TEST:53.113 seconds]
[sig-network] Networking
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:52:33.328: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov  7 01:52:46.453: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:46.453: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:47.160: INFO: Exec stderr: ""
Nov  7 01:52:47.160: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:47.160: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:47.893: INFO: Exec stderr: ""
Nov  7 01:52:47.893: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:47.893: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:48.638: INFO: Exec stderr: ""
Nov  7 01:52:48.638: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:48.638: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:49.339: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov  7 01:52:49.339: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:49.339: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:50.031: INFO: Exec stderr: ""
Nov  7 01:52:50.031: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:50.031: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:50.734: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov  7 01:52:50.734: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:50.734: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:51.467: INFO: Exec stderr: ""
Nov  7 01:52:51.467: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:51.467: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:52.190: INFO: Exec stderr: ""
Nov  7 01:52:52.190: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:52.190: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:52.878: INFO: Exec stderr: ""
Nov  7 01:52:52.878: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2622 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 01:52:52.878: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 01:52:53.570: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:52:53.570: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2622" for this suite.
Nov  7 01:53:42.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:53:50.685: INFO: namespace e2e-kubelet-etc-hosts-2622 deletion completed in 56.843855257s

• [SLOW TEST:77.358 seconds]
[k8s.io] KubeletManagedEtcHosts
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:53:50.685: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:53:51.149: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-723e3abe-0101-11ea-aaa6-525400524259
STEP: Creating secret with name s-test-opt-upd-723e3b2b-0101-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-723e3abe-0101-11ea-aaa6-525400524259
STEP: Updating secret s-test-opt-upd-723e3b2b-0101-11ea-aaa6-525400524259
STEP: Creating secret with name s-test-opt-create-723e3b3e-0101-11ea-aaa6-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:55:13.784: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-7866" for this suite.
Nov  7 01:55:34.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:55:42.910: INFO: namespace secrets-7866 deletion completed in 28.853685957s

• [SLOW TEST:112.225 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:55:42.911: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Nov  7 01:55:43.284: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig cluster-info'
Nov  7 01:55:47.546: INFO: stderr: ""
Nov  7 01:55:47.546: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.jeder-42-test1.g7g7.p1.openshiftapps.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:55:47.546: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6305" for this suite.
Nov  7 01:55:54.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:56:02.689: INFO: namespace kubectl-6305 deletion completed in 14.870335259s

• [SLOW TEST:19.779 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:56:02.691: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-c0de7ca6-0101-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 01:56:03.257: INFO: Waiting up to 5m0s for pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259" in namespace "secrets-6274" to be "success or failure"
Nov  7 01:56:03.349: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.069918ms
Nov  7 01:56:05.442: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185484885s
Nov  7 01:56:07.535: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277975418s
Nov  7 01:56:09.628: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370802438s
Nov  7 01:56:11.721: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464340436s
Nov  7 01:56:13.815: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.557817715s
STEP: Saw pod success
Nov  7 01:56:13.815: INFO: Pod "pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 01:56:13.907: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 01:56:14.102: INFO: Waiting for pod pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259 to disappear
Nov  7 01:56:14.194: INFO: Pod pod-secrets-c0ecb0a3-0101-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:56:14.194: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6274" for this suite.
Nov  7 01:56:20.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:56:29.380: INFO: namespace secrets-6274 deletion completed in 14.914161468s

• [SLOW TEST:26.690 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:56:29.382: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4759.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4759.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  7 01:56:52.876: INFO: DNS probes using dns-4759/dns-test-d0c72be1-0101-11ea-aaa6-525400524259 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:56:52.976: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-4759" for this suite.
Nov  7 01:56:59.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:57:08.033: INFO: namespace dns-4759 deletion completed in 14.874804421s

• [SLOW TEST:38.651 seconds]
[sig-network] DNS
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:57:08.034: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 01:57:08.497: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-e7df1570-0101-11ea-aaa6-525400524259
STEP: Creating configMap with name cm-test-opt-upd-e7df3238-0101-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e7df1570-0101-11ea-aaa6-525400524259
STEP: Updating configmap cm-test-opt-upd-e7df3238-0101-11ea-aaa6-525400524259
STEP: Creating configMap with name cm-test-opt-create-e7df3265-0101-11ea-aaa6-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:58:35.355: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8770" for this suite.
Nov  7 01:59:03.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:59:12.486: INFO: namespace configmap-8770 deletion completed in 36.857926244s

• [SLOW TEST:124.452 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:59:12.488: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov  7 01:59:21.932: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-340 pod-service-account-327686e0-0102-11ea-aaa6-525400524259 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov  7 01:59:23.142: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-340 pod-service-account-327686e0-0102-11ea-aaa6-525400524259 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov  7 01:59:24.322: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-340 pod-service-account-327686e0-0102-11ea-aaa6-525400524259 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 01:59:25.543: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-340" for this suite.
Nov  7 01:59:32.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 01:59:40.673: INFO: namespace svcaccounts-340 deletion completed in 14.857736313s

• [SLOW TEST:28.186 seconds]
[sig-auth] ServiceAccounts
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 01:59:40.674: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Nov  7 01:59:41.051: INFO: PodSpec: initContainers in spec.initContainers
Nov  7 02:00:28.687: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-42ccf81c-0102-11ea-aaa6-525400524259", GenerateName:"", Namespace:"init-container-5816", SelfLink:"/api/v1/namespaces/init-container-5816/pods/pod-init-42ccf81c-0102-11ea-aaa6-525400524259", UID:"2dcf2c45-0102-11ea-98c4-0655ae4e8e56", ResourceVersion:"98391", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63708688745, loc:(*time.Location)(0x8828100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"51296692"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"openshift-sdn\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.37\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ztpbl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002f4c280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ztpbl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003863d60), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ztpbl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003863e00), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ztpbl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003863cc0), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002d8c2b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-139-168.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002dab560), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-gvsfc"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d8c370)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d8c390)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002d8c398), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002d8c39c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688745, loc:(*time.Location)(0x8828100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688745, loc:(*time.Location)(0x8828100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688745, loc:(*time.Location)(0x8828100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708688745, loc:(*time.Location)(0x8828100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.139.168", PodIP:"10.131.0.37", StartTime:(*v1.Time)(0xc002f2a0c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001bb7b90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001bb7c00)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"cri-o://5b381857d4bf4a2dedc4381d7f956e4b80fd0b55129e56146d4db9b6d71acc63"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f2a100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f2a0e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Burstable"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:00:28.689: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-5816" for this suite.
Nov  7 02:00:57.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:01:05.822: INFO: namespace init-container-5816 deletion completed in 36.859944439s

• [SLOW TEST:85.148 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:01:05.824: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:01:06.296: INFO: Waiting up to 5m0s for pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259" in namespace "downward-api-2152" to be "success or failure"
Nov  7 02:01:06.388: INFO: Pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.015146ms
Nov  7 02:01:08.481: INFO: Pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184557545s
Nov  7 02:01:10.573: INFO: Pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277194263s
Nov  7 02:01:12.667: INFO: Pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371242168s
Nov  7 02:01:14.760: INFO: Pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.463792694s
STEP: Saw pod success
Nov  7 02:01:14.760: INFO: Pod "downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:01:14.852: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:01:15.058: INFO: Waiting for pod downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:01:15.150: INFO: Pod downwardapi-volume-758cde22-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:01:15.150: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2152" for this suite.
Nov  7 02:01:21.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:01:30.286: INFO: namespace downward-api-2152 deletion completed in 14.864826616s

• [SLOW TEST:24.462 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:01:30.289: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-8421ccad-0102-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 02:01:31.222: INFO: Waiting up to 5m0s for pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259" in namespace "secrets-6652" to be "success or failure"
Nov  7 02:01:31.315: INFO: Pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.103413ms
Nov  7 02:01:33.408: INFO: Pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186255082s
Nov  7 02:01:35.501: INFO: Pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278841863s
Nov  7 02:01:37.595: INFO: Pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.37241187s
Nov  7 02:01:39.687: INFO: Pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.464925932s
STEP: Saw pod success
Nov  7 02:01:39.690: INFO: Pod "pod-secrets-84683ca3-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:01:39.782: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-84683ca3-0102-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 02:01:39.977: INFO: Waiting for pod pod-secrets-84683ca3-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:01:40.069: INFO: Pod pod-secrets-84683ca3-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:01:40.069: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6652" for this suite.
Nov  7 02:01:46.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:01:55.265: INFO: namespace secrets-6652 deletion completed in 14.923243495s
STEP: Destroying namespace "secret-namespace-6566" for this suite.
Nov  7 02:02:01.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:02:10.125: INFO: namespace secret-namespace-6566 deletion completed in 14.859732933s

• [SLOW TEST:39.835 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:02:10.126: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:02:10.686: INFO: (0) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.282511ms)
Nov  7 02:02:10.779: INFO: (1) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.722324ms)
Nov  7 02:02:10.873: INFO: (2) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 94.033051ms)
Nov  7 02:02:10.966: INFO: (3) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.702779ms)
Nov  7 02:02:11.059: INFO: (4) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.119247ms)
Nov  7 02:02:11.152: INFO: (5) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.702259ms)
Nov  7 02:02:11.245: INFO: (6) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.883989ms)
Nov  7 02:02:11.338: INFO: (7) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.63565ms)
Nov  7 02:02:11.430: INFO: (8) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.868349ms)
Nov  7 02:02:11.523: INFO: (9) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.818089ms)
Nov  7 02:02:11.616: INFO: (10) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.794402ms)
Nov  7 02:02:11.709: INFO: (11) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.969353ms)
Nov  7 02:02:11.802: INFO: (12) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.945927ms)
Nov  7 02:02:11.895: INFO: (13) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.912844ms)
Nov  7 02:02:11.988: INFO: (14) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.952605ms)
Nov  7 02:02:12.081: INFO: (15) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.865847ms)
Nov  7 02:02:12.174: INFO: (16) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.799413ms)
Nov  7 02:02:12.267: INFO: (17) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 92.686979ms)
Nov  7 02:02:12.360: INFO: (18) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.04079ms)
Nov  7 02:02:12.454: INFO: (19) /api/v1/nodes/ip-10-0-132-48.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 93.470873ms)
[AfterEach] version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:02:12.454: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-9231" for this suite.
Nov  7 02:02:18.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:02:26.649: INFO: namespace proxy-9231 deletion completed in 14.10053214s

• [SLOW TEST:16.523 seconds]
[sig-network] Proxy
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:02:26.650: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:02:27.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259" in namespace "downward-api-1877" to be "success or failure"
Nov  7 02:02:27.228: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.240505ms
Nov  7 02:02:29.320: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184659643s
Nov  7 02:02:31.413: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277797776s
Nov  7 02:02:33.508: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372808992s
Nov  7 02:02:35.601: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.465686795s
Nov  7 02:02:37.694: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.558150719s
STEP: Saw pod success
Nov  7 02:02:37.694: INFO: Pod "downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:02:37.787: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:02:37.980: INFO: Waiting for pod downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:02:38.072: INFO: Pod downwardapi-volume-a5ba4f1b-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:02:38.072: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1877" for this suite.
Nov  7 02:02:44.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:02:53.212: INFO: namespace downward-api-1877 deletion completed in 14.866712333s

• [SLOW TEST:26.563 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:02:53.214: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Nov  7 02:02:53.727: INFO: Waiting up to 5m0s for pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259" in namespace "downward-api-8314" to be "success or failure"
Nov  7 02:02:53.824: INFO: Pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 97.264628ms
Nov  7 02:02:55.917: INFO: Pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19030263s
Nov  7 02:02:58.010: INFO: Pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.283172378s
Nov  7 02:03:00.103: INFO: Pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376804143s
Nov  7 02:03:02.198: INFO: Pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.471640413s
STEP: Saw pod success
Nov  7 02:03:02.199: INFO: Pod "downward-api-b5921de6-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:03:02.291: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downward-api-b5921de6-0102-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 02:03:02.485: INFO: Waiting for pod downward-api-b5921de6-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:03:02.577: INFO: Pod downward-api-b5921de6-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:03:02.577: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8314" for this suite.
Nov  7 02:03:09.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:03:17.745: INFO: namespace downward-api-8314 deletion completed in 14.896077164s

• [SLOW TEST:24.531 seconds]
[sig-node] Downward API
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:03:17.746: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-c42e773d-0102-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 02:03:18.312: INFO: Waiting up to 5m0s for pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259" in namespace "configmap-9723" to be "success or failure"
Nov  7 02:03:18.405: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.451415ms
Nov  7 02:03:20.497: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185040251s
Nov  7 02:03:22.590: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277979866s
Nov  7 02:03:24.683: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370566962s
Nov  7 02:03:26.776: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463176443s
Nov  7 02:03:28.868: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.555515982s
STEP: Saw pod success
Nov  7 02:03:28.868: INFO: Pod "pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:03:28.960: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 02:03:29.154: INFO: Waiting for pod pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:03:29.246: INFO: Pod pod-configmaps-c43ccc98-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:03:29.246: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9723" for this suite.
Nov  7 02:03:35.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:03:44.430: INFO: namespace configmap-9723 deletion completed in 14.911261926s

• [SLOW TEST:26.684 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:03:44.430: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Nov  7 02:03:44.905: INFO: Waiting up to 5m0s for pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259" in namespace "var-expansion-4631" to be "success or failure"
Nov  7 02:03:44.997: INFO: Pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.298328ms
Nov  7 02:03:47.090: INFO: Pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185143176s
Nov  7 02:03:49.185: INFO: Pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280091102s
Nov  7 02:03:51.278: INFO: Pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373243461s
Nov  7 02:03:53.371: INFO: Pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.46595721s
STEP: Saw pod success
Nov  7 02:03:53.371: INFO: Pod "var-expansion-d416beea-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:03:53.463: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod var-expansion-d416beea-0102-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 02:03:53.658: INFO: Waiting for pod var-expansion-d416beea-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:03:53.750: INFO: Pod var-expansion-d416beea-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:03:53.750: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-4631" for this suite.
Nov  7 02:04:00.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:04:08.881: INFO: namespace var-expansion-4631 deletion completed in 14.858803209s

• [SLOW TEST:24.451 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:04:08.882: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov  7 02:04:09.354: INFO: Waiting up to 5m0s for pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259" in namespace "emptydir-8585" to be "success or failure"
Nov  7 02:04:09.447: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.583665ms
Nov  7 02:04:11.541: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186773777s
Nov  7 02:04:13.633: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279364597s
Nov  7 02:04:15.727: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373336154s
Nov  7 02:04:17.820: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.466446566s
Nov  7 02:04:19.913: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.559128821s
STEP: Saw pod success
Nov  7 02:04:19.913: INFO: Pod "pod-e2a96cf3-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:04:20.005: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-e2a96cf3-0102-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:04:20.199: INFO: Waiting for pod pod-e2a96cf3-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:04:20.291: INFO: Pod pod-e2a96cf3-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:04:20.291: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8585" for this suite.
Nov  7 02:04:26.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:04:35.414: INFO: namespace emptydir-8585 deletion completed in 14.851019653s

• [SLOW TEST:26.532 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:04:35.416: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-f279ec78-0102-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 02:04:35.988: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259" in namespace "projected-832" to be "success or failure"
Nov  7 02:04:36.080: INFO: Pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.096318ms
Nov  7 02:04:38.173: INFO: Pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184708477s
Nov  7 02:04:40.266: INFO: Pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277499294s
Nov  7 02:04:42.359: INFO: Pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370662541s
Nov  7 02:04:44.452: INFO: Pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.463281581s
STEP: Saw pod success
Nov  7 02:04:44.452: INFO: Pod "pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:04:44.544: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 02:04:44.738: INFO: Waiting for pod pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259 to disappear
Nov  7 02:04:44.829: INFO: Pod pod-projected-configmaps-f2883ce4-0102-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:04:44.829: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-832" for this suite.
Nov  7 02:04:51.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:05:00.030: INFO: namespace projected-832 deletion completed in 14.929222403s

• [SLOW TEST:24.615 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:05:00.032: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Nov  7 02:05:00.505: INFO: Waiting up to 5m0s for pod "downward-api-01264a62-0103-11ea-aaa6-525400524259" in namespace "downward-api-7765" to be "success or failure"
Nov  7 02:05:00.599: INFO: Pod "downward-api-01264a62-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.406877ms
Nov  7 02:05:02.692: INFO: Pod "downward-api-01264a62-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186278072s
Nov  7 02:05:04.785: INFO: Pod "downward-api-01264a62-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279096143s
Nov  7 02:05:06.877: INFO: Pod "downward-api-01264a62-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371984197s
Nov  7 02:05:08.971: INFO: Pod "downward-api-01264a62-0103-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.465064275s
STEP: Saw pod success
Nov  7 02:05:08.971: INFO: Pod "downward-api-01264a62-0103-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:05:09.063: INFO: Trying to get logs from node ip-10-0-136-94.us-west-2.compute.internal pod downward-api-01264a62-0103-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 02:05:09.268: INFO: Waiting for pod downward-api-01264a62-0103-11ea-aaa6-525400524259 to disappear
Nov  7 02:05:09.360: INFO: Pod downward-api-01264a62-0103-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:05:09.360: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7765" for this suite.
Nov  7 02:05:15.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:05:24.498: INFO: namespace downward-api-7765 deletion completed in 14.865183947s

• [SLOW TEST:24.465 seconds]
[sig-node] Downward API
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:05:24.498: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Nov  7 02:05:24.869: INFO: Asynchronously running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix502410334/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:05:24.961: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5693" for this suite.
Nov  7 02:05:31.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:05:39.922: INFO: namespace kubectl-5693 deletion completed in 14.868516291s

• [SLOW TEST:15.425 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:05:39.923: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:05:50.873: INFO: Waiting up to 5m0s for pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259" in namespace "pods-3793" to be "success or failure"
Nov  7 02:05:50.966: INFO: Pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.216295ms
Nov  7 02:05:53.059: INFO: Pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185480801s
Nov  7 02:05:55.151: INFO: Pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278000866s
Nov  7 02:05:57.244: INFO: Pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370657696s
Nov  7 02:05:59.337: INFO: Pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.463743152s
STEP: Saw pod success
Nov  7 02:05:59.337: INFO: Pod "client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:05:59.430: INFO: Trying to get logs from node ip-10-0-136-94.us-west-2.compute.internal pod client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259 container env3cont: <nil>
STEP: delete the pod
Nov  7 02:05:59.624: INFO: Waiting for pod client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259 to disappear
Nov  7 02:05:59.716: INFO: Pod client-envvars-1f2c0d5d-0103-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:05:59.716: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3793" for this suite.
Nov  7 02:06:44.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:06:52.876: INFO: namespace pods-3793 deletion completed in 52.885396266s

• [SLOW TEST:72.953 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:06:52.876: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-3508
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3508
STEP: Deleting pre-stop pod
Nov  7 02:07:21.103: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:07:21.199: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "prestop-3508" for this suite.
Nov  7 02:08:03.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:08:12.327: INFO: namespace prestop-3508 deletion completed in 50.855153083s

• [SLOW TEST:79.451 seconds]
[k8s.io] [sig-node] PreStop
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:08:12.328: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3262
Nov  7 02:08:24.985: INFO: Started pod liveness-http in namespace container-probe-3262
STEP: checking the pod's current state and verifying that restartCount is present
Nov  7 02:08:25.077: INFO: Initial restart count of pod liveness-http is 0
Nov  7 02:08:46.098: INFO: Restart count of pod container-probe-3262/liveness-http is now 1 (21.020784359s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:08:46.196: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-3262" for this suite.
Nov  7 02:08:52.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:09:01.410: INFO: namespace container-probe-3262 deletion completed in 14.941925599s

• [SLOW TEST:49.082 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:09:01.410: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:09:01.874: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9113cd8f-0103-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-9113cd8f-0103-11ea-aaa6-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:10:36.440: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1862" for this suite.
Nov  7 02:10:48.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:10:57.742: INFO: namespace projected-1862 deletion completed in 21.0293151s

• [SLOW TEST:116.332 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:10:57.742: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov  7 02:10:58.219: INFO: Waiting up to 5m0s for pod "pod-d65d1694-0103-11ea-aaa6-525400524259" in namespace "emptydir-6515" to be "success or failure"
Nov  7 02:10:58.312: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.990262ms
Nov  7 02:11:00.406: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186614678s
Nov  7 02:11:02.500: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281074612s
Nov  7 02:11:04.593: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373688415s
Nov  7 02:11:06.687: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.467433259s
Nov  7 02:11:08.781: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.56119834s
STEP: Saw pod success
Nov  7 02:11:08.781: INFO: Pod "pod-d65d1694-0103-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:11:08.874: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-d65d1694-0103-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:11:09.071: INFO: Waiting for pod pod-d65d1694-0103-11ea-aaa6-525400524259 to disappear
Nov  7 02:11:09.164: INFO: Pod pod-d65d1694-0103-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:11:09.164: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6515" for this suite.
Nov  7 02:11:15.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:11:24.400: INFO: namespace emptydir-6515 deletion completed in 14.960723674s

• [SLOW TEST:26.658 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:11:24.400: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Nov  7 02:11:24.779: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-9476'
Nov  7 02:11:30.150: INFO: stderr: ""
Nov  7 02:11:30.150: INFO: stdout: "pod/pause created\n"
Nov  7 02:11:30.150: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov  7 02:11:30.150: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9476" to be "running and ready"
Nov  7 02:11:30.246: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 95.991121ms
Nov  7 02:11:32.340: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.190722906s
Nov  7 02:11:34.435: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285145291s
Nov  7 02:11:36.529: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.379032181s
Nov  7 02:11:38.624: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.473910152s
Nov  7 02:11:40.717: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.567800722s
Nov  7 02:11:40.718: INFO: Pod "pause" satisfied condition "running and ready"
Nov  7 02:11:40.718: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Nov  7 02:11:40.718: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig label pods pause testing-label=testing-label-value --namespace=kubectl-9476'
Nov  7 02:11:41.286: INFO: stderr: ""
Nov  7 02:11:41.286: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov  7 02:11:41.286: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pod pause -L testing-label --namespace=kubectl-9476'
Nov  7 02:11:41.720: INFO: stderr: ""
Nov  7 02:11:41.720: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          12s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov  7 02:11:41.720: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig label pods pause testing-label- --namespace=kubectl-9476'
Nov  7 02:11:42.270: INFO: stderr: ""
Nov  7 02:11:42.270: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov  7 02:11:42.270: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pod pause -L testing-label --namespace=kubectl-9476'
Nov  7 02:11:42.715: INFO: stderr: ""
Nov  7 02:11:42.715: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          13s   \n"
[AfterEach] [k8s.io] Kubectl label
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Nov  7 02:11:42.715: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9476'
Nov  7 02:11:43.247: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:11:43.247: INFO: stdout: "pod \"pause\" force deleted\n"
Nov  7 02:11:43.247: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get rc,svc -l name=pause --no-headers --namespace=kubectl-9476'
Nov  7 02:11:43.796: INFO: stderr: "No resources found.\n"
Nov  7 02:11:43.796: INFO: stdout: ""
Nov  7 02:11:43.796: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -l name=pause --namespace=kubectl-9476 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  7 02:11:44.235: INFO: stderr: ""
Nov  7 02:11:44.235: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:11:44.235: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9476" for this suite.
Nov  7 02:11:50.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:11:59.472: INFO: namespace kubectl-9476 deletion completed in 14.961884613s

• [SLOW TEST:35.072 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:11:59.475: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:11:59.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259" in namespace "projected-3715" to be "success or failure"
Nov  7 02:12:00.046: INFO: Pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.29538ms
Nov  7 02:12:02.140: INFO: Pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186937905s
Nov  7 02:12:04.233: INFO: Pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280799407s
Nov  7 02:12:06.327: INFO: Pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.37450157s
Nov  7 02:12:08.423: INFO: Pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.470246837s
STEP: Saw pod success
Nov  7 02:12:08.423: INFO: Pod "downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:12:08.516: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:12:08.712: INFO: Waiting for pod downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259 to disappear
Nov  7 02:12:08.806: INFO: Pod downwardapi-volume-fb28b6b3-0103-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:12:08.807: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3715" for this suite.
Nov  7 02:12:15.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:12:24.031: INFO: namespace projected-3715 deletion completed in 14.949086059s

• [SLOW TEST:24.558 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:12:24.034: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:12:24.606: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-9224" for this suite.
Nov  7 02:12:36.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:12:45.663: INFO: namespace kubelet-test-9224 deletion completed in 20.961935226s

• [SLOW TEST:21.630 seconds]
[k8s.io] Kubelet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:12:45.665: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-16b0b6aa-0104-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 02:12:46.236: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259" in namespace "projected-4040" to be "success or failure"
Nov  7 02:12:46.330: INFO: Pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 94.446996ms
Nov  7 02:12:48.424: INFO: Pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188729054s
Nov  7 02:12:50.519: INFO: Pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28308081s
Nov  7 02:12:52.613: INFO: Pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.377710402s
Nov  7 02:12:54.708: INFO: Pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.472034292s
STEP: Saw pod success
Nov  7 02:12:54.708: INFO: Pod "pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:12:54.803: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  7 02:12:54.999: INFO: Waiting for pod pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259 to disappear
Nov  7 02:12:55.092: INFO: Pod pod-projected-secrets-16bf1517-0104-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:12:55.092: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4040" for this suite.
Nov  7 02:13:01.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:13:10.317: INFO: namespace projected-4040 deletion completed in 14.949177829s

• [SLOW TEST:24.652 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:13:10.319: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:13:11.447: INFO: Create a RollingUpdate DaemonSet
Nov  7 02:13:11.543: INFO: Check that daemon pods launch on every node of the cluster
Nov  7 02:13:11.637: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:11.637: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:11.637: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:11.730: INFO: Number of nodes with available pods: 0
Nov  7 02:13:11.730: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:13.006: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:13.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:13.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:13.100: INFO: Number of nodes with available pods: 0
Nov  7 02:13:13.100: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:14.007: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:14.007: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:14.007: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:14.100: INFO: Number of nodes with available pods: 0
Nov  7 02:13:14.100: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:15.006: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:15.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:15.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:15.099: INFO: Number of nodes with available pods: 0
Nov  7 02:13:15.099: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:16.006: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:16.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:16.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:16.100: INFO: Number of nodes with available pods: 0
Nov  7 02:13:16.100: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:17.005: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:17.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:17.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:17.100: INFO: Number of nodes with available pods: 0
Nov  7 02:13:17.100: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:18.006: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:18.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:18.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:18.100: INFO: Number of nodes with available pods: 0
Nov  7 02:13:18.100: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:19.006: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:19.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:19.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:19.099: INFO: Number of nodes with available pods: 0
Nov  7 02:13:19.099: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:20.007: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:20.007: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:20.007: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:20.101: INFO: Number of nodes with available pods: 3
Nov  7 02:13:20.101: INFO: Node ip-10-0-136-94.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:13:21.006: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:21.006: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:21.006: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:21.099: INFO: Number of nodes with available pods: 4
Nov  7 02:13:21.100: INFO: Number of running nodes: 4, number of available pods: 4
Nov  7 02:13:21.100: INFO: Update the DaemonSet to trigger a rollout
Nov  7 02:13:21.288: INFO: Updating DaemonSet daemon-set
Nov  7 02:13:24.753: INFO: Roll back the DaemonSet before rollout is complete
Nov  7 02:13:24.942: INFO: Updating DaemonSet daemon-set
Nov  7 02:13:24.942: INFO: Make sure DaemonSet rollback is complete
Nov  7 02:13:25.035: INFO: Wrong image for pod: daemon-set-jxp9c. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov  7 02:13:25.035: INFO: Pod daemon-set-jxp9c is not available
Nov  7 02:13:25.130: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:25.130: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:25.130: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:26.224: INFO: Wrong image for pod: daemon-set-jxp9c. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov  7 02:13:26.224: INFO: Pod daemon-set-jxp9c is not available
Nov  7 02:13:26.409: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:26.409: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:26.409: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:27.223: INFO: Wrong image for pod: daemon-set-jxp9c. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov  7 02:13:27.223: INFO: Pod daemon-set-jxp9c is not available
Nov  7 02:13:27.408: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:27.408: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:27.408: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:28.225: INFO: Wrong image for pod: daemon-set-jxp9c. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov  7 02:13:28.225: INFO: Pod daemon-set-jxp9c is not available
Nov  7 02:13:28.409: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:28.409: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:28.409: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:29.224: INFO: Wrong image for pod: daemon-set-jxp9c. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov  7 02:13:29.224: INFO: Pod daemon-set-jxp9c is not available
Nov  7 02:13:29.409: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:29.409: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:29.409: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:30.224: INFO: Wrong image for pod: daemon-set-jxp9c. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov  7 02:13:30.224: INFO: Pod daemon-set-jxp9c is not available
Nov  7 02:13:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:30.409: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
Nov  7 02:13:31.223: INFO: Pod daemon-set-2s7m9 is not available
Nov  7 02:13:31.410: INFO: DaemonSet pods can't tolerate node ip-10-0-131-250.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:20 +0000 UTC}], skip checking this node
Nov  7 02:13:31.410: INFO: DaemonSet pods can't tolerate node ip-10-0-132-41.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:21 +0000 UTC}], skip checking this node
Nov  7 02:13:31.410: INFO: DaemonSet pods can't tolerate node ip-10-0-138-87.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-11-07 00:28:22 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6330, will wait for the garbage collector to delete the pods
Nov  7 02:13:31.886: INFO: Deleting DaemonSet.extensions daemon-set took: 95.992282ms
Nov  7 02:13:31.986: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.292923ms
Nov  7 02:13:43.980: INFO: Number of nodes with available pods: 0
Nov  7 02:13:43.980: INFO: Number of running nodes: 0, number of available pods: 0
Nov  7 02:13:44.073: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6330/daemonsets","resourceVersion":"104290"},"items":null}

Nov  7 02:13:44.166: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6330/pods","resourceVersion":"104290"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:13:44.725: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-6330" for this suite.
Nov  7 02:13:51.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:13:59.893: INFO: namespace daemonsets-6330 deletion completed in 14.982887671s

• [SLOW TEST:49.574 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:13:59.894: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov  7 02:14:00.372: INFO: Waiting up to 5m0s for pod "pod-42ef77a2-0104-11ea-aaa6-525400524259" in namespace "emptydir-4959" to be "success or failure"
Nov  7 02:14:00.465: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.008027ms
Nov  7 02:14:02.560: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187591423s
Nov  7 02:14:04.654: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281324041s
Nov  7 02:14:06.749: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376819982s
Nov  7 02:14:08.843: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470672826s
Nov  7 02:14:10.937: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.564547359s
STEP: Saw pod success
Nov  7 02:14:10.937: INFO: Pod "pod-42ef77a2-0104-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:14:11.030: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-42ef77a2-0104-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:14:11.226: INFO: Waiting for pod pod-42ef77a2-0104-11ea-aaa6-525400524259 to disappear
Nov  7 02:14:11.319: INFO: Pod pod-42ef77a2-0104-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:14:11.319: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4959" for this suite.
Nov  7 02:14:17.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:14:26.565: INFO: namespace emptydir-4959 deletion completed in 14.968948397s

• [SLOW TEST:26.671 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:14:26.566: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Nov  7 02:14:26.943: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-5653'
Nov  7 02:14:28.439: INFO: stderr: ""
Nov  7 02:14:28.439: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  7 02:14:28.439: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:28.872: INFO: stderr: ""
Nov  7 02:14:28.872: INFO: stdout: "update-demo-nautilus-k76fw update-demo-nautilus-l7r68 "
Nov  7 02:14:28.872: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:29.325: INFO: stderr: ""
Nov  7 02:14:29.325: INFO: stdout: ""
Nov  7 02:14:29.325: INFO: update-demo-nautilus-k76fw is created but not running
Nov  7 02:14:34.325: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:34.763: INFO: stderr: ""
Nov  7 02:14:34.763: INFO: stdout: "update-demo-nautilus-k76fw update-demo-nautilus-l7r68 "
Nov  7 02:14:34.763: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:35.219: INFO: stderr: ""
Nov  7 02:14:35.219: INFO: stdout: ""
Nov  7 02:14:35.219: INFO: update-demo-nautilus-k76fw is created but not running
Nov  7 02:14:40.220: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:40.657: INFO: stderr: ""
Nov  7 02:14:40.657: INFO: stdout: "update-demo-nautilus-k76fw update-demo-nautilus-l7r68 "
Nov  7 02:14:40.658: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:41.111: INFO: stderr: ""
Nov  7 02:14:41.111: INFO: stdout: "true"
Nov  7 02:14:41.111: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:41.559: INFO: stderr: ""
Nov  7 02:14:41.559: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:14:41.559: INFO: validating pod update-demo-nautilus-k76fw
Nov  7 02:14:41.656: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:14:41.656: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:14:41.656: INFO: update-demo-nautilus-k76fw is verified up and running
Nov  7 02:14:41.656: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-l7r68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:42.091: INFO: stderr: ""
Nov  7 02:14:42.091: INFO: stdout: "true"
Nov  7 02:14:42.091: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-l7r68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:42.527: INFO: stderr: ""
Nov  7 02:14:42.527: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:14:42.527: INFO: validating pod update-demo-nautilus-l7r68
Nov  7 02:14:42.623: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:14:42.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:14:42.623: INFO: update-demo-nautilus-l7r68 is verified up and running
STEP: scaling down the replication controller
Nov  7 02:14:42.801: INFO: scanned /home/jeder for discovery docs: <nil>
Nov  7 02:14:42.801: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5653'
Nov  7 02:14:43.528: INFO: stderr: ""
Nov  7 02:14:43.528: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  7 02:14:43.528: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:43.961: INFO: stderr: ""
Nov  7 02:14:43.961: INFO: stdout: "update-demo-nautilus-k76fw update-demo-nautilus-l7r68 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov  7 02:14:48.962: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:49.414: INFO: stderr: ""
Nov  7 02:14:49.414: INFO: stdout: "update-demo-nautilus-k76fw update-demo-nautilus-l7r68 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov  7 02:14:54.416: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:54.900: INFO: stderr: ""
Nov  7 02:14:54.900: INFO: stdout: "update-demo-nautilus-k76fw "
Nov  7 02:14:54.901: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:55.329: INFO: stderr: ""
Nov  7 02:14:55.329: INFO: stdout: "true"
Nov  7 02:14:55.329: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:55.783: INFO: stderr: ""
Nov  7 02:14:55.783: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:14:55.783: INFO: validating pod update-demo-nautilus-k76fw
Nov  7 02:14:55.877: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:14:55.877: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:14:55.878: INFO: update-demo-nautilus-k76fw is verified up and running
STEP: scaling up the replication controller
Nov  7 02:14:56.009: INFO: scanned /home/jeder for discovery docs: <nil>
Nov  7 02:14:56.009: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5653'
Nov  7 02:14:56.728: INFO: stderr: ""
Nov  7 02:14:56.728: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  7 02:14:56.728: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:14:57.183: INFO: stderr: ""
Nov  7 02:14:57.183: INFO: stdout: "update-demo-nautilus-hxqcd update-demo-nautilus-k76fw "
Nov  7 02:14:57.183: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-hxqcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:14:57.623: INFO: stderr: ""
Nov  7 02:14:57.623: INFO: stdout: ""
Nov  7 02:14:57.623: INFO: update-demo-nautilus-hxqcd is created but not running
Nov  7 02:15:02.623: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:15:03.063: INFO: stderr: ""
Nov  7 02:15:03.063: INFO: stdout: "update-demo-nautilus-hxqcd update-demo-nautilus-k76fw "
Nov  7 02:15:03.063: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-hxqcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:15:03.490: INFO: stderr: ""
Nov  7 02:15:03.490: INFO: stdout: ""
Nov  7 02:15:03.490: INFO: update-demo-nautilus-hxqcd is created but not running
Nov  7 02:15:08.491: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5653'
Nov  7 02:15:08.922: INFO: stderr: ""
Nov  7 02:15:08.922: INFO: stdout: "update-demo-nautilus-hxqcd update-demo-nautilus-k76fw "
Nov  7 02:15:08.922: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-hxqcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:15:09.367: INFO: stderr: ""
Nov  7 02:15:09.367: INFO: stdout: "true"
Nov  7 02:15:09.367: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-hxqcd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:15:09.794: INFO: stderr: ""
Nov  7 02:15:09.794: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:15:09.794: INFO: validating pod update-demo-nautilus-hxqcd
Nov  7 02:15:09.891: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:15:09.891: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:15:09.891: INFO: update-demo-nautilus-hxqcd is verified up and running
Nov  7 02:15:09.891: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:15:10.329: INFO: stderr: ""
Nov  7 02:15:10.329: INFO: stdout: "true"
Nov  7 02:15:10.329: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-k76fw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5653'
Nov  7 02:15:10.776: INFO: stderr: ""
Nov  7 02:15:10.776: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:15:10.776: INFO: validating pod update-demo-nautilus-k76fw
Nov  7 02:15:10.872: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:15:10.872: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:15:10.872: INFO: update-demo-nautilus-k76fw is verified up and running
STEP: using delete to clean up resources
Nov  7 02:15:10.872: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-5653'
Nov  7 02:15:11.413: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:15:11.413: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov  7 02:15:11.413: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5653'
Nov  7 02:15:11.974: INFO: stderr: "No resources found.\n"
Nov  7 02:15:11.974: INFO: stdout: ""
Nov  7 02:15:11.974: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -l name=update-demo --namespace=kubectl-5653 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  7 02:15:12.400: INFO: stderr: ""
Nov  7 02:15:12.400: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:15:12.400: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5653" for this suite.
Nov  7 02:15:24.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:15:33.705: INFO: namespace kubectl-5653 deletion completed in 21.028978412s

• [SLOW TEST:67.138 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:15:33.705: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:16:13.304: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-783" for this suite.
Nov  7 02:16:19.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:16:28.589: INFO: namespace namespaces-783 deletion completed in 15.008319639s
STEP: Destroying namespace "nsdeletetest-269" for this suite.
Nov  7 02:16:28.682: INFO: Namespace nsdeletetest-269 was already deleted
STEP: Destroying namespace "nsdeletetest-5318" for this suite.
Nov  7 02:16:34.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:16:43.652: INFO: namespace nsdeletetest-5318 deletion completed in 14.969802412s

• [SLOW TEST:69.947 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:16:43.652: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7964
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7964
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7964
Nov  7 02:16:44.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 02:16:54.406: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov  7 02:16:54.499: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 02:16:55.711: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 02:16:55.712: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 02:16:55.712: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 02:16:55.805: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov  7 02:17:05.899: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 02:17:05.899: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 02:17:06.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999379s
Nov  7 02:17:07.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.905814932s
Nov  7 02:17:08.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.812172292s
Nov  7 02:17:09.557: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.718079081s
Nov  7 02:17:10.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.624037462s
Nov  7 02:17:11.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.529125825s
Nov  7 02:17:12.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.435008729s
Nov  7 02:17:13.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.341181553s
Nov  7 02:17:15.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.247489178s
Nov  7 02:17:16.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 153.941993ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7964
Nov  7 02:17:17.217: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 02:17:18.397: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov  7 02:17:18.397: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 02:17:18.397: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 02:17:18.397: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 02:17:19.583: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov  7 02:17:19.583: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 02:17:19.583: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 02:17:19.583: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 02:17:20.732: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov  7 02:17:20.732: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 02:17:20.732: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 02:17:20.827: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:17:20.827: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:17:20.827: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov  7 02:17:20.920: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 02:17:22.070: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 02:17:22.070: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 02:17:22.070: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 02:17:22.070: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 02:17:23.224: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 02:17:23.224: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 02:17:23.224: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 02:17:23.225: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7964 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 02:17:24.377: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 02:17:24.377: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 02:17:24.377: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 02:17:24.377: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 02:17:24.471: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Nov  7 02:17:34.659: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 02:17:34.660: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 02:17:34.660: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov  7 02:17:34.942: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:34.942: INFO: ss-0  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  }]
Nov  7 02:17:34.942: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:34.942: INFO: ss-2  ip-10-0-136-94.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:34.942: INFO: 
Nov  7 02:17:34.942: INFO: StatefulSet ss has not reached scale 0, at 3
Nov  7 02:17:36.036: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:36.036: INFO: ss-0  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  }]
Nov  7 02:17:36.036: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:36.036: INFO: ss-2  ip-10-0-136-94.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:36.036: INFO: 
Nov  7 02:17:36.036: INFO: StatefulSet ss has not reached scale 0, at 3
Nov  7 02:17:37.131: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:37.131: INFO: ss-0  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  }]
Nov  7 02:17:37.131: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:37.131: INFO: 
Nov  7 02:17:37.131: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  7 02:17:38.226: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:38.226: INFO: ss-0  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  }]
Nov  7 02:17:38.226: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:38.226: INFO: 
Nov  7 02:17:38.226: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  7 02:17:39.320: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:39.320: INFO: ss-0  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  }]
Nov  7 02:17:39.320: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:39.320: INFO: 
Nov  7 02:17:39.320: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  7 02:17:40.414: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:40.414: INFO: ss-0  ip-10-0-132-48.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:08 +0000 UTC  }]
Nov  7 02:17:40.414: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:40.414: INFO: 
Nov  7 02:17:40.414: INFO: StatefulSet ss has not reached scale 0, at 2
Nov  7 02:17:41.523: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:41.523: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:41.523: INFO: 
Nov  7 02:17:41.523: INFO: StatefulSet ss has not reached scale 0, at 1
Nov  7 02:17:42.617: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Nov  7 02:17:42.617: INFO: ss-1  ip-10-0-139-168.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:16:30 +0000 UTC  }]
Nov  7 02:17:42.617: INFO: 
Nov  7 02:17:42.617: INFO: StatefulSet ss has not reached scale 0, at 1
Nov  7 02:17:43.710: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.232029641s
Nov  7 02:17:44.804: INFO: Verifying statefulset ss doesn't scale past 0 for another 138.267721ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7964
Nov  7 02:17:45.898: INFO: Scaling statefulset ss to 0
Nov  7 02:17:46.179: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Nov  7 02:17:46.272: INFO: Deleting all statefulset in ns statefulset-7964
Nov  7 02:17:46.365: INFO: Scaling statefulset ss to 0
Nov  7 02:17:46.646: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 02:17:46.739: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:17:47.050: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-7964" for this suite.
Nov  7 02:17:53.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:18:02.219: INFO: namespace statefulset-7964 deletion completed in 14.983318568s

• [SLOW TEST:78.567 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:18:02.222: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:18:13.463: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8073" for this suite.
Nov  7 02:18:20.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:18:28.776: INFO: namespace emptydir-wrapper-8073 deletion completed in 15.03801602s

• [SLOW TEST:26.554 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:18:28.776: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov  7 02:18:29.254: INFO: Waiting up to 5m0s for pod "pod-e3337f22-0104-11ea-aaa6-525400524259" in namespace "emptydir-8863" to be "success or failure"
Nov  7 02:18:29.348: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.149141ms
Nov  7 02:18:31.442: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187620376s
Nov  7 02:18:33.536: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281254739s
Nov  7 02:18:35.630: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375590954s
Nov  7 02:18:37.724: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469517141s
Nov  7 02:18:39.818: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.563331245s
STEP: Saw pod success
Nov  7 02:18:39.818: INFO: Pod "pod-e3337f22-0104-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:18:39.911: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-e3337f22-0104-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:18:40.109: INFO: Waiting for pod pod-e3337f22-0104-11ea-aaa6-525400524259 to disappear
Nov  7 02:18:40.202: INFO: Pod pod-e3337f22-0104-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:18:40.202: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8863" for this suite.
Nov  7 02:18:46.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:18:55.446: INFO: namespace emptydir-8863 deletion completed in 14.969051188s

• [SLOW TEST:26.670 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:18:55.446: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov  7 02:18:55.923: INFO: Waiting up to 5m0s for pod "pod-f3190e41-0104-11ea-aaa6-525400524259" in namespace "emptydir-4637" to be "success or failure"
Nov  7 02:18:56.017: INFO: Pod "pod-f3190e41-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.370448ms
Nov  7 02:18:58.111: INFO: Pod "pod-f3190e41-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18728583s
Nov  7 02:19:00.204: INFO: Pod "pod-f3190e41-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281048997s
Nov  7 02:19:02.298: INFO: Pod "pod-f3190e41-0104-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374556662s
Nov  7 02:19:04.392: INFO: Pod "pod-f3190e41-0104-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.468443169s
STEP: Saw pod success
Nov  7 02:19:04.392: INFO: Pod "pod-f3190e41-0104-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:19:04.485: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-f3190e41-0104-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:19:04.680: INFO: Waiting for pod pod-f3190e41-0104-11ea-aaa6-525400524259 to disappear
Nov  7 02:19:04.773: INFO: Pod pod-f3190e41-0104-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:19:04.773: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4637" for this suite.
Nov  7 02:19:11.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:19:20.034: INFO: namespace emptydir-4637 deletion completed in 14.985705373s

• [SLOW TEST:24.587 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:19:20.034: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Nov  7 02:19:21.483: INFO: created pod pod-service-account-defaultsa
Nov  7 02:19:21.483: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov  7 02:19:21.584: INFO: created pod pod-service-account-mountsa
Nov  7 02:19:21.584: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov  7 02:19:21.684: INFO: created pod pod-service-account-nomountsa
Nov  7 02:19:21.684: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov  7 02:19:21.785: INFO: created pod pod-service-account-defaultsa-mountspec
Nov  7 02:19:21.785: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov  7 02:19:21.884: INFO: created pod pod-service-account-mountsa-mountspec
Nov  7 02:19:21.884: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov  7 02:19:21.984: INFO: created pod pod-service-account-nomountsa-mountspec
Nov  7 02:19:21.984: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov  7 02:19:22.083: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov  7 02:19:22.083: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov  7 02:19:22.182: INFO: created pod pod-service-account-mountsa-nomountspec
Nov  7 02:19:22.183: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov  7 02:19:22.281: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov  7 02:19:22.281: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:19:22.281: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-6222" for this suite.
Nov  7 02:19:40.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:19:49.490: INFO: namespace svcaccounts-6222 deletion completed in 27.023589517s

• [SLOW TEST:29.455 seconds]
[sig-auth] ServiceAccounts
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:19:49.491: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov  7 02:20:12.733: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:12.835: INFO: Pod pod-with-prestop-http-hook still exists
Nov  7 02:20:14.835: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:14.929: INFO: Pod pod-with-prestop-http-hook still exists
Nov  7 02:20:16.835: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:16.929: INFO: Pod pod-with-prestop-http-hook still exists
Nov  7 02:20:18.836: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:18.929: INFO: Pod pod-with-prestop-http-hook still exists
Nov  7 02:20:20.836: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:20.930: INFO: Pod pod-with-prestop-http-hook still exists
Nov  7 02:20:22.836: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:22.929: INFO: Pod pod-with-prestop-http-hook still exists
Nov  7 02:20:24.835: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov  7 02:20:24.929: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:20:25.026: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-207" for this suite.
Nov  7 02:20:53.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:21:02.285: INFO: namespace container-lifecycle-hook-207 deletion completed in 36.980085988s

• [SLOW TEST:72.795 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:21:02.286: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:21:02.661: INFO: Creating ReplicaSet my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259
Nov  7 02:21:02.850: INFO: Pod name my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259: Found 1 pods out of 1
Nov  7 02:21:02.850: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259" is running
Nov  7 02:21:13.038: INFO: Pod "my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259-9w2hr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 02:20:27 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 02:20:27 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 02:20:27 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-07 02:20:27 +0000 UTC Reason: Message:}])
Nov  7 02:21:13.038: INFO: Trying to dial the pod
Nov  7 02:21:18.320: INFO: Controller my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259: Got expected result from replica 1 [my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259-9w2hr]: "my-hostname-basic-3eb33a53-0105-11ea-aaa6-525400524259-9w2hr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:21:18.320: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-9189" for this suite.
Nov  7 02:21:24.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:21:33.633: INFO: namespace replicaset-9189 deletion completed in 15.03769459s

• [SLOW TEST:31.347 seconds]
[sig-apps] ReplicaSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:21:33.634: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Nov  7 02:21:34.108: INFO: Waiting up to 5m0s for pod "var-expansion-51623992-0105-11ea-aaa6-525400524259" in namespace "var-expansion-5010" to be "success or failure"
Nov  7 02:21:34.201: INFO: Pod "var-expansion-51623992-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.643139ms
Nov  7 02:21:36.295: INFO: Pod "var-expansion-51623992-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187214816s
Nov  7 02:21:38.388: INFO: Pod "var-expansion-51623992-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280679826s
Nov  7 02:21:40.482: INFO: Pod "var-expansion-51623992-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373770603s
Nov  7 02:21:42.576: INFO: Pod "var-expansion-51623992-0105-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.468237652s
STEP: Saw pod success
Nov  7 02:21:42.576: INFO: Pod "var-expansion-51623992-0105-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:21:42.669: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod var-expansion-51623992-0105-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 02:21:42.866: INFO: Waiting for pod var-expansion-51623992-0105-11ea-aaa6-525400524259 to disappear
Nov  7 02:21:42.959: INFO: Pod var-expansion-51623992-0105-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:21:42.959: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-5010" for this suite.
Nov  7 02:21:49.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:21:58.159: INFO: namespace var-expansion-5010 deletion completed in 14.925692763s

• [SLOW TEST:24.526 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:21:58.160: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6000cd25-0105-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 02:21:58.732: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259" in namespace "projected-7954" to be "success or failure"
Nov  7 02:21:58.825: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.117808ms
Nov  7 02:22:00.919: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186675639s
Nov  7 02:22:03.014: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282048367s
Nov  7 02:22:05.107: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.375232029s
Nov  7 02:22:07.202: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.469708107s
Nov  7 02:22:09.296: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.563903448s
STEP: Saw pod success
Nov  7 02:22:09.296: INFO: Pod "pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:22:09.389: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov  7 02:22:09.586: INFO: Waiting for pod pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259 to disappear
Nov  7 02:22:09.679: INFO: Pod pod-projected-secrets-600f20bf-0105-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:22:09.679: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7954" for this suite.
Nov  7 02:22:16.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:22:24.875: INFO: namespace projected-7954 deletion completed in 14.921297899s

• [SLOW TEST:26.715 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:22:24.877: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:22:25.352: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259" in namespace "projected-3090" to be "success or failure"
Nov  7 02:22:25.445: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.033102ms
Nov  7 02:22:27.538: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186811066s
Nov  7 02:22:29.632: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280163967s
Nov  7 02:22:31.725: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373538269s
Nov  7 02:22:33.819: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.466980876s
Nov  7 02:22:35.912: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.560216944s
STEP: Saw pod success
Nov  7 02:22:35.912: INFO: Pod "downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:22:36.005: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:22:36.199: INFO: Waiting for pod downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259 to disappear
Nov  7 02:22:36.292: INFO: Pod downwardapi-volume-6fed4e42-0105-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:22:36.292: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3090" for this suite.
Nov  7 02:22:42.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:22:51.503: INFO: namespace projected-3090 deletion completed in 14.936262778s

• [SLOW TEST:26.626 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:22:51.504: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:22:52.534: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov  7 02:22:52.723: INFO: Number of nodes with available pods: 0
Nov  7 02:22:52.723: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov  7 02:22:53.101: INFO: Number of nodes with available pods: 0
Nov  7 02:22:53.101: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:22:54.195: INFO: Number of nodes with available pods: 0
Nov  7 02:22:54.195: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:22:55.196: INFO: Number of nodes with available pods: 0
Nov  7 02:22:55.196: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:22:56.195: INFO: Number of nodes with available pods: 0
Nov  7 02:22:56.195: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:22:57.195: INFO: Number of nodes with available pods: 0
Nov  7 02:22:57.195: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:22:58.194: INFO: Number of nodes with available pods: 0
Nov  7 02:22:58.195: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:22:59.194: INFO: Number of nodes with available pods: 0
Nov  7 02:22:59.194: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:00.205: INFO: Number of nodes with available pods: 0
Nov  7 02:23:00.205: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:01.195: INFO: Number of nodes with available pods: 0
Nov  7 02:23:01.195: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:02.194: INFO: Number of nodes with available pods: 1
Nov  7 02:23:02.195: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov  7 02:23:02.660: INFO: Number of nodes with available pods: 0
Nov  7 02:23:02.660: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov  7 02:23:02.848: INFO: Number of nodes with available pods: 0
Nov  7 02:23:02.848: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:03.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:03.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:04.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:04.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:05.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:05.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:06.941: INFO: Number of nodes with available pods: 0
Nov  7 02:23:06.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:07.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:07.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:08.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:08.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:09.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:09.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:10.941: INFO: Number of nodes with available pods: 0
Nov  7 02:23:10.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:11.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:11.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:12.942: INFO: Number of nodes with available pods: 0
Nov  7 02:23:12.942: INFO: Node ip-10-0-132-48.us-west-2.compute.internal is running more than one daemon pod
Nov  7 02:23:13.942: INFO: Number of nodes with available pods: 1
Nov  7 02:23:13.942: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3488, will wait for the garbage collector to delete the pods
Nov  7 02:23:14.417: INFO: Deleting DaemonSet.extensions daemon-set took: 95.841848ms
Nov  7 02:23:14.518: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.222129ms
Nov  7 02:23:17.511: INFO: Number of nodes with available pods: 0
Nov  7 02:23:17.511: INFO: Number of running nodes: 0, number of available pods: 0
Nov  7 02:23:17.604: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3488/daemonsets","resourceVersion":"108784"},"items":null}

Nov  7 02:23:17.696: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3488/pods","resourceVersion":"108784"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:23:18.438: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-3488" for this suite.
Nov  7 02:23:24.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:23:33.593: INFO: namespace daemonsets-3488 deletion completed in 14.971244166s

• [SLOW TEST:42.089 seconds]
[sig-apps] Daemon set [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:23:33.594: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov  7 02:23:34.069: INFO: Waiting up to 5m0s for pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259" in namespace "emptydir-4656" to be "success or failure"
Nov  7 02:23:34.162: INFO: Pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.231001ms
Nov  7 02:23:36.256: INFO: Pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186710183s
Nov  7 02:23:38.353: INFO: Pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28430718s
Nov  7 02:23:40.447: INFO: Pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378404019s
Nov  7 02:23:42.541: INFO: Pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.471777342s
STEP: Saw pod success
Nov  7 02:23:42.541: INFO: Pod "pod-98e2bfe9-0105-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:23:42.634: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-98e2bfe9-0105-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:23:42.829: INFO: Waiting for pod pod-98e2bfe9-0105-11ea-aaa6-525400524259 to disappear
Nov  7 02:23:42.922: INFO: Pod pod-98e2bfe9-0105-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:23:42.922: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-4656" for this suite.
Nov  7 02:23:49.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:23:58.124: INFO: namespace emptydir-4656 deletion completed in 14.927881449s

• [SLOW TEST:24.531 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:23:58.125: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-a781cbca-0105-11ea-aaa6-525400524259
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:23:58.591: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-859" for this suite.
Nov  7 02:24:04.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:24:13.705: INFO: namespace configmap-859 deletion completed in 15.019978476s

• [SLOW TEST:15.581 seconds]
[sig-node] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:24:13.708: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Nov  7 02:24:14.083: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Nov  7 02:24:14.281: INFO: Waiting for terminating namespaces to be deleted...
Nov  7 02:24:14.374: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Nov  7 02:24:14.565: INFO: managed-velero-operator-7bcd76bc7b-sbc6n from openshift-velero started at 2019-11-06 22:33:42 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container managed-velero-operator ready: true, restart count 0
Nov  7 02:24:14.565: INFO: dns-default-pld7m from openshift-dns started at 2019-11-06 22:25:08 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:24:14.565: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:24:14.565: INFO: osd-curated-redhat-operators-5c5d89cf9d-q48c8 from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Nov  7 02:24:14.565: INFO: dedicated-admin-operator-registry-jxtzh from openshift-dedicated-admin started at 2019-11-06 23:00:13 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container registry-server ready: true, restart count 0
Nov  7 02:24:14.565: INFO: sre-machine-api-status-exporter-1-mkpqx from openshift-monitoring started at 2019-11-06 22:33:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.565: INFO: builds-pruner-1573092000-8c6d4 from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:24:14.565: INFO: sre-machine-api-status-exporter-1-deploy from openshift-monitoring started at 2019-11-06 22:33:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:24:14.565: INFO: osd-curated-community-operators-54bddd79c9-4tgxv from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Nov  7 02:24:14.565: INFO: sdn-qkgb9 from openshift-sdn started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:24:14.565: INFO: router-default-867fb8468b-rck4m from openshift-ingress started at 2019-11-06 22:37:38 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container router ready: true, restart count 0
Nov  7 02:24:14.565: INFO: builds-pruner-1573084800-sbv5t from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:24:14.565: INFO: multus-m476d from openshift-multus started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:24:14.565: INFO: ovs-2cn5m from openshift-sdn started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:24:14.565: INFO: node-exporter-rqmvm from openshift-monitoring started at 2019-11-06 22:25:08 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.565: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:24:14.565: INFO: machine-config-daemon-d454m from openshift-machine-config-operator started at 2019-11-06 22:25:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:24:14.565: INFO: dedicated-admin-operator-7bcdbb466-j7rkh from openshift-dedicated-admin started at 2019-11-06 23:02:23 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container dedicated-admin-operator ready: true, restart count 0
Nov  7 02:24:14.565: INFO: deployments-pruner-1573088400-n24rg from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:24:14.565: INFO: tuned-zlvb7 from openshift-cluster-node-tuning-operator started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:24:14.565: INFO: sre-dns-latency-exporter-26s9f from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.565: INFO: osd-curated-certified-operators-5749fb9b6d-dhtws from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Nov  7 02:24:14.565: INFO: sre-stuck-ebs-vols-1-deploy from openshift-monitoring started at 2019-11-06 22:33:44 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:24:14.565: INFO: builds-pruner-1573088400-q7ps8 from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:24:14.565: INFO: node-ca-jmqmn from openshift-image-registry started at 2019-11-06 22:25:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:24:14.565: INFO: configure-alertmanager-operator-registry-j8p8d from openshift-monitoring started at 2019-11-06 22:33:33 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container registry-server ready: true, restart count 0
Nov  7 02:24:14.565: INFO: sre-ebs-iops-reporter-1-p8sd7 from openshift-monitoring started at 2019-11-06 22:34:00 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.565: INFO: sre-stuck-ebs-vols-1-l8s9n from openshift-monitoring started at 2019-11-06 22:34:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.565: INFO: configure-alertmanager-operator-7c54957bd5-rrc89 from openshift-monitoring started at 2019-11-06 22:34:30 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.565: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Nov  7 02:24:14.565: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-136-94.us-west-2.compute.internal before test
Nov  7 02:24:14.757: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:24:14.757: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:24:14.757: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:24:14.757: INFO: multus-qz978 from openshift-multus started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:24:14.757: INFO: sre-dns-latency-exporter-k4lcz from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.757: INFO: sre-ebs-iops-reporter-1-deploy from openshift-monitoring started at 2019-11-06 22:33:44 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:24:14.757: INFO: router-default-867fb8468b-ch6kh from openshift-ingress started at 2019-11-06 22:38:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container router ready: true, restart count 0
Nov  7 02:24:14.757: INFO: dns-default-fr54k from openshift-dns started at 2019-11-06 22:21:29 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:24:14.757: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:24:14.757: INFO: velero-665f8fd84c-npmsd from openshift-velero started at 2019-11-06 22:34:38 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container velero ready: true, restart count 0
Nov  7 02:24:14.757: INFO: node-exporter-v9klx from openshift-monitoring started at 2019-11-06 22:21:29 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.757: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:24:14.757: INFO: ovs-5sqtb from openshift-sdn started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:24:14.757: INFO: node-ca-sbwtm from openshift-image-registry started at 2019-11-06 22:22:10 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:24:14.757: INFO: prometheus-adapter-577cfd84b5-fscj8 from openshift-monitoring started at 2019-11-06 22:22:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov  7 02:24:14.757: INFO: sdn-7pz7q from openshift-sdn started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container sdn ready: true, restart count 1
Nov  7 02:24:14.757: INFO: image-pruner-1573088400-wzm2x from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:24:14.757: INFO: deployments-pruner-1573092000-bzwcd from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:24:14.757: INFO: tuned-8j9zw from openshift-cluster-node-tuning-operator started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:24:14.757: INFO: machine-config-daemon-vdp88 from openshift-machine-config-operator started at 2019-11-06 22:22:10 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:24:14.757: INFO: telemeter-client-75fd44c765-r4rcv from openshift-monitoring started at 2019-11-06 22:22:46 +0000 UTC (3 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.757: INFO: 	Container reload ready: true, restart count 0
Nov  7 02:24:14.757: INFO: 	Container telemeter-client ready: true, restart count 0
Nov  7 02:24:14.757: INFO: image-pruner-1573084800-dbs5b from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.757: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:24:14.757: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-139-168.us-west-2.compute.internal before test
Nov  7 02:24:14.862: INFO: multus-nqw4r from openshift-multus started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:24:14.862: INFO: machine-config-daemon-6npdl from openshift-machine-config-operator started at 2019-11-06 22:21:31 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:24:14.862: INFO: node-ca-mq5jd from openshift-image-registry started at 2019-11-06 22:21:31 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:24:14.862: INFO: image-pruner-1573092000-qw99r from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:24:14.862: INFO: sdn-prk8p from openshift-sdn started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:24:14.862: INFO: tuned-x8lhp from openshift-cluster-node-tuning-operator started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:24:14.862: INFO: node-exporter-2xsxn from openshift-monitoring started at 2019-11-06 22:20:51 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:24:14.862: INFO: sre-dns-latency-exporter-s8p4s from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.862: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:24:14.862: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-11-06 22:34:23 +0000 UTC (6 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container prometheus ready: true, restart count 1
Nov  7 02:24:14.862: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov  7 02:24:14.862: INFO: deployments-pruner-1573084800-ls92f from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:24:14.862: INFO: dns-default-dqmjz from openshift-dns started at 2019-11-06 22:20:51 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:24:14.862: INFO: ovs-b49js from openshift-sdn started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:24:14.862: INFO: image-registry-6464fd6f4d-zcr27 from openshift-image-registry started at 2019-11-06 22:21:37 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.862: INFO: 	Container registry ready: true, restart count 0
Nov  7 02:24:14.862: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-143-165.us-west-2.compute.internal before test
Nov  7 02:24:14.969: INFO: multus-8mnvs from openshift-multus started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:24:14.969: INFO: node-ca-xg4sn from openshift-image-registry started at 2019-11-06 22:21:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:24:14.969: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:24:14.969: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-11-06 22:34:23 +0000 UTC (6 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container prometheus ready: true, restart count 1
Nov  7 02:24:14.969: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov  7 02:24:14.969: INFO: node-exporter-bs4nw from openshift-monitoring started at 2019-11-06 22:20:55 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:24:14.969: INFO: sdn-cd2fd from openshift-sdn started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:24:14.969: INFO: prometheus-adapter-577cfd84b5-bf87d from openshift-monitoring started at 2019-11-06 22:22:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov  7 02:24:14.969: INFO: sre-dns-latency-exporter-r9xlc from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container main ready: true, restart count 0
Nov  7 02:24:14.969: INFO: grafana-69f4f95645-t98kn from openshift-monitoring started at 2019-11-06 22:21:59 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container grafana ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov  7 02:24:14.969: INFO: ovs-4g6fz from openshift-sdn started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:24:14.969: INFO: tuned-8sfnc from openshift-cluster-node-tuning-operator started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:24:14.969: INFO: kube-state-metrics-646c968f77-hl75k from openshift-monitoring started at 2019-11-06 22:21:35 +0000 UTC (3 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov  7 02:24:14.969: INFO: dns-default-7v8r4 from openshift-dns started at 2019-11-06 22:20:55 +0000 UTC (2 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:24:14.969: INFO: machine-config-daemon-6kfmf from openshift-machine-config-operator started at 2019-11-06 22:21:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:24:14.969: INFO: openshift-state-metrics-7f4bdfbdf9-xfb8l from openshift-monitoring started at 2019-11-06 22:21:35 +0000 UTC (3 container statuses recorded)
Nov  7 02:24:14.969: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov  7 02:24:14.969: INFO: 	Container openshift-state-metrics ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-10-0-132-48.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-136-94.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-139-168.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod tuned-8j9zw requesting resource cpu=10m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod tuned-8sfnc requesting resource cpu=10m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod tuned-x8lhp requesting resource cpu=10m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod tuned-zlvb7 requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod dedicated-admin-operator-7bcdbb466-j7rkh requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod dedicated-admin-operator-registry-jxtzh requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod dns-default-7v8r4 requesting resource cpu=110m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod dns-default-dqmjz requesting resource cpu=110m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod dns-default-fr54k requesting resource cpu=110m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod dns-default-pld7m requesting resource cpu=110m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod image-registry-6464fd6f4d-zcr27 requesting resource cpu=100m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-ca-jmqmn requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-ca-mq5jd requesting resource cpu=10m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-ca-sbwtm requesting resource cpu=10m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-ca-xg4sn requesting resource cpu=10m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod router-default-867fb8468b-ch6kh requesting resource cpu=100m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod router-default-867fb8468b-rck4m requesting resource cpu=100m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod machine-config-daemon-6kfmf requesting resource cpu=20m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod machine-config-daemon-6npdl requesting resource cpu=20m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod machine-config-daemon-d454m requesting resource cpu=20m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod machine-config-daemon-vdp88 requesting resource cpu=20m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod osd-curated-certified-operators-5749fb9b6d-dhtws requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod osd-curated-community-operators-54bddd79c9-4tgxv requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod osd-curated-redhat-operators-5c5d89cf9d-q48c8 requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod configure-alertmanager-operator-7c54957bd5-rrc89 requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod configure-alertmanager-operator-registry-j8p8d requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod grafana-69f4f95645-t98kn requesting resource cpu=100m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod kube-state-metrics-646c968f77-hl75k requesting resource cpu=30m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-exporter-2xsxn requesting resource cpu=10m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-exporter-bs4nw requesting resource cpu=10m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-exporter-rqmvm requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod node-exporter-v9klx requesting resource cpu=10m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod openshift-state-metrics-7f4bdfbdf9-xfb8l requesting resource cpu=120m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod prometheus-adapter-577cfd84b5-bf87d requesting resource cpu=10m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod prometheus-adapter-577cfd84b5-fscj8 requesting resource cpu=10m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod prometheus-k8s-0 requesting resource cpu=430m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod prometheus-k8s-1 requesting resource cpu=430m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-dns-latency-exporter-26s9f requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-dns-latency-exporter-k4lcz requesting resource cpu=0m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-dns-latency-exporter-r9xlc requesting resource cpu=0m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-dns-latency-exporter-s8p4s requesting resource cpu=0m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-ebs-iops-reporter-1-p8sd7 requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-machine-api-status-exporter-1-mkpqx requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sre-stuck-ebs-vols-1-l8s9n requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod telemeter-client-75fd44c765-r4rcv requesting resource cpu=10m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod multus-8mnvs requesting resource cpu=10m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod multus-m476d requesting resource cpu=10m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod multus-nqw4r requesting resource cpu=10m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod multus-qz978 requesting resource cpu=10m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod ovs-2cn5m requesting resource cpu=200m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod ovs-4g6fz requesting resource cpu=200m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod ovs-5sqtb requesting resource cpu=200m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod ovs-b49js requesting resource cpu=200m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sdn-7pz7q requesting resource cpu=100m on Node ip-10-0-136-94.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sdn-cd2fd requesting resource cpu=100m on Node ip-10-0-143-165.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sdn-prk8p requesting resource cpu=100m on Node ip-10-0-139-168.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod sdn-qkgb9 requesting resource cpu=100m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod managed-velero-operator-7bcd76bc7b-sbc6n requesting resource cpu=0m on Node ip-10-0-132-48.us-west-2.compute.internal
Nov  7 02:24:16.414: INFO: Pod velero-665f8fd84c-npmsd requesting resource cpu=0m on Node ip-10-0-136-94.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259.15d4c021c97bb180], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1849/filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259 to ip-10-0-132-48.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259.15d4c02395b25e1c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259.15d4c0239dc802c9], Reason = [Created], Message = [Created container filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259.15d4c0239eeced8f], Reason = [Started], Message = [Started container filler-pod-b22fa0ee-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b23f182d-0105-11ea-aaa6-525400524259.15d4c021cf5dfad6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1849/filler-pod-b23f182d-0105-11ea-aaa6-525400524259 to ip-10-0-136-94.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b23f182d-0105-11ea-aaa6-525400524259.15d4c0238c15790a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b23f182d-0105-11ea-aaa6-525400524259.15d4c02394fed337], Reason = [Created], Message = [Created container filler-pod-b23f182d-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b23f182d-0105-11ea-aaa6-525400524259.15d4c02396308042], Reason = [Started], Message = [Started container filler-pod-b23f182d-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b24e43aa-0105-11ea-aaa6-525400524259.15d4c021d54d1f2a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1849/filler-pod-b24e43aa-0105-11ea-aaa6-525400524259 to ip-10-0-139-168.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b24e43aa-0105-11ea-aaa6-525400524259.15d4c023bef7e336], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b24e43aa-0105-11ea-aaa6-525400524259.15d4c023c761eb94], Reason = [Created], Message = [Created container filler-pod-b24e43aa-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b24e43aa-0105-11ea-aaa6-525400524259.15d4c023c8734b57], Reason = [Started], Message = [Started container filler-pod-b24e43aa-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25d6e77-0105-11ea-aaa6-525400524259.15d4c021db3a13a2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1849/filler-pod-b25d6e77-0105-11ea-aaa6-525400524259 to ip-10-0-143-165.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25d6e77-0105-11ea-aaa6-525400524259.15d4c023b1b448d1], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25d6e77-0105-11ea-aaa6-525400524259.15d4c02431467db5], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25d6e77-0105-11ea-aaa6-525400524259.15d4c02438d68c6d], Reason = [Created], Message = [Created container filler-pod-b25d6e77-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b25d6e77-0105-11ea-aaa6-525400524259.15d4c02439e3397a], Reason = [Started], Message = [Started container filler-pod-b25d6e77-0105-11ea-aaa6-525400524259]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15d4c024de45df31], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) were unschedulable, 4 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-132-48.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-136-94.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-139-168.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-143-165.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:24:32.067: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-1849" for this suite.
Nov  7 02:24:38.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:24:47.244: INFO: namespace sched-pred-1849 deletion completed in 14.993062907s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:33.537 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:24:47.245: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Nov  7 02:24:47.620: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:24:59.068: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-3627" for this suite.
Nov  7 02:25:27.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:25:36.281: INFO: namespace init-container-3627 deletion completed in 36.938899473s

• [SLOW TEST:49.036 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:25:36.283: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6100.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6100.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 195.61.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.61.195_udp@PTR;check="$$(dig +tcp +noall +answer +search 195.61.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.61.195_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6100.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6100.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6100.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6100.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6100.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6100.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 195.61.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.61.195_udp@PTR;check="$$(dig +tcp +noall +answer +search 195.61.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.61.195_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov  7 02:25:47.347: INFO: Unable to read wheezy_udp@dns-test-service.dns-6100.svc.cluster.local from pod dns-6100/dns-test-e221b9ff-0105-11ea-aaa6-525400524259: the server could not find the requested resource (get pods dns-test-e221b9ff-0105-11ea-aaa6-525400524259)
Nov  7 02:25:47.535: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6100.svc.cluster.local from pod dns-6100/dns-test-e221b9ff-0105-11ea-aaa6-525400524259: the server could not find the requested resource (get pods dns-test-e221b9ff-0105-11ea-aaa6-525400524259)
Nov  7 02:25:49.129: INFO: Lookups using dns-6100/dns-test-e221b9ff-0105-11ea-aaa6-525400524259 failed for: [wheezy_udp@dns-test-service.dns-6100.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6100.svc.cluster.local]

Nov  7 02:25:56.004: INFO: DNS probes using dns-6100/dns-test-e221b9ff-0105-11ea-aaa6-525400524259 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:25:56.318: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-6100" for this suite.
Nov  7 02:26:02.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:26:11.455: INFO: namespace dns-6100 deletion completed in 14.950891169s

• [SLOW TEST:35.173 seconds]
[sig-network] DNS
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:26:11.455: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Nov  7 02:26:11.829: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Nov  7 02:26:11.829: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-6647'
Nov  7 02:26:21.273: INFO: stderr: ""
Nov  7 02:26:21.273: INFO: stdout: "service/redis-slave created\n"
Nov  7 02:26:21.274: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Nov  7 02:26:21.274: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-6647'
Nov  7 02:26:22.281: INFO: stderr: ""
Nov  7 02:26:22.281: INFO: stdout: "service/redis-master created\n"
Nov  7 02:26:22.282: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov  7 02:26:22.282: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-6647'
Nov  7 02:26:23.866: INFO: stderr: ""
Nov  7 02:26:23.866: INFO: stdout: "service/frontend created\n"
Nov  7 02:26:23.868: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Nov  7 02:26:23.868: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-6647'
Nov  7 02:26:25.339: INFO: stderr: ""
Nov  7 02:26:25.339: INFO: stdout: "deployment.apps/frontend created\n"
Nov  7 02:26:25.339: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov  7 02:26:25.339: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-6647'
Nov  7 02:26:26.743: INFO: stderr: ""
Nov  7 02:26:26.743: INFO: stdout: "deployment.apps/redis-master created\n"
Nov  7 02:26:26.744: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Nov  7 02:26:26.744: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-6647'
Nov  7 02:26:28.175: INFO: stderr: ""
Nov  7 02:26:28.175: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Nov  7 02:26:28.175: INFO: Waiting for all frontend pods to be Running.
Nov  7 02:26:53.276: INFO: Waiting for frontend to serve content.
Nov  7 02:26:58.386: INFO: Trying to add a new entry to the guestbook.
Nov  7 02:26:58.488: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov  7 02:27:03.600: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6647'
Nov  7 02:27:04.168: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:27:04.168: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov  7 02:27:04.168: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6647'
Nov  7 02:27:04.714: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:27:04.714: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov  7 02:27:04.714: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6647'
Nov  7 02:27:05.250: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:27:05.250: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov  7 02:27:05.250: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6647'
Nov  7 02:27:05.787: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:27:05.787: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov  7 02:27:05.787: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6647'
Nov  7 02:27:06.310: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:27:06.310: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov  7 02:27:06.310: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-6647'
Nov  7 02:27:06.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:27:06.830: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:27:06.830: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6647" for this suite.
Nov  7 02:27:47.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:27:56.050: INFO: namespace kubectl-6647 deletion completed in 48.944655606s

• [SLOW TEST:104.596 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:27:56.054: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Nov  7 02:27:56.433: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Nov  7 02:27:56.623: INFO: Waiting for terminating namespaces to be deleted...
Nov  7 02:27:56.718: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Nov  7 02:27:56.908: INFO: multus-m476d from openshift-multus started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:27:56.909: INFO: ovs-2cn5m from openshift-sdn started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:27:56.909: INFO: node-exporter-rqmvm from openshift-monitoring started at 2019-11-06 22:25:08 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:56.909: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:27:56.909: INFO: machine-config-daemon-d454m from openshift-machine-config-operator started at 2019-11-06 22:25:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:27:56.909: INFO: dedicated-admin-operator-7bcdbb466-j7rkh from openshift-dedicated-admin started at 2019-11-06 23:02:23 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container dedicated-admin-operator ready: true, restart count 0
Nov  7 02:27:56.909: INFO: deployments-pruner-1573088400-n24rg from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:27:56.909: INFO: tuned-zlvb7 from openshift-cluster-node-tuning-operator started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:27:56.909: INFO: sre-dns-latency-exporter-26s9f from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:56.909: INFO: osd-curated-certified-operators-5749fb9b6d-dhtws from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Nov  7 02:27:56.909: INFO: sre-stuck-ebs-vols-1-deploy from openshift-monitoring started at 2019-11-06 22:33:44 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:27:56.909: INFO: builds-pruner-1573088400-q7ps8 from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:27:56.909: INFO: node-ca-jmqmn from openshift-image-registry started at 2019-11-06 22:25:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:27:56.909: INFO: configure-alertmanager-operator-registry-j8p8d from openshift-monitoring started at 2019-11-06 22:33:33 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container registry-server ready: true, restart count 0
Nov  7 02:27:56.909: INFO: sre-ebs-iops-reporter-1-p8sd7 from openshift-monitoring started at 2019-11-06 22:34:00 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:56.909: INFO: sre-stuck-ebs-vols-1-l8s9n from openshift-monitoring started at 2019-11-06 22:34:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:56.909: INFO: configure-alertmanager-operator-7c54957bd5-rrc89 from openshift-monitoring started at 2019-11-06 22:34:30 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Nov  7 02:27:56.909: INFO: managed-velero-operator-7bcd76bc7b-sbc6n from openshift-velero started at 2019-11-06 22:33:42 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container managed-velero-operator ready: true, restart count 0
Nov  7 02:27:56.909: INFO: dns-default-pld7m from openshift-dns started at 2019-11-06 22:25:08 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:27:56.909: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:27:56.909: INFO: osd-curated-redhat-operators-5c5d89cf9d-q48c8 from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Nov  7 02:27:56.909: INFO: dedicated-admin-operator-registry-jxtzh from openshift-dedicated-admin started at 2019-11-06 23:00:13 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container registry-server ready: true, restart count 0
Nov  7 02:27:56.909: INFO: sre-machine-api-status-exporter-1-mkpqx from openshift-monitoring started at 2019-11-06 22:33:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:56.909: INFO: builds-pruner-1573092000-8c6d4 from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:27:56.909: INFO: sre-machine-api-status-exporter-1-deploy from openshift-monitoring started at 2019-11-06 22:33:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:27:56.909: INFO: osd-curated-community-operators-54bddd79c9-4tgxv from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Nov  7 02:27:56.909: INFO: sdn-qkgb9 from openshift-sdn started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:27:56.909: INFO: router-default-867fb8468b-rck4m from openshift-ingress started at 2019-11-06 22:37:38 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container router ready: true, restart count 0
Nov  7 02:27:56.909: INFO: builds-pruner-1573084800-sbv5t from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:56.909: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:27:56.909: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-136-94.us-west-2.compute.internal before test
Nov  7 02:27:57.007: INFO: dns-default-fr54k from openshift-dns started at 2019-11-06 22:21:29 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:27:57.007: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:27:57.007: INFO: velero-665f8fd84c-npmsd from openshift-velero started at 2019-11-06 22:34:38 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container velero ready: true, restart count 0
Nov  7 02:27:57.007: INFO: node-exporter-v9klx from openshift-monitoring started at 2019-11-06 22:21:29 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:57.007: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:27:57.007: INFO: ovs-5sqtb from openshift-sdn started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:27:57.007: INFO: node-ca-sbwtm from openshift-image-registry started at 2019-11-06 22:22:10 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:27:57.007: INFO: prometheus-adapter-577cfd84b5-fscj8 from openshift-monitoring started at 2019-11-06 22:22:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov  7 02:27:57.007: INFO: sdn-7pz7q from openshift-sdn started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container sdn ready: true, restart count 1
Nov  7 02:27:57.007: INFO: image-pruner-1573088400-wzm2x from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:27:57.007: INFO: deployments-pruner-1573092000-bzwcd from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:27:57.007: INFO: image-pruner-1573084800-dbs5b from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:27:57.007: INFO: tuned-8j9zw from openshift-cluster-node-tuning-operator started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:27:57.007: INFO: machine-config-daemon-vdp88 from openshift-machine-config-operator started at 2019-11-06 22:22:10 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:27:57.007: INFO: telemeter-client-75fd44c765-r4rcv from openshift-monitoring started at 2019-11-06 22:22:46 +0000 UTC (3 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:57.007: INFO: 	Container reload ready: true, restart count 0
Nov  7 02:27:57.007: INFO: 	Container telemeter-client ready: true, restart count 0
Nov  7 02:27:57.007: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:27:57.007: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:27:57.007: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:27:57.007: INFO: router-default-867fb8468b-ch6kh from openshift-ingress started at 2019-11-06 22:38:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container router ready: true, restart count 0
Nov  7 02:27:57.007: INFO: multus-qz978 from openshift-multus started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:27:57.007: INFO: sre-dns-latency-exporter-k4lcz from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:57.007: INFO: sre-ebs-iops-reporter-1-deploy from openshift-monitoring started at 2019-11-06 22:33:44 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.007: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:27:57.007: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-139-168.us-west-2.compute.internal before test
Nov  7 02:27:57.106: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-11-06 22:34:23 +0000 UTC (6 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container prometheus ready: true, restart count 1
Nov  7 02:27:57.106: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov  7 02:27:57.106: INFO: sre-dns-latency-exporter-s8p4s from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:57.106: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:27:57.106: INFO: image-registry-6464fd6f4d-zcr27 from openshift-image-registry started at 2019-11-06 22:21:37 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container registry ready: true, restart count 0
Nov  7 02:27:57.106: INFO: deployments-pruner-1573084800-ls92f from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:27:57.106: INFO: dns-default-dqmjz from openshift-dns started at 2019-11-06 22:20:51 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:27:57.106: INFO: ovs-b49js from openshift-sdn started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:27:57.106: INFO: multus-nqw4r from openshift-multus started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:27:57.106: INFO: machine-config-daemon-6npdl from openshift-machine-config-operator started at 2019-11-06 22:21:31 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:27:57.106: INFO: node-exporter-2xsxn from openshift-monitoring started at 2019-11-06 22:20:51 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:27:57.106: INFO: node-ca-mq5jd from openshift-image-registry started at 2019-11-06 22:21:31 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:27:57.106: INFO: image-pruner-1573092000-qw99r from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:27:57.106: INFO: sdn-prk8p from openshift-sdn started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:27:57.106: INFO: tuned-x8lhp from openshift-cluster-node-tuning-operator started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.106: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:27:57.106: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-143-165.us-west-2.compute.internal before test
Nov  7 02:27:57.205: INFO: dns-default-7v8r4 from openshift-dns started at 2019-11-06 22:20:55 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:27:57.205: INFO: machine-config-daemon-6kfmf from openshift-machine-config-operator started at 2019-11-06 22:21:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:27:57.205: INFO: openshift-state-metrics-7f4bdfbdf9-xfb8l from openshift-monitoring started at 2019-11-06 22:21:35 +0000 UTC (3 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov  7 02:27:57.205: INFO: multus-8mnvs from openshift-multus started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:27:57.205: INFO: node-ca-xg4sn from openshift-image-registry started at 2019-11-06 22:21:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:27:57.205: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:27:57.205: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-11-06 22:34:23 +0000 UTC (6 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container prometheus ready: true, restart count 1
Nov  7 02:27:57.205: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov  7 02:27:57.205: INFO: node-exporter-bs4nw from openshift-monitoring started at 2019-11-06 22:20:55 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:27:57.205: INFO: sdn-cd2fd from openshift-sdn started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:27:57.205: INFO: prometheus-adapter-577cfd84b5-bf87d from openshift-monitoring started at 2019-11-06 22:22:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov  7 02:27:57.205: INFO: sre-dns-latency-exporter-r9xlc from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container main ready: true, restart count 0
Nov  7 02:27:57.205: INFO: ovs-4g6fz from openshift-sdn started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:27:57.205: INFO: tuned-8sfnc from openshift-cluster-node-tuning-operator started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:27:57.205: INFO: kube-state-metrics-646c968f77-hl75k from openshift-monitoring started at 2019-11-06 22:21:35 +0000 UTC (3 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov  7 02:27:57.205: INFO: grafana-69f4f95645-t98kn from openshift-monitoring started at 2019-11-06 22:21:59 +0000 UTC (2 container statuses recorded)
Nov  7 02:27:57.205: INFO: 	Container grafana ready: true, restart count 0
Nov  7 02:27:57.205: INFO: 	Container grafana-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15d4c0554e40b8cd], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) were unschedulable, 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:27:58.886: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-2161" for this suite.
Nov  7 02:28:05.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:28:13.907: INFO: namespace sched-pred-2161 deletion completed in 14.926396094s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:17.854 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:28:13.909: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8132
Nov  7 02:28:22.570: INFO: Started pod liveness-exec in namespace container-probe-8132
STEP: checking the pod's current state and verifying that restartCount is present
Nov  7 02:28:22.669: INFO: Initial restart count of pod liveness-exec is 0
Nov  7 02:29:17.193: INFO: Restart count of pod container-probe-8132/liveness-exec is now 1 (54.523619574s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:29:17.292: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-8132" for this suite.
Nov  7 02:29:23.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:29:32.573: INFO: namespace container-probe-8132 deletion completed in 15.007320597s

• [SLOW TEST:78.664 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:29:32.574: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:29:33.134: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov  7 02:29:43.319: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov  7 02:29:45.413: INFO: Creating deployment "test-rollover-deployment"
Nov  7 02:29:45.601: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov  7 02:29:45.694: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov  7 02:29:45.884: INFO: Ensure that both replica sets have 1 created replica
Nov  7 02:29:46.069: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov  7 02:29:46.257: INFO: Updating deployment test-rollover-deployment
Nov  7 02:29:46.257: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov  7 02:29:46.350: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov  7 02:29:46.535: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov  7 02:29:46.721: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:46.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690551, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:29:48.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:48.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690551, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:29:50.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:50.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690551, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:29:52.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:52.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690551, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:29:54.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:54.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690559, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:29:56.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:56.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690559, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:29:58.908: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:29:58.908: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690559, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:30:00.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:30:00.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690559, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:30:02.907: INFO: all replica sets need to contain the pod-template-hash label
Nov  7 02:30:02.907: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690559, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708690550, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 02:30:04.907: INFO: 
Nov  7 02:30:04.907: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Nov  7 02:30:05.186: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3441,SelfLink:/apis/apps/v1/namespaces/deployment-3441/deployments/test-rollover-deployment,UID:6149ddf6-0106-11ea-98c4-0655ae4e8e56,ResourceVersion:111842,Generation:2,CreationTimestamp:2019-11-07 02:29:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-07 02:29:10 +0000 UTC 2019-11-07 02:29:10 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-07 02:29:29 +0000 UTC 2019-11-07 02:29:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov  7 02:30:05.279: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-3441,SelfLink:/apis/apps/v1/namespaces/deployment-3441/replicasets/test-rollover-deployment-659c699649,UID:61bcdfb5-0106-11ea-98c4-0655ae4e8e56,ResourceVersion:111831,Generation:2,CreationTimestamp:2019-11-07 02:29:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 6149ddf6-0106-11ea-98c4-0655ae4e8e56 0xc0036986e7 0xc0036986e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov  7 02:30:05.280: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov  7 02:30:05.280: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3441,SelfLink:/apis/apps/v1/namespaces/deployment-3441/replicasets/test-rollover-controller,UID:59dbb4ac-0106-11ea-98c4-0655ae4e8e56,ResourceVersion:111840,Generation:2,CreationTimestamp:2019-11-07 02:28:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 6149ddf6-0106-11ea-98c4-0655ae4e8e56 0xc003698617 0xc003698618}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov  7 02:30:05.280: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-3441,SelfLink:/apis/apps/v1/namespaces/deployment-3441/replicasets/test-rollover-deployment-7b45b6464,UID:614b97e1-0106-11ea-98c4-0655ae4e8e56,ResourceVersion:111734,Generation:2,CreationTimestamp:2019-11-07 02:29:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 6149ddf6-0106-11ea-98c4-0655ae4e8e56 0xc0036987b0 0xc0036987b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov  7 02:30:05.373: INFO: Pod "test-rollover-deployment-659c699649-7sh76" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-7sh76,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-3441,SelfLink:/api/v1/namespaces/deployment-3441/pods/test-rollover-deployment-659c699649-7sh76,UID:61c18037-0106-11ea-98c4-0655ae4e8e56,ResourceVersion:111783,Generation:0,CreationTimestamp:2019-11-07 02:29:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.129.2.55"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 61bcdfb5-0106-11ea-98c4-0655ae4e8e56 0xc003036227 0xc003036228}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pjdmq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pjdmq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pjdmq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-136-94.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-2fvqf}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003036290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030362b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:29:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:29:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:29:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:29:10 +0000 UTC  }],Message:,Reason:,HostIP:10.0.136.94,PodIP:10.129.2.55,StartTime:2019-11-07 02:29:10 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-07 02:29:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://386c6d98cf8293c00a9f1de469bf917e235beaf612531ac770d9da0ae376d82f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:30:05.373: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-3441" for this suite.
Nov  7 02:30:11.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:30:20.515: INFO: namespace deployment-3441 deletion completed in 14.866779349s

• [SLOW TEST:47.941 seconds]
[sig-apps] Deployment
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:30:20.515: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9309
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  7 02:30:20.883: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Nov  7 02:30:57.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.181:8080/dial?request=hostName&protocol=udp&host=10.128.2.31&port=8081&tries=1'] Namespace:pod-network-test-9309 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:30:57.047: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:30:57.839: INFO: Waiting for endpoints: map[]
Nov  7 02:30:57.930: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.181:8080/dial?request=hostName&protocol=udp&host=10.131.0.49&port=8081&tries=1'] Namespace:pod-network-test-9309 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:30:57.930: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:30:58.625: INFO: Waiting for endpoints: map[]
Nov  7 02:30:58.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.181:8080/dial?request=hostName&protocol=udp&host=10.129.2.56&port=8081&tries=1'] Namespace:pod-network-test-9309 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:30:58.718: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:30:59.452: INFO: Waiting for endpoints: map[]
Nov  7 02:30:59.545: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.181:8080/dial?request=hostName&protocol=udp&host=10.130.2.180&port=8081&tries=1'] Namespace:pod-network-test-9309 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:30:59.545: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:31:00.281: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:31:00.282: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-9309" for this suite.
Nov  7 02:31:06.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:31:15.452: INFO: namespace pod-network-test-9309 deletion completed in 14.897950059s

• [SLOW TEST:54.938 seconds]
[sig-network] Networking
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:31:15.453: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:31:15.914: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-ac3a34b3-0106-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ac3a34b3-0106-11ea-aaa6-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:32:42.077: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3260" for this suite.
Nov  7 02:32:54.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:33:03.254: INFO: namespace configmap-3260 deletion completed in 20.904521719s

• [SLOW TEST:107.801 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:33:03.255: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1107 02:33:44.455753    5393 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov  7 02:33:44.455: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:33:44.455: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-8743" for this suite.
Nov  7 02:33:50.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:33:59.410: INFO: namespace gc-8743 deletion completed in 14.861941069s

• [SLOW TEST:56.156 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:33:59.412: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov  7 02:33:59.963: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:34:21.227: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-8795" for this suite.
Nov  7 02:34:27.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:34:36.328: INFO: namespace pods-8795 deletion completed in 14.829341794s

• [SLOW TEST:36.916 seconds]
[k8s.io] Pods
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:34:36.328: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Nov  7 02:34:47.877: INFO: Successfully updated pod "labelsupdate23e77cb1-0107-11ea-aaa6-525400524259"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:34:50.066: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2962" for this suite.
Nov  7 02:35:02.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:35:11.181: INFO: namespace projected-2962 deletion completed in 20.842773785s

• [SLOW TEST:34.852 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:35:11.182: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7614
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Nov  7 02:35:11.831: INFO: Found 1 stateful pods, waiting for 3
Nov  7 02:35:21.924: INFO: Found 2 stateful pods, waiting for 3
Nov  7 02:35:31.924: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:35:31.924: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:35:31.924: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Nov  7 02:35:41.924: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:35:41.924: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:35:41.924: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov  7 02:35:42.201: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7614 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 02:35:43.417: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 02:35:43.417: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 02:35:43.417: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Nov  7 02:35:43.799: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov  7 02:35:44.074: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7614 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 02:35:45.245: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov  7 02:35:45.245: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 02:35:45.245: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 02:35:55.798: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
Nov  7 02:35:55.798: INFO: Waiting for Pod statefulset-7614/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 02:35:55.798: INFO: Waiting for Pod statefulset-7614/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 02:36:05.982: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
Nov  7 02:36:05.982: INFO: Waiting for Pod statefulset-7614/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 02:36:05.982: INFO: Waiting for Pod statefulset-7614/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 02:36:15.984: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
Nov  7 02:36:15.984: INFO: Waiting for Pod statefulset-7614/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov  7 02:36:25.984: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
STEP: Rolling back to a previous revision
Nov  7 02:36:35.987: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7614 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov  7 02:36:37.179: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov  7 02:36:37.179: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov  7 02:36:37.179: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov  7 02:36:47.745: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov  7 02:36:48.022: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig exec --namespace=statefulset-7614 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov  7 02:36:49.180: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov  7 02:36:49.180: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov  7 02:36:49.180: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov  7 02:36:59.735: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
Nov  7 02:36:59.735: INFO: Waiting for Pod statefulset-7614/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Nov  7 02:36:59.735: INFO: Waiting for Pod statefulset-7614/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Nov  7 02:37:09.922: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
Nov  7 02:37:09.923: INFO: Waiting for Pod statefulset-7614/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Nov  7 02:37:09.923: INFO: Waiting for Pod statefulset-7614/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Nov  7 02:37:19.922: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
Nov  7 02:37:19.922: INFO: Waiting for Pod statefulset-7614/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Nov  7 02:37:29.921: INFO: Waiting for StatefulSet statefulset-7614/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Nov  7 02:37:39.922: INFO: Deleting all statefulset in ns statefulset-7614
Nov  7 02:37:40.015: INFO: Scaling statefulset ss2 to 0
Nov  7 02:38:00.387: INFO: Waiting for statefulset status.replicas updated to 0
Nov  7 02:38:00.479: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:38:00.760: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-7614" for this suite.
Nov  7 02:38:07.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:38:15.830: INFO: namespace statefulset-7614 deletion completed in 14.886180384s

• [SLOW TEST:184.648 seconds]
[sig-apps] StatefulSet
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:38:15.832: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-a6bd1ddb-0107-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 02:38:16.396: INFO: Waiting up to 5m0s for pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259" in namespace "secrets-9015" to be "success or failure"
Nov  7 02:38:16.488: INFO: Pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.97276ms
Nov  7 02:38:18.581: INFO: Pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184371551s
Nov  7 02:38:20.677: INFO: Pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28041233s
Nov  7 02:38:22.769: INFO: Pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372806673s
Nov  7 02:38:24.861: INFO: Pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.465193772s
STEP: Saw pod success
Nov  7 02:38:24.861: INFO: Pod "pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:38:24.953: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 02:38:25.148: INFO: Waiting for pod pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259 to disappear
Nov  7 02:38:25.240: INFO: Pod pod-secrets-a6cb506c-0107-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:38:25.240: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9015" for this suite.
Nov  7 02:38:31.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:38:40.346: INFO: namespace secrets-9015 deletion completed in 14.835266391s

• [SLOW TEST:24.514 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:38:40.347: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-dqtk
STEP: Creating a pod to test atomic-volume-subpath
Nov  7 02:38:41.005: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dqtk" in namespace "subpath-3066" to be "success or failure"
Nov  7 02:38:41.098: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 92.478884ms
Nov  7 02:38:43.190: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185144741s
Nov  7 02:38:45.283: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278198924s
Nov  7 02:38:47.376: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370605701s
Nov  7 02:38:49.468: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463310293s
Nov  7 02:38:51.561: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 10.555575448s
Nov  7 02:38:53.653: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 12.648008001s
Nov  7 02:38:55.747: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 14.74206315s
Nov  7 02:38:57.839: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 16.83417916s
Nov  7 02:38:59.936: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 18.930455076s
Nov  7 02:39:02.028: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 21.022718153s
Nov  7 02:39:04.123: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 23.117835989s
Nov  7 02:39:06.218: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 25.212864302s
Nov  7 02:39:08.312: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Running", Reason="", readiness=true. Elapsed: 27.306541053s
Nov  7 02:39:10.404: INFO: Pod "pod-subpath-test-downwardapi-dqtk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.399336871s
STEP: Saw pod success
Nov  7 02:39:10.405: INFO: Pod "pod-subpath-test-downwardapi-dqtk" satisfied condition "success or failure"
Nov  7 02:39:10.498: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-subpath-test-downwardapi-dqtk container test-container-subpath-downwardapi-dqtk: <nil>
STEP: delete the pod
Nov  7 02:39:10.693: INFO: Waiting for pod pod-subpath-test-downwardapi-dqtk to disappear
Nov  7 02:39:10.785: INFO: Pod pod-subpath-test-downwardapi-dqtk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dqtk
Nov  7 02:39:10.786: INFO: Deleting pod "pod-subpath-test-downwardapi-dqtk" in namespace "subpath-3066"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:39:10.879: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-3066" for this suite.
Nov  7 02:39:17.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:39:25.987: INFO: namespace subpath-3066 deletion completed in 14.835005053s

• [SLOW TEST:45.641 seconds]
[sig-storage] Subpath
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:39:25.989: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Nov  7 02:39:26.469: INFO: Waiting up to 5m0s for pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259" in namespace "containers-9939" to be "success or failure"
Nov  7 02:39:26.561: INFO: Pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.995852ms
Nov  7 02:39:28.654: INFO: Pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184725954s
Nov  7 02:39:30.746: INFO: Pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277322815s
Nov  7 02:39:32.839: INFO: Pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370137878s
Nov  7 02:39:34.931: INFO: Pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.462462999s
STEP: Saw pod success
Nov  7 02:39:34.932: INFO: Pod "client-containers-d08f1886-0107-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:39:35.024: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod client-containers-d08f1886-0107-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:39:35.225: INFO: Waiting for pod client-containers-d08f1886-0107-11ea-aaa6-525400524259 to disappear
Nov  7 02:39:35.316: INFO: Pod client-containers-d08f1886-0107-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:39:35.316: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-9939" for this suite.
Nov  7 02:39:41.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:39:50.504: INFO: namespace containers-9939 deletion completed in 14.916020766s

• [SLOW TEST:24.514 seconds]
[k8s.io] Docker Containers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:39:50.504: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov  7 02:40:09.817: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  7 02:40:09.909: INFO: Pod pod-with-poststart-http-hook still exists
Nov  7 02:40:11.909: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov  7 02:40:12.001: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:40:12.003: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9047" for this suite.
Nov  7 02:40:24.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:40:33.171: INFO: namespace container-lifecycle-hook-9047 deletion completed in 20.896759539s

• [SLOW TEST:42.667 seconds]
[k8s.io] Container Lifecycle Hook
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:40:33.172: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-f8997bbc-0107-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 02:40:33.740: INFO: Waiting up to 5m0s for pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259" in namespace "secrets-5318" to be "success or failure"
Nov  7 02:40:33.832: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.808965ms
Nov  7 02:40:35.924: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183918696s
Nov  7 02:40:38.017: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276527296s
Nov  7 02:40:40.109: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369198018s
Nov  7 02:40:42.202: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.461799381s
Nov  7 02:40:44.296: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.555540612s
STEP: Saw pod success
Nov  7 02:40:44.296: INFO: Pod "pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:40:44.387: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 02:40:44.580: INFO: Waiting for pod pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259 to disappear
Nov  7 02:40:44.672: INFO: Pod pod-secrets-f8a7ccd0-0107-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:40:44.672: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5318" for this suite.
Nov  7 02:40:51.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:40:59.879: INFO: namespace secrets-5318 deletion completed in 14.935525881s

• [SLOW TEST:26.707 seconds]
[sig-storage] Secrets
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:40:59.879: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Nov  7 02:41:00.248: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-3865'
Nov  7 02:41:05.547: INFO: stderr: ""
Nov  7 02:41:05.547: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov  7 02:41:05.547: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3865'
Nov  7 02:41:06.000: INFO: stderr: ""
Nov  7 02:41:06.000: INFO: stdout: "update-demo-nautilus-c6dhq update-demo-nautilus-f8m2x "
Nov  7 02:41:06.000: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-c6dhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3865'
Nov  7 02:41:06.431: INFO: stderr: ""
Nov  7 02:41:06.431: INFO: stdout: ""
Nov  7 02:41:06.431: INFO: update-demo-nautilus-c6dhq is created but not running
Nov  7 02:41:11.431: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3865'
Nov  7 02:41:11.874: INFO: stderr: ""
Nov  7 02:41:11.874: INFO: stdout: "update-demo-nautilus-c6dhq update-demo-nautilus-f8m2x "
Nov  7 02:41:11.874: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-c6dhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3865'
Nov  7 02:41:12.325: INFO: stderr: ""
Nov  7 02:41:12.325: INFO: stdout: ""
Nov  7 02:41:12.325: INFO: update-demo-nautilus-c6dhq is created but not running
Nov  7 02:41:17.326: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3865'
Nov  7 02:41:17.779: INFO: stderr: ""
Nov  7 02:41:17.779: INFO: stdout: "update-demo-nautilus-c6dhq update-demo-nautilus-f8m2x "
Nov  7 02:41:17.780: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-c6dhq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3865'
Nov  7 02:41:18.229: INFO: stderr: ""
Nov  7 02:41:18.229: INFO: stdout: "true"
Nov  7 02:41:18.229: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-c6dhq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3865'
Nov  7 02:41:18.683: INFO: stderr: ""
Nov  7 02:41:18.683: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:41:18.683: INFO: validating pod update-demo-nautilus-c6dhq
Nov  7 02:41:18.779: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:41:18.779: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:41:18.779: INFO: update-demo-nautilus-c6dhq is verified up and running
Nov  7 02:41:18.779: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-f8m2x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3865'
Nov  7 02:41:19.233: INFO: stderr: ""
Nov  7 02:41:19.234: INFO: stdout: "true"
Nov  7 02:41:19.234: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods update-demo-nautilus-f8m2x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3865'
Nov  7 02:41:19.658: INFO: stderr: ""
Nov  7 02:41:19.658: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov  7 02:41:19.658: INFO: validating pod update-demo-nautilus-f8m2x
Nov  7 02:41:19.753: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov  7 02:41:19.753: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov  7 02:41:19.753: INFO: update-demo-nautilus-f8m2x is verified up and running
STEP: using delete to clean up resources
Nov  7 02:41:19.753: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-3865'
Nov  7 02:41:20.269: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov  7 02:41:20.269: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov  7 02:41:20.269: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3865'
Nov  7 02:41:20.823: INFO: stderr: "No resources found.\n"
Nov  7 02:41:20.823: INFO: stdout: ""
Nov  7 02:41:20.823: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig get pods -l name=update-demo --namespace=kubectl-3865 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov  7 02:41:21.261: INFO: stderr: ""
Nov  7 02:41:21.261: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:41:21.261: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3865" for this suite.
Nov  7 02:41:27.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:41:36.367: INFO: namespace kubectl-3865 deletion completed in 14.834097476s

• [SLOW TEST:36.488 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:41:36.367: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-1e44555b-0108-11ea-aaa6-525400524259
STEP: Creating a pod to test consume secrets
Nov  7 02:41:36.932: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259" in namespace "projected-8755" to be "success or failure"
Nov  7 02:41:37.024: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.873559ms
Nov  7 02:41:39.116: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183977685s
Nov  7 02:41:41.209: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277285361s
Nov  7 02:41:43.302: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3699252s
Nov  7 02:41:45.394: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462102351s
Nov  7 02:41:47.486: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.554170012s
STEP: Saw pod success
Nov  7 02:41:47.486: INFO: Pod "pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:41:47.578: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259 container secret-volume-test: <nil>
STEP: delete the pod
Nov  7 02:41:47.774: INFO: Waiting for pod pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259 to disappear
Nov  7 02:41:47.865: INFO: Pod pod-projected-secrets-1e528b64-0108-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:41:47.865: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8755" for this suite.
Nov  7 02:41:54.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:42:03.066: INFO: namespace projected-8755 deletion completed in 14.929001091s

• [SLOW TEST:26.698 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:42:03.067: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:42:03.539: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259" in namespace "projected-1522" to be "success or failure"
Nov  7 02:42:03.631: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.095553ms
Nov  7 02:42:05.723: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184476082s
Nov  7 02:42:07.817: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277789625s
Nov  7 02:42:09.910: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370897942s
Nov  7 02:42:12.002: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463194574s
Nov  7 02:42:14.094: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.555518138s
STEP: Saw pod success
Nov  7 02:42:14.095: INFO: Pod "downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:42:14.186: INFO: Trying to get logs from node ip-10-0-139-168.us-west-2.compute.internal pod downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:42:14.379: INFO: Waiting for pod downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259 to disappear
Nov  7 02:42:14.471: INFO: Pod downwardapi-volume-2e2e6e9a-0108-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:42:14.471: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1522" for this suite.
Nov  7 02:42:21.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:42:29.686: INFO: namespace projected-1522 deletion completed in 14.943370591s

• [SLOW TEST:26.619 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:42:29.688: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Nov  7 02:42:30.057: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Nov  7 02:42:30.242: INFO: Waiting for terminating namespaces to be deleted...
Nov  7 02:42:30.335: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-132-48.us-west-2.compute.internal before test
Nov  7 02:42:30.433: INFO: dns-default-pld7m from openshift-dns started at 2019-11-06 22:25:08 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:42:30.434: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:42:30.434: INFO: osd-curated-redhat-operators-5c5d89cf9d-q48c8 from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container osd-curated-redhat-operators ready: true, restart count 0
Nov  7 02:42:30.434: INFO: dedicated-admin-operator-registry-jxtzh from openshift-dedicated-admin started at 2019-11-06 23:00:13 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container registry-server ready: true, restart count 0
Nov  7 02:42:30.434: INFO: sre-machine-api-status-exporter-1-mkpqx from openshift-monitoring started at 2019-11-06 22:33:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.434: INFO: builds-pruner-1573092000-8c6d4 from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:42:30.434: INFO: sre-machine-api-status-exporter-1-deploy from openshift-monitoring started at 2019-11-06 22:33:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:42:30.434: INFO: osd-curated-community-operators-54bddd79c9-4tgxv from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container osd-curated-community-operators ready: true, restart count 0
Nov  7 02:42:30.434: INFO: sdn-qkgb9 from openshift-sdn started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:42:30.434: INFO: router-default-867fb8468b-rck4m from openshift-ingress started at 2019-11-06 22:37:38 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container router ready: true, restart count 0
Nov  7 02:42:30.434: INFO: builds-pruner-1573084800-sbv5t from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:42:30.434: INFO: multus-m476d from openshift-multus started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:42:30.434: INFO: ovs-2cn5m from openshift-sdn started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:42:30.434: INFO: node-exporter-rqmvm from openshift-monitoring started at 2019-11-06 22:25:08 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.434: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:42:30.434: INFO: machine-config-daemon-d454m from openshift-machine-config-operator started at 2019-11-06 22:25:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:42:30.434: INFO: dedicated-admin-operator-7bcdbb466-j7rkh from openshift-dedicated-admin started at 2019-11-06 23:02:23 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container dedicated-admin-operator ready: true, restart count 0
Nov  7 02:42:30.434: INFO: deployments-pruner-1573088400-n24rg from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:42:30.434: INFO: tuned-zlvb7 from openshift-cluster-node-tuning-operator started at 2019-11-06 22:25:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:42:30.434: INFO: sre-dns-latency-exporter-26s9f from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.434: INFO: osd-curated-certified-operators-5749fb9b6d-dhtws from openshift-marketplace started at 2019-11-06 22:33:39 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container osd-curated-certified-operators ready: true, restart count 0
Nov  7 02:42:30.434: INFO: sre-stuck-ebs-vols-1-deploy from openshift-monitoring started at 2019-11-06 22:33:44 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:42:30.434: INFO: builds-pruner-1573088400-q7ps8 from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container builds-pruner ready: false, restart count 0
Nov  7 02:42:30.434: INFO: node-ca-jmqmn from openshift-image-registry started at 2019-11-06 22:25:48 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:42:30.434: INFO: configure-alertmanager-operator-registry-j8p8d from openshift-monitoring started at 2019-11-06 22:33:33 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container registry-server ready: true, restart count 0
Nov  7 02:42:30.434: INFO: sre-ebs-iops-reporter-1-p8sd7 from openshift-monitoring started at 2019-11-06 22:34:00 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.434: INFO: sre-stuck-ebs-vols-1-l8s9n from openshift-monitoring started at 2019-11-06 22:34:08 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.434: INFO: configure-alertmanager-operator-7c54957bd5-rrc89 from openshift-monitoring started at 2019-11-06 22:34:30 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Nov  7 02:42:30.434: INFO: managed-velero-operator-7bcd76bc7b-sbc6n from openshift-velero started at 2019-11-06 22:33:42 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.434: INFO: 	Container managed-velero-operator ready: true, restart count 0
Nov  7 02:42:30.434: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-136-94.us-west-2.compute.internal before test
Nov  7 02:42:30.532: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:42:30.532: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:42:30.532: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:42:30.532: INFO: multus-qz978 from openshift-multus started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:42:30.532: INFO: sre-dns-latency-exporter-k4lcz from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.532: INFO: sre-ebs-iops-reporter-1-deploy from openshift-monitoring started at 2019-11-06 22:33:44 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container deployment ready: false, restart count 0
Nov  7 02:42:30.532: INFO: router-default-867fb8468b-ch6kh from openshift-ingress started at 2019-11-06 22:38:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container router ready: true, restart count 0
Nov  7 02:42:30.532: INFO: dns-default-fr54k from openshift-dns started at 2019-11-06 22:21:29 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:42:30.532: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:42:30.532: INFO: velero-665f8fd84c-npmsd from openshift-velero started at 2019-11-06 22:34:38 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container velero ready: true, restart count 0
Nov  7 02:42:30.532: INFO: node-exporter-v9klx from openshift-monitoring started at 2019-11-06 22:21:29 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.532: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:42:30.532: INFO: ovs-5sqtb from openshift-sdn started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:42:30.532: INFO: node-ca-sbwtm from openshift-image-registry started at 2019-11-06 22:22:10 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:42:30.532: INFO: prometheus-adapter-577cfd84b5-fscj8 from openshift-monitoring started at 2019-11-06 22:22:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov  7 02:42:30.532: INFO: sdn-7pz7q from openshift-sdn started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container sdn ready: true, restart count 1
Nov  7 02:42:30.532: INFO: image-pruner-1573088400-wzm2x from openshift-sre-pruning started at 2019-11-07 01:00:05 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:42:30.532: INFO: deployments-pruner-1573092000-bzwcd from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:42:30.532: INFO: tuned-8j9zw from openshift-cluster-node-tuning-operator started at 2019-11-06 22:21:29 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:42:30.532: INFO: machine-config-daemon-vdp88 from openshift-machine-config-operator started at 2019-11-06 22:22:10 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:42:30.532: INFO: telemeter-client-75fd44c765-r4rcv from openshift-monitoring started at 2019-11-06 22:22:46 +0000 UTC (3 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.532: INFO: 	Container reload ready: true, restart count 0
Nov  7 02:42:30.532: INFO: 	Container telemeter-client ready: true, restart count 0
Nov  7 02:42:30.532: INFO: image-pruner-1573084800-dbs5b from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.532: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:42:30.532: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-139-168.us-west-2.compute.internal before test
Nov  7 02:42:30.629: INFO: sre-dns-latency-exporter-s8p4s from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.630: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:42:30.630: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-11-06 22:34:23 +0000 UTC (6 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container prometheus ready: true, restart count 1
Nov  7 02:42:30.630: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Nov  7 02:42:30.630: INFO: dns-default-dqmjz from openshift-dns started at 2019-11-06 22:20:51 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:42:30.630: INFO: ovs-b49js from openshift-sdn started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:42:30.630: INFO: image-registry-6464fd6f4d-zcr27 from openshift-image-registry started at 2019-11-06 22:21:37 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container registry ready: true, restart count 0
Nov  7 02:42:30.630: INFO: deployments-pruner-1573084800-ls92f from openshift-sre-pruning started at 2019-11-07 00:00:06 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container deployments-pruner ready: false, restart count 0
Nov  7 02:42:30.630: INFO: multus-nqw4r from openshift-multus started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:42:30.630: INFO: machine-config-daemon-6npdl from openshift-machine-config-operator started at 2019-11-06 22:21:31 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:42:30.630: INFO: sdn-prk8p from openshift-sdn started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:42:30.630: INFO: tuned-x8lhp from openshift-cluster-node-tuning-operator started at 2019-11-06 22:20:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:42:30.630: INFO: node-exporter-2xsxn from openshift-monitoring started at 2019-11-06 22:20:51 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.630: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:42:30.630: INFO: node-ca-mq5jd from openshift-image-registry started at 2019-11-06 22:21:31 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:42:30.630: INFO: image-pruner-1573092000-qw99r from openshift-sre-pruning started at 2019-11-07 02:00:03 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.630: INFO: 	Container image-pruner ready: false, restart count 0
Nov  7 02:42:30.630: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-143-165.us-west-2.compute.internal before test
Nov  7 02:42:30.728: INFO: node-exporter-bs4nw from openshift-monitoring started at 2019-11-06 22:20:55 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.728: INFO: 	Container node-exporter ready: true, restart count 0
Nov  7 02:42:30.728: INFO: sdn-cd2fd from openshift-sdn started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container sdn ready: true, restart count 0
Nov  7 02:42:30.728: INFO: prometheus-adapter-577cfd84b5-bf87d from openshift-monitoring started at 2019-11-06 22:22:51 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container prometheus-adapter ready: true, restart count 0
Nov  7 02:42:30.728: INFO: sre-dns-latency-exporter-r9xlc from openshift-monitoring started at 2019-11-06 22:33:34 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container main ready: true, restart count 0
Nov  7 02:42:30.728: INFO: ovs-4g6fz from openshift-sdn started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container openvswitch ready: true, restart count 0
Nov  7 02:42:30.728: INFO: tuned-8sfnc from openshift-cluster-node-tuning-operator started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container tuned ready: true, restart count 0
Nov  7 02:42:30.728: INFO: kube-state-metrics-646c968f77-hl75k from openshift-monitoring started at 2019-11-06 22:21:35 +0000 UTC (3 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov  7 02:42:30.728: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov  7 02:42:30.728: INFO: 	Container kube-state-metrics ready: true, restart count 0
Nov  7 02:42:30.728: INFO: grafana-69f4f95645-t98kn from openshift-monitoring started at 2019-11-06 22:21:59 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.728: INFO: 	Container grafana ready: true, restart count 0
Nov  7 02:42:30.728: INFO: 	Container grafana-proxy ready: true, restart count 0
Nov  7 02:42:30.728: INFO: dns-default-7v8r4 from openshift-dns started at 2019-11-06 22:20:55 +0000 UTC (2 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container dns ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container dns-node-resolver ready: true, restart count 0
Nov  7 02:42:30.729: INFO: machine-config-daemon-6kfmf from openshift-machine-config-operator started at 2019-11-06 22:21:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container machine-config-daemon ready: true, restart count 0
Nov  7 02:42:30.729: INFO: openshift-state-metrics-7f4bdfbdf9-xfb8l from openshift-monitoring started at 2019-11-06 22:21:35 +0000 UTC (3 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Nov  7 02:42:30.729: INFO: multus-8mnvs from openshift-multus started at 2019-11-06 22:20:55 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container kube-multus ready: true, restart count 0
Nov  7 02:42:30.729: INFO: node-ca-xg4sn from openshift-image-registry started at 2019-11-06 22:21:35 +0000 UTC (1 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container node-ca ready: true, restart count 0
Nov  7 02:42:30.729: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-11-06 22:34:18 +0000 UTC (3 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container alertmanager ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container config-reloader ready: true, restart count 0
Nov  7 02:42:30.729: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-11-06 22:34:23 +0000 UTC (6 container statuses recorded)
Nov  7 02:42:30.729: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container prom-label-proxy ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container prometheus ready: true, restart count 1
Nov  7 02:42:30.729: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container prometheus-proxy ready: true, restart count 0
Nov  7 02:42:30.729: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-44b1105f-0108-11ea-aaa6-525400524259 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-44b1105f-0108-11ea-aaa6-525400524259 off the node ip-10-0-132-48.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-44b1105f-0108-11ea-aaa6-525400524259
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:42:50.227: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-6408" for this suite.
Nov  7 02:43:02.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:43:11.258: INFO: namespace sched-pred-6408 deletion completed in 20.848852202s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:41.571 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:43:11.261: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:43:11.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259" in namespace "downward-api-1771" to be "success or failure"
Nov  7 02:43:11.822: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.671925ms
Nov  7 02:43:13.916: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185341852s
Nov  7 02:43:16.008: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277750887s
Nov  7 02:43:18.101: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370145939s
Nov  7 02:43:20.193: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462081922s
Nov  7 02:43:22.285: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.554632144s
STEP: Saw pod success
Nov  7 02:43:22.285: INFO: Pod "downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:43:22.377: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:43:22.570: INFO: Waiting for pod downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259 to disappear
Nov  7 02:43:22.661: INFO: Pod downwardapi-volume-56d3b5f4-0108-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:43:22.661: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1771" for this suite.
Nov  7 02:43:29.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:43:37.761: INFO: namespace downward-api-1771 deletion completed in 14.828652572s

• [SLOW TEST:26.500 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:43:37.761: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Nov  7 02:43:38.231: INFO: Waiting up to 5m0s for pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259" in namespace "downward-api-5658" to be "success or failure"
Nov  7 02:43:38.323: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.731399ms
Nov  7 02:43:40.415: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184155567s
Nov  7 02:43:42.507: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27638371s
Nov  7 02:43:44.600: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.369074673s
Nov  7 02:43:46.693: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.462435439s
Nov  7 02:43:48.786: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.554914818s
STEP: Saw pod success
Nov  7 02:43:48.786: INFO: Pod "downward-api-669f4fbb-0108-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:43:48.878: INFO: Trying to get logs from node ip-10-0-136-94.us-west-2.compute.internal pod downward-api-669f4fbb-0108-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 02:43:49.070: INFO: Waiting for pod downward-api-669f4fbb-0108-11ea-aaa6-525400524259 to disappear
Nov  7 02:43:49.162: INFO: Pod downward-api-669f4fbb-0108-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] Downward API
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:43:49.162: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5658" for this suite.
Nov  7 02:43:55.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:44:04.367: INFO: namespace downward-api-5658 deletion completed in 14.933432312s

• [SLOW TEST:26.605 seconds]
[sig-node] Downward API
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:44:04.368: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov  7 02:44:04.931: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:44:05.214: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-8170" for this suite.
Nov  7 02:44:11.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:44:20.157: INFO: namespace replication-controller-8170 deletion completed in 14.84948195s

• [SLOW TEST:15.789 seconds]
[sig-apps] ReplicationController
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:44:20.157: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Nov  7 02:44:20.529: INFO: namespace kubectl-998
Nov  7 02:44:20.529: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-998'
Nov  7 02:44:21.536: INFO: stderr: ""
Nov  7 02:44:21.536: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov  7 02:44:22.632: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:22.632: INFO: Found 0 / 1
Nov  7 02:44:23.628: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:23.628: INFO: Found 0 / 1
Nov  7 02:44:24.628: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:24.628: INFO: Found 0 / 1
Nov  7 02:44:25.630: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:25.630: INFO: Found 0 / 1
Nov  7 02:44:26.631: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:26.631: INFO: Found 0 / 1
Nov  7 02:44:27.628: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:27.628: INFO: Found 0 / 1
Nov  7 02:44:28.628: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:28.628: INFO: Found 0 / 1
Nov  7 02:44:29.628: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:29.628: INFO: Found 0 / 1
Nov  7 02:44:30.629: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:30.629: INFO: Found 1 / 1
Nov  7 02:44:30.629: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov  7 02:44:30.721: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:44:30.721: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov  7 02:44:30.721: INFO: wait on redis-master startup in kubectl-998 
Nov  7 02:44:30.721: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig logs redis-master-7vpqm redis-master --namespace=kubectl-998'
Nov  7 02:44:31.304: INFO: stderr: ""
Nov  7 02:44:31.305: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 07 Nov 02:43:54.512 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 07 Nov 02:43:54.512 # Server started, Redis version 3.2.12\n1:M 07 Nov 02:43:54.512 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 07 Nov 02:43:54.512 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Nov  7 02:44:31.305: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-998'
Nov  7 02:44:31.879: INFO: stderr: ""
Nov  7 02:44:31.879: INFO: stdout: "service/rm2 exposed\n"
Nov  7 02:44:31.971: INFO: Service rm2 in namespace kubectl-998 found.
STEP: exposing service
Nov  7 02:44:34.154: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-998'
Nov  7 02:44:34.726: INFO: stderr: ""
Nov  7 02:44:34.726: INFO: stdout: "service/rm3 exposed\n"
Nov  7 02:44:34.817: INFO: Service rm3 in namespace kubectl-998 found.
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:44:37.001: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-998" for this suite.
Nov  7 02:45:05.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:45:14.107: INFO: namespace kubectl-998 deletion completed in 36.834021965s

• [SLOW TEST:53.950 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:45:14.108: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9171
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov  7 02:45:14.476: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Nov  7 02:45:52.727: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.204:8080/dial?request=hostName&protocol=http&host=10.129.2.64&port=8080&tries=1'] Namespace:pod-network-test-9171 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:45:52.727: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:45:53.498: INFO: Waiting for endpoints: map[]
Nov  7 02:45:53.590: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.204:8080/dial?request=hostName&protocol=http&host=10.130.2.203&port=8080&tries=1'] Namespace:pod-network-test-9171 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:45:53.590: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:45:54.286: INFO: Waiting for endpoints: map[]
Nov  7 02:45:54.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.204:8080/dial?request=hostName&protocol=http&host=10.131.0.57&port=8080&tries=1'] Namespace:pod-network-test-9171 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:45:54.378: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:45:55.110: INFO: Waiting for endpoints: map[]
Nov  7 02:45:55.204: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.130.2.204:8080/dial?request=hostName&protocol=http&host=10.128.2.34&port=8080&tries=1'] Namespace:pod-network-test-9171 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov  7 02:45:55.204: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
Nov  7 02:45:55.902: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:45:55.902: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-9171" for this suite.
Nov  7 02:46:02.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:46:11.026: INFO: namespace pod-network-test-9171 deletion completed in 14.852029932s

• [SLOW TEST:56.918 seconds]
[sig-network] Networking
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:46:11.027: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1107 02:46:18.233542    5393 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov  7 02:46:18.233: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:46:18.233: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-5594" for this suite.
Nov  7 02:46:24.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:46:33.361: INFO: namespace gc-5594 deletion completed in 14.946363982s

• [SLOW TEST:22.335 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:46:33.366: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:46:33.843: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259" in namespace "downward-api-6706" to be "success or failure"
Nov  7 02:46:33.935: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.990597ms
Nov  7 02:46:36.027: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.184834451s
Nov  7 02:46:38.121: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278718948s
Nov  7 02:46:40.214: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371117876s
Nov  7 02:46:42.306: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.463467967s
Nov  7 02:46:44.400: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.55733272s
STEP: Saw pod success
Nov  7 02:46:44.400: INFO: Pod "downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:46:44.492: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:46:44.685: INFO: Waiting for pod downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259 to disappear
Nov  7 02:46:44.777: INFO: Pod downwardapi-volume-cf4aa77d-0108-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:46:44.778: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6706" for this suite.
Nov  7 02:46:51.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:46:59.886: INFO: namespace downward-api-6706 deletion completed in 14.836550614s

• [SLOW TEST:26.520 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:46:59.889: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:47:00.259: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:47:00.925: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5215" for this suite.
Nov  7 02:47:07.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:47:15.939: INFO: namespace custom-resource-definition-5215 deletion completed in 14.918204675s

• [SLOW TEST:16.051 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:47:15.940: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Nov  7 02:47:16.410: INFO: Waiting up to 5m0s for pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259" in namespace "var-expansion-2663" to be "success or failure"
Nov  7 02:47:16.502: INFO: Pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.701082ms
Nov  7 02:47:18.594: INFO: Pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183825116s
Nov  7 02:47:20.689: INFO: Pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278180497s
Nov  7 02:47:22.780: INFO: Pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.370104518s
Nov  7 02:47:24.875: INFO: Pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.464571331s
STEP: Saw pod success
Nov  7 02:47:24.875: INFO: Pod "var-expansion-e8aafd05-0108-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:47:24.967: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod var-expansion-e8aafd05-0108-11ea-aaa6-525400524259 container dapi-container: <nil>
STEP: delete the pod
Nov  7 02:47:25.159: INFO: Waiting for pod var-expansion-e8aafd05-0108-11ea-aaa6-525400524259 to disappear
Nov  7 02:47:25.252: INFO: Pod var-expansion-e8aafd05-0108-11ea-aaa6-525400524259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:47:25.252: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-2663" for this suite.
Nov  7 02:47:31.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:47:40.351: INFO: namespace var-expansion-2663 deletion completed in 14.827513891s

• [SLOW TEST:24.411 seconds]
[k8s.io] Variable Expansion
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:47:40.352: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1107 02:48:11.650490    5393 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov  7 02:48:11.650: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:48:11.650: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-71" for this suite.
Nov  7 02:48:18.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:48:26.768: INFO: namespace gc-71 deletion completed in 14.929143783s

• [SLOW TEST:46.416 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:48:26.770: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:48:27.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259" in namespace "projected-8500" to be "success or failure"
Nov  7 02:48:27.410: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 165.070041ms
Nov  7 02:48:29.506: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260416911s
Nov  7 02:48:31.600: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.354222967s
Nov  7 02:48:33.693: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448131245s
Nov  7 02:48:35.787: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.541628164s
Nov  7 02:48:37.882: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.636617446s
STEP: Saw pod success
Nov  7 02:48:37.882: INFO: Pod "downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:48:37.975: INFO: Trying to get logs from node ip-10-0-139-168.us-west-2.compute.internal pod downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:48:38.171: INFO: Waiting for pod downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259 to disappear
Nov  7 02:48:38.264: INFO: Pod downwardapi-volume-12e34e38-0109-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:48:38.264: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8500" for this suite.
Nov  7 02:48:44.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:48:53.473: INFO: namespace projected-8500 deletion completed in 14.935240915s

• [SLOW TEST:26.704 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:48:53.475: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov  7 02:48:53.953: INFO: Waiting up to 5m0s for pod "pod-22ce9d86-0109-11ea-aaa6-525400524259" in namespace "emptydir-6231" to be "success or failure"
Nov  7 02:48:54.047: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.146665ms
Nov  7 02:48:56.139: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186044161s
Nov  7 02:48:58.236: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28241633s
Nov  7 02:49:00.332: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378419122s
Nov  7 02:49:02.426: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.472837005s
Nov  7 02:49:04.524: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.570741613s
STEP: Saw pod success
Nov  7 02:49:04.525: INFO: Pod "pod-22ce9d86-0109-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:49:04.618: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-22ce9d86-0109-11ea-aaa6-525400524259 container test-container: <nil>
STEP: delete the pod
Nov  7 02:49:04.816: INFO: Waiting for pod pod-22ce9d86-0109-11ea-aaa6-525400524259 to disappear
Nov  7 02:49:04.909: INFO: Pod pod-22ce9d86-0109-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:49:04.911: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6231" for this suite.
Nov  7 02:49:11.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:49:20.241: INFO: namespace emptydir-6231 deletion completed in 15.052913506s

• [SLOW TEST:26.766 seconds]
[sig-storage] EmptyDir volumes
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:49:20.248: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-32c3b222-0109-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 02:49:20.821: INFO: Waiting up to 5m0s for pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259" in namespace "configmap-80" to be "success or failure"
Nov  7 02:49:20.914: INFO: Pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.909935ms
Nov  7 02:49:23.008: INFO: Pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186639816s
Nov  7 02:49:25.103: INFO: Pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282000073s
Nov  7 02:49:27.197: INFO: Pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376309209s
Nov  7 02:49:29.291: INFO: Pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.469661683s
STEP: Saw pod success
Nov  7 02:49:29.292: INFO: Pod "pod-configmaps-32d24095-0109-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:49:29.385: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-32d24095-0109-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 02:49:29.579: INFO: Waiting for pod pod-configmaps-32d24095-0109-11ea-aaa6-525400524259 to disappear
Nov  7 02:49:29.672: INFO: Pod pod-configmaps-32d24095-0109-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:49:29.672: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-80" for this suite.
Nov  7 02:49:36.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:49:44.965: INFO: namespace configmap-80 deletion completed in 15.017502096s

• [SLOW TEST:24.717 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:49:44.965: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1107 02:49:56.857347    5393 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov  7 02:49:56.857: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:49:56.857: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-6597" for this suite.
Nov  7 02:50:03.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:50:11.887: INFO: namespace gc-6597 deletion completed in 14.935882542s

• [SLOW TEST:26.922 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:50:11.888: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:50:12.451: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov  7 02:50:22.638: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Nov  7 02:50:33.383: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7304,SelfLink:/apis/apps/v1/namespaces/deployment-7304/deployments/test-cleanup-deployment,UID:42d74a3c-0109-11ea-98c4-0655ae4e8e56,ResourceVersion:121349,Generation:1,CreationTimestamp:2019-11-07 02:49:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-07 02:49:47 +0000 UTC 2019-11-07 02:49:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-07 02:49:56 +0000 UTC 2019-11-07 02:49:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6865c98b76" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov  7 02:50:33.476: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-7304,SelfLink:/apis/apps/v1/namespaces/deployment-7304/replicasets/test-cleanup-deployment-6865c98b76,UID:42d8e10e-0109-11ea-98c4-0655ae4e8e56,ResourceVersion:121337,Generation:1,CreationTimestamp:2019-11-07 02:49:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 42d74a3c-0109-11ea-98c4-0655ae4e8e56 0xc0036997a7 0xc0036997a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov  7 02:50:33.570: INFO: Pod "test-cleanup-deployment-6865c98b76-ptlzd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-ptlzd,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-7304,SelfLink:/api/v1/namespaces/deployment-7304/pods/test-cleanup-deployment-6865c98b76-ptlzd,UID:42d9fc82-0109-11ea-98c4-0655ae4e8e56,ResourceVersion:121336,Generation:0,CreationTimestamp:2019-11-07 02:49:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{k8s.v1.cni.cncf.io/networks-status: [{
    "name": "openshift-sdn",
    "interface": "eth0",
    "ips": [
        "10.130.2.213"
    ],
    "default": true,
    "dns": {}
}],openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 42d8e10e-0109-11ea-98c4-0655ae4e8e56 0xc003699db7 0xc003699db8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-75rl5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-75rl5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-75rl5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-48.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-mwj92}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003699e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003699e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:49:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:49:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:49:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-07 02:49:47 +0000 UTC  }],Message:,Reason:,HostIP:10.0.132.48,PodIP:10.130.2.213,StartTime:2019-11-07 02:49:47 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-07 02:49:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://e596d5bfe1c14cf97e6333f2911ae5e31dc0633c92a9dbf5e50adf9a43ff8c6d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:50:33.571: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-7304" for this suite.
Nov  7 02:50:40.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:50:48.838: INFO: namespace deployment-7304 deletion completed in 14.992825396s

• [SLOW TEST:36.949 seconds]
[sig-apps] Deployment
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:50:48.838: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:50:49.609: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"52b1195f-0109-11ea-98c4-0655ae4e8e56", Controller:(*bool)(0xc0035ff4d2), BlockOwnerDeletion:(*bool)(0xc0035ff4d3)}}
Nov  7 02:50:49.706: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5292b076-0109-11ea-98c4-0655ae4e8e56", Controller:(*bool)(0xc00261b932), BlockOwnerDeletion:(*bool)(0xc00261b933)}}
Nov  7 02:50:49.802: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"52a1dc44-0109-11ea-98c4-0655ae4e8e56", Controller:(*bool)(0xc0027fd656), BlockOwnerDeletion:(*bool)(0xc0027fd657)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:50:54.991: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1230" for this suite.
Nov  7 02:51:01.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:51:10.196: INFO: namespace gc-1230 deletion completed in 14.931285785s

• [SLOW TEST:21.359 seconds]
[sig-api-machinery] Garbage collector
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:51:10.199: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:51:10.573: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-7472'
Nov  7 02:51:16.942: INFO: stderr: ""
Nov  7 02:51:16.942: INFO: stdout: "replicationcontroller/redis-master created\n"
Nov  7 02:51:16.942: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig create -f - --namespace=kubectl-7472'
Nov  7 02:51:18.316: INFO: stderr: ""
Nov  7 02:51:18.316: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov  7 02:51:19.414: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:19.414: INFO: Found 0 / 1
Nov  7 02:51:20.411: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:20.411: INFO: Found 0 / 1
Nov  7 02:51:21.412: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:21.412: INFO: Found 0 / 1
Nov  7 02:51:22.411: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:22.411: INFO: Found 0 / 1
Nov  7 02:51:23.412: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:23.412: INFO: Found 0 / 1
Nov  7 02:51:24.412: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:24.413: INFO: Found 0 / 1
Nov  7 02:51:25.411: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:25.411: INFO: Found 1 / 1
Nov  7 02:51:25.411: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov  7 02:51:25.504: INFO: Selector matched 1 pods for map[app:redis]
Nov  7 02:51:25.504: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov  7 02:51:25.505: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig describe pod redis-master-qn5f8 --namespace=kubectl-7472'
Nov  7 02:51:26.130: INFO: stderr: ""
Nov  7 02:51:26.130: INFO: stdout: "Name:               redis-master-qn5f8\nNamespace:          kubectl-7472\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-0-132-48.us-west-2.compute.internal/10.0.132.48\nStart Time:         Thu, 07 Nov 2019 02:50:41 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        k8s.v1.cni.cncf.io/networks-status:\n                      [{\n                          \"name\": \"openshift-sdn\",\n                          \"interface\": \"eth0\",\n                          \"ips\": [\n                              \"10.130.2.214\"\n                          ],\n                          \"default\": true,\n                          \"dns\": {}\n                      }]\n                    openshift.io/scc: privileged\nStatus:             Running\nIP:                 10.130.2.214\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://b33195902398c7b823d3bd3cb5d8db79ad0dabab015aa6908ba896e1cebeeb83\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 07 Nov 2019 02:50:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xffjx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xffjx:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xffjx\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                Message\n  ----    ------     ----  ----                                                -------\n  Normal  Scheduled  45s   default-scheduler                                   Successfully assigned kubectl-7472/redis-master-qn5f8 to ip-10-0-132-48.us-west-2.compute.internal\n  Normal  Pulled     37s   kubelet, ip-10-0-132-48.us-west-2.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    37s   kubelet, ip-10-0-132-48.us-west-2.compute.internal  Created container redis-master\n  Normal  Started    37s   kubelet, ip-10-0-132-48.us-west-2.compute.internal  Started container redis-master\n"
Nov  7 02:51:26.130: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig describe rc redis-master --namespace=kubectl-7472'
Nov  7 02:51:26.859: INFO: stderr: ""
Nov  7 02:51:26.859: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7472\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  45s   replication-controller  Created pod: redis-master-qn5f8\n"
Nov  7 02:51:26.859: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig describe service redis-master --namespace=kubectl-7472'
Nov  7 02:51:27.610: INFO: stderr: ""
Nov  7 02:51:27.610: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7472\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.30.17.131\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.130.2.214:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov  7 02:51:27.886: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig describe node ip-10-0-131-250.us-west-2.compute.internal'
Nov  7 02:51:28.987: INFO: stderr: ""
Nov  7 02:51:28.987: INFO: stdout: "Name:               ip-10-0-131-250.us-west-2.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-131-250\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.openshift.io/os_id=rhcos\nAnnotations:        machine.openshift.io/machine: openshift-machine-api/jeder-42-test1-v49vz-master-1\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-13a55dc699da0fdd61f2577a5ba466fd\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-13a55dc699da0fdd61f2577a5ba466fd\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 06 Nov 2019 22:13:15 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\n                    node.kubernetes.io/unschedulable:NoSchedule\nUnschedulable:      true\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 07 Nov 2019 02:50:22 +0000   Wed, 06 Nov 2019 22:13:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 07 Nov 2019 02:50:22 +0000   Wed, 06 Nov 2019 22:13:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 07 Nov 2019 02:50:22 +0000   Wed, 06 Nov 2019 22:13:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 07 Nov 2019 02:50:22 +0000   Wed, 06 Nov 2019 22:15:15 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.131.250\n  Hostname:     ip-10-0-131-250.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-131-250.us-west-2.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         4\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      15946308Ki\n pods:                        250\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         3500m\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      15331908Ki\n pods:                        250\nSystem Info:\n Machine ID:                                  ec2f0715f403be5c13ffd26f9094d550\n System UUID:                                 ec2f0715-f403-be5c-13ff-d26f9094d550\n Boot ID:                                     70f8ee59-2241-4354-b48d-0182ed2d9684\n Kernel Version:                              4.18.0-80.11.2.el8_0.x86_64\n OS Image:                                    Red Hat Enterprise Linux CoreOS 42.80.20191022.0 (Ootpa)\n Operating System:                            linux\n Architecture:                                amd64\n Container Runtime Version:                   cri-o://1.14.11-0.23.dev.rhaos4.2.gitc41de67.el8\n Kubelet Version:                             v1.14.6+7e13ab9a7\n Kube-Proxy Version:                          v1.14.6+7e13ab9a7\nProviderID:                                   aws:///us-west-2a/i-0b51234b8e0765516\nNon-terminated Pods:                          (40 in total)\n  Namespace                                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver-operator                openshift-apiserver-operator-7944d76cc8-mnwp5                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-apiserver                         apiserver-6f5hj                                                        150m (4%)     0 (0%)      200Mi (1%)       0 (0%)         4h15m\n  openshift-authentication                    oauth-openshift-5b5b88c9c6-b5xwt                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h\n  openshift-cloud-credential-operator         cloud-credential-operator-7468f6c665-w882b                             10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         4h38m\n  openshift-cluster-machine-approver          machine-approver-d97cbbb79-qm9gp                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-cluster-node-tuning-operator      tuned-t9c8h                                                            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h32m\n  openshift-cluster-samples-operator          cluster-samples-operator-5f855475b5-zs2qz                              10m (0%)      0 (0%)      0 (0%)           0 (0%)         4h31m\n  openshift-cluster-version                   cluster-version-operator-5869c8f646-8csdk                              20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-console                           console-5fb8f4c49b-r88nw                                               10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         4h17m\n  openshift-console                           downloads-df59f64db-kbzjw                                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h31m\n  openshift-controller-manager-operator       openshift-controller-manager-operator-5558cf84dc-gq8sf                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-controller-manager                controller-manager-9z4md                                               100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         4h25m\n  openshift-dns-operator                      dns-operator-b5cfc448d-6tm5h                                           10m (0%)      0 (0%)      0 (0%)           0 (0%)         4h38m\n  openshift-dns                               dns-default-jdpf5                                                      110m (3%)     0 (0%)      70Mi (0%)        512Mi (3%)     4h35m\n  openshift-etcd                              etcd-member-ip-10-0-131-250.us-west-2.compute.internal                 300m (8%)     0 (0%)      600Mi (4%)       0 (0%)         4h37m\n  openshift-image-registry                    node-ca-8vzkv                                                          10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h30m\n  openshift-ingress-operator                  ingress-operator-7c846bf595-sskcz                                      10m (0%)      0 (0%)      0 (0%)           0 (0%)         4h30m\n  openshift-insights                          insights-operator-596fff95bf-tzcgw                                     10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         4h38m\n  openshift-kube-apiserver-operator           kube-apiserver-operator-8cdcb887f-5pt42                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-kube-apiserver                    kube-apiserver-ip-10-0-131-250.us-west-2.compute.internal              170m (4%)     0 (0%)      1124Mi (7%)      0 (0%)         4h8m\n  openshift-kube-controller-manager-operator  kube-controller-manager-operator-74557b754c-fm9pl                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-kube-controller-manager           kube-controller-manager-ip-10-0-131-250.us-west-2.compute.internal     110m (3%)     0 (0%)      250Mi (1%)       0 (0%)         4h11m\n  openshift-kube-scheduler-operator           openshift-kube-scheduler-operator-59cc967c79-d5xbq                     0 (0%)        0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-kube-scheduler                    openshift-kube-scheduler-ip-10-0-131-250.us-west-2.compute.internal    0 (0%)        0 (0%)      50Mi (0%)        0 (0%)         4h13m\n  openshift-machine-api                       cluster-autoscaler-operator-5ddfdb8c89-c4bt6                           20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h30m\n  openshift-machine-api                       machine-api-operator-78b74fcf6f-h7xkz                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h37m\n  openshift-machine-config-operator           etcd-quorum-guard-6899c458b4-wvkqf                                     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         4h34m\n  openshift-machine-config-operator           machine-config-daemon-892mj                                            20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h35m\n  openshift-machine-config-operator           machine-config-operator-785449b679-wcccl                               20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-machine-config-operator           machine-config-server-b6rlr                                            20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h35m\n  openshift-monitoring                        node-exporter-tqqdv                                                    10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h30m\n  openshift-multus                            multus-5knhg                                                           10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         4h37m\n  openshift-multus                            multus-admission-controller-2v8vh                                      10m (0%)      0 (0%)      0 (0%)           0 (0%)         4h36m\n  openshift-network-operator                  network-operator-5b6546b745-cz9zk                                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h38m\n  openshift-operator-lifecycle-manager        catalog-operator-8b45fc999-6w42m                                       10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         4h38m\n  openshift-operator-lifecycle-manager        olm-operator-5fb85c646b-pfgcm                                          10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         4h38m\n  openshift-sdn                               ovs-pjnl7                                                              200m (5%)     0 (0%)      400Mi (2%)       0 (0%)         4h37m\n  openshift-sdn                               sdn-controller-b667p                                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h37m\n  openshift-sdn                               sdn-r5qgd                                                              100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         4h37m\n  openshift-service-ca-operator               service-ca-operator-bff9f4f85-btbmj                                    10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         4h38m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         1590m (45%)   0 (0%)\n  memory                      4629Mi (30%)  512Mi (3%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type    Reason              Age   From                                                 Message\n  ----    ------              ----  ----                                                 -------\n  Normal  NodeNotSchedulable  142m  kubelet, ip-10-0-131-250.us-west-2.compute.internal  Node ip-10-0-131-250.us-west-2.compute.internal status is now: NodeNotSchedulable\n"
Nov  7 02:51:28.988: INFO: Running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig describe namespace kubectl-7472'
Nov  7 02:51:29.732: INFO: stderr: ""
Nov  7 02:51:29.732: INFO: stdout: "Name:         kubectl-7472\nLabels:       e2e-framework=kubectl\n              e2e-run=e191d10d-00f5-11ea-aaa6-525400524259\nAnnotations:  openshift.io/sa.scc.mcs: s0:c50,c5\n              openshift.io/sa.scc.supplemental-groups: 1002460000/10000\n              openshift.io/sa.scc.uid-range: 1002460000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:51:29.732: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-7472" for this suite.
Nov  7 02:51:42.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:51:50.944: INFO: namespace kubectl-7472 deletion completed in 20.937545694s

• [SLOW TEST:40.746 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:51:50.945: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:51:51.424: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259" in namespace "downward-api-450" to be "success or failure"
Nov  7 02:51:51.517: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.832457ms
Nov  7 02:51:53.611: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186262302s
Nov  7 02:51:55.705: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280160202s
Nov  7 02:51:57.798: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373701839s
Nov  7 02:51:59.892: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.467180329s
Nov  7 02:52:01.986: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.561159934s
STEP: Saw pod success
Nov  7 02:52:01.986: INFO: Pod "downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:52:02.079: INFO: Trying to get logs from node ip-10-0-139-168.us-west-2.compute.internal pod downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:52:02.273: INFO: Waiting for pod downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259 to disappear
Nov  7 02:52:02.368: INFO: Pod downwardapi-volume-8c964029-0109-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:52:02.368: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-450" for this suite.
Nov  7 02:52:08.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:52:17.581: INFO: namespace downward-api-450 deletion completed in 14.938035508s

• [SLOW TEST:26.636 seconds]
[sig-storage] Downward API volume
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:52:17.581: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-9666/configmap-test-9c768a47-0109-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 02:52:18.153: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259" in namespace "configmap-9666" to be "success or failure"
Nov  7 02:52:18.246: INFO: Pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 92.616665ms
Nov  7 02:52:20.340: INFO: Pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186249596s
Nov  7 02:52:22.433: INFO: Pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.279175653s
Nov  7 02:52:24.526: INFO: Pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.372296902s
Nov  7 02:52:26.619: INFO: Pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.465753126s
STEP: Saw pod success
Nov  7 02:52:26.619: INFO: Pod "pod-configmaps-9c852979-0109-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:52:26.712: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-9c852979-0109-11ea-aaa6-525400524259 container env-test: <nil>
STEP: delete the pod
Nov  7 02:52:26.909: INFO: Waiting for pod pod-configmaps-9c852979-0109-11ea-aaa6-525400524259 to disappear
Nov  7 02:52:27.002: INFO: Pod pod-configmaps-9c852979-0109-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:52:27.002: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9666" for this suite.
Nov  7 02:52:33.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:52:42.255: INFO: namespace configmap-9666 deletion completed in 14.978298936s

• [SLOW TEST:24.674 seconds]
[sig-node] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:52:42.256: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6777
Nov  7 02:52:50.919: INFO: Started pod liveness-http in namespace container-probe-6777
STEP: checking the pod's current state and verifying that restartCount is present
Nov  7 02:52:51.012: INFO: Initial restart count of pod liveness-http is 0
Nov  7 02:53:09.947: INFO: Restart count of pod container-probe-6777/liveness-http is now 1 (18.934895075s elapsed)
Nov  7 02:53:30.882: INFO: Restart count of pod container-probe-6777/liveness-http is now 2 (39.870226874s elapsed)
Nov  7 02:53:49.723: INFO: Restart count of pod container-probe-6777/liveness-http is now 3 (58.711289113s elapsed)
Nov  7 02:54:10.657: INFO: Restart count of pod container-probe-6777/liveness-http is now 4 (1m19.645303554s elapsed)
Nov  7 02:55:13.462: INFO: Restart count of pod container-probe-6777/liveness-http is now 5 (2m22.450083228s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:55:13.563: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6777" for this suite.
Nov  7 02:55:20.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:55:28.906: INFO: namespace container-probe-6777 deletion completed in 15.067952412s

• [SLOW TEST:166.651 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:55:28.908: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 02:55:29.384: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259" in namespace "projected-3219" to be "success or failure"
Nov  7 02:55:29.477: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.390017ms
Nov  7 02:55:31.573: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188710191s
Nov  7 02:55:33.667: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.282807307s
Nov  7 02:55:35.761: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.377228095s
Nov  7 02:55:37.855: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.47083962s
Nov  7 02:55:39.948: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.56435447s
STEP: Saw pod success
Nov  7 02:55:39.948: INFO: Pod "downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:55:40.042: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 02:55:40.237: INFO: Waiting for pod downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259 to disappear
Nov  7 02:55:40.330: INFO: Pod downwardapi-volume-0e8079bd-010a-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:55:40.330: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3219" for this suite.
Nov  7 02:55:46.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:55:55.972: INFO: namespace projected-3219 deletion completed in 15.366921243s

• [SLOW TEST:27.065 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:55:55.974: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov  7 02:55:56.718: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123560,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov  7 02:55:56.718: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123560,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov  7 02:56:06.907: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123615,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov  7 02:56:06.907: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123615,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov  7 02:56:17.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123667,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov  7 02:56:17.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123667,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov  7 02:56:27.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123734,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov  7 02:56:27.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-a,UID:09cd345f-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123734,Generation:0,CreationTimestamp:2019-11-07 02:55:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov  7 02:56:37.300: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-b,UID:21fc1c28-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123825,Generation:0,CreationTimestamp:2019-11-07 02:56:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov  7 02:56:37.300: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-b,UID:21fc1c28-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123825,Generation:0,CreationTimestamp:2019-11-07 02:56:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov  7 02:56:47.397: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-b,UID:21fc1c28-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123880,Generation:0,CreationTimestamp:2019-11-07 02:56:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov  7 02:56:47.397: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1237,SelfLink:/api/v1/namespaces/watch-1237/configmaps/e2e-watch-test-configmap-b,UID:21fc1c28-010a-11ea-98c4-0655ae4e8e56,ResourceVersion:123880,Generation:0,CreationTimestamp:2019-11-07 02:56:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:56:57.398: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-1237" for this suite.
Nov  7 02:57:03.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:57:12.612: INFO: namespace watch-1237 deletion completed in 14.938328175s

• [SLOW TEST:76.638 seconds]
[sig-api-machinery] Watchers
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:57:12.613: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Nov  7 02:57:12.995: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:57:23.212: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-6565" for this suite.
Nov  7 02:57:29.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:57:38.440: INFO: namespace init-container-6565 deletion completed in 14.951993996s

• [SLOW TEST:25.829 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:57:38.442: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 02:57:38.913: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-5bc48fc9-010a-11ea-aaa6-525400524259
STEP: Creating secret with name s-test-opt-upd-5bc49035-010a-11ea-aaa6-525400524259
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5bc48fc9-010a-11ea-aaa6-525400524259
STEP: Updating secret s-test-opt-upd-5bc49035-010a-11ea-aaa6-525400524259
STEP: Creating secret with name s-test-opt-create-5bc49048-010a-11ea-aaa6-525400524259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:58:59.508: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8976" for this suite.
Nov  7 02:59:14.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:59:22.735: INFO: namespace projected-8976 deletion completed in 22.951197198s

• [SLOW TEST:104.293 seconds]
[sig-storage] Projected secret
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:59:22.735: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-99e03cb1-010a-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 02:59:23.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259" in namespace "configmap-8364" to be "success or failure"
Nov  7 02:59:23.404: INFO: Pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.165773ms
Nov  7 02:59:25.498: INFO: Pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186912534s
Nov  7 02:59:27.593: INFO: Pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281466795s
Nov  7 02:59:29.686: INFO: Pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374619183s
Nov  7 02:59:31.779: INFO: Pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.468063146s
STEP: Saw pod success
Nov  7 02:59:31.779: INFO: Pod "pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:59:31.872: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259 container configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 02:59:32.068: INFO: Waiting for pod pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259 to disappear
Nov  7 02:59:32.161: INFO: Pod pod-configmaps-99eebc57-010a-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:59:32.161: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8364" for this suite.
Nov  7 02:59:38.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 02:59:47.472: INFO: namespace configmap-8364 deletion completed in 15.035796047s

• [SLOW TEST:24.737 seconds]
[sig-storage] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 02:59:47.474: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a89e9df9-010a-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 02:59:48.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259" in namespace "projected-1530" to be "success or failure"
Nov  7 02:59:48.138: INFO: Pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.220713ms
Nov  7 02:59:50.231: INFO: Pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186413519s
Nov  7 02:59:52.326: INFO: Pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280674038s
Nov  7 02:59:54.420: INFO: Pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.374608267s
Nov  7 02:59:56.513: INFO: Pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.468115502s
STEP: Saw pod success
Nov  7 02:59:56.513: INFO: Pod "pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 02:59:56.606: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 02:59:56.802: INFO: Waiting for pod pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259 to disappear
Nov  7 02:59:56.895: INFO: Pod pod-projected-configmaps-a8ad152f-010a-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 02:59:56.895: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1530" for this suite.
Nov  7 03:00:03.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:00:12.163: INFO: namespace projected-1530 deletion completed in 14.991565344s

• [SLOW TEST:24.690 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:00:12.165: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:00:19.588: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-3456" for this suite.
Nov  7 03:00:26.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:00:34.808: INFO: namespace namespaces-3456 deletion completed in 14.94393912s
STEP: Destroying namespace "nsdeletetest-8535" for this suite.
Nov  7 03:00:34.901: INFO: Namespace nsdeletetest-8535 was already deleted
STEP: Destroying namespace "nsdeletetest-4939" for this suite.
Nov  7 03:00:41.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:00:49.840: INFO: namespace nsdeletetest-4939 deletion completed in 14.939237543s

• [SLOW TEST:37.675 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:00:49.841: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Nov  7 03:01:18.600: INFO: Container started at 2019-11-07 03:00:23 +0000 UTC, pod became ready at 2019-11-07 03:00:41 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:01:18.600: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-9061" for this suite.
Nov  7 03:01:47.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:01:55.809: INFO: namespace container-probe-9061 deletion completed in 36.935027682s

• [SLOW TEST:65.968 seconds]
[k8s.io] Probing container
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:01:55.812: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Nov  7 03:01:56.186: INFO: Asynchronously running '/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:01:56.735: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3768" for this suite.
Nov  7 03:02:03.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:02:11.939: INFO: namespace kubectl-3768 deletion completed in 14.930114442s

• [SLOW TEST:16.128 seconds]
[sig-cli] Kubectl client
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:02:11.940: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-1951/configmap-test-feb9fdaa-010a-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 03:02:12.508: INFO: Waiting up to 5m0s for pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259" in namespace "configmap-1951" to be "success or failure"
Nov  7 03:02:12.601: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 93.166078ms
Nov  7 03:02:14.695: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186857606s
Nov  7 03:02:16.788: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280356778s
Nov  7 03:02:18.882: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.373749465s
Nov  7 03:02:20.975: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.467510056s
Nov  7 03:02:23.069: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.561211255s
STEP: Saw pod success
Nov  7 03:02:23.070: INFO: Pod "pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 03:02:23.163: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259 container env-test: <nil>
STEP: delete the pod
Nov  7 03:02:23.359: INFO: Waiting for pod pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259 to disappear
Nov  7 03:02:23.452: INFO: Pod pod-configmaps-fec86b8e-010a-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:02:23.452: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-1951" for this suite.
Nov  7 03:02:30.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:02:38.655: INFO: namespace configmap-1951 deletion completed in 14.927943184s

• [SLOW TEST:26.715 seconds]
[sig-node] ConfigMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:02:38.657: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-xkhj
STEP: Creating a pod to test atomic-volume-subpath
Nov  7 03:02:39.326: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xkhj" in namespace "subpath-5662" to be "success or failure"
Nov  7 03:02:39.422: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Pending", Reason="", readiness=false. Elapsed: 95.184854ms
Nov  7 03:02:41.533: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20670355s
Nov  7 03:02:43.627: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300521254s
Nov  7 03:02:45.719: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.392509976s
Nov  7 03:02:47.811: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 8.484245451s
Nov  7 03:02:49.904: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 10.577389656s
Nov  7 03:02:51.996: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 12.669760157s
Nov  7 03:02:54.088: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 14.761485052s
Nov  7 03:02:56.181: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 16.854468248s
Nov  7 03:02:58.274: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 18.947314321s
Nov  7 03:03:00.366: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 21.039306681s
Nov  7 03:03:02.458: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 23.131435622s
Nov  7 03:03:04.550: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 25.223397086s
Nov  7 03:03:06.642: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Running", Reason="", readiness=true. Elapsed: 27.315466805s
Nov  7 03:03:08.734: INFO: Pod "pod-subpath-test-secret-xkhj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.407596268s
STEP: Saw pod success
Nov  7 03:03:08.734: INFO: Pod "pod-subpath-test-secret-xkhj" satisfied condition "success or failure"
Nov  7 03:03:08.826: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-subpath-test-secret-xkhj container test-container-subpath-secret-xkhj: <nil>
STEP: delete the pod
Nov  7 03:03:09.019: INFO: Waiting for pod pod-subpath-test-secret-xkhj to disappear
Nov  7 03:03:09.111: INFO: Pod pod-subpath-test-secret-xkhj no longer exists
STEP: Deleting pod pod-subpath-test-secret-xkhj
Nov  7 03:03:09.111: INFO: Deleting pod "pod-subpath-test-secret-xkhj" in namespace "subpath-5662"
[AfterEach] [sig-storage] Subpath
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:03:09.202: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-5662" for this suite.
Nov  7 03:03:15.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:03:24.284: INFO: namespace subpath-5662 deletion completed in 14.811239713s

• [SLOW TEST:45.628 seconds]
[sig-storage] Subpath
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:03:24.289: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Nov  7 03:03:24.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259" in namespace "projected-3932" to be "success or failure"
Nov  7 03:03:24.849: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.583683ms
Nov  7 03:03:26.941: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18357568s
Nov  7 03:03:29.034: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276390164s
Nov  7 03:03:31.126: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368299064s
Nov  7 03:03:33.218: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.46024667s
Nov  7 03:03:35.310: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.552492499s
STEP: Saw pod success
Nov  7 03:03:35.310: INFO: Pod "downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 03:03:35.402: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259 container client-container: <nil>
STEP: delete the pod
Nov  7 03:03:35.595: INFO: Waiting for pod downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259 to disappear
Nov  7 03:03:35.688: INFO: Pod downwardapi-volume-29d8d5e5-010b-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:03:35.688: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3932" for this suite.
Nov  7 03:03:42.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:03:50.860: INFO: namespace projected-3932 deletion completed in 14.901393898s

• [SLOW TEST:26.572 seconds]
[sig-storage] Projected downwardAPI
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:03:50.860: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-699 to expose endpoints map[]
Nov  7 03:03:51.417: INFO: successfully validated that service endpoint-test2 in namespace services-699 exposes endpoints map[] (91.383847ms elapsed)
STEP: Creating pod pod1 in namespace services-699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-699 to expose endpoints map[pod1:[80]]
Nov  7 03:03:56.438: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.917815119s elapsed, will retry)
Nov  7 03:04:01.172: INFO: successfully validated that service endpoint-test2 in namespace services-699 exposes endpoints map[pod1:[80]] (9.652249064s elapsed)
STEP: Creating pod pod2 in namespace services-699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-699 to expose endpoints map[pod1:[80] pod2:[80]]
Nov  7 03:04:06.658: INFO: Unexpected endpoints: found map[24ce06a2-010b-11ea-98c4-0655ae4e8e56:[80]], expected map[pod1:[80] pod2:[80]] (5.386496533s elapsed, will retry)
Nov  7 03:04:10.482: INFO: successfully validated that service endpoint-test2 in namespace services-699 exposes endpoints map[pod1:[80] pod2:[80]] (9.210669071s elapsed)
STEP: Deleting pod pod1 in namespace services-699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-699 to expose endpoints map[pod2:[80]]
Nov  7 03:04:10.761: INFO: successfully validated that service endpoint-test2 in namespace services-699 exposes endpoints map[pod2:[80]] (183.401057ms elapsed)
STEP: Deleting pod pod2 in namespace services-699
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-699 to expose endpoints map[]
Nov  7 03:04:10.946: INFO: successfully validated that service endpoint-test2 in namespace services-699 exposes endpoints map[] (91.308113ms elapsed)
[AfterEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:04:11.051: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-699" for this suite.
Nov  7 03:04:29.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:04:38.246: INFO: namespace services-699 deletion completed in 26.836789088s
[AfterEach] [sig-network] Services
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:47.386 seconds]
[sig-network] Services
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:04:38.247: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-55ee4fe9-010b-11ea-aaa6-525400524259
STEP: Creating a pod to test consume configMaps
Nov  7 03:04:38.811: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259" in namespace "projected-1003" to be "success or failure"
Nov  7 03:04:38.902: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 91.626193ms
Nov  7 03:04:40.994: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183693085s
Nov  7 03:04:43.087: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.276066511s
Nov  7 03:04:45.179: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368477199s
Nov  7 03:04:47.271: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.460842979s
Nov  7 03:04:49.364: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.553243945s
STEP: Saw pod success
Nov  7 03:04:49.364: INFO: Pod "pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259" satisfied condition "success or failure"
Nov  7 03:04:49.456: INFO: Trying to get logs from node ip-10-0-132-48.us-west-2.compute.internal pod pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov  7 03:04:49.650: INFO: Waiting for pod pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259 to disappear
Nov  7 03:04:49.741: INFO: Pod pod-projected-configmaps-55fc96a2-010b-11ea-aaa6-525400524259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:04:49.741: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1003" for this suite.
Nov  7 03:04:56.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:05:04.852: INFO: namespace projected-1003 deletion completed in 14.838822129s

• [SLOW TEST:26.605 seconds]
[sig-storage] Projected configMap
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Nov  7 03:05:04.854: INFO: >>> kubeConfig: /home/jeder/198f6h2ub0413bevcvb7k6827s1p43p6.kubeconfig
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Nov  7 03:05:06.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:08.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:10.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:12.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:14.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:16.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:18.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:20.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:22.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63708692670, loc:(*time.Location)(0x8828100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov  7 03:05:40.412: INFO: Waited 15.38349066s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Nov  7 03:05:44.528: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "aggregator-8036" for this suite.
Nov  7 03:05:51.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov  7 03:05:59.728: INFO: namespace aggregator-8036 deletion completed in 14.928297781s

• [SLOW TEST:54.875 seconds]
[sig-api-machinery] Aggregator
/home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /home/jeder/osd_42_conformance/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSNov  7 03:05:59.730: INFO: Running AfterSuite actions on all nodes
Nov  7 03:05:59.730: INFO: Running AfterSuite actions on node 1
Nov  7 03:05:59.730: INFO: Dumping logs locally to: /home/jeder/osd_42_conformance/origin/_output/scripts/conformance-k8s/artifacts
Nov  7 03:05:59.731: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 204 of 3586 Specs in 9294.912 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS
