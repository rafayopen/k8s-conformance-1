Aug 26 23:15:16.747: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I0826 23:15:16.748211     729 e2e.go:240] Starting e2e run "5ce0c26b-c857-11e9-974e-0a58ac107583" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1566861315 - Will randomize all specs
Will run 204 of 3586 specs

Aug 26 23:15:16.823: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:15:16.827: INFO: Waiting up to 30m0s for all (but 3) nodes to be schedulable
Aug 26 23:15:16.930: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 26 23:15:16.995: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 26 23:15:16.995: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Aug 26 23:15:16.995: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 26 23:15:17.015: INFO: e2e test version: v1.14.7-beta.0.14+3fe98d8e0f1966
Aug 26 23:15:17.030: INFO: kube-apiserver version: v1.14.0+204cc07
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:15:17.030: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
Aug 26 23:15:17.188: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Aug 26 23:15:17.206: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-2716'
Aug 26 23:15:17.697: INFO: stderr: ""
Aug 26 23:15:17.697: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 26 23:15:18.715: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:18.716: INFO: Found 0 / 1
Aug 26 23:15:19.716: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:19.716: INFO: Found 0 / 1
Aug 26 23:15:20.716: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:20.716: INFO: Found 0 / 1
Aug 26 23:15:21.716: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:21.716: INFO: Found 0 / 1
Aug 26 23:15:22.715: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:22.715: INFO: Found 0 / 1
Aug 26 23:15:23.715: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:23.715: INFO: Found 1 / 1
Aug 26 23:15:23.715: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 26 23:15:23.732: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:23.732: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 26 23:15:23.732: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig patch pod redis-master-8rl79 --namespace=kubectl-2716 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 26 23:15:23.916: INFO: stderr: ""
Aug 26 23:15:23.916: INFO: stdout: "pod/redis-master-8rl79 patched\n"
STEP: checking annotations
Aug 26 23:15:23.933: INFO: Selector matched 1 pods for map[app:redis]
Aug 26 23:15:23.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:15:23.933: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2716" for this suite.
Aug 26 23:15:48.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:15:50.177: INFO: namespace kubectl-2716 deletion completed in 26.212721099s

â€¢ [SLOW TEST:33.147 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:15:50.178: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6780
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-6780
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6780
Aug 26 23:15:50.401: INFO: Found 0 stateful pods, waiting for 1
Aug 26 23:16:00.420: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 26 23:16:00.438: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 26 23:16:00.782: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 26 23:16:00.782: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 26 23:16:00.782: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 26 23:16:00.799: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 26 23:16:10.817: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 23:16:10.817: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 23:16:10.891: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:10.891: INFO: ss-0  ip-10-0-129-239.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  }]
Aug 26 23:16:10.891: INFO: ss-1  ip-10-0-155-122.ec2.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:10.891: INFO: 
Aug 26 23:16:10.891: INFO: StatefulSet ss has not reached scale 3, at 2
Aug 26 23:16:11.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979653118s
Aug 26 23:16:12.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961627706s
Aug 26 23:16:13.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.943959405s
Aug 26 23:16:14.962: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.926561551s
Aug 26 23:16:15.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.908942917s
Aug 26 23:16:16.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.891012253s
Aug 26 23:16:18.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.873391481s
Aug 26 23:16:19.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.855397664s
Aug 26 23:16:20.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 838.038763ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6780
Aug 26 23:16:21.068: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 26 23:16:21.443: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 26 23:16:21.443: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 26 23:16:21.443: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 26 23:16:21.443: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 26 23:16:21.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 26 23:16:21.798: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 26 23:16:21.798: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 26 23:16:21.798: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 26 23:16:22.146: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 26 23:16:22.146: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 26 23:16:22.146: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 26 23:16:22.163: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 26 23:16:32.183: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:16:32.183: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:16:32.183: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 26 23:16:32.200: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 26 23:16:32.795: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 26 23:16:32.795: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 26 23:16:32.795: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 26 23:16:32.795: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 26 23:16:33.334: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 26 23:16:33.334: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 26 23:16:33.334: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 26 23:16:33.334: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-6780 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 26 23:16:33.809: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 26 23:16:33.809: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 26 23:16:33.809: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 26 23:16:33.809: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 23:16:33.828: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 26 23:16:43.863: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 23:16:43.863: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 23:16:43.863: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 26 23:16:43.924: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:43.924: INFO: ss-0  ip-10-0-129-239.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  }]
Aug 26 23:16:43.924: INFO: ss-1  ip-10-0-155-122.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:43.924: INFO: ss-2  ip-10-0-139-255.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:43.924: INFO: 
Aug 26 23:16:43.924: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 26 23:16:44.941: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:44.941: INFO: ss-0  ip-10-0-129-239.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  }]
Aug 26 23:16:44.941: INFO: ss-1  ip-10-0-155-122.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:44.941: INFO: ss-2  ip-10-0-139-255.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:44.941: INFO: 
Aug 26 23:16:44.941: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 26 23:16:45.960: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:45.960: INFO: ss-0  ip-10-0-129-239.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  }]
Aug 26 23:16:45.960: INFO: ss-1  ip-10-0-155-122.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:45.960: INFO: ss-2  ip-10-0-139-255.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:45.960: INFO: 
Aug 26 23:16:45.960: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 26 23:16:46.978: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:46.978: INFO: ss-0  ip-10-0-129-239.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  }]
Aug 26 23:16:46.978: INFO: ss-1  ip-10-0-155-122.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:46.978: INFO: ss-2  ip-10-0-139-255.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:46.978: INFO: 
Aug 26 23:16:46.978: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 26 23:16:47.995: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:47.995: INFO: ss-0  ip-10-0-129-239.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:15:50 +0000 UTC  }]
Aug 26 23:16:47.995: INFO: ss-1  ip-10-0-155-122.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:47.995: INFO: ss-2  ip-10-0-139-255.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:47.995: INFO: 
Aug 26 23:16:47.995: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 26 23:16:49.013: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 26 23:16:49.013: INFO: ss-2  ip-10-0-139-255.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:16:10 +0000 UTC  }]
Aug 26 23:16:49.013: INFO: 
Aug 26 23:16:49.013: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 26 23:16:50.030: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.888367154s
Aug 26 23:16:51.048: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.871410009s
Aug 26 23:16:52.065: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.853086002s
Aug 26 23:16:53.083: INFO: Verifying statefulset ss doesn't scale past 0 for another 835.780657ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6780
Aug 26 23:16:54.101: INFO: Scaling statefulset ss to 0
Aug 26 23:16:54.154: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 26 23:16:54.170: INFO: Deleting all statefulset in ns statefulset-6780
Aug 26 23:16:54.186: INFO: Scaling statefulset ss to 0
Aug 26 23:16:54.236: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 23:16:54.252: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:16:54.308: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-6780" for this suite.
Aug 26 23:17:00.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:17:02.557: INFO: namespace statefulset-6780 deletion completed in 8.217192319s

â€¢ [SLOW TEST:72.380 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:17:02.558: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 26 23:17:09.371: INFO: Successfully updated pod "annotationupdate9c9da9b1-c857-11e9-974e-0a58ac107583"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:17:11.413: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-4354" for this suite.
Aug 26 23:17:35.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:17:37.661: INFO: namespace downward-api-4354 deletion completed in 26.213075894s

â€¢ [SLOW TEST:35.104 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:17:37.661: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:17:37.820: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 26 23:17:41.855: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 26 23:17:43.873: INFO: Creating deployment "test-rollover-deployment"
Aug 26 23:17:43.912: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 26 23:17:45.950: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 26 23:17:45.984: INFO: Ensure that both replica sets have 1 created replica
Aug 26 23:17:46.017: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 26 23:17:46.066: INFO: Updating deployment test-rollover-deployment
Aug 26 23:17:46.066: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 26 23:17:48.101: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 26 23:17:48.135: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 26 23:17:48.168: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 23:17:48.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458266, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 23:17:50.204: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 23:17:50.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458269, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 23:17:52.204: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 23:17:52.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458269, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 23:17:54.203: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 23:17:54.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458269, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 23:17:56.203: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 23:17:56.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458269, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 23:17:58.203: INFO: all replica sets need to contain the pod-template-hash label
Aug 26 23:17:58.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458269, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702458263, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 26 23:18:00.203: INFO: 
Aug 26 23:18:00.203: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 26 23:18:00.253: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-521,SelfLink:/apis/apps/v1/namespaces/deployment-521/deployments/test-rollover-deployment,UID:b52c8d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18368,Generation:2,CreationTimestamp:2019-08-26 23:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-26 23:17:43 +0000 UTC 2019-08-26 23:17:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-26 23:17:59 +0000 UTC 2019-08-26 23:17:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 26 23:18:00.272: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-521,SelfLink:/apis/apps/v1/namespaces/deployment-521/replicasets/test-rollover-deployment-659c699649,UID:b6792b12-c857-11e9-8aff-12fef04b6120,ResourceVersion:18356,Generation:2,CreationTimestamp:2019-08-26 23:17:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b52c8d7e-c857-11e9-8aff-12fef04b6120 0xc002944e37 0xc002944e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 26 23:18:00.272: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 26 23:18:00.272: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-521,SelfLink:/apis/apps/v1/namespaces/deployment-521/replicasets/test-rollover-controller,UID:b18b3ac4-c857-11e9-8aff-12fef04b6120,ResourceVersion:18366,Generation:2,CreationTimestamp:2019-08-26 23:17:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b52c8d7e-c857-11e9-8aff-12fef04b6120 0xc002944d67 0xc002944d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 26 23:18:00.272: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-521,SelfLink:/apis/apps/v1/namespaces/deployment-521/replicasets/test-rollover-deployment-7b45b6464,UID:b52f65ac-c857-11e9-8aff-12fef04b6120,ResourceVersion:18281,Generation:2,CreationTimestamp:2019-08-26 23:17:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b52c8d7e-c857-11e9-8aff-12fef04b6120 0xc002944f00 0xc002944f01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 26 23:18:00.289: INFO: Pod "test-rollover-deployment-659c699649-bvnhd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-bvnhd,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-521,SelfLink:/api/v1/namespaces/deployment-521/pods/test-rollover-deployment-659c699649-bvnhd,UID:b6800d3b-c857-11e9-8aff-12fef04b6120,ResourceVersion:18314,Generation:0,CreationTimestamp:2019-08-26 23:17:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 b6792b12-c857-11e9-8aff-12fef04b6120 0xc002945b37 0xc002945b38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lmzl7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lmzl7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lmzl7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-bbc6j}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:17:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:17:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:17:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-26 23:17:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:10.129.2.14,StartTime:2019-08-26 23:17:46 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-26 23:17:48 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://7e472bae199b1dbd7641f40fae9052f7411ebdb2a2610b949ae1dd0826a3a9e0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:18:00.289: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-521" for this suite.
Aug 26 23:18:06.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:18:08.537: INFO: namespace deployment-521 deletion completed in 8.21612644s

â€¢ [SLOW TEST:30.875 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:18:08.538: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 26 23:18:08.721: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18490,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 26 23:18:08.721: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18490,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 26 23:18:18.761: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18533,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 26 23:18:18.761: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18533,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 26 23:18:28.799: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18572,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 26 23:18:28.800: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18572,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 26 23:18:38.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18615,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 26 23:18:38.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-a,UID:c3f91d7e-c857-11e9-8aff-12fef04b6120,ResourceVersion:18615,Generation:0,CreationTimestamp:2019-08-26 23:18:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 26 23:18:48.849: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-b,UID:dbe39cad-c857-11e9-8aff-12fef04b6120,ResourceVersion:18656,Generation:0,CreationTimestamp:2019-08-26 23:18:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 26 23:18:48.849: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-b,UID:dbe39cad-c857-11e9-8aff-12fef04b6120,ResourceVersion:18656,Generation:0,CreationTimestamp:2019-08-26 23:18:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 26 23:18:58.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-b,UID:dbe39cad-c857-11e9-8aff-12fef04b6120,ResourceVersion:18699,Generation:0,CreationTimestamp:2019-08-26 23:18:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 26 23:18:58.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6837,SelfLink:/api/v1/namespaces/watch-6837/configmaps/e2e-watch-test-configmap-b,UID:dbe39cad-c857-11e9-8aff-12fef04b6120,ResourceVersion:18699,Generation:0,CreationTimestamp:2019-08-26 23:18:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:19:08.873: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-6837" for this suite.
Aug 26 23:19:14.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:19:17.108: INFO: namespace watch-6837 deletion completed in 8.202803195s

â€¢ [SLOW TEST:68.570 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:19:17.108: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:19:17.296: INFO: (0) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 40.41058ms)
Aug 26 23:19:17.314: INFO: (1) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.619772ms)
Aug 26 23:19:17.332: INFO: (2) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.31466ms)
Aug 26 23:19:17.350: INFO: (3) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.238322ms)
Aug 26 23:19:17.368: INFO: (4) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.55836ms)
Aug 26 23:19:17.385: INFO: (5) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.444173ms)
Aug 26 23:19:17.407: INFO: (6) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 21.800654ms)
Aug 26 23:19:17.425: INFO: (7) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.789404ms)
Aug 26 23:19:17.442: INFO: (8) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.639451ms)
Aug 26 23:19:17.460: INFO: (9) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.438643ms)
Aug 26 23:19:17.477: INFO: (10) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.431629ms)
Aug 26 23:19:17.496: INFO: (11) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.428309ms)
Aug 26 23:19:17.515: INFO: (12) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.249329ms)
Aug 26 23:19:17.533: INFO: (13) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.84278ms)
Aug 26 23:19:17.551: INFO: (14) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.25205ms)
Aug 26 23:19:17.568: INFO: (15) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.608396ms)
Aug 26 23:19:17.586: INFO: (16) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.345999ms)
Aug 26 23:19:17.603: INFO: (17) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.314791ms)
Aug 26 23:19:17.620: INFO: (18) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.060938ms)
Aug 26 23:19:17.640: INFO: (19) /api/v1/nodes/ip-10-0-129-239.ec2.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 19.434977ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:19:17.640: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-9732" for this suite.
Aug 26 23:19:23.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:19:24.936: INFO: namespace proxy-9732 deletion completed in 7.277914818s

â€¢ [SLOW TEST:7.828 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:19:24.936: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0826 23:20:05.217724     729 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 26 23:20:05.217: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:20:05.217: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-2332" for this suite.
Aug 26 23:20:13.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:20:15.447: INFO: namespace gc-2332 deletion completed in 10.211955807s

â€¢ [SLOW TEST:50.512 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:20:15.448: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2662
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 26 23:20:15.564: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug 26 23:20:37.984: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.21:8080/dial?request=hostName&protocol=http&host=10.129.2.18&port=8080&tries=1'] Namespace:pod-network-test-2662 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:20:37.984: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:20:38.221: INFO: Waiting for endpoints: map[]
Aug 26 23:20:38.238: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.21:8080/dial?request=hostName&protocol=http&host=10.128.2.16&port=8080&tries=1'] Namespace:pod-network-test-2662 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:20:38.238: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:20:38.444: INFO: Waiting for endpoints: map[]
Aug 26 23:20:38.461: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.21:8080/dial?request=hostName&protocol=http&host=10.131.0.20&port=8080&tries=1'] Namespace:pod-network-test-2662 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:20:38.461: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:20:38.656: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:20:38.656: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-2662" for this suite.
Aug 26 23:21:02.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:21:04.888: INFO: namespace pod-network-test-2662 deletion completed in 26.200104496s

â€¢ [SLOW TEST:49.440 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:21:04.889: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Aug 26 23:21:05.044: INFO: Waiting up to 5m0s for pod "var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583" in namespace "var-expansion-8602" to be "success or failure"
Aug 26 23:21:05.063: INFO: Pod "var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.963982ms
Aug 26 23:21:07.080: INFO: Pod "var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036401186s
Aug 26 23:21:09.098: INFO: Pod "var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053970929s
Aug 26 23:21:11.115: INFO: Pod "var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071086511s
STEP: Saw pod success
Aug 26 23:21:11.115: INFO: Pod "var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:21:11.132: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:21:11.179: INFO: Waiting for pod var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:21:11.196: INFO: Pod var-expansion-2d0eeb00-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:21:11.196: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-8602" for this suite.
Aug 26 23:21:17.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:21:19.453: INFO: namespace var-expansion-8602 deletion completed in 8.22623902s

â€¢ [SLOW TEST:14.565 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:21:19.453: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-35bc878c-c858-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 26 23:21:19.621: INFO: Waiting up to 5m0s for pod "pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583" in namespace "secrets-1134" to be "success or failure"
Aug 26 23:21:19.640: INFO: Pod "pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 19.107862ms
Aug 26 23:21:21.658: INFO: Pod "pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037248141s
Aug 26 23:21:23.675: INFO: Pod "pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054565311s
Aug 26 23:21:25.692: INFO: Pod "pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071437404s
STEP: Saw pod success
Aug 26 23:21:25.692: INFO: Pod "pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:21:25.709: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 26 23:21:25.775: INFO: Waiting for pod pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:21:25.791: INFO: Pod pod-secrets-35bf6984-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:21:25.791: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1134" for this suite.
Aug 26 23:21:31.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:21:34.015: INFO: namespace secrets-1134 deletion completed in 8.191998977s

â€¢ [SLOW TEST:14.562 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:21:34.015: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:22:02.094: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-runtime-5588" for this suite.
Aug 26 23:22:08.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:22:10.323: INFO: namespace container-runtime-5588 deletion completed in 8.196078478s

â€¢ [SLOW TEST:36.308 seconds]
[k8s.io] Container Runtime
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:22:10.323: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:22:10.502: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583" in namespace "projected-3591" to be "success or failure"
Aug 26 23:22:10.519: INFO: Pod "downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.074633ms
Aug 26 23:22:12.539: INFO: Pod "downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036687506s
Aug 26 23:22:14.557: INFO: Pod "downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05431462s
STEP: Saw pod success
Aug 26 23:22:14.557: INFO: Pod "downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:22:14.576: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:22:14.625: INFO: Waiting for pod downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:22:14.641: INFO: Pod downwardapi-volume-5411d792-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:22:14.641: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3591" for this suite.
Aug 26 23:22:20.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:22:22.873: INFO: namespace projected-3591 deletion completed in 8.20031505s

â€¢ [SLOW TEST:12.549 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:22:22.873: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Aug 26 23:22:23.034: INFO: Waiting up to 5m0s for pod "var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583" in namespace "var-expansion-7714" to be "success or failure"
Aug 26 23:22:23.051: INFO: Pod "var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.927919ms
Aug 26 23:22:25.068: INFO: Pod "var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034382298s
Aug 26 23:22:27.087: INFO: Pod "var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053104528s
STEP: Saw pod success
Aug 26 23:22:27.087: INFO: Pod "var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:22:27.105: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:22:27.151: INFO: Waiting for pod var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:22:27.168: INFO: Pod var-expansion-5b8b75a2-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:22:27.168: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-7714" for this suite.
Aug 26 23:22:33.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:22:35.405: INFO: namespace var-expansion-7714 deletion completed in 8.204776316s

â€¢ [SLOW TEST:12.532 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:22:35.405: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-901
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 26 23:22:35.550: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug 26 23:22:57.970: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-901 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:22:57.970: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:22:59.189: INFO: Found all expected endpoints: [netserver-0]
Aug 26 23:22:59.206: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.20 8081 | grep -v '^\s*$'] Namespace:pod-network-test-901 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:22:59.206: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:23:00.397: INFO: Found all expected endpoints: [netserver-1]
Aug 26 23:23:00.414: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.24 8081 | grep -v '^\s*$'] Namespace:pod-network-test-901 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:23:00.414: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:23:01.589: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:23:01.589: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-901" for this suite.
Aug 26 23:23:25.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:23:27.848: INFO: namespace pod-network-test-901 deletion completed in 26.214205454s

â€¢ [SLOW TEST:52.443 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:23:27.848: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 26 23:23:28.122: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:28.122: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:28.122: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:28.140: INFO: Number of nodes with available pods: 0
Aug 26 23:23:28.140: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:23:29.172: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:29.173: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:29.173: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:29.190: INFO: Number of nodes with available pods: 0
Aug 26 23:23:29.190: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:23:30.171: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:30.172: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:30.172: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:30.188: INFO: Number of nodes with available pods: 1
Aug 26 23:23:30.188: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 26 23:23:31.172: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:31.172: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:31.172: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:31.189: INFO: Number of nodes with available pods: 2
Aug 26 23:23:31.189: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 26 23:23:32.172: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:32.172: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:32.172: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:32.190: INFO: Number of nodes with available pods: 3
Aug 26 23:23:32.190: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 26 23:23:32.264: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:32.264: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:32.264: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:32.282: INFO: Number of nodes with available pods: 2
Aug 26 23:23:32.282: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:23:33.315: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:33.315: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:33.315: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:33.333: INFO: Number of nodes with available pods: 2
Aug 26 23:23:33.333: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:23:34.314: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:34.314: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:34.314: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:34.331: INFO: Number of nodes with available pods: 2
Aug 26 23:23:34.331: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:23:35.315: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:23:35.315: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:35.315: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:23:35.332: INFO: Number of nodes with available pods: 3
Aug 26 23:23:35.332: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8412, will wait for the garbage collector to delete the pods
Aug 26 23:23:35.455: INFO: Deleting DaemonSet.extensions daemon-set took: 22.491999ms
Aug 26 23:23:35.656: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.436305ms
Aug 26 23:23:48.173: INFO: Number of nodes with available pods: 0
Aug 26 23:23:48.173: INFO: Number of running nodes: 0, number of available pods: 0
Aug 26 23:23:48.191: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8412/daemonsets","resourceVersion":"21076"},"items":null}

Aug 26 23:23:48.208: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8412/pods","resourceVersion":"21076"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:23:48.276: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-8412" for this suite.
Aug 26 23:23:54.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:23:56.512: INFO: namespace daemonsets-8412 deletion completed in 8.204128818s

â€¢ [SLOW TEST:28.664 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:23:56.512: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Aug 26 23:23:56.677: INFO: Waiting up to 5m0s for pod "client-containers-935ad4ad-c858-11e9-974e-0a58ac107583" in namespace "containers-8076" to be "success or failure"
Aug 26 23:23:56.695: INFO: Pod "client-containers-935ad4ad-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.612534ms
Aug 26 23:23:58.714: INFO: Pod "client-containers-935ad4ad-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036406471s
Aug 26 23:24:00.731: INFO: Pod "client-containers-935ad4ad-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053609642s
Aug 26 23:24:02.748: INFO: Pod "client-containers-935ad4ad-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071397089s
STEP: Saw pod success
Aug 26 23:24:02.749: INFO: Pod "client-containers-935ad4ad-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:24:02.766: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod client-containers-935ad4ad-c858-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:24:02.816: INFO: Waiting for pod client-containers-935ad4ad-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:24:02.832: INFO: Pod client-containers-935ad4ad-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:24:02.832: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-8076" for this suite.
Aug 26 23:24:08.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:24:11.076: INFO: namespace containers-8076 deletion completed in 8.197648451s

â€¢ [SLOW TEST:14.564 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:24:11.076: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:24:11.236: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583" in namespace "downward-api-6184" to be "success or failure"
Aug 26 23:24:11.260: INFO: Pod "downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 24.857682ms
Aug 26 23:24:13.278: INFO: Pod "downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042551567s
Aug 26 23:24:15.296: INFO: Pod "downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060434555s
Aug 26 23:24:17.314: INFO: Pod "downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078250191s
STEP: Saw pod success
Aug 26 23:24:17.314: INFO: Pod "downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:24:17.331: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:24:17.377: INFO: Waiting for pod downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:24:17.394: INFO: Pod downwardapi-volume-9c095500-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:24:17.394: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6184" for this suite.
Aug 26 23:24:23.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:24:25.632: INFO: namespace downward-api-6184 deletion completed in 8.207601152s

â€¢ [SLOW TEST:14.556 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:24:25.633: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 26 23:24:35.939: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 26 23:24:35.956: INFO: Pod pod-with-prestop-http-hook still exists
Aug 26 23:24:37.956: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 26 23:24:37.973: INFO: Pod pod-with-prestop-http-hook still exists
Aug 26 23:24:39.956: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 26 23:24:39.973: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:24:40.001: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1488" for this suite.
Aug 26 23:25:04.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:25:06.252: INFO: namespace container-lifecycle-hook-1488 deletion completed in 26.217460093s

â€¢ [SLOW TEST:40.619 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:25:06.252: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 26 23:25:06.409: INFO: Waiting up to 5m0s for pod "pod-bcec9490-c858-11e9-974e-0a58ac107583" in namespace "emptydir-7570" to be "success or failure"
Aug 26 23:25:06.426: INFO: Pod "pod-bcec9490-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.779772ms
Aug 26 23:25:08.444: INFO: Pod "pod-bcec9490-c858-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03560373s
Aug 26 23:25:10.462: INFO: Pod "pod-bcec9490-c858-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053061693s
STEP: Saw pod success
Aug 26 23:25:10.462: INFO: Pod "pod-bcec9490-c858-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:25:10.479: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-bcec9490-c858-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:25:10.532: INFO: Waiting for pod pod-bcec9490-c858-11e9-974e-0a58ac107583 to disappear
Aug 26 23:25:10.549: INFO: Pod pod-bcec9490-c858-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:25:10.549: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-7570" for this suite.
Aug 26 23:25:16.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:25:18.792: INFO: namespace emptydir-7570 deletion completed in 8.198975772s

â€¢ [SLOW TEST:12.540 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:25:18.792: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 26 23:25:18.922: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:25:23.812: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-4152" for this suite.
Aug 26 23:25:29.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:25:32.051: INFO: namespace init-container-4152 deletion completed in 8.207772374s

â€¢ [SLOW TEST:13.259 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:25:32.051: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:25:36.250: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-3816" for this suite.
Aug 26 23:25:42.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:25:44.512: INFO: namespace kubelet-test-3816 deletion completed in 8.231196867s

â€¢ [SLOW TEST:12.461 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:25:44.512: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Aug 26 23:25:50.732: INFO: Pod pod-hostip-d3b9a6fd-c858-11e9-974e-0a58ac107583 has hostIP: 10.0.155.122
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:25:50.733: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-816" for this suite.
Aug 26 23:26:14.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:26:16.965: INFO: namespace pods-816 deletion completed in 26.201205799s

â€¢ [SLOW TEST:32.453 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:26:16.966: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5135
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 26 23:26:17.140: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug 26 23:26:39.580: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.32:8080/dial?request=hostName&protocol=udp&host=10.131.0.31&port=8081&tries=1'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:26:39.580: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:26:39.824: INFO: Waiting for endpoints: map[]
Aug 26 23:26:39.842: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.32:8080/dial?request=hostName&protocol=udp&host=10.129.2.25&port=8081&tries=1'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:26:39.842: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:26:40.045: INFO: Waiting for endpoints: map[]
Aug 26 23:26:40.062: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.32:8080/dial?request=hostName&protocol=udp&host=10.128.2.25&port=8081&tries=1'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:26:40.062: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:26:40.259: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:26:40.259: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-5135" for this suite.
Aug 26 23:27:04.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:27:06.519: INFO: namespace pod-network-test-5135 deletion completed in 26.22833528s

â€¢ [SLOW TEST:49.553 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:27:06.519: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:27:06.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583" in namespace "downward-api-8297" to be "success or failure"
Aug 26 23:27:06.713: INFO: Pod "downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131732ms
Aug 26 23:27:08.730: INFO: Pod "downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034486987s
Aug 26 23:27:10.750: INFO: Pod "downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05447036s
STEP: Saw pod success
Aug 26 23:27:10.750: INFO: Pod "downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:27:10.767: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:27:10.815: INFO: Waiting for pod downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:27:10.831: INFO: Pod downwardapi-volume-049df70c-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:27:10.831: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8297" for this suite.
Aug 26 23:27:16.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:27:19.101: INFO: namespace downward-api-8297 deletion completed in 8.238910939s

â€¢ [SLOW TEST:12.582 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:27:19.102: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Aug 26 23:27:19.232: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig api-versions'
Aug 26 23:27:19.395: INFO: stderr: ""
Aug 26 23:27:19.396: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhealthchecking.openshift.io/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:27:19.396: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8963" for this suite.
Aug 26 23:27:25.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:27:27.632: INFO: namespace kubectl-8963 deletion completed in 8.21669452s

â€¢ [SLOW TEST:8.530 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:27:27.632: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Aug 26 23:27:27.799: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:27:27.948: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-6689" for this suite.
Aug 26 23:27:34.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:27:36.198: INFO: namespace kubectl-6689 deletion completed in 8.229660245s

â€¢ [SLOW TEST:8.566 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:27:36.198: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Aug 26 23:27:36.339: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-8766'
Aug 26 23:27:36.949: INFO: stderr: ""
Aug 26 23:27:36.949: INFO: stdout: "pod/pause created\n"
Aug 26 23:27:36.949: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 26 23:27:36.949: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8766" to be "running and ready"
Aug 26 23:27:36.966: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.170411ms
Aug 26 23:27:38.984: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035206582s
Aug 26 23:27:41.002: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052812203s
Aug 26 23:27:43.020: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.070784774s
Aug 26 23:27:43.020: INFO: Pod "pause" satisfied condition "running and ready"
Aug 26 23:27:43.020: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 26 23:27:43.020: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label=testing-label-value --namespace=kubectl-8766'
Aug 26 23:27:43.287: INFO: stderr: ""
Aug 26 23:27:43.287: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 26 23:27:43.287: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=kubectl-8766'
Aug 26 23:27:43.471: INFO: stderr: ""
Aug 26 23:27:43.471: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 26 23:27:43.471: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig label pods pause testing-label- --namespace=kubectl-8766'
Aug 26 23:27:43.686: INFO: stderr: ""
Aug 26 23:27:43.686: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 26 23:27:43.686: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod pause -L testing-label --namespace=kubectl-8766'
Aug 26 23:27:43.866: INFO: stderr: ""
Aug 26 23:27:43.866: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Aug 26 23:27:43.866: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-8766'
Aug 26 23:27:44.070: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:27:44.070: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 26 23:27:44.070: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=pause --no-headers --namespace=kubectl-8766'
Aug 26 23:27:44.269: INFO: stderr: "No resources found.\n"
Aug 26 23:27:44.269: INFO: stdout: ""
Aug 26 23:27:44.269: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=pause --namespace=kubectl-8766 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 26 23:27:44.444: INFO: stderr: ""
Aug 26 23:27:44.444: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:27:44.444: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8766" for this suite.
Aug 26 23:27:50.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:27:52.678: INFO: namespace kubectl-8766 deletion completed in 8.202174246s

â€¢ [SLOW TEST:16.480 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:27:52.679: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 26 23:27:52.831: INFO: Waiting up to 5m0s for pod "downward-api-201de137-c859-11e9-974e-0a58ac107583" in namespace "downward-api-2263" to be "success or failure"
Aug 26 23:27:52.848: INFO: Pod "downward-api-201de137-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.816981ms
Aug 26 23:27:54.865: INFO: Pod "downward-api-201de137-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034171053s
Aug 26 23:27:56.882: INFO: Pod "downward-api-201de137-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051467073s
STEP: Saw pod success
Aug 26 23:27:56.882: INFO: Pod "downward-api-201de137-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:27:56.899: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod downward-api-201de137-c859-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:27:56.949: INFO: Waiting for pod downward-api-201de137-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:27:56.966: INFO: Pod downward-api-201de137-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:27:56.966: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2263" for this suite.
Aug 26 23:28:03.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:28:05.215: INFO: namespace downward-api-2263 deletion completed in 8.203517367s

â€¢ [SLOW TEST:12.536 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:28:05.215: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 26 23:28:05.431: INFO: Waiting up to 5m0s for pod "downward-api-27a00574-c859-11e9-974e-0a58ac107583" in namespace "downward-api-5673" to be "success or failure"
Aug 26 23:28:05.451: INFO: Pod "downward-api-27a00574-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 19.889671ms
Aug 26 23:28:07.469: INFO: Pod "downward-api-27a00574-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037950156s
Aug 26 23:28:09.487: INFO: Pod "downward-api-27a00574-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056024852s
STEP: Saw pod success
Aug 26 23:28:09.487: INFO: Pod "downward-api-27a00574-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:28:09.505: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downward-api-27a00574-c859-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:28:09.552: INFO: Waiting for pod downward-api-27a00574-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:28:09.568: INFO: Pod downward-api-27a00574-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:28:09.568: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5673" for this suite.
Aug 26 23:28:15.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:28:17.787: INFO: namespace downward-api-5673 deletion completed in 8.187870148s

â€¢ [SLOW TEST:12.572 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:28:17.788: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Aug 26 23:28:17.929: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-2420'
Aug 26 23:28:18.446: INFO: stderr: ""
Aug 26 23:28:18.446: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 26 23:28:18.446: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2420'
Aug 26 23:28:18.604: INFO: stderr: ""
Aug 26 23:28:18.604: INFO: stdout: "update-demo-nautilus-cmwhd update-demo-nautilus-jsbvj "
Aug 26 23:28:18.604: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-cmwhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:18.769: INFO: stderr: ""
Aug 26 23:28:18.769: INFO: stdout: ""
Aug 26 23:28:18.769: INFO: update-demo-nautilus-cmwhd is created but not running
Aug 26 23:28:23.769: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2420'
Aug 26 23:28:23.945: INFO: stderr: ""
Aug 26 23:28:23.945: INFO: stdout: "update-demo-nautilus-cmwhd update-demo-nautilus-jsbvj "
Aug 26 23:28:23.945: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-cmwhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:24.119: INFO: stderr: ""
Aug 26 23:28:24.119: INFO: stdout: "true"
Aug 26 23:28:24.119: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-cmwhd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:24.284: INFO: stderr: ""
Aug 26 23:28:24.284: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:28:24.284: INFO: validating pod update-demo-nautilus-cmwhd
Aug 26 23:28:24.304: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:28:24.304: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:28:24.304: INFO: update-demo-nautilus-cmwhd is verified up and running
Aug 26 23:28:24.304: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-jsbvj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:24.477: INFO: stderr: ""
Aug 26 23:28:24.477: INFO: stdout: "true"
Aug 26 23:28:24.477: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-jsbvj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:24.649: INFO: stderr: ""
Aug 26 23:28:24.649: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:28:24.649: INFO: validating pod update-demo-nautilus-jsbvj
Aug 26 23:28:24.670: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:28:24.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:28:24.670: INFO: update-demo-nautilus-jsbvj is verified up and running
STEP: rolling-update to new replication controller
Aug 26 23:28:24.673: INFO: scanned /tmp/home for discovery docs: <nil>
Aug 26 23:28:24.673: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2420'
Aug 26 23:28:51.952: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 26 23:28:51.952: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 26 23:28:51.952: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2420'
Aug 26 23:28:52.125: INFO: stderr: ""
Aug 26 23:28:52.125: INFO: stdout: "update-demo-kitten-jfd2f update-demo-kitten-pg4kg "
Aug 26 23:28:52.125: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-jfd2f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:52.283: INFO: stderr: ""
Aug 26 23:28:52.283: INFO: stdout: "true"
Aug 26 23:28:52.283: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-jfd2f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:52.440: INFO: stderr: ""
Aug 26 23:28:52.440: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 26 23:28:52.440: INFO: validating pod update-demo-kitten-jfd2f
Aug 26 23:28:52.460: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 26 23:28:52.460: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 26 23:28:52.460: INFO: update-demo-kitten-jfd2f is verified up and running
Aug 26 23:28:52.460: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-pg4kg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:52.622: INFO: stderr: ""
Aug 26 23:28:52.622: INFO: stdout: "true"
Aug 26 23:28:52.622: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-kitten-pg4kg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2420'
Aug 26 23:28:52.782: INFO: stderr: ""
Aug 26 23:28:52.782: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 26 23:28:52.782: INFO: validating pod update-demo-kitten-pg4kg
Aug 26 23:28:52.803: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 26 23:28:52.803: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 26 23:28:52.803: INFO: update-demo-kitten-pg4kg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:28:52.803: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2420" for this suite.
Aug 26 23:29:16.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:29:19.027: INFO: namespace kubectl-2420 deletion completed in 26.192652676s

â€¢ [SLOW TEST:61.240 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:29:19.028: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 26 23:29:19.141: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Aug 26 23:29:19.176: INFO: Waiting for terminating namespaces to be deleted...
Aug 26 23:29:19.194: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-239.ec2.internal before test
Aug 26 23:29:19.216: INFO: dns-default-fnqls from openshift-dns started at 2019-08-26 23:00:41 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container dns ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 26 23:29:19.216: INFO: kube-state-metrics-5f57cc8b6f-p56kl from openshift-monitoring started at 2019-08-26 23:01:41 +0000 UTC (3 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 26 23:29:19.216: INFO: router-default-7879f59486-qp8d2 from openshift-ingress started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container router ready: true, restart count 0
Aug 26 23:29:19.216: INFO: ovs-qnv4t from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container openvswitch ready: true, restart count 0
Aug 26 23:29:19.216: INFO: redhat-operators-67f45f9754-t96bh from openshift-marketplace started at 2019-08-26 23:01:47 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 26 23:29:19.216: INFO: certified-operators-575dfbb7d8-frm4v from openshift-marketplace started at 2019-08-26 23:01:47 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container certified-operators ready: true, restart count 0
Aug 26 23:29:19.216: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-08-26 23:03:08 +0000 UTC (3 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container alertmanager ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container config-reloader ready: true, restart count 0
Aug 26 23:29:19.216: INFO: sdn-xnf2w from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container sdn ready: true, restart count 0
Aug 26 23:29:19.216: INFO: node-exporter-nrtz2 from openshift-monitoring started at 2019-08-26 23:00:41 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container node-exporter ready: true, restart count 0
Aug 26 23:29:19.216: INFO: machine-config-daemon-qtvl9 from openshift-machine-config-operator started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 26 23:29:19.216: INFO: image-registry-78bd5f6f67-49xzm from openshift-image-registry started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container registry ready: true, restart count 0
Aug 26 23:29:19.216: INFO: node-ca-jsqtx from openshift-image-registry started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container node-ca ready: true, restart count 0
Aug 26 23:29:19.216: INFO: multus-mj66c from openshift-multus started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container kube-multus ready: true, restart count 0
Aug 26 23:29:19.216: INFO: tuned-p49z9 from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container tuned ready: true, restart count 0
Aug 26 23:29:19.216: INFO: community-operators-848c796fb7-94fds from openshift-marketplace started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container community-operators ready: true, restart count 0
Aug 26 23:29:19.216: INFO: openshift-state-metrics-6f45d6c785-jnjkl from openshift-monitoring started at 2019-08-26 23:01:43 +0000 UTC (3 container statuses recorded)
Aug 26 23:29:19.216: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 26 23:29:19.216: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-139-255.ec2.internal before test
Aug 26 23:29:19.241: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-08-26 23:03:20 +0000 UTC (3 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container alertmanager ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container config-reloader ready: true, restart count 0
Aug 26 23:29:19.241: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-08-26 23:03:00 +0000 UTC (6 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container prometheus ready: true, restart count 1
Aug 26 23:29:19.241: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 26 23:29:19.241: INFO: multus-qtzdx from openshift-multus started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container kube-multus ready: true, restart count 0
Aug 26 23:29:19.241: INFO: ovs-7mgp8 from openshift-sdn started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container openvswitch ready: true, restart count 0
Aug 26 23:29:19.241: INFO: grafana-79767c7d48-clxjd from openshift-monitoring started at 2019-08-26 23:02:16 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container grafana ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 26 23:29:19.241: INFO: downloads-67cbd588c5-qgc4d from openshift-console started at 2019-08-26 23:02:16 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container download-server ready: true, restart count 0
Aug 26 23:29:19.241: INFO: prometheus-adapter-664f76c4d5-98pc9 from openshift-monitoring started at 2019-08-26 23:02:58 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 26 23:29:19.241: INFO: dns-default-x64xg from openshift-dns started at 2019-08-26 23:00:44 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container dns ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 26 23:29:19.241: INFO: sdn-lpp6w from openshift-sdn started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container sdn ready: true, restart count 0
Aug 26 23:29:19.241: INFO: node-ca-4p4gt from openshift-image-registry started at 2019-08-26 23:01:53 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container node-ca ready: true, restart count 0
Aug 26 23:29:19.241: INFO: node-exporter-mhdpr from openshift-monitoring started at 2019-08-26 23:00:44 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 	Container node-exporter ready: true, restart count 0
Aug 26 23:29:19.241: INFO: tuned-4w8vz from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container tuned ready: true, restart count 0
Aug 26 23:29:19.241: INFO: machine-config-daemon-csqcg from openshift-machine-config-operator started at 2019-08-26 23:01:53 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.241: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 26 23:29:19.241: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-155-122.ec2.internal before test
Aug 26 23:29:19.263: INFO: sdn-lvs9b from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container sdn ready: true, restart count 0
Aug 26 23:29:19.263: INFO: node-ca-ncxgw from openshift-image-registry started at 2019-08-26 23:01:51 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container node-ca ready: true, restart count 0
Aug 26 23:29:19.263: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-08-26 23:03:00 +0000 UTC (6 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container prometheus ready: true, restart count 1
Aug 26 23:29:19.263: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 26 23:29:19.263: INFO: ovs-hq6b7 from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container openvswitch ready: true, restart count 0
Aug 26 23:29:19.263: INFO: multus-gmgxj from openshift-multus started at 2019-08-26 23:00:42 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container kube-multus ready: true, restart count 0
Aug 26 23:29:19.263: INFO: tuned-rwhck from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:42 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container tuned ready: true, restart count 0
Aug 26 23:29:19.263: INFO: machine-config-daemon-jtl47 from openshift-machine-config-operator started at 2019-08-26 23:01:50 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 26 23:29:19.263: INFO: telemeter-client-9c775b868-4wbnh from openshift-monitoring started at 2019-08-26 23:02:56 +0000 UTC (3 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container reload ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 26 23:29:19.263: INFO: node-exporter-7xwt8 from openshift-monitoring started at 2019-08-26 23:00:42 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container node-exporter ready: true, restart count 0
Aug 26 23:29:19.263: INFO: dns-default-gggf8 from openshift-dns started at 2019-08-26 23:00:42 +0000 UTC (2 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container dns ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 26 23:29:19.263: INFO: downloads-67cbd588c5-7fz6d from openshift-console started at 2019-08-26 23:02:16 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container download-server ready: true, restart count 0
Aug 26 23:29:19.263: INFO: router-default-7879f59486-d98jc from openshift-ingress started at 2019-08-26 23:01:52 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container router ready: true, restart count 0
Aug 26 23:29:19.263: INFO: prometheus-adapter-664f76c4d5-5l7g7 from openshift-monitoring started at 2019-08-26 23:02:58 +0000 UTC (1 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 26 23:29:19.263: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-08-26 23:03:28 +0000 UTC (3 container statuses recorded)
Aug 26 23:29:19.263: INFO: 	Container alertmanager ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 26 23:29:19.263: INFO: 	Container config-reloader ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-10-0-129-239.ec2.internal
STEP: verifying the node has the label node ip-10-0-139-255.ec2.internal
STEP: verifying the node has the label node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod tuned-4w8vz requesting resource cpu=10m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod tuned-p49z9 requesting resource cpu=10m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod tuned-rwhck requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod downloads-67cbd588c5-7fz6d requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod downloads-67cbd588c5-qgc4d requesting resource cpu=10m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod dns-default-fnqls requesting resource cpu=110m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod dns-default-gggf8 requesting resource cpu=110m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod dns-default-x64xg requesting resource cpu=110m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod image-registry-78bd5f6f67-49xzm requesting resource cpu=100m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod node-ca-4p4gt requesting resource cpu=10m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod node-ca-jsqtx requesting resource cpu=10m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod node-ca-ncxgw requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod router-default-7879f59486-d98jc requesting resource cpu=100m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod router-default-7879f59486-qp8d2 requesting resource cpu=100m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod machine-config-daemon-csqcg requesting resource cpu=20m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod machine-config-daemon-jtl47 requesting resource cpu=20m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod machine-config-daemon-qtvl9 requesting resource cpu=20m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod certified-operators-575dfbb7d8-frm4v requesting resource cpu=0m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod community-operators-848c796fb7-94fds requesting resource cpu=0m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod redhat-operators-67f45f9754-t96bh requesting resource cpu=0m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod grafana-79767c7d48-clxjd requesting resource cpu=100m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod kube-state-metrics-5f57cc8b6f-p56kl requesting resource cpu=30m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod node-exporter-7xwt8 requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod node-exporter-mhdpr requesting resource cpu=10m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod node-exporter-nrtz2 requesting resource cpu=10m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod openshift-state-metrics-6f45d6c785-jnjkl requesting resource cpu=120m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod prometheus-adapter-664f76c4d5-5l7g7 requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod prometheus-adapter-664f76c4d5-98pc9 requesting resource cpu=10m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod prometheus-k8s-0 requesting resource cpu=430m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod prometheus-k8s-1 requesting resource cpu=430m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod telemeter-client-9c775b868-4wbnh requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod multus-gmgxj requesting resource cpu=10m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod multus-mj66c requesting resource cpu=10m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod multus-qtzdx requesting resource cpu=10m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod ovs-7mgp8 requesting resource cpu=200m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod ovs-hq6b7 requesting resource cpu=200m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod ovs-qnv4t requesting resource cpu=200m on Node ip-10-0-129-239.ec2.internal
Aug 26 23:29:19.494: INFO: Pod sdn-lpp6w requesting resource cpu=100m on Node ip-10-0-139-255.ec2.internal
Aug 26 23:29:19.494: INFO: Pod sdn-lvs9b requesting resource cpu=100m on Node ip-10-0-155-122.ec2.internal
Aug 26 23:29:19.494: INFO: Pod sdn-xnf2w requesting resource cpu=100m on Node ip-10-0-129-239.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583.15be9cd51ed2d75d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-584/filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583 to ip-10-0-139-255.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583.15be9cd5a266f56e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583.15be9cd5ab37ade1], Reason = [Created], Message = [Created container filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583.15be9cd5ac6ae6e1], Reason = [Started], Message = [Started container filler-pod-53caa5a2-c859-11e9-974e-0a58ac107583]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d05a25-c859-11e9-974e-0a58ac107583.15be9cd520663c82], Reason = [Scheduled], Message = [Successfully assigned sched-pred-584/filler-pod-53d05a25-c859-11e9-974e-0a58ac107583 to ip-10-0-155-122.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d05a25-c859-11e9-974e-0a58ac107583.15be9cd59f396523], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d05a25-c859-11e9-974e-0a58ac107583.15be9cd5a888e4b1], Reason = [Created], Message = [Created container filler-pod-53d05a25-c859-11e9-974e-0a58ac107583]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d05a25-c859-11e9-974e-0a58ac107583.15be9cd5a9ed9e58], Reason = [Started], Message = [Started container filler-pod-53d05a25-c859-11e9-974e-0a58ac107583]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d47b04-c859-11e9-974e-0a58ac107583.15be9cd521fd9f7b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-584/filler-pod-53d47b04-c859-11e9-974e-0a58ac107583 to ip-10-0-129-239.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d47b04-c859-11e9-974e-0a58ac107583.15be9cd5be2fb22d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d47b04-c859-11e9-974e-0a58ac107583.15be9cd5c8bf6a7d], Reason = [Created], Message = [Created container filler-pod-53d47b04-c859-11e9-974e-0a58ac107583]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53d47b04-c859-11e9-974e-0a58ac107583.15be9cd5ca0310c1], Reason = [Started], Message = [Started container filler-pod-53d47b04-c859-11e9-974e-0a58ac107583]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15be9cd617eb62ce], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) were unschedulable.]
STEP: removing the label node off the node ip-10-0-129-239.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-139-255.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-155-122.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:29:24.905: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-584" for this suite.
Aug 26 23:29:30.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:29:33.129: INFO: namespace sched-pred-584 deletion completed in 8.192963023s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:14.101 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:29:33.129: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:29:33.267: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5c004477-c859-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-5c004477-c859-11e9-974e-0a58ac107583
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:30:50.160: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5679" for this suite.
Aug 26 23:31:14.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:31:16.401: INFO: namespace projected-5679 deletion completed in 26.196064774s

â€¢ [SLOW TEST:103.272 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:31:16.402: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Aug 26 23:31:16.527: INFO: Asynchronously running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl kubectl --kubeconfig=/tmp/admin.kubeconfig proxy --unix-socket=/tmp/kubectl-proxy-unix632026309/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:31:16.627: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-2425" for this suite.
Aug 26 23:31:22.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:31:24.835: INFO: namespace kubectl-2425 deletion completed in 8.189387913s

â€¢ [SLOW TEST:8.433 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:31:24.835: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-9e958e8e-c859-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 26 23:31:25.028: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583" in namespace "projected-3253" to be "success or failure"
Aug 26 23:31:25.045: INFO: Pod "pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.162912ms
Aug 26 23:31:27.062: INFO: Pod "pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034852832s
Aug 26 23:31:29.080: INFO: Pod "pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05241556s
STEP: Saw pod success
Aug 26 23:31:29.080: INFO: Pod "pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:31:29.097: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 26 23:31:29.147: INFO: Waiting for pod pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:31:29.163: INFO: Pod pod-projected-secrets-9e98f275-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:31:29.163: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3253" for this suite.
Aug 26 23:31:35.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:31:37.457: INFO: namespace projected-3253 deletion completed in 8.249923047s

â€¢ [SLOW TEST:12.621 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:31:37.457: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:31:37.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583" in namespace "projected-7486" to be "success or failure"
Aug 26 23:31:37.632: INFO: Pod "downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.575378ms
Aug 26 23:31:39.649: INFO: Pod "downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033894535s
Aug 26 23:31:41.666: INFO: Pod "downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051075859s
STEP: Saw pod success
Aug 26 23:31:41.666: INFO: Pod "downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:31:41.683: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:31:41.730: INFO: Waiting for pod downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:31:41.746: INFO: Pod downwardapi-volume-a6190564-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:31:41.746: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7486" for this suite.
Aug 26 23:31:47.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:31:49.984: INFO: namespace projected-7486 deletion completed in 8.193905462s

â€¢ [SLOW TEST:12.527 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:31:49.985: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ad918ac7-c859-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 26 23:31:50.173: INFO: Waiting up to 5m0s for pod "pod-secrets-ad946717-c859-11e9-974e-0a58ac107583" in namespace "secrets-8442" to be "success or failure"
Aug 26 23:31:50.191: INFO: Pod "pod-secrets-ad946717-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.364812ms
Aug 26 23:31:52.209: INFO: Pod "pod-secrets-ad946717-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035547277s
Aug 26 23:31:54.227: INFO: Pod "pod-secrets-ad946717-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053244046s
STEP: Saw pod success
Aug 26 23:31:54.227: INFO: Pod "pod-secrets-ad946717-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:31:54.243: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-secrets-ad946717-c859-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 26 23:31:54.290: INFO: Waiting for pod pod-secrets-ad946717-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:31:54.306: INFO: Pod pod-secrets-ad946717-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:31:54.306: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-8442" for this suite.
Aug 26 23:32:00.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:32:02.554: INFO: namespace secrets-8442 deletion completed in 8.203667167s

â€¢ [SLOW TEST:12.569 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:32:02.554: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 26 23:32:02.728: INFO: Waiting up to 5m0s for pod "downward-api-b510df45-c859-11e9-974e-0a58ac107583" in namespace "downward-api-645" to be "success or failure"
Aug 26 23:32:02.746: INFO: Pod "downward-api-b510df45-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.720048ms
Aug 26 23:32:04.763: INFO: Pod "downward-api-b510df45-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035510791s
Aug 26 23:32:06.783: INFO: Pod "downward-api-b510df45-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055317303s
STEP: Saw pod success
Aug 26 23:32:06.783: INFO: Pod "downward-api-b510df45-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:32:06.801: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downward-api-b510df45-c859-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:32:06.848: INFO: Waiting for pod downward-api-b510df45-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:32:06.865: INFO: Pod downward-api-b510df45-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:32:06.865: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-645" for this suite.
Aug 26 23:32:12.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:32:15.144: INFO: namespace downward-api-645 deletion completed in 8.234719771s

â€¢ [SLOW TEST:12.590 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:32:15.144: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-3703
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3703 to expose endpoints map[]
Aug 26 23:32:15.320: INFO: successfully validated that service endpoint-test2 in namespace services-3703 exposes endpoints map[] (16.750364ms elapsed)
STEP: Creating pod pod1 in namespace services-3703
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3703 to expose endpoints map[pod1:[80]]
Aug 26 23:32:19.539: INFO: successfully validated that service endpoint-test2 in namespace services-3703 exposes endpoints map[pod1:[80]] (4.175641761s elapsed)
STEP: Creating pod pod2 in namespace services-3703
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3703 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 26 23:32:22.765: INFO: successfully validated that service endpoint-test2 in namespace services-3703 exposes endpoints map[pod1:[80] pod2:[80]] (3.200572056s elapsed)
STEP: Deleting pod pod1 in namespace services-3703
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3703 to expose endpoints map[pod2:[80]]
Aug 26 23:32:22.820: INFO: successfully validated that service endpoint-test2 in namespace services-3703 exposes endpoints map[pod2:[80]] (33.617723ms elapsed)
STEP: Deleting pod pod2 in namespace services-3703
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3703 to expose endpoints map[]
Aug 26 23:32:22.858: INFO: successfully validated that service endpoint-test2 in namespace services-3703 exposes endpoints map[] (16.367574ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:32:22.899: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-3703" for this suite.
Aug 26 23:32:28.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:32:31.146: INFO: namespace services-3703 deletion completed in 8.202289971s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:16.001 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:32:31.146: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-851
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-851
STEP: Deleting pre-stop pod
Aug 26 23:32:48.483: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:32:48.504: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "prestop-851" for this suite.
Aug 26 23:33:28.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:33:30.744: INFO: namespace prestop-851 deletion completed in 42.195671342s

â€¢ [SLOW TEST:59.598 seconds]
[k8s.io] [sig-node] PreStop
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:33:30.744: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 26 23:33:35.503: INFO: Successfully updated pod "pod-update-e99d71cd-c859-11e9-974e-0a58ac107583"
STEP: verifying the updated pod is in kubernetes
Aug 26 23:33:35.537: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:33:35.537: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-1249" for this suite.
Aug 26 23:33:59.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:34:01.769: INFO: namespace pods-1249 deletion completed in 26.18793477s

â€¢ [SLOW TEST:31.025 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:34:01.769: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:34:01.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583" in namespace "downward-api-8645" to be "success or failure"
Aug 26 23:34:01.944: INFO: Pod "downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.546258ms
Aug 26 23:34:03.962: INFO: Pod "downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035490365s
Aug 26 23:34:05.981: INFO: Pod "downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053870685s
STEP: Saw pod success
Aug 26 23:34:05.981: INFO: Pod "downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:34:05.998: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:34:06.047: INFO: Waiting for pod downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583 to disappear
Aug 26 23:34:06.064: INFO: Pod downwardapi-volume-fc1ca584-c859-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:34:06.064: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-8645" for this suite.
Aug 26 23:34:12.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:34:14.339: INFO: namespace downward-api-8645 deletion completed in 8.230008275s

â€¢ [SLOW TEST:12.569 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:34:14.339: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 26 23:34:14.499: INFO: Waiting up to 5m0s for pod "pod-039c0cb4-c85a-11e9-974e-0a58ac107583" in namespace "emptydir-7189" to be "success or failure"
Aug 26 23:34:14.516: INFO: Pod "pod-039c0cb4-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.274089ms
Aug 26 23:34:16.534: INFO: Pod "pod-039c0cb4-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034622838s
Aug 26 23:34:18.551: INFO: Pod "pod-039c0cb4-c85a-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052506786s
STEP: Saw pod success
Aug 26 23:34:18.552: INFO: Pod "pod-039c0cb4-c85a-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:34:18.568: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-039c0cb4-c85a-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:34:18.614: INFO: Waiting for pod pod-039c0cb4-c85a-11e9-974e-0a58ac107583 to disappear
Aug 26 23:34:18.630: INFO: Pod pod-039c0cb4-c85a-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:34:18.630: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-7189" for this suite.
Aug 26 23:34:24.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:34:26.882: INFO: namespace emptydir-7189 deletion completed in 8.207510544s

â€¢ [SLOW TEST:12.543 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:34:26.882: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 26 23:34:27.048: INFO: Waiting up to 5m0s for pod "pod-0b16e331-c85a-11e9-974e-0a58ac107583" in namespace "emptydir-5493" to be "success or failure"
Aug 26 23:34:27.066: INFO: Pod "pod-0b16e331-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.465898ms
Aug 26 23:34:29.084: INFO: Pod "pod-0b16e331-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036226075s
Aug 26 23:34:31.101: INFO: Pod "pod-0b16e331-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053679678s
Aug 26 23:34:33.119: INFO: Pod "pod-0b16e331-c85a-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07164477s
STEP: Saw pod success
Aug 26 23:34:33.119: INFO: Pod "pod-0b16e331-c85a-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:34:33.137: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-0b16e331-c85a-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:34:33.188: INFO: Waiting for pod pod-0b16e331-c85a-11e9-974e-0a58ac107583 to disappear
Aug 26 23:34:33.205: INFO: Pod pod-0b16e331-c85a-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:34:33.205: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-5493" for this suite.
Aug 26 23:34:39.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:34:41.441: INFO: namespace emptydir-5493 deletion completed in 8.192173734s

â€¢ [SLOW TEST:14.559 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:34:41.441: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:34:41.621: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583" in namespace "projected-1391" to be "success or failure"
Aug 26 23:34:41.638: INFO: Pod "downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.231496ms
Aug 26 23:34:43.656: INFO: Pod "downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034745706s
Aug 26 23:34:45.673: INFO: Pod "downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052182675s
STEP: Saw pod success
Aug 26 23:34:45.673: INFO: Pod "downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:34:45.690: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:34:45.736: INFO: Waiting for pod downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583 to disappear
Aug 26 23:34:45.753: INFO: Pod downwardapi-volume-13c5ad04-c85a-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:34:45.753: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1391" for this suite.
Aug 26 23:34:51.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:34:53.994: INFO: namespace projected-1391 deletion completed in 8.196266197s

â€¢ [SLOW TEST:12.552 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:34:53.994: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 26 23:34:54.122: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4481'
Aug 26 23:34:54.329: INFO: stderr: ""
Aug 26 23:34:54.329: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 26 23:34:59.380: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pod e2e-test-nginx-pod --namespace=kubectl-4481 -o json'
Aug 26 23:34:59.545: INFO: stderr: ""
Aug 26 23:34:59.545: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2019-08-26T23:34:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4481\",\n        \"resourceVersion\": \"26311\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4481/pods/e2e-test-nginx-pod\",\n        \"uid\": \"1b5a24f4-c85a-11e9-8aff-12fef04b6120\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-7jjms\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-ljgxv\"\n            }\n        ],\n        \"nodeName\": \"ip-10-0-155-122.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c31,c0\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-7jjms\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-7jjms\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-26T23:34:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-26T23:34:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-26T23:34:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-26T23:34:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://7d19c3d8da8b51b63733464fecf9eb15c925881353cb4eac1121e5838375bad8\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-26T23:34:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.155.122\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.2.36\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-26T23:34:54Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 26 23:34:59.546: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig replace -f - --namespace=kubectl-4481'
Aug 26 23:34:59.976: INFO: stderr: ""
Aug 26 23:34:59.976: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Aug 26 23:34:59.995: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=kubectl-4481'
Aug 26 23:35:02.361: INFO: stderr: ""
Aug 26 23:35:02.361: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:35:02.361: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4481" for this suite.
Aug 26 23:35:08.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:35:10.605: INFO: namespace kubectl-4481 deletion completed in 8.200125308s

â€¢ [SLOW TEST:16.611 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:35:10.605: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Aug 26 23:35:10.719: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 26 23:35:10.719: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9337'
Aug 26 23:35:11.137: INFO: stderr: ""
Aug 26 23:35:11.137: INFO: stdout: "service/redis-slave created\n"
Aug 26 23:35:11.137: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 26 23:35:11.137: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9337'
Aug 26 23:35:11.548: INFO: stderr: ""
Aug 26 23:35:11.548: INFO: stdout: "service/redis-master created\n"
Aug 26 23:35:11.549: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 26 23:35:11.549: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9337'
Aug 26 23:35:11.955: INFO: stderr: ""
Aug 26 23:35:11.955: INFO: stdout: "service/frontend created\n"
Aug 26 23:35:11.956: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 26 23:35:11.956: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9337'
Aug 26 23:35:12.380: INFO: stderr: ""
Aug 26 23:35:12.380: INFO: stdout: "deployment.apps/frontend created\n"
Aug 26 23:35:12.380: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 26 23:35:12.380: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9337'
Aug 26 23:35:12.786: INFO: stderr: ""
Aug 26 23:35:12.786: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 26 23:35:12.787: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 26 23:35:12.787: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9337'
Aug 26 23:35:13.169: INFO: stderr: ""
Aug 26 23:35:13.169: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 26 23:35:13.169: INFO: Waiting for all frontend pods to be Running.
Aug 26 23:35:33.221: INFO: Waiting for frontend to serve content.
Aug 26 23:35:38.260: INFO: Trying to add a new entry to the guestbook.
Aug 26 23:35:43.299: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 26 23:35:48.333: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9337'
Aug 26 23:35:48.556: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:35:48.556: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 26 23:35:48.556: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9337'
Aug 26 23:35:48.746: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:35:48.746: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 26 23:35:48.746: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9337'
Aug 26 23:35:48.935: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:35:48.935: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 26 23:35:48.936: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9337'
Aug 26 23:35:49.111: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:35:49.112: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 26 23:35:49.112: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9337'
Aug 26 23:35:49.297: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:35:49.297: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 26 23:35:49.298: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-9337'
Aug 26 23:35:49.489: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:35:49.489: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:35:49.490: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9337" for this suite.
Aug 26 23:36:31.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:36:33.728: INFO: namespace kubectl-9337 deletion completed in 44.193520105s

â€¢ [SLOW TEST:83.123 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:36:33.729: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:36:33.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583" in namespace "downward-api-7652" to be "success or failure"
Aug 26 23:36:33.913: INFO: Pod "downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 19.124159ms
Aug 26 23:36:35.933: INFO: Pod "downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039712591s
Aug 26 23:36:37.951: INFO: Pod "downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057927299s
STEP: Saw pod success
Aug 26 23:36:37.951: INFO: Pod "downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:36:37.969: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:36:38.015: INFO: Waiting for pod downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583 to disappear
Aug 26 23:36:38.034: INFO: Pod downwardapi-volume-56b24a63-c85a-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:36:38.034: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7652" for this suite.
Aug 26 23:36:44.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:36:46.273: INFO: namespace downward-api-7652 deletion completed in 8.1949779s

â€¢ [SLOW TEST:12.545 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:36:46.274: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:36:46.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583" in namespace "downward-api-2643" to be "success or failure"
Aug 26 23:36:46.444: INFO: Pod "downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.286276ms
Aug 26 23:36:48.461: INFO: Pod "downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034624182s
Aug 26 23:36:50.479: INFO: Pod "downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052405234s
STEP: Saw pod success
Aug 26 23:36:50.479: INFO: Pod "downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:36:50.496: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:36:50.543: INFO: Waiting for pod downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583 to disappear
Aug 26 23:36:50.559: INFO: Pod downwardapi-volume-5e2aaa5e-c85a-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:36:50.559: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-2643" for this suite.
Aug 26 23:36:56.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:36:58.797: INFO: namespace downward-api-2643 deletion completed in 8.194151019s

â€¢ [SLOW TEST:12.523 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:36:58.798: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 26 23:36:58.963: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3111'
Aug 26 23:36:59.167: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 26 23:36:59.168: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 26 23:36:59.202: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-qhhd7]
Aug 26 23:36:59.202: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-qhhd7" in namespace "kubectl-3111" to be "running and ready"
Aug 26 23:36:59.218: INFO: Pod "e2e-test-nginx-rc-qhhd7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.325088ms
Aug 26 23:37:01.236: INFO: Pod "e2e-test-nginx-rc-qhhd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034195938s
Aug 26 23:37:03.253: INFO: Pod "e2e-test-nginx-rc-qhhd7": Phase="Running", Reason="", readiness=true. Elapsed: 4.051344231s
Aug 26 23:37:03.254: INFO: Pod "e2e-test-nginx-rc-qhhd7" satisfied condition "running and ready"
Aug 26 23:37:03.254: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-qhhd7]
Aug 26 23:37:03.254: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs rc/e2e-test-nginx-rc --namespace=kubectl-3111'
Aug 26 23:37:03.483: INFO: stderr: ""
Aug 26 23:37:03.483: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Aug 26 23:37:03.483: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=kubectl-3111'
Aug 26 23:37:03.668: INFO: stderr: ""
Aug 26 23:37:03.668: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:37:03.668: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-3111" for this suite.
Aug 26 23:37:09.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:37:11.905: INFO: namespace kubectl-3111 deletion completed in 8.19157152s

â€¢ [SLOW TEST:13.107 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:37:11.905: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 26 23:37:16.689: INFO: Successfully updated pod "labelsupdate6d735302-c85a-11e9-974e-0a58ac107583"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:37:20.749: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6657" for this suite.
Aug 26 23:37:44.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:37:46.976: INFO: namespace projected-6657 deletion completed in 26.182874944s

â€¢ [SLOW TEST:35.071 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:37:46.976: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 26 23:37:47.120: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:37:52.959: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-572" for this suite.
Aug 26 23:37:59.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:38:01.210: INFO: namespace init-container-572 deletion completed in 8.206715363s

â€¢ [SLOW TEST:14.234 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:38:01.211: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:38:01.388: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-7312" for this suite.
Aug 26 23:38:25.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:38:27.610: INFO: namespace kubelet-test-7312 deletion completed in 26.204140251s

â€¢ [SLOW TEST:26.400 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:38:27.611: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0826 23:38:37.892263     729 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 26 23:38:37.892: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:38:37.892: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-9255" for this suite.
Aug 26 23:38:43.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:38:46.477: INFO: namespace gc-9255 deletion completed in 8.566329063s

â€¢ [SLOW TEST:18.866 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:38:46.477: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:38:50.758: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-1447" for this suite.
Aug 26 23:39:14.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:39:17.006: INFO: namespace replication-controller-1447 deletion completed in 26.202757394s

â€¢ [SLOW TEST:30.529 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:39:17.006: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:39:17.173: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-b80934df-c85a-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b80934df-c85a-11e9-974e-0a58ac107583
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:40:26.010: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2171" for this suite.
Aug 26 23:40:50.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:40:52.287: INFO: namespace configmap-2171 deletion completed in 26.232141376s

â€¢ [SLOW TEST:95.281 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:40:52.288: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Aug 26 23:40:53.022: INFO: created pod pod-service-account-defaultsa
Aug 26 23:40:53.022: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 26 23:40:53.050: INFO: created pod pod-service-account-mountsa
Aug 26 23:40:53.050: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 26 23:40:53.079: INFO: created pod pod-service-account-nomountsa
Aug 26 23:40:53.079: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 26 23:40:53.168: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 26 23:40:53.168: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 26 23:40:53.194: INFO: created pod pod-service-account-mountsa-mountspec
Aug 26 23:40:53.194: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 26 23:40:53.220: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 26 23:40:53.220: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 26 23:40:53.251: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 26 23:40:53.251: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 26 23:40:53.278: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 26 23:40:53.278: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 26 23:40:53.303: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 26 23:40:53.303: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:40:53.303: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-3621" for this suite.
Aug 26 23:41:17.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:41:19.531: INFO: namespace svcaccounts-3621 deletion completed in 26.196629499s

â€¢ [SLOW TEST:27.243 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:41:19.531: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:41:19.690: INFO: Waiting up to 5m0s for pod "downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583" in namespace "downward-api-7456" to be "success or failure"
Aug 26 23:41:19.707: INFO: Pod "downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.943303ms
Aug 26 23:41:21.727: INFO: Pod "downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037400529s
Aug 26 23:41:23.745: INFO: Pod "downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055151049s
STEP: Saw pod success
Aug 26 23:41:23.745: INFO: Pod "downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:41:23.761: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:41:23.809: INFO: Waiting for pod downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583 to disappear
Aug 26 23:41:23.826: INFO: Pod downwardapi-volume-010b2fc7-c85b-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:41:23.826: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-7456" for this suite.
Aug 26 23:41:29.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:41:32.069: INFO: namespace downward-api-7456 deletion completed in 8.198848616s

â€¢ [SLOW TEST:12.538 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:41:32.070: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8676
[It] Should recreate evicted statefulset [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8676
STEP: Creating statefulset with conflicting port in namespace statefulset-8676
STEP: Waiting until pod test-pod will start running in namespace statefulset-8676
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8676
Aug 26 23:41:38.345: INFO: Observed stateful pod in namespace: statefulset-8676, name: ss-0, uid: 0c274377-c85b-11e9-8aff-12fef04b6120, status phase: Pending. Waiting for statefulset controller to delete.
Aug 26 23:41:40.287: INFO: Observed stateful pod in namespace: statefulset-8676, name: ss-0, uid: 0c274377-c85b-11e9-8aff-12fef04b6120, status phase: Failed. Waiting for statefulset controller to delete.
Aug 26 23:41:40.297: INFO: Observed stateful pod in namespace: statefulset-8676, name: ss-0, uid: 0c274377-c85b-11e9-8aff-12fef04b6120, status phase: Failed. Waiting for statefulset controller to delete.
Aug 26 23:41:40.302: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8676
STEP: Removing pod with conflicting port in namespace statefulset-8676
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8676 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 26 23:41:46.398: INFO: Deleting all statefulset in ns statefulset-8676
Aug 26 23:41:46.414: INFO: Scaling statefulset ss to 0
Aug 26 23:41:56.485: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 23:41:56.502: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:41:56.556: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-8676" for this suite.
Aug 26 23:42:02.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:42:04.795: INFO: namespace statefulset-8676 deletion completed in 8.193931935s

â€¢ [SLOW TEST:32.725 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:42:04.796: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Aug 26 23:42:04.929: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-660'
Aug 26 23:42:05.570: INFO: stderr: ""
Aug 26 23:42:05.570: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 26 23:42:05.570: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-660'
Aug 26 23:42:05.728: INFO: stderr: ""
Aug 26 23:42:05.728: INFO: stdout: "update-demo-nautilus-6bvf2 update-demo-nautilus-bfkxh "
Aug 26 23:42:05.728: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-6bvf2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-660'
Aug 26 23:42:05.895: INFO: stderr: ""
Aug 26 23:42:05.895: INFO: stdout: ""
Aug 26 23:42:05.895: INFO: update-demo-nautilus-6bvf2 is created but not running
Aug 26 23:42:10.895: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-660'
Aug 26 23:42:11.057: INFO: stderr: ""
Aug 26 23:42:11.057: INFO: stdout: "update-demo-nautilus-6bvf2 update-demo-nautilus-bfkxh "
Aug 26 23:42:11.057: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-6bvf2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-660'
Aug 26 23:42:11.210: INFO: stderr: ""
Aug 26 23:42:11.210: INFO: stdout: "true"
Aug 26 23:42:11.211: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-6bvf2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-660'
Aug 26 23:42:11.369: INFO: stderr: ""
Aug 26 23:42:11.369: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:42:11.369: INFO: validating pod update-demo-nautilus-6bvf2
Aug 26 23:42:11.389: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:42:11.389: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:42:11.389: INFO: update-demo-nautilus-6bvf2 is verified up and running
Aug 26 23:42:11.389: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-bfkxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-660'
Aug 26 23:42:11.548: INFO: stderr: ""
Aug 26 23:42:11.548: INFO: stdout: "true"
Aug 26 23:42:11.548: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-bfkxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-660'
Aug 26 23:42:11.705: INFO: stderr: ""
Aug 26 23:42:11.705: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:42:11.705: INFO: validating pod update-demo-nautilus-bfkxh
Aug 26 23:42:11.725: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:42:11.725: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:42:11.725: INFO: update-demo-nautilus-bfkxh is verified up and running
STEP: using delete to clean up resources
Aug 26 23:42:11.725: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-660'
Aug 26 23:42:11.906: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:42:11.906: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 26 23:42:11.906: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-660'
Aug 26 23:42:12.095: INFO: stderr: "No resources found.\n"
Aug 26 23:42:12.095: INFO: stdout: ""
Aug 26 23:42:12.095: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=kubectl-660 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 26 23:42:12.252: INFO: stderr: ""
Aug 26 23:42:12.252: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:42:12.253: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-660" for this suite.
Aug 26 23:42:36.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:42:38.491: INFO: namespace kubectl-660 deletion completed in 26.193795265s

â€¢ [SLOW TEST:33.695 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:42:38.491: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 26 23:42:43.405: INFO: Successfully updated pod "labelsupdate302ea378-c85b-11e9-974e-0a58ac107583"
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:42:47.469: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6301" for this suite.
Aug 26 23:43:11.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:43:13.720: INFO: namespace downward-api-6301 deletion completed in 26.206667416s

â€¢ [SLOW TEST:35.229 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:43:13.720: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 26 23:43:13.856: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-5963'
Aug 26 23:43:14.077: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 26 23:43:14.077: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Aug 26 23:43:16.118: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=kubectl-5963'
Aug 26 23:43:16.312: INFO: stderr: ""
Aug 26 23:43:16.312: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:43:16.312: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5963" for this suite.
Aug 26 23:43:40.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:43:42.551: INFO: namespace kubectl-5963 deletion completed in 26.193012897s

â€¢ [SLOW TEST:28.831 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:43:42.551: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:43:42.690: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:43:46.864: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-2544" for this suite.
Aug 26 23:44:24.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:44:27.107: INFO: namespace pods-2544 deletion completed in 40.198371468s

â€¢ [SLOW TEST:44.556 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:44:27.107: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-70d9d3a8-c85b-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 26 23:44:27.297: INFO: Waiting up to 5m0s for pod "pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583" in namespace "secrets-9024" to be "success or failure"
Aug 26 23:44:27.317: INFO: Pod "pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 19.338859ms
Aug 26 23:44:29.334: INFO: Pod "pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036989901s
Aug 26 23:44:31.354: INFO: Pod "pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056879364s
STEP: Saw pod success
Aug 26 23:44:31.354: INFO: Pod "pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:44:31.371: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583 container secret-env-test: <nil>
STEP: delete the pod
Aug 26 23:44:31.420: INFO: Waiting for pod pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583 to disappear
Aug 26 23:44:31.437: INFO: Pod pod-secrets-70ddc1a6-c85b-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:44:31.437: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9024" for this suite.
Aug 26 23:44:37.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:44:39.675: INFO: namespace secrets-9024 deletion completed in 8.193384883s

â€¢ [SLOW TEST:12.568 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:44:39.675: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:44:39.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583" in namespace "projected-138" to be "success or failure"
Aug 26 23:44:39.848: INFO: Pod "downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.706879ms
Aug 26 23:44:41.866: INFO: Pod "downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035479392s
Aug 26 23:44:43.883: INFO: Pod "downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052760804s
STEP: Saw pod success
Aug 26 23:44:43.883: INFO: Pod "downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:44:43.900: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:44:43.947: INFO: Waiting for pod downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583 to disappear
Aug 26 23:44:43.963: INFO: Pod downwardapi-volume-78566377-c85b-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:44:43.963: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-138" for this suite.
Aug 26 23:44:50.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:44:52.207: INFO: namespace projected-138 deletion completed in 8.198584578s

â€¢ [SLOW TEST:12.532 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:44:52.208: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:44:52.350: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583" in namespace "downward-api-1572" to be "success or failure"
Aug 26 23:44:52.367: INFO: Pod "downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.718241ms
Aug 26 23:44:54.385: INFO: Pod "downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034663888s
Aug 26 23:44:56.404: INFO: Pod "downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05391482s
STEP: Saw pod success
Aug 26 23:44:56.404: INFO: Pod "downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:44:56.421: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:44:56.470: INFO: Waiting for pod downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583 to disappear
Aug 26 23:44:56.486: INFO: Pod downwardapi-volume-7fcbb248-c85b-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:44:56.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1572" for this suite.
Aug 26 23:45:02.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:45:04.755: INFO: namespace downward-api-1572 deletion completed in 8.223617819s

â€¢ [SLOW TEST:12.547 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:45:04.755: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:45:26.976: INFO: Container started at 2019-08-26 23:45:09 +0000 UTC, pod became ready at 2019-08-26 23:45:25 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:45:26.976: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-551" for this suite.
Aug 26 23:45:51.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:45:53.234: INFO: namespace container-probe-551 deletion completed in 26.214230948s

â€¢ [SLOW TEST:48.480 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:45:53.236: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a42f6160-c85b-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 26 23:45:53.416: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583" in namespace "projected-862" to be "success or failure"
Aug 26 23:45:53.434: INFO: Pod "pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.16372ms
Aug 26 23:45:55.452: INFO: Pod "pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036108806s
Aug 26 23:45:57.471: INFO: Pod "pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054491779s
STEP: Saw pod success
Aug 26 23:45:57.471: INFO: Pod "pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:45:57.487: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 26 23:45:57.541: INFO: Waiting for pod pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583 to disappear
Aug 26 23:45:57.558: INFO: Pod pod-projected-configmaps-a43258ab-c85b-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:45:57.558: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-862" for this suite.
Aug 26 23:46:03.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:46:05.817: INFO: namespace projected-862 deletion completed in 8.213827021s

â€¢ [SLOW TEST:12.581 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:46:05.817: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:46:06.058: INFO: Create a RollingUpdate DaemonSet
Aug 26 23:46:06.081: INFO: Check that daemon pods launch on every node of the cluster
Aug 26 23:46:06.099: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:06.100: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:06.100: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:06.119: INFO: Number of nodes with available pods: 0
Aug 26 23:46:06.119: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:46:07.165: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:07.165: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:07.165: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:07.182: INFO: Number of nodes with available pods: 0
Aug 26 23:46:07.182: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:46:08.165: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:08.165: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:08.165: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:08.184: INFO: Number of nodes with available pods: 1
Aug 26 23:46:08.184: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:46:09.164: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:09.165: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:09.165: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:09.182: INFO: Number of nodes with available pods: 2
Aug 26 23:46:09.182: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 26 23:46:10.165: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:10.165: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:10.165: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:10.182: INFO: Number of nodes with available pods: 3
Aug 26 23:46:10.182: INFO: Number of running nodes: 3, number of available pods: 3
Aug 26 23:46:10.182: INFO: Update the DaemonSet to trigger a rollout
Aug 26 23:46:10.220: INFO: Updating DaemonSet daemon-set
Aug 26 23:46:18.280: INFO: Roll back the DaemonSet before rollout is complete
Aug 26 23:46:18.318: INFO: Updating DaemonSet daemon-set
Aug 26 23:46:18.318: INFO: Make sure DaemonSet rollback is complete
Aug 26 23:46:18.335: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:18.335: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:18.367: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:18.368: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:18.368: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:19.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:19.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:19.418: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:19.418: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:19.418: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:20.385: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:20.385: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:20.417: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:20.418: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:20.418: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:21.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:21.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:21.417: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:21.417: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:21.417: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:22.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:22.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:22.417: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:22.417: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:22.417: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:23.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:23.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:23.418: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:23.418: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:23.418: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:24.388: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:24.388: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:24.420: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:24.420: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:24.420: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:25.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:25.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:25.418: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:25.418: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:25.418: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:26.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:26.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:26.417: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:26.417: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:26.417: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:27.386: INFO: Wrong image for pod: daemon-set-f7kwp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 26 23:46:27.386: INFO: Pod daemon-set-f7kwp is not available
Aug 26 23:46:27.418: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:27.418: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:27.418: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:28.388: INFO: Pod daemon-set-7dxth is not available
Aug 26 23:46:28.422: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 26 23:46:28.422: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 26 23:46:28.423: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9881, will wait for the garbage collector to delete the pods
Aug 26 23:46:28.546: INFO: Deleting DaemonSet.extensions daemon-set took: 22.420178ms
Aug 26 23:46:28.747: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.48451ms
Aug 26 23:46:30.664: INFO: Number of nodes with available pods: 0
Aug 26 23:46:30.664: INFO: Number of running nodes: 0, number of available pods: 0
Aug 26 23:46:30.681: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9881/daemonsets","resourceVersion":"31549"},"items":null}

Aug 26 23:46:30.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9881/pods","resourceVersion":"31549"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:46:30.781: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-9881" for this suite.
Aug 26 23:46:36.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:46:39.006: INFO: namespace daemonsets-9881 deletion completed in 8.193874491s

â€¢ [SLOW TEST:33.189 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:46:39.006: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Aug 26 23:46:39.152: INFO: Waiting up to 5m0s for pod "var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583" in namespace "var-expansion-3585" to be "success or failure"
Aug 26 23:46:39.169: INFO: Pod "var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.783136ms
Aug 26 23:46:41.187: INFO: Pod "var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034560447s
Aug 26 23:46:43.204: INFO: Pod "var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05228641s
STEP: Saw pod success
Aug 26 23:46:43.204: INFO: Pod "var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:46:43.221: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:46:43.277: INFO: Waiting for pod var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583 to disappear
Aug 26 23:46:43.294: INFO: Pod var-expansion-bf7579c9-c85b-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:46:43.294: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "var-expansion-3585" for this suite.
Aug 26 23:46:49.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:46:51.553: INFO: namespace var-expansion-3585 deletion completed in 8.214199393s

â€¢ [SLOW TEST:12.547 seconds]
[k8s.io] Variable Expansion
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:46:51.553: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:47:51.718: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-6734" for this suite.
Aug 26 23:48:15.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:48:17.968: INFO: namespace container-probe-6734 deletion completed in 26.205069603s

â€¢ [SLOW TEST:86.415 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:48:17.968: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Aug 26 23:48:18.103: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-5228'
Aug 26 23:48:18.548: INFO: stderr: ""
Aug 26 23:48:18.548: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 26 23:48:18.548: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5228'
Aug 26 23:48:18.721: INFO: stderr: ""
Aug 26 23:48:18.721: INFO: stdout: "update-demo-nautilus-gwggq update-demo-nautilus-twkkl "
Aug 26 23:48:18.721: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:18.888: INFO: stderr: ""
Aug 26 23:48:18.888: INFO: stdout: ""
Aug 26 23:48:18.888: INFO: update-demo-nautilus-gwggq is created but not running
Aug 26 23:48:23.889: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5228'
Aug 26 23:48:24.065: INFO: stderr: ""
Aug 26 23:48:24.065: INFO: stdout: "update-demo-nautilus-gwggq update-demo-nautilus-twkkl "
Aug 26 23:48:24.065: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:24.247: INFO: stderr: ""
Aug 26 23:48:24.247: INFO: stdout: "true"
Aug 26 23:48:24.247: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:24.411: INFO: stderr: ""
Aug 26 23:48:24.411: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:48:24.411: INFO: validating pod update-demo-nautilus-gwggq
Aug 26 23:48:24.432: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:48:24.432: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:48:24.432: INFO: update-demo-nautilus-gwggq is verified up and running
Aug 26 23:48:24.432: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-twkkl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:24.598: INFO: stderr: ""
Aug 26 23:48:24.598: INFO: stdout: "true"
Aug 26 23:48:24.598: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-twkkl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:24.772: INFO: stderr: ""
Aug 26 23:48:24.772: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:48:24.772: INFO: validating pod update-demo-nautilus-twkkl
Aug 26 23:48:24.792: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:48:24.792: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:48:24.792: INFO: update-demo-nautilus-twkkl is verified up and running
STEP: scaling down the replication controller
Aug 26 23:48:24.795: INFO: scanned /tmp/home for discovery docs: <nil>
Aug 26 23:48:24.795: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5228'
Aug 26 23:48:26.077: INFO: stderr: ""
Aug 26 23:48:26.077: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 26 23:48:26.077: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5228'
Aug 26 23:48:26.247: INFO: stderr: ""
Aug 26 23:48:26.247: INFO: stdout: "update-demo-nautilus-gwggq update-demo-nautilus-twkkl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 26 23:48:31.247: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5228'
Aug 26 23:48:31.425: INFO: stderr: ""
Aug 26 23:48:31.425: INFO: stdout: "update-demo-nautilus-gwggq "
Aug 26 23:48:31.425: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:31.598: INFO: stderr: ""
Aug 26 23:48:31.598: INFO: stdout: "true"
Aug 26 23:48:31.598: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:31.767: INFO: stderr: ""
Aug 26 23:48:31.767: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:48:31.767: INFO: validating pod update-demo-nautilus-gwggq
Aug 26 23:48:31.785: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:48:31.785: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:48:31.785: INFO: update-demo-nautilus-gwggq is verified up and running
STEP: scaling up the replication controller
Aug 26 23:48:31.788: INFO: scanned /tmp/home for discovery docs: <nil>
Aug 26 23:48:31.788: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5228'
Aug 26 23:48:33.080: INFO: stderr: ""
Aug 26 23:48:33.080: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 26 23:48:33.080: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5228'
Aug 26 23:48:33.259: INFO: stderr: ""
Aug 26 23:48:33.259: INFO: stdout: "update-demo-nautilus-62kzz update-demo-nautilus-gwggq "
Aug 26 23:48:33.260: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-62kzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:33.433: INFO: stderr: ""
Aug 26 23:48:33.433: INFO: stdout: ""
Aug 26 23:48:33.433: INFO: update-demo-nautilus-62kzz is created but not running
Aug 26 23:48:38.433: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5228'
Aug 26 23:48:38.596: INFO: stderr: ""
Aug 26 23:48:38.596: INFO: stdout: "update-demo-nautilus-62kzz update-demo-nautilus-gwggq "
Aug 26 23:48:38.596: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-62kzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:38.759: INFO: stderr: ""
Aug 26 23:48:38.759: INFO: stdout: "true"
Aug 26 23:48:38.759: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-62kzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:38.917: INFO: stderr: ""
Aug 26 23:48:38.917: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:48:38.917: INFO: validating pod update-demo-nautilus-62kzz
Aug 26 23:48:38.938: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:48:38.938: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:48:38.938: INFO: update-demo-nautilus-62kzz is verified up and running
Aug 26 23:48:38.938: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:39.102: INFO: stderr: ""
Aug 26 23:48:39.102: INFO: stdout: "true"
Aug 26 23:48:39.103: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods update-demo-nautilus-gwggq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5228'
Aug 26 23:48:39.264: INFO: stderr: ""
Aug 26 23:48:39.264: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 26 23:48:39.264: INFO: validating pod update-demo-nautilus-gwggq
Aug 26 23:48:39.283: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 26 23:48:39.283: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 26 23:48:39.283: INFO: update-demo-nautilus-gwggq is verified up and running
STEP: using delete to clean up resources
Aug 26 23:48:39.283: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-5228'
Aug 26 23:48:39.461: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 26 23:48:39.461: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 26 23:48:39.461: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5228'
Aug 26 23:48:39.660: INFO: stderr: "No resources found.\n"
Aug 26 23:48:39.660: INFO: stdout: ""
Aug 26 23:48:39.660: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=update-demo --namespace=kubectl-5228 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 26 23:48:39.824: INFO: stderr: ""
Aug 26 23:48:39.824: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:48:39.824: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5228" for this suite.
Aug 26 23:49:03.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:49:06.081: INFO: namespace kubectl-5228 deletion completed in 26.21214445s

â€¢ [SLOW TEST:48.113 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:49:06.081: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:49:06.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583" in namespace "projected-8452" to be "success or failure"
Aug 26 23:49:06.254: INFO: Pod "downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.103907ms
Aug 26 23:49:08.273: INFO: Pod "downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035338588s
Aug 26 23:49:10.290: INFO: Pod "downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052845975s
STEP: Saw pod success
Aug 26 23:49:10.290: INFO: Pod "downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:49:10.307: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:49:10.354: INFO: Waiting for pod downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:49:10.371: INFO: Pod downwardapi-volume-172062d3-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:49:10.371: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8452" for this suite.
Aug 26 23:49:16.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:49:18.646: INFO: namespace projected-8452 deletion completed in 8.230849866s

â€¢ [SLOW TEST:12.565 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:49:18.647: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Aug 26 23:49:18.774: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig --namespace=kubectl-1619 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 26 23:49:22.910: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 26 23:49:22.910: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:49:24.944: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1619" for this suite.
Aug 26 23:49:31.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:49:33.195: INFO: namespace kubectl-1619 deletion completed in 8.205729522s

â€¢ [SLOW TEST:14.548 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:49:33.195: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 26 23:49:33.409: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8997,SelfLink:/api/v1/namespaces/watch-8997/configmaps/e2e-watch-test-watch-closed,UID:274e9cc1-c85c-11e9-8aff-12fef04b6120,ResourceVersion:32862,Generation:0,CreationTimestamp:2019-08-26 23:49:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 26 23:49:33.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8997,SelfLink:/api/v1/namespaces/watch-8997/configmaps/e2e-watch-test-watch-closed,UID:274e9cc1-c85c-11e9-8aff-12fef04b6120,ResourceVersion:32864,Generation:0,CreationTimestamp:2019-08-26 23:49:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 26 23:49:33.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8997,SelfLink:/api/v1/namespaces/watch-8997/configmaps/e2e-watch-test-watch-closed,UID:274e9cc1-c85c-11e9-8aff-12fef04b6120,ResourceVersion:32866,Generation:0,CreationTimestamp:2019-08-26 23:49:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 26 23:49:33.486: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8997,SelfLink:/api/v1/namespaces/watch-8997/configmaps/e2e-watch-test-watch-closed,UID:274e9cc1-c85c-11e9-8aff-12fef04b6120,ResourceVersion:32867,Generation:0,CreationTimestamp:2019-08-26 23:49:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:49:33.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-8997" for this suite.
Aug 26 23:49:39.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:49:41.712: INFO: namespace watch-8997 deletion completed in 8.206792162s

â€¢ [SLOW TEST:8.516 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:49:41.712: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 26 23:49:41.868: INFO: Waiting up to 5m0s for pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583" in namespace "emptydir-2607" to be "success or failure"
Aug 26 23:49:41.885: INFO: Pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962592ms
Aug 26 23:49:43.902: INFO: Pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034868958s
Aug 26 23:49:45.920: INFO: Pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052197724s
Aug 26 23:49:47.937: INFO: Pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069751314s
Aug 26 23:49:49.956: INFO: Pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.088750498s
STEP: Saw pod success
Aug 26 23:49:49.956: INFO: Pod "pod-2c5d9043-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:49:49.973: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-2c5d9043-c85c-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:49:50.025: INFO: Waiting for pod pod-2c5d9043-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:49:50.041: INFO: Pod pod-2c5d9043-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:49:50.042: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2607" for this suite.
Aug 26 23:49:56.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:49:58.294: INFO: namespace emptydir-2607 deletion completed in 8.207502383s

â€¢ [SLOW TEST:16.582 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:49:58.294: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 26 23:49:58.458: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583" in namespace "downward-api-1766" to be "success or failure"
Aug 26 23:49:58.477: INFO: Pod "downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.360735ms
Aug 26 23:50:00.495: INFO: Pod "downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03611656s
Aug 26 23:50:02.513: INFO: Pod "downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054685252s
STEP: Saw pod success
Aug 26 23:50:02.513: INFO: Pod "downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:50:02.531: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 26 23:50:02.586: INFO: Waiting for pod downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:50:02.603: INFO: Pod downwardapi-volume-3640fb33-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:50:02.603: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-1766" for this suite.
Aug 26 23:50:08.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:50:10.845: INFO: namespace downward-api-1766 deletion completed in 8.197197246s

â€¢ [SLOW TEST:12.551 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:50:10.845: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7604
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Aug 26 23:50:11.015: INFO: Found 0 stateful pods, waiting for 3
Aug 26 23:50:21.033: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:50:21.033: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:50:21.033: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Aug 26 23:50:31.034: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:50:31.034: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:50:31.034: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 26 23:50:31.085: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7604 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 26 23:50:31.470: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 26 23:50:31.470: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 26 23:50:31.470: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 26 23:50:41.587: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 26 23:50:51.674: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7604 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 26 23:50:52.017: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 26 23:50:52.017: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 26 23:50:52.017: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 26 23:51:02.120: INFO: Waiting for StatefulSet statefulset-7604/ss2 to complete update
Aug 26 23:51:02.120: INFO: Waiting for Pod statefulset-7604/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 26 23:51:02.120: INFO: Waiting for Pod statefulset-7604/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 26 23:51:12.155: INFO: Waiting for StatefulSet statefulset-7604/ss2 to complete update
Aug 26 23:51:12.156: INFO: Waiting for Pod statefulset-7604/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 26 23:51:12.156: INFO: Waiting for Pod statefulset-7604/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 26 23:51:22.155: INFO: Waiting for StatefulSet statefulset-7604/ss2 to complete update
Aug 26 23:51:22.155: INFO: Waiting for Pod statefulset-7604/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 26 23:51:32.156: INFO: Waiting for StatefulSet statefulset-7604/ss2 to complete update
STEP: Rolling back to a previous revision
Aug 26 23:51:42.155: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7604 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 26 23:51:42.559: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 26 23:51:42.559: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 26 23:51:42.559: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 26 23:51:52.674: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 26 23:51:52.724: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7604 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 26 23:51:53.069: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 26 23:51:53.069: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 26 23:51:53.069: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 26 23:52:03.172: INFO: Waiting for StatefulSet statefulset-7604/ss2 to complete update
Aug 26 23:52:03.172: INFO: Waiting for Pod statefulset-7604/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 26 23:52:03.172: INFO: Waiting for Pod statefulset-7604/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 26 23:52:13.207: INFO: Waiting for StatefulSet statefulset-7604/ss2 to complete update
Aug 26 23:52:13.207: INFO: Waiting for Pod statefulset-7604/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 26 23:52:23.207: INFO: Deleting all statefulset in ns statefulset-7604
Aug 26 23:52:23.223: INFO: Scaling statefulset ss2 to 0
Aug 26 23:52:43.299: INFO: Waiting for statefulset status.replicas updated to 0
Aug 26 23:52:43.315: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:52:43.373: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-7604" for this suite.
Aug 26 23:52:49.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:52:51.637: INFO: namespace statefulset-7604 deletion completed in 8.218239725s

â€¢ [SLOW TEST:160.792 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:52:51.637: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 26 23:52:51.797: INFO: Waiting up to 5m0s for pod "pod-9d928675-c85c-11e9-974e-0a58ac107583" in namespace "emptydir-3448" to be "success or failure"
Aug 26 23:52:51.813: INFO: Pod "pod-9d928675-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.787951ms
Aug 26 23:52:53.831: INFO: Pod "pod-9d928675-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034753367s
Aug 26 23:52:55.849: INFO: Pod "pod-9d928675-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052162531s
STEP: Saw pod success
Aug 26 23:52:55.849: INFO: Pod "pod-9d928675-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:52:55.866: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-9d928675-c85c-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:52:55.914: INFO: Waiting for pod pod-9d928675-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:52:55.931: INFO: Pod pod-9d928675-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:52:55.931: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-3448" for this suite.
Aug 26 23:53:02.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:53:04.181: INFO: namespace emptydir-3448 deletion completed in 8.204788814s

â€¢ [SLOW TEST:12.544 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:53:04.181: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-a50cc552-c85c-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 26 23:53:04.363: INFO: Waiting up to 5m0s for pod "pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583" in namespace "configmap-7415" to be "success or failure"
Aug 26 23:53:04.381: INFO: Pod "pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.18644ms
Aug 26 23:53:06.399: INFO: Pod "pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035249742s
Aug 26 23:53:08.416: INFO: Pod "pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052806109s
STEP: Saw pod success
Aug 26 23:53:08.416: INFO: Pod "pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:53:08.433: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 26 23:53:08.482: INFO: Waiting for pod pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:53:08.498: INFO: Pod pod-configmaps-a50fb3f6-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:53:08.498: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-7415" for this suite.
Aug 26 23:53:14.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:53:16.747: INFO: namespace configmap-7415 deletion completed in 8.203265527s

â€¢ [SLOW TEST:12.566 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:53:16.747: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 26 23:53:16.907: INFO: Waiting up to 5m0s for pod "pod-ac897c48-c85c-11e9-974e-0a58ac107583" in namespace "emptydir-8896" to be "success or failure"
Aug 26 23:53:16.926: INFO: Pod "pod-ac897c48-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 19.032387ms
Aug 26 23:53:18.943: INFO: Pod "pod-ac897c48-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03638541s
Aug 26 23:53:20.961: INFO: Pod "pod-ac897c48-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054304614s
Aug 26 23:53:22.980: INFO: Pod "pod-ac897c48-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072842253s
STEP: Saw pod success
Aug 26 23:53:22.980: INFO: Pod "pod-ac897c48-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:53:22.997: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-ac897c48-c85c-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 26 23:53:23.043: INFO: Waiting for pod pod-ac897c48-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:53:23.059: INFO: Pod pod-ac897c48-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:53:23.059: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-8896" for this suite.
Aug 26 23:53:29.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:53:31.310: INFO: namespace emptydir-8896 deletion completed in 8.206190546s

â€¢ [SLOW TEST:14.563 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:53:31.310: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 26 23:53:31.470: INFO: Waiting up to 5m0s for pod "downward-api-b537e39c-c85c-11e9-974e-0a58ac107583" in namespace "downward-api-5415" to be "success or failure"
Aug 26 23:53:31.488: INFO: Pod "downward-api-b537e39c-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.698245ms
Aug 26 23:53:33.506: INFO: Pod "downward-api-b537e39c-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035415926s
Aug 26 23:53:35.524: INFO: Pod "downward-api-b537e39c-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053581307s
STEP: Saw pod success
Aug 26 23:53:35.524: INFO: Pod "downward-api-b537e39c-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:53:35.541: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod downward-api-b537e39c-c85c-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 26 23:53:35.589: INFO: Waiting for pod downward-api-b537e39c-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:53:35.605: INFO: Pod downward-api-b537e39c-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:53:35.605: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-5415" for this suite.
Aug 26 23:53:41.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:53:43.861: INFO: namespace downward-api-5415 deletion completed in 8.211385259s

â€¢ [SLOW TEST:12.551 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:53:43.862: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-zj6r
STEP: Creating a pod to test atomic-volume-subpath
Aug 26 23:53:44.064: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zj6r" in namespace "subpath-7569" to be "success or failure"
Aug 26 23:53:44.081: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Pending", Reason="", readiness=false. Elapsed: 17.451025ms
Aug 26 23:53:46.099: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035229429s
Aug 26 23:53:48.117: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 4.053515721s
Aug 26 23:53:50.135: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 6.070950529s
Aug 26 23:53:52.153: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 8.088620077s
Aug 26 23:53:54.170: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 10.106191644s
Aug 26 23:53:56.187: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 12.12342647s
Aug 26 23:53:58.205: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 14.141437687s
Aug 26 23:54:00.223: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 16.159492263s
Aug 26 23:54:02.241: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 18.177337278s
Aug 26 23:54:04.259: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 20.194795988s
Aug 26 23:54:06.276: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Running", Reason="", readiness=true. Elapsed: 22.212360836s
Aug 26 23:54:08.298: INFO: Pod "pod-subpath-test-configmap-zj6r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.234112337s
STEP: Saw pod success
Aug 26 23:54:08.298: INFO: Pod "pod-subpath-test-configmap-zj6r" satisfied condition "success or failure"
Aug 26 23:54:08.315: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-subpath-test-configmap-zj6r container test-container-subpath-configmap-zj6r: <nil>
STEP: delete the pod
Aug 26 23:54:08.363: INFO: Waiting for pod pod-subpath-test-configmap-zj6r to disappear
Aug 26 23:54:08.379: INFO: Pod pod-subpath-test-configmap-zj6r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zj6r
Aug 26 23:54:08.379: INFO: Deleting pod "pod-subpath-test-configmap-zj6r" in namespace "subpath-7569"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:54:08.396: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-7569" for this suite.
Aug 26 23:54:14.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:54:16.650: INFO: namespace subpath-7569 deletion completed in 8.209827016s

â€¢ [SLOW TEST:32.788 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:54:16.651: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3355.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3355.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 26 23:54:33.040: INFO: DNS probes using dns-3355/dns-test-d03d92b4-c85c-11e9-974e-0a58ac107583 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:54:33.067: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-3355" for this suite.
Aug 26 23:54:39.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:54:41.331: INFO: namespace dns-3355 deletion completed in 8.212592451s

â€¢ [SLOW TEST:24.681 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:54:41.332: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Aug 26 23:54:41.457: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig cluster-info'
Aug 26 23:54:41.713: INFO: stderr: ""
Aug 26 23:54:41.713: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.ci-op-dpgbhtjj-1ac2a.origin-ci-int-aws.dev.rhcloud.com:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:54:41.713: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1146" for this suite.
Aug 26 23:54:47.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:54:49.942: INFO: namespace kubectl-1146 deletion completed in 8.19795906s

â€¢ [SLOW TEST:8.610 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:54:49.943: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-e415a534-c85c-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 26 23:54:50.127: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583" in namespace "projected-2202" to be "success or failure"
Aug 26 23:54:50.144: INFO: Pod "pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.787011ms
Aug 26 23:54:52.161: INFO: Pod "pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034192261s
Aug 26 23:54:54.179: INFO: Pod "pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051762193s
STEP: Saw pod success
Aug 26 23:54:54.179: INFO: Pod "pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:54:54.197: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 26 23:54:54.243: INFO: Waiting for pod pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583 to disappear
Aug 26 23:54:54.259: INFO: Pod pod-projected-secrets-e419cd1d-c85c-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:54:54.259: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2202" for this suite.
Aug 26 23:55:00.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:55:02.523: INFO: namespace projected-2202 deletion completed in 8.21944985s

â€¢ [SLOW TEST:12.580 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:55:02.523: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:55:06.758: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-277" for this suite.
Aug 26 23:55:48.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:55:51.022: INFO: namespace kubelet-test-277 deletion completed in 44.217624223s

â€¢ [SLOW TEST:48.499 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:55:51.022: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 26 23:55:51.147: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8662'
Aug 26 23:55:51.360: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 26 23:55:51.360: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Aug 26 23:55:51.398: INFO: scanned /tmp/home for discovery docs: <nil>
Aug 26 23:55:51.398: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8662'
Aug 26 23:56:04.521: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 26 23:56:04.521: INFO: stdout: "Created e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751\nScaling up e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 26 23:56:04.521: INFO: stdout: "Created e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751\nScaling up e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 26 23:56:04.521: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8662'
Aug 26 23:56:04.687: INFO: stderr: ""
Aug 26 23:56:04.687: INFO: stdout: "e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751-292nt "
Aug 26 23:56:04.687: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751-292nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8662'
Aug 26 23:56:04.847: INFO: stderr: ""
Aug 26 23:56:04.847: INFO: stdout: "true"
Aug 26 23:56:04.847: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751-292nt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8662'
Aug 26 23:56:05.000: INFO: stderr: ""
Aug 26 23:56:05.000: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 26 23:56:05.000: INFO: e2e-test-nginx-rc-e5fbc30fe23220ff5616d4bf078cd751-292nt is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Aug 26 23:56:05.000: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete rc e2e-test-nginx-rc --namespace=kubectl-8662'
Aug 26 23:56:05.182: INFO: stderr: ""
Aug 26 23:56:05.182: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:56:05.182: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-8662" for this suite.
Aug 26 23:56:29.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:56:31.438: INFO: namespace kubectl-8662 deletion completed in 26.210771442s

â€¢ [SLOW TEST:40.415 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:56:31.438: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 26 23:56:36.724: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:56:36.789: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-8576" for this suite.
Aug 26 23:57:00.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:57:03.048: INFO: namespace replicaset-8576 deletion completed in 26.205960115s

â€¢ [SLOW TEST:31.610 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:57:03.048: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-269
Aug 26 23:57:09.242: INFO: Started pod liveness-http in namespace container-probe-269
STEP: checking the pod's current state and verifying that restartCount is present
Aug 26 23:57:09.260: INFO: Initial restart count of pod liveness-http is 0
Aug 26 23:57:27.438: INFO: Restart count of pod container-probe-269/liveness-http is now 1 (18.177555562s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:57:27.464: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-269" for this suite.
Aug 26 23:57:33.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:57:35.723: INFO: namespace container-probe-269 deletion completed in 8.21230956s

â€¢ [SLOW TEST:32.675 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:57:35.723: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-5021/configmap-test-46e618e0-c85d-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 26 23:57:35.902: INFO: Waiting up to 5m0s for pod "pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583" in namespace "configmap-5021" to be "success or failure"
Aug 26 23:57:35.921: INFO: Pod "pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.260167ms
Aug 26 23:57:37.939: INFO: Pod "pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036318839s
Aug 26 23:57:39.956: INFO: Pod "pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054054875s
STEP: Saw pod success
Aug 26 23:57:39.957: INFO: Pod "pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:57:39.974: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583 container env-test: <nil>
STEP: delete the pod
Aug 26 23:57:40.034: INFO: Waiting for pod pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583 to disappear
Aug 26 23:57:40.052: INFO: Pod pod-configmaps-46e9244e-c85d-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:57:40.052: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-5021" for this suite.
Aug 26 23:57:46.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:57:48.304: INFO: namespace configmap-5021 deletion completed in 8.207086279s

â€¢ [SLOW TEST:12.581 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:57:48.305: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 26 23:57:48.439: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:57:49.615: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5587" for this suite.
Aug 26 23:57:55.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:57:57.897: INFO: namespace custom-resource-definition-5587 deletion completed in 8.235815226s

â€¢ [SLOW TEST:9.592 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:57:57.897: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 26 23:58:04.188: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:04.188: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:04.434: INFO: Exec stderr: ""
Aug 26 23:58:04.434: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:04.434: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:04.606: INFO: Exec stderr: ""
Aug 26 23:58:04.606: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:04.606: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:04.782: INFO: Exec stderr: ""
Aug 26 23:58:04.782: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:04.782: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:04.952: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 26 23:58:04.952: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:04.952: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:05.125: INFO: Exec stderr: ""
Aug 26 23:58:05.125: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:05.125: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:05.317: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 26 23:58:05.317: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:05.317: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:05.512: INFO: Exec stderr: ""
Aug 26 23:58:05.512: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:05.512: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:05.683: INFO: Exec stderr: ""
Aug 26 23:58:05.683: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:05.683: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:05.866: INFO: Exec stderr: ""
Aug 26 23:58:05.866: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9401 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 26 23:58:05.866: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 26 23:58:06.036: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:58:06.036: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9401" for this suite.
Aug 26 23:58:52.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:58:54.297: INFO: namespace e2e-kubelet-etc-hosts-9401 deletion completed in 48.215040046s

â€¢ [SLOW TEST:56.400 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:58:54.298: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-75bd724d-c85d-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 26 23:58:54.486: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583" in namespace "projected-4105" to be "success or failure"
Aug 26 23:58:54.506: INFO: Pod "pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 19.275241ms
Aug 26 23:58:56.524: INFO: Pod "pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037998311s
Aug 26 23:58:58.542: INFO: Pod "pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055887051s
STEP: Saw pod success
Aug 26 23:58:58.542: INFO: Pod "pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 26 23:58:58.559: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 26 23:58:58.606: INFO: Waiting for pod pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583 to disappear
Aug 26 23:58:58.623: INFO: Pod pod-projected-configmaps-75c061ec-c85d-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:58:58.623: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4105" for this suite.
Aug 26 23:59:04.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:59:06.922: INFO: namespace projected-4105 deletion completed in 8.253936614s

â€¢ [SLOW TEST:12.624 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:59:06.923: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-p2nh
STEP: Creating a pod to test atomic-volume-subpath
Aug 26 23:59:07.111: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p2nh" in namespace "subpath-7594" to be "success or failure"
Aug 26 23:59:07.129: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Pending", Reason="", readiness=false. Elapsed: 17.655458ms
Aug 26 23:59:09.147: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035704788s
Aug 26 23:59:11.165: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 4.053424325s
Aug 26 23:59:13.182: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 6.070761878s
Aug 26 23:59:15.200: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 8.088799345s
Aug 26 23:59:17.218: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 10.106962097s
Aug 26 23:59:19.236: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 12.124654734s
Aug 26 23:59:21.253: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 14.142093971s
Aug 26 23:59:23.272: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 16.160556899s
Aug 26 23:59:25.289: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 18.178347413s
Aug 26 23:59:27.307: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 20.195765522s
Aug 26 23:59:29.324: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Running", Reason="", readiness=true. Elapsed: 22.213279961s
Aug 26 23:59:31.342: INFO: Pod "pod-subpath-test-projected-p2nh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.230860092s
STEP: Saw pod success
Aug 26 23:59:31.342: INFO: Pod "pod-subpath-test-projected-p2nh" satisfied condition "success or failure"
Aug 26 23:59:31.359: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-subpath-test-projected-p2nh container test-container-subpath-projected-p2nh: <nil>
STEP: delete the pod
Aug 26 23:59:31.408: INFO: Waiting for pod pod-subpath-test-projected-p2nh to disappear
Aug 26 23:59:31.425: INFO: Pod pod-subpath-test-projected-p2nh no longer exists
STEP: Deleting pod pod-subpath-test-projected-p2nh
Aug 26 23:59:31.426: INFO: Deleting pod "pod-subpath-test-projected-p2nh" in namespace "subpath-7594"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:59:31.442: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-7594" for this suite.
Aug 26 23:59:37.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 26 23:59:39.696: INFO: namespace subpath-7594 deletion completed in 8.208013405s

â€¢ [SLOW TEST:32.774 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 26 23:59:39.696: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 26 23:59:39.888: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-561" for this suite.
Aug 27 00:00:03.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:00:06.134: INFO: namespace pods-561 deletion completed in 26.227515935s

â€¢ [SLOW TEST:26.438 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:00:06.134: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:00:06.267: INFO: Creating deployment "nginx-deployment"
Aug 27 00:00:06.288: INFO: Waiting for observed generation 1
Aug 27 00:00:08.324: INFO: Waiting for all required pods to come up
Aug 27 00:00:08.355: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 27 00:00:12.404: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 27 00:00:12.441: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 27 00:00:12.499: INFO: Updating deployment nginx-deployment
Aug 27 00:00:12.499: INFO: Waiting for observed generation 2
Aug 27 00:00:14.536: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 27 00:00:14.556: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 27 00:00:14.573: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 27 00:00:14.625: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 27 00:00:14.625: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 27 00:00:14.643: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 27 00:00:14.680: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 27 00:00:14.680: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 27 00:00:14.719: INFO: Updating deployment nginx-deployment
Aug 27 00:00:14.719: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 27 00:00:14.760: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 27 00:00:16.808: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 00:00:16.842: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-526,SelfLink:/apis/apps/v1/namespaces/deployment-526/deployments/nginx-deployment,UID:a08f1ed7-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37926,Generation:3,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-27 00:00:14 +0000 UTC 2019-08-27 00:00:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-27 00:00:14 +0000 UTC 2019-08-27 00:00:06 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 27 00:00:16.861: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-526,SelfLink:/apis/apps/v1/namespaces/deployment-526/replicasets/nginx-deployment-b79c9d74d,UID:a4439617-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37924,Generation:3,CreationTimestamp:2019-08-27 00:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a08f1ed7-c85d-11e9-8aff-12fef04b6120 0xc0030b2927 0xc0030b2928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 00:00:16.861: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 27 00:00:16.861: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-526,SelfLink:/apis/apps/v1/namespaces/deployment-526/replicasets/nginx-deployment-85db8c99c5,UID:a0904f7f-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37872,Generation:3,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a08f1ed7-c85d-11e9-8aff-12fef04b6120 0xc0030b2857 0xc0030b2858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 27 00:00:16.908: INFO: Pod "nginx-deployment-85db8c99c5-2lslv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-2lslv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-2lslv,UID:a59ed2c8-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37866,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319f157 0xc00319f158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319f1c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319f1e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.909: INFO: Pod "nginx-deployment-85db8c99c5-52b7b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-52b7b,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-52b7b,UID:a59ec441-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37876,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319f2c7 0xc00319f2c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc00319f330} {node.kubernetes.io/not-ready Exists  NoExecute 0xc00319f350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.909: INFO: Pod "nginx-deployment-85db8c99c5-7gxbr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-7gxbr,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-7gxbr,UID:a59b8b5b-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37840,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319f437 0xc00319f438}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319f4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319f4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.909: INFO: Pod "nginx-deployment-85db8c99c5-7xfcc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-7xfcc,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-7xfcc,UID:a59817d2-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37828,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319f5a7 0xc00319f5a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319f610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319f630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.909: INFO: Pod "nginx-deployment-85db8c99c5-9cvmv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-9cvmv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-9cvmv,UID:a59ec14b-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37861,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319f717 0xc00319f718}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319f780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319f7a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.909: INFO: Pod "nginx-deployment-85db8c99c5-9wsjp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-9wsjp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-9wsjp,UID:a5a2e080-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37893,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319f897 0xc00319f898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319f900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319f920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.910: INFO: Pod "nginx-deployment-85db8c99c5-c5j4j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-c5j4j,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-c5j4j,UID:a59bf4ef-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37843,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319fa07 0xc00319fa08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319fa70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319fa90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.910: INFO: Pod "nginx-deployment-85db8c99c5-cbg8c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-cbg8c,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-cbg8c,UID:a099802c-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37722,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319fb77 0xc00319fb78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319fbe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319fc00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:10.129.2.62,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:09 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://1028ae422a5d7f8353f72079efca4a1ee6aaec3426f3b283bb1db2637d2b13db}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.910: INFO: Pod "nginx-deployment-85db8c99c5-f68fg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-f68fg,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-f68fg,UID:a5a401f2-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37882,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319fcf0 0xc00319fcf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319fd50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319fd70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.910: INFO: Pod "nginx-deployment-85db8c99c5-gvp22" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-gvp22,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-gvp22,UID:a5a41484-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37888,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319fe67 0xc00319fe68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00319fed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00319fef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.910: INFO: Pod "nginx-deployment-85db8c99c5-gw2zd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-gw2zd,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-gw2zd,UID:a5a44b90-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37892,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc00319ffd7 0xc00319ffd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.911: INFO: Pod "nginx-deployment-85db8c99c5-hg7jl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-hg7jl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-hg7jl,UID:a0958eaa-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37716,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7a147 0xc001d7a148}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:10.129.2.60,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:09 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://fc9ddce56915b8b68945ae490038218b6101bfadc5ff0324b0fedb45e6e094ac}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.911: INFO: Pod "nginx-deployment-85db8c99c5-m4pf9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-m4pf9,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-m4pf9,UID:a0995ab5-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37729,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7a2c0 0xc001d7a2c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:10.128.2.60,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:08 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://48948a7125fe54fc9b1fcc69c2de6d420d94851ca664f98dac60b0701f859148}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.911: INFO: Pod "nginx-deployment-85db8c99c5-nbfpv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-nbfpv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-nbfpv,UID:a095bb08-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37704,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7a430 0xc001d7a431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:10.131.0.69,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:08 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://fabd2816b1376147260dfb0e6565ec7e366bc181b3c405efd7d766d9947e391b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.911: INFO: Pod "nginx-deployment-85db8c99c5-pz2wx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-pz2wx,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-pz2wx,UID:a099b5fd-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37726,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7a5b0 0xc001d7a5b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:10.128.2.61,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:09 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://1a44186aeb62dac5eca6f340644fc83cf9e28368f3b7e9c3f9f80498ad2f014f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.911: INFO: Pod "nginx-deployment-85db8c99c5-ql284" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-ql284,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-ql284,UID:a59eb2fd-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37870,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7a720 0xc001d7a721}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a7a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.911: INFO: Pod "nginx-deployment-85db8c99c5-r597x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-r597x,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-r597x,UID:a5a39665-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37887,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7a8b7 0xc001d7a8b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7a920} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7a940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.912: INFO: Pod "nginx-deployment-85db8c99c5-vrl8w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-vrl8w,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-vrl8w,UID:a09d5213-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37719,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7aa27 0xc001d7aa28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7aa90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7aab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:10.129.2.61,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:09 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://d4a8423a18368a01ed1fff8c64c4ee9ab7bfc6bb653b929c106d04064dd7e893}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.912: INFO: Pod "nginx-deployment-85db8c99c5-w7g58" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-w7g58,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-w7g58,UID:a092a1d0-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37732,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7aba0 0xc001d7aba1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7ac00} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7ac20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:10.128.2.59,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:08 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://7f056ef8b8150cc337a7577377ce8902247eda99f58932e4d7cfaa06116cebd5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.912: INFO: Pod "nginx-deployment-85db8c99c5-wv6x5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-wv6x5,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-85db8c99c5-wv6x5,UID:a09e047a-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37698,Generation:0,CreationTimestamp:2019-08-27 00:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a0904f7f-c85d-11e9-8aff-12fef04b6120 0xc001d7ad10 0xc001d7ad11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7ad70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7ad90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:10.131.0.71,StartTime:2019-08-27 00:00:06 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-27 00:00:08 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://d162d90260f6269131530b990fb3e4fb15a7b6529904fc0fd2b4c8c840f7dc78}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.912: INFO: Pod "nginx-deployment-b79c9d74d-5j5dq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-5j5dq,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-5j5dq,UID:a5b27b22-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37947,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7ae80 0xc001d7ae81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7aef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7af10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.913: INFO: Pod "nginx-deployment-b79c9d74d-6pb8h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-6pb8h,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-6pb8h,UID:a5b32b92-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37948,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b000 0xc001d7b001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.913: INFO: Pod "nginx-deployment-b79c9d74d-6xs7c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-6xs7c,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-6xs7c,UID:a456b5c8-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37796,Generation:0,CreationTimestamp:2019-08-27 00:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b180 0xc001d7b181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.913: INFO: Pod "nginx-deployment-b79c9d74d-7x66d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-7x66d,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-7x66d,UID:a59c1597-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37855,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b300 0xc001d7b301}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.913: INFO: Pod "nginx-deployment-b79c9d74d-87xsr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-87xsr,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-87xsr,UID:a44eb6a3-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37789,Generation:0,CreationTimestamp:2019-08-27 00:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b480 0xc001d7b481}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b4f0} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.913: INFO: Pod "nginx-deployment-b79c9d74d-g7z6t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-g7z6t,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-g7z6t,UID:a45b40b7-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37804,Generation:0,CreationTimestamp:2019-08-27 00:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b600 0xc001d7b601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.913: INFO: Pod "nginx-deployment-b79c9d74d-hhwvm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-hhwvm,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-hhwvm,UID:a5a4d274-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37932,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b780 0xc001d7b781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.914: INFO: Pod "nginx-deployment-b79c9d74d-l2gtt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-l2gtt,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-l2gtt,UID:a5b3b50b-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37943,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7b910 0xc001d7b911}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7b980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7b9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.914: INFO: Pod "nginx-deployment-b79c9d74d-n2clj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-n2clj,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-n2clj,UID:a44f57b8-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37952,Generation:0,CreationTimestamp:2019-08-27 00:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7baa0 0xc001d7baa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7bb10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7bb30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:10.131.0.72,StartTime:2019-08-27 00:00:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/nginx: manifest unknown: manifest unknown,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.914: INFO: Pod "nginx-deployment-b79c9d74d-wtnzl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-wtnzl,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-wtnzl,UID:a446ea3d-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37779,Generation:0,CreationTimestamp:2019-08-27 00:00:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7bc40 0xc001d7bc41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7bcb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7bcd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:12 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.914: INFO: Pod "nginx-deployment-b79c9d74d-z47d2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-z47d2,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-z47d2,UID:a5bc250c-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37950,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7bdc0 0xc001d7bdc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7be30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7be50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.914: INFO: Pod "nginx-deployment-b79c9d74d-zhf6v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-zhf6v,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-zhf6v,UID:a5b2cadf-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37917,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc001d7bf40 0xc001d7bf41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-129-239.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/unreachable Exists  NoExecute 0xc001d7bfb0} {node.kubernetes.io/not-ready Exists  NoExecute 0xc001d7bfd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.129.239,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 27 00:00:16.915: INFO: Pod "nginx-deployment-b79c9d74d-zwgtj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-zwgtj,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-526,SelfLink:/api/v1/namespaces/deployment-526/pods/nginx-deployment-b79c9d74d-zwgtj,UID:a5a4830e-c85d-11e9-8aff-12fef04b6120,ResourceVersion:37898,Generation:0,CreationTimestamp:2019-08-27 00:00:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d a4439617-c85d-11e9-8aff-12fef04b6120 0xc002e6a290 0xc002e6a291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tt7t4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tt7t4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-tt7t4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-27995}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e6a450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e6a470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:00:14 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:00:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:00:16.915: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-526" for this suite.
Aug 27 00:00:24.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:00:27.180: INFO: namespace deployment-526 deletion completed in 10.246903855s

â€¢ [SLOW TEST:21.046 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:00:27.181: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 27 00:00:27.354: INFO: Waiting up to 5m0s for pod "pod-ad1a06ba-c85d-11e9-974e-0a58ac107583" in namespace "emptydir-7519" to be "success or failure"
Aug 27 00:00:27.373: INFO: Pod "pod-ad1a06ba-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.638194ms
Aug 27 00:00:29.390: INFO: Pod "pod-ad1a06ba-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036142629s
Aug 27 00:00:31.408: INFO: Pod "pod-ad1a06ba-c85d-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053952758s
STEP: Saw pod success
Aug 27 00:00:31.408: INFO: Pod "pod-ad1a06ba-c85d-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:00:31.428: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-ad1a06ba-c85d-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:00:31.483: INFO: Waiting for pod pod-ad1a06ba-c85d-11e9-974e-0a58ac107583 to disappear
Aug 27 00:00:31.504: INFO: Pod pod-ad1a06ba-c85d-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:00:31.504: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-7519" for this suite.
Aug 27 00:00:37.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:00:39.803: INFO: namespace emptydir-7519 deletion completed in 8.2546656s

â€¢ [SLOW TEST:12.623 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:00:39.803: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9300
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Aug 27 00:00:40.027: INFO: Found 0 stateful pods, waiting for 3
Aug 27 00:00:50.046: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:00:50.046: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:00:50.046: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Aug 27 00:01:00.045: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:01:00.046: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:01:00.046: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 27 00:01:00.146: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 27 00:01:00.229: INFO: Updating stateful set ss2
Aug 27 00:01:00.265: INFO: Waiting for Pod statefulset-9300/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 27 00:01:10.379: INFO: Found 2 stateful pods, waiting for 3
Aug 27 00:01:20.397: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:01:20.398: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:01:20.398: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 27 00:01:20.479: INFO: Updating stateful set ss2
Aug 27 00:01:20.517: INFO: Waiting for Pod statefulset-9300/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 27 00:01:30.600: INFO: Updating stateful set ss2
Aug 27 00:01:30.635: INFO: Waiting for StatefulSet statefulset-9300/ss2 to complete update
Aug 27 00:01:30.635: INFO: Waiting for Pod statefulset-9300/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 00:01:40.670: INFO: Deleting all statefulset in ns statefulset-9300
Aug 27 00:01:40.688: INFO: Scaling statefulset ss2 to 0
Aug 27 00:01:50.763: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 00:01:50.779: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:01:50.834: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-9300" for this suite.
Aug 27 00:01:58.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:02:01.130: INFO: namespace statefulset-9300 deletion completed in 10.250684366s

â€¢ [SLOW TEST:81.327 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:02:01.130: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-e5168b33-c85d-11e9-974e-0a58ac107583
STEP: Creating secret with name secret-projected-all-test-volume-e5168b19-c85d-11e9-974e-0a58ac107583
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 27 00:02:01.322: INFO: Waiting up to 5m0s for pod "projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583" in namespace "projected-8661" to be "success or failure"
Aug 27 00:02:01.339: INFO: Pod "projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.167372ms
Aug 27 00:02:03.359: INFO: Pod "projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036619785s
Aug 27 00:02:05.377: INFO: Pod "projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055248269s
STEP: Saw pod success
Aug 27 00:02:05.378: INFO: Pod "projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:02:05.396: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 27 00:02:05.449: INFO: Waiting for pod projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583 to disappear
Aug 27 00:02:05.466: INFO: Pod projected-volume-e5168ae3-c85d-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:02:05.466: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8661" for this suite.
Aug 27 00:02:11.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:02:13.707: INFO: namespace projected-8661 deletion completed in 8.194298169s

â€¢ [SLOW TEST:12.577 seconds]
[sig-storage] Projected combined
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:02:13.708: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ec9675b5-c85d-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:02:13.882: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583" in namespace "configmap-5337" to be "success or failure"
Aug 27 00:02:13.900: INFO: Pod "pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.613394ms
Aug 27 00:02:15.918: INFO: Pod "pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036429745s
Aug 27 00:02:17.936: INFO: Pod "pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054458406s
STEP: Saw pod success
Aug 27 00:02:17.936: INFO: Pod "pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:02:17.953: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:02:18.002: INFO: Waiting for pod pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583 to disappear
Aug 27 00:02:18.018: INFO: Pod pod-configmaps-ec99631e-c85d-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:02:18.018: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-5337" for this suite.
Aug 27 00:02:24.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:02:26.261: INFO: namespace configmap-5337 deletion completed in 8.196325806s

â€¢ [SLOW TEST:12.553 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:02:26.262: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 00:02:26.406: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583" in namespace "projected-7980" to be "success or failure"
Aug 27 00:02:26.423: INFO: Pod "downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.778936ms
Aug 27 00:02:28.444: INFO: Pod "downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037368979s
Aug 27 00:02:30.462: INFO: Pod "downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055861274s
STEP: Saw pod success
Aug 27 00:02:30.462: INFO: Pod "downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:02:30.480: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 27 00:02:30.530: INFO: Waiting for pod downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583 to disappear
Aug 27 00:02:30.547: INFO: Pod downwardapi-volume-f410b6e5-c85d-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:02:30.547: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7980" for this suite.
Aug 27 00:02:36.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:02:38.789: INFO: namespace projected-7980 deletion completed in 8.19681552s

â€¢ [SLOW TEST:12.527 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:02:38.789: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:02:38.958: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-fb90adcb-c85d-11e9-974e-0a58ac107583
STEP: Creating configMap with name cm-test-opt-upd-fb90ae2f-c85d-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fb90adcb-c85d-11e9-974e-0a58ac107583
STEP: Updating configmap cm-test-opt-upd-fb90ae2f-c85d-11e9-974e-0a58ac107583
STEP: Creating configMap with name cm-test-opt-create-fb90ae51-c85d-11e9-974e-0a58ac107583
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:04:08.193: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6323" for this suite.
Aug 27 00:04:26.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:04:28.458: INFO: namespace configmap-6323 deletion completed in 20.232543989s

â€¢ [SLOW TEST:109.669 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:04:28.459: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7032
Aug 27 00:04:32.652: INFO: Started pod liveness-http in namespace container-probe-7032
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 00:04:32.670: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:08:33.011: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7032" for this suite.
Aug 27 00:08:39.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:08:41.311: INFO: namespace container-probe-7032 deletion completed in 8.249899942s

â€¢ [SLOW TEST:252.853 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:08:41.312: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-d39e15f4-c85e-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:08:41.483: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583" in namespace "projected-1658" to be "success or failure"
Aug 27 00:08:41.501: INFO: Pod "pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.932267ms
Aug 27 00:08:43.520: INFO: Pod "pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036542438s
Aug 27 00:08:45.549: INFO: Pod "pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066000024s
STEP: Saw pod success
Aug 27 00:08:45.550: INFO: Pod "pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:08:45.570: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:08:45.621: INFO: Waiting for pod pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583 to disappear
Aug 27 00:08:45.639: INFO: Pod pod-projected-secrets-d3a11fb6-c85e-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:08:45.639: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-1658" for this suite.
Aug 27 00:08:51.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:08:53.943: INFO: namespace projected-1658 deletion completed in 8.249542707s

â€¢ [SLOW TEST:12.631 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:08:53.943: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7259
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7259
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7259
Aug 27 00:08:54.175: INFO: Found 0 stateful pods, waiting for 1
Aug 27 00:09:04.200: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 27 00:09:04.218: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 00:09:05.105: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 00:09:05.105: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 00:09:05.105: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 00:09:05.122: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 27 00:09:15.156: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 00:09:15.157: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 00:09:15.229: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999348s
Aug 27 00:09:16.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.982077332s
Aug 27 00:09:17.265: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.964325094s
Aug 27 00:09:18.284: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.946673446s
Aug 27 00:09:19.304: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.925785105s
Aug 27 00:09:20.322: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.906965641s
Aug 27 00:09:21.340: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.889345504s
Aug 27 00:09:22.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.871831753s
Aug 27 00:09:23.375: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.853529837s
Aug 27 00:09:24.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 836.404693ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7259
Aug 27 00:09:25.412: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 00:09:25.760: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 00:09:25.760: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 00:09:25.760: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 00:09:25.778: INFO: Found 1 stateful pods, waiting for 3
Aug 27 00:09:35.796: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:09:35.797: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 27 00:09:35.797: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 27 00:09:35.830: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 00:09:36.183: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 00:09:36.183: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 00:09:36.183: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 00:09:36.183: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 00:09:36.565: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 00:09:36.565: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 00:09:36.565: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 00:09:36.565: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 27 00:09:37.510: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 27 00:09:37.510: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 27 00:09:37.510: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 27 00:09:37.510: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 00:09:37.527: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 27 00:09:47.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 00:09:47.562: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 00:09:47.562: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 27 00:09:47.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999266s
Aug 27 00:09:48.634: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982566355s
Aug 27 00:09:49.652: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.964717724s
Aug 27 00:09:50.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.946752384s
Aug 27 00:09:51.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.929272385s
Aug 27 00:09:52.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.91173561s
Aug 27 00:09:53.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.892292001s
Aug 27 00:09:54.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.874266066s
Aug 27 00:09:55.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.85670949s
Aug 27 00:09:56.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 838.789006ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7259
Aug 27 00:09:57.799: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 00:09:58.167: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 00:09:58.167: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 00:09:58.167: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 00:09:58.167: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 00:09:58.544: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 00:09:58.544: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 00:09:58.544: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 00:09:58.544: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig exec --namespace=statefulset-7259 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 27 00:09:58.902: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 27 00:09:58.902: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 27 00:09:58.902: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 27 00:09:58.902: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 27 00:10:28.973: INFO: Deleting all statefulset in ns statefulset-7259
Aug 27 00:10:28.990: INFO: Scaling statefulset ss to 0
Aug 27 00:10:29.041: INFO: Waiting for statefulset status.replicas updated to 0
Aug 27 00:10:29.057: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:10:29.113: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "statefulset-7259" for this suite.
Aug 27 00:10:35.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:10:37.448: INFO: namespace statefulset-7259 deletion completed in 8.289014402s

â€¢ [SLOW TEST:103.505 seconds]
[sig-apps] StatefulSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:10:37.448: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Aug 27 00:10:37.610: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-64" to be "success or failure"
Aug 27 00:10:37.629: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.733831ms
Aug 27 00:10:39.647: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036451103s
Aug 27 00:10:41.664: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053781241s
STEP: Saw pod success
Aug 27 00:10:41.665: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 27 00:10:41.682: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 27 00:10:41.729: INFO: Waiting for pod pod-host-path-test to disappear
Aug 27 00:10:41.746: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:10:41.746: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "hostpath-64" for this suite.
Aug 27 00:10:47.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:10:50.012: INFO: namespace hostpath-64 deletion completed in 8.2133425s

â€¢ [SLOW TEST:12.564 seconds]
[sig-storage] HostPath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:10:50.012: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:10:50.165: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-2058e646-c85f-11e9-974e-0a58ac107583
STEP: Creating secret with name s-test-opt-upd-2058e995-c85f-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2058e646-c85f-11e9-974e-0a58ac107583
STEP: Updating secret s-test-opt-upd-2058e995-c85f-11e9-974e-0a58ac107583
STEP: Creating secret with name s-test-opt-create-2058e9c1-c85f-11e9-974e-0a58ac107583
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:12:17.361: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-4150" for this suite.
Aug 27 00:12:41.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:12:43.627: INFO: namespace projected-4150 deletion completed in 26.220399592s

â€¢ [SLOW TEST:113.615 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:12:43.627: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Aug 27 00:12:43.787: INFO: Waiting up to 5m0s for pod "client-containers-640db085-c85f-11e9-974e-0a58ac107583" in namespace "containers-713" to be "success or failure"
Aug 27 00:12:43.804: INFO: Pod "client-containers-640db085-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.941413ms
Aug 27 00:12:45.822: INFO: Pod "client-containers-640db085-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034725899s
Aug 27 00:12:47.840: INFO: Pod "client-containers-640db085-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052592179s
Aug 27 00:12:49.858: INFO: Pod "client-containers-640db085-c85f-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070281225s
STEP: Saw pod success
Aug 27 00:12:49.858: INFO: Pod "client-containers-640db085-c85f-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:12:49.875: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod client-containers-640db085-c85f-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:12:49.922: INFO: Waiting for pod client-containers-640db085-c85f-11e9-974e-0a58ac107583 to disappear
Aug 27 00:12:49.938: INFO: Pod client-containers-640db085-c85f-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:12:49.938: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-713" for this suite.
Aug 27 00:12:56.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:12:58.235: INFO: namespace containers-713 deletion completed in 8.25203757s

â€¢ [SLOW TEST:14.608 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:12:58.236: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6cc0e154-c85f-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:12:58.403: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583" in namespace "projected-832" to be "success or failure"
Aug 27 00:12:58.420: INFO: Pod "pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.417033ms
Aug 27 00:13:00.438: INFO: Pod "pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035341687s
Aug 27 00:13:02.455: INFO: Pod "pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052800761s
STEP: Saw pod success
Aug 27 00:13:02.456: INFO: Pod "pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:13:02.472: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:13:02.517: INFO: Waiting for pod pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583 to disappear
Aug 27 00:13:02.534: INFO: Pod pod-projected-secrets-6cc3f0e9-c85f-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:13:02.534: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-832" for this suite.
Aug 27 00:13:08.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:13:10.891: INFO: namespace projected-832 deletion completed in 8.312501752s

â€¢ [SLOW TEST:12.655 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:13:10.891: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:13:11.133: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"74595345-c85f-11e9-8aff-12fef04b6120", Controller:(*bool)(0xc0026858d2), BlockOwnerDeletion:(*bool)(0xc0026858d3)}}
Aug 27 00:13:11.158: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"74509644-c85f-11e9-8aff-12fef04b6120", Controller:(*bool)(0xc0032e2416), BlockOwnerDeletion:(*bool)(0xc0032e2417)}}
Aug 27 00:13:11.178: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"74548ca3-c85f-11e9-8aff-12fef04b6120", Controller:(*bool)(0xc002337136), BlockOwnerDeletion:(*bool)(0xc002337137)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:13:16.217: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-5268" for this suite.
Aug 27 00:13:22.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:13:24.503: INFO: namespace gc-5268 deletion completed in 8.240077712s

â€¢ [SLOW TEST:13.612 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:13:24.503: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Aug 27 00:13:24.658: INFO: Waiting up to 5m0s for pod "client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583" in namespace "containers-8568" to be "success or failure"
Aug 27 00:13:24.675: INFO: Pod "client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.070833ms
Aug 27 00:13:26.692: INFO: Pod "client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034703522s
Aug 27 00:13:28.710: INFO: Pod "client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052751795s
Aug 27 00:13:30.729: INFO: Pod "client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071401237s
STEP: Saw pod success
Aug 27 00:13:30.729: INFO: Pod "client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:13:30.746: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:13:30.797: INFO: Waiting for pod client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583 to disappear
Aug 27 00:13:30.815: INFO: Pod client-containers-7c6a2389-c85f-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:13:30.815: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-8568" for this suite.
Aug 27 00:13:36.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:13:39.071: INFO: namespace containers-8568 deletion completed in 8.208386501s

â€¢ [SLOW TEST:14.568 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:13:39.072: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 27 00:13:43.836: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-9291 pod-service-account-85710019-c85f-11e9-974e-0a58ac107583 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 27 00:13:44.215: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-9291 pod-service-account-85710019-c85f-11e9-974e-0a58ac107583 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 27 00:13:44.559: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl exec --namespace=svcaccounts-9291 pod-service-account-85710019-c85f-11e9-974e-0a58ac107583 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:13:44.936: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svcaccounts-9291" for this suite.
Aug 27 00:13:51.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:13:53.191: INFO: namespace svcaccounts-9291 deletion completed in 8.209080736s

â€¢ [SLOW TEST:14.120 seconds]
[sig-auth] ServiceAccounts
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:13:53.191: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:13:53.324: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 27 00:13:53.363: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 27 00:13:57.403: INFO: Creating deployment "test-rolling-update-deployment"
Aug 27 00:13:57.426: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 27 00:13:57.467: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Aug 27 00:13:59.501: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 27 00:13:59.522: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:14:01.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702461637, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:14:03.539: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 00:14:03.589: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2436,SelfLink:/apis/apps/v1/namespaces/deployment-2436/deployments/test-rolling-update-deployment,UID:8ff46244-c85f-11e9-8aff-12fef04b6120,ResourceVersion:43522,Generation:1,CreationTimestamp:2019-08-27 00:13:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-27 00:13:57 +0000 UTC 2019-08-27 00:13:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-27 00:14:03 +0000 UTC 2019-08-27 00:13:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 27 00:14:03.607: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-2436,SelfLink:/apis/apps/v1/namespaces/deployment-2436/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:8ff6e2f3-c85f-11e9-8aff-12fef04b6120,ResourceVersion:43511,Generation:1,CreationTimestamp:2019-08-27 00:13:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 8ff46244-c85f-11e9-8aff-12fef04b6120 0xc00172ae57 0xc00172ae58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 27 00:14:03.607: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 27 00:14:03.607: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2436,SelfLink:/apis/apps/v1/namespaces/deployment-2436/replicasets/test-rolling-update-controller,UID:8d85fe11-c85f-11e9-8aff-12fef04b6120,ResourceVersion:43520,Generation:2,CreationTimestamp:2019-08-27 00:13:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 8ff46244-c85f-11e9-8aff-12fef04b6120 0xc00172ad77 0xc00172ad78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 00:14:03.624: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-prcbz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-prcbz,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-2436,SelfLink:/api/v1/namespaces/deployment-2436/pods/test-rolling-update-deployment-57b6b5bb54-prcbz,UID:8ff8de47-c85f-11e9-8aff-12fef04b6120,ResourceVersion:43510,Generation:0,CreationTimestamp:2019-08-27 00:13:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 8ff6e2f3-c85f-11e9-8aff-12fef04b6120 0xc00172b727 0xc00172b728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sd7xv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sd7xv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sd7xv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-zt9rh}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00172b790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00172b7b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:13:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:14:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:14:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:13:57 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:10.128.2.80,StartTime:2019-08-27 00:13:57 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-27 00:14:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://131cfbfff267634ac047907d03354862741ec1e393c3c5c40968a1fba6e4196a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:14:03.624: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-2436" for this suite.
Aug 27 00:14:09.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:14:11.886: INFO: namespace deployment-2436 deletion completed in 8.217543721s

â€¢ [SLOW TEST:18.695 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:14:11.886: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-98a90f7f-c85f-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:14:12.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583" in namespace "projected-3165" to be "success or failure"
Aug 27 00:14:12.089: INFO: Pod "pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.945622ms
Aug 27 00:14:14.107: INFO: Pod "pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036723362s
Aug 27 00:14:16.125: INFO: Pod "pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054637402s
STEP: Saw pod success
Aug 27 00:14:16.125: INFO: Pod "pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:14:16.142: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:14:16.192: INFO: Waiting for pod pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583 to disappear
Aug 27 00:14:16.208: INFO: Pod pod-projected-configmaps-98ac3ee4-c85f-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:14:16.208: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3165" for this suite.
Aug 27 00:14:22.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:14:24.495: INFO: namespace projected-3165 deletion completed in 8.241850003s

â€¢ [SLOW TEST:12.609 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:14:24.495: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-a02c4e67-c85f-11e9-974e-0a58ac107583
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:14:24.638: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6820" for this suite.
Aug 27 00:14:30.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:14:32.893: INFO: namespace configmap-6820 deletion completed in 8.235207454s

â€¢ [SLOW TEST:8.399 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:14:32.894: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 27 00:14:39.113: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-a52d40b4-c85f-11e9-974e-0a58ac107583,GenerateName:,Namespace:events-6653,SelfLink:/api/v1/namespaces/events-6653/pods/send-events-a52d40b4-c85f-11e9-974e-0a58ac107583,UID:a52f69e3-c85f-11e9-8aff-12fef04b6120,ResourceVersion:43893,Generation:0,CreationTimestamp:2019-08-27 00:14:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 16547669,},Annotations:map[string]string{openshift.io/scc: anyuid,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vf95k {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vf95k,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-vf95k true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c41,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-gfgf6}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00321bd70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00321bd90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:14:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:14:33 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:10.128.2.81,StartTime:2019-08-27 00:14:33 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-27 00:14:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 cri-o://53e9b5ae5ac18c7b939d76d184f02c167d8fd6972da96399c3b54ce6ebfd22cd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 27 00:14:41.130: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 27 00:14:43.148: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:14:43.169: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "events-6653" for this suite.
Aug 27 00:15:23.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:15:25.427: INFO: namespace events-6653 deletion completed in 42.214160188s

â€¢ [SLOW TEST:52.534 seconds]
[k8s.io] [sig-node] Events
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:15:25.428: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:15:29.675: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-1354" for this suite.
Aug 27 00:16:25.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:16:27.920: INFO: namespace kubelet-test-1354 deletion completed in 58.19967966s

â€¢ [SLOW TEST:62.492 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:16:27.920: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 00:16:28.072: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1290'
Aug 27 00:16:28.338: INFO: stderr: ""
Aug 27 00:16:28.338: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Aug 27 00:16:28.355: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete pods e2e-test-nginx-pod --namespace=kubectl-1290'
Aug 27 00:16:39.965: INFO: stderr: ""
Aug 27 00:16:39.965: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:16:39.965: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1290" for this suite.
Aug 27 00:16:46.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:16:48.217: INFO: namespace kubectl-1290 deletion completed in 8.204709374s

â€¢ [SLOW TEST:20.297 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:16:48.217: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 27 00:16:48.384: INFO: Waiting up to 5m0s for pod "pod-f5d77d27-c85f-11e9-974e-0a58ac107583" in namespace "emptydir-920" to be "success or failure"
Aug 27 00:16:48.402: INFO: Pod "pod-f5d77d27-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.192889ms
Aug 27 00:16:50.421: INFO: Pod "pod-f5d77d27-c85f-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036952118s
Aug 27 00:16:52.439: INFO: Pod "pod-f5d77d27-c85f-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054596944s
STEP: Saw pod success
Aug 27 00:16:52.439: INFO: Pod "pod-f5d77d27-c85f-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:16:52.456: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-f5d77d27-c85f-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:16:52.549: INFO: Waiting for pod pod-f5d77d27-c85f-11e9-974e-0a58ac107583 to disappear
Aug 27 00:16:52.566: INFO: Pod pod-f5d77d27-c85f-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:16:52.567: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-920" for this suite.
Aug 27 00:16:58.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:17:00.882: INFO: namespace emptydir-920 deletion completed in 8.261802592s

â€¢ [SLOW TEST:12.665 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:17:00.883: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 27 00:17:01.116: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7547,SelfLink:/api/v1/namespaces/watch-7547/configmaps/e2e-watch-test-label-changed,UID:fd6684e5-c85f-11e9-8aff-12fef04b6120,ResourceVersion:44795,Generation:0,CreationTimestamp:2019-08-27 00:17:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 27 00:17:01.116: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7547,SelfLink:/api/v1/namespaces/watch-7547/configmaps/e2e-watch-test-label-changed,UID:fd6684e5-c85f-11e9-8aff-12fef04b6120,ResourceVersion:44797,Generation:0,CreationTimestamp:2019-08-27 00:17:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 27 00:17:01.116: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7547,SelfLink:/api/v1/namespaces/watch-7547/configmaps/e2e-watch-test-label-changed,UID:fd6684e5-c85f-11e9-8aff-12fef04b6120,ResourceVersion:44798,Generation:0,CreationTimestamp:2019-08-27 00:17:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 27 00:17:11.247: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7547,SelfLink:/api/v1/namespaces/watch-7547/configmaps/e2e-watch-test-label-changed,UID:fd6684e5-c85f-11e9-8aff-12fef04b6120,ResourceVersion:44844,Generation:0,CreationTimestamp:2019-08-27 00:17:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 00:17:11.247: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7547,SelfLink:/api/v1/namespaces/watch-7547/configmaps/e2e-watch-test-label-changed,UID:fd6684e5-c85f-11e9-8aff-12fef04b6120,ResourceVersion:44845,Generation:0,CreationTimestamp:2019-08-27 00:17:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 27 00:17:11.247: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7547,SelfLink:/api/v1/namespaces/watch-7547/configmaps/e2e-watch-test-label-changed,UID:fd6684e5-c85f-11e9-8aff-12fef04b6120,ResourceVersion:44846,Generation:0,CreationTimestamp:2019-08-27 00:17:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:17:11.247: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-7547" for this suite.
Aug 27 00:17:17.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:17:19.490: INFO: namespace watch-7547 deletion completed in 8.198068633s

â€¢ [SLOW TEST:18.607 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:17:19.490: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 27 00:17:24.265: INFO: Successfully updated pod "pod-update-activedeadlineseconds-087b24a0-c860-11e9-974e-0a58ac107583"
Aug 27 00:17:24.265: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-087b24a0-c860-11e9-974e-0a58ac107583" in namespace "pods-4558" to be "terminated due to deadline exceeded"
Aug 27 00:17:24.283: INFO: Pod "pod-update-activedeadlineseconds-087b24a0-c860-11e9-974e-0a58ac107583": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 17.926362ms
Aug 27 00:17:24.283: INFO: Pod "pod-update-activedeadlineseconds-087b24a0-c860-11e9-974e-0a58ac107583" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:17:24.283: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-4558" for this suite.
Aug 27 00:17:30.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:17:32.574: INFO: namespace pods-4558 deletion completed in 8.245228417s

â€¢ [SLOW TEST:13.084 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:17:32.574: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 27 00:17:32.740: INFO: Waiting up to 5m0s for pod "pod-104868f0-c860-11e9-974e-0a58ac107583" in namespace "emptydir-6400" to be "success or failure"
Aug 27 00:17:32.757: INFO: Pod "pod-104868f0-c860-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.677543ms
Aug 27 00:17:34.775: INFO: Pod "pod-104868f0-c860-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034985851s
Aug 27 00:17:36.793: INFO: Pod "pod-104868f0-c860-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053289188s
STEP: Saw pod success
Aug 27 00:17:36.793: INFO: Pod "pod-104868f0-c860-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:17:36.811: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-104868f0-c860-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:17:36.860: INFO: Waiting for pod pod-104868f0-c860-11e9-974e-0a58ac107583 to disappear
Aug 27 00:17:36.876: INFO: Pod pod-104868f0-c860-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:17:36.876: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6400" for this suite.
Aug 27 00:17:42.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:17:45.221: INFO: namespace emptydir-6400 deletion completed in 8.29886756s

â€¢ [SLOW TEST:12.647 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:17:45.221: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 27 00:17:46.385: INFO: Pod name wrapped-volume-race-1863f5a1-c860-11e9-974e-0a58ac107583: Found 1 pods out of 5
Aug 27 00:17:51.418: INFO: Pod name wrapped-volume-race-1863f5a1-c860-11e9-974e-0a58ac107583: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1863f5a1-c860-11e9-974e-0a58ac107583 in namespace emptydir-wrapper-6716, will wait for the garbage collector to delete the pods
Aug 27 00:18:03.629: INFO: Deleting ReplicationController wrapped-volume-race-1863f5a1-c860-11e9-974e-0a58ac107583 took: 26.80272ms
Aug 27 00:18:03.829: INFO: Terminating ReplicationController wrapped-volume-race-1863f5a1-c860-11e9-974e-0a58ac107583 pods took: 200.333694ms
STEP: Creating RC which spawns configmap-volume pods
Aug 27 00:18:49.104: INFO: Pod name wrapped-volume-race-3dc5cc83-c860-11e9-974e-0a58ac107583: Found 0 pods out of 5
Aug 27 00:18:54.137: INFO: Pod name wrapped-volume-race-3dc5cc83-c860-11e9-974e-0a58ac107583: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3dc5cc83-c860-11e9-974e-0a58ac107583 in namespace emptydir-wrapper-6716, will wait for the garbage collector to delete the pods
Aug 27 00:19:32.357: INFO: Deleting ReplicationController wrapped-volume-race-3dc5cc83-c860-11e9-974e-0a58ac107583 took: 22.822509ms
Aug 27 00:19:32.557: INFO: Terminating ReplicationController wrapped-volume-race-3dc5cc83-c860-11e9-974e-0a58ac107583 pods took: 200.425164ms
STEP: Creating RC which spawns configmap-volume pods
Aug 27 00:20:10.536: INFO: Pod name wrapped-volume-race-6e4ec5a1-c860-11e9-974e-0a58ac107583: Found 0 pods out of 5
Aug 27 00:20:15.569: INFO: Pod name wrapped-volume-race-6e4ec5a1-c860-11e9-974e-0a58ac107583: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6e4ec5a1-c860-11e9-974e-0a58ac107583 in namespace emptydir-wrapper-6716, will wait for the garbage collector to delete the pods
Aug 27 00:20:39.775: INFO: Deleting ReplicationController wrapped-volume-race-6e4ec5a1-c860-11e9-974e-0a58ac107583 took: 23.853111ms
Aug 27 00:20:39.976: INFO: Terminating ReplicationController wrapped-volume-race-6e4ec5a1-c860-11e9-974e-0a58ac107583 pods took: 200.421371ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:21:20.283: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6716" for this suite.
Aug 27 00:21:28.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:21:30.522: INFO: namespace emptydir-wrapper-6716 deletion completed in 10.207211659s

â€¢ [SLOW TEST:225.301 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:21:30.523: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 00:21:30.658: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1606'
Aug 27 00:21:30.924: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 00:21:30.924: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Aug 27 00:21:32.960: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete deployment e2e-test-nginx-deployment --namespace=kubectl-1606'
Aug 27 00:21:33.151: INFO: stderr: ""
Aug 27 00:21:33.151: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:21:33.151: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-1606" for this suite.
Aug 27 00:21:39.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:21:41.398: INFO: namespace kubectl-1606 deletion completed in 8.20047844s

â€¢ [SLOW TEST:10.875 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:21:41.398: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:21:43.669: INFO: Waiting up to 5m0s for pod "client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583" in namespace "pods-9170" to be "success or failure"
Aug 27 00:21:43.687: INFO: Pod "client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.645714ms
Aug 27 00:21:45.705: INFO: Pod "client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035608754s
Aug 27 00:21:47.722: INFO: Pod "client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05286172s
STEP: Saw pod success
Aug 27 00:21:47.722: INFO: Pod "client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:21:47.741: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583 container env3cont: <nil>
STEP: delete the pod
Aug 27 00:21:47.789: INFO: Waiting for pod client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583 to disappear
Aug 27 00:21:47.806: INFO: Pod client-envvars-a5d93f46-c860-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:21:47.806: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-9170" for this suite.
Aug 27 00:22:29.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:22:32.154: INFO: namespace pods-9170 deletion completed in 44.302847021s

â€¢ [SLOW TEST:50.756 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:22:32.154: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-t2fg
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 00:22:32.352: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-t2fg" in namespace "subpath-9419" to be "success or failure"
Aug 27 00:22:32.369: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Pending", Reason="", readiness=false. Elapsed: 17.527934ms
Aug 27 00:22:34.387: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035012074s
Aug 27 00:22:36.405: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 4.052962856s
Aug 27 00:22:38.423: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 6.070847291s
Aug 27 00:22:40.440: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 8.088495883s
Aug 27 00:22:42.458: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 10.105808414s
Aug 27 00:22:44.476: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 12.124155369s
Aug 27 00:22:46.494: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 14.141768748s
Aug 27 00:22:48.511: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 16.15939892s
Aug 27 00:22:50.529: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 18.176593537s
Aug 27 00:22:52.546: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 20.193938681s
Aug 27 00:22:54.563: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Running", Reason="", readiness=true. Elapsed: 22.211235834s
Aug 27 00:22:56.581: INFO: Pod "pod-subpath-test-secret-t2fg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.228542405s
STEP: Saw pod success
Aug 27 00:22:56.581: INFO: Pod "pod-subpath-test-secret-t2fg" satisfied condition "success or failure"
Aug 27 00:22:56.598: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-subpath-test-secret-t2fg container test-container-subpath-secret-t2fg: <nil>
STEP: delete the pod
Aug 27 00:22:56.650: INFO: Waiting for pod pod-subpath-test-secret-t2fg to disappear
Aug 27 00:22:56.668: INFO: Pod pod-subpath-test-secret-t2fg no longer exists
STEP: Deleting pod pod-subpath-test-secret-t2fg
Aug 27 00:22:56.668: INFO: Deleting pod "pod-subpath-test-secret-t2fg" in namespace "subpath-9419"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:22:56.686: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-9419" for this suite.
Aug 27 00:23:02.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:23:04.933: INFO: namespace subpath-9419 deletion completed in 8.199345833s

â€¢ [SLOW TEST:32.779 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:23:04.933: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 27 00:23:05.204: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:05.204: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:05.204: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:05.227: INFO: Number of nodes with available pods: 0
Aug 27 00:23:05.227: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:23:06.275: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:06.275: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:06.275: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:06.292: INFO: Number of nodes with available pods: 0
Aug 27 00:23:06.292: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:23:07.273: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:07.273: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:07.273: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:07.290: INFO: Number of nodes with available pods: 0
Aug 27 00:23:07.290: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:23:08.273: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:08.273: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:08.273: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:08.291: INFO: Number of nodes with available pods: 2
Aug 27 00:23:08.291: INFO: Node ip-10-0-155-122.ec2.internal is running more than one daemon pod
Aug 27 00:23:09.273: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:09.273: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:09.273: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:09.293: INFO: Number of nodes with available pods: 3
Aug 27 00:23:09.293: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 27 00:23:09.380: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:09.380: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:09.380: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:09.397: INFO: Number of nodes with available pods: 2
Aug 27 00:23:09.397: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:10.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:10.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:10.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:10.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:10.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:11.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:11.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:11.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:11.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:11.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:12.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:12.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:12.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:12.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:12.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:13.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:13.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:13.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:13.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:13.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:14.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:14.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:14.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:14.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:14.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:15.444: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:15.444: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:15.444: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:15.462: INFO: Number of nodes with available pods: 2
Aug 27 00:23:15.462: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:16.444: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:16.444: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:16.444: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:16.461: INFO: Number of nodes with available pods: 2
Aug 27 00:23:16.461: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:17.445: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:17.445: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:17.445: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:17.462: INFO: Number of nodes with available pods: 2
Aug 27 00:23:17.462: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:18.444: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:18.444: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:18.444: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:18.461: INFO: Number of nodes with available pods: 2
Aug 27 00:23:18.461: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:19.445: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:19.445: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:19.446: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:19.466: INFO: Number of nodes with available pods: 2
Aug 27 00:23:19.466: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:20.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:20.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:20.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:20.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:20.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:21.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:21.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:21.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:21.460: INFO: Number of nodes with available pods: 2
Aug 27 00:23:21.460: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:22.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:22.444: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:22.444: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:22.461: INFO: Number of nodes with available pods: 2
Aug 27 00:23:22.461: INFO: Node ip-10-0-139-255.ec2.internal is running more than one daemon pod
Aug 27 00:23:23.443: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:23:23.443: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:23.443: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:23:23.460: INFO: Number of nodes with available pods: 3
Aug 27 00:23:23.460: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8792, will wait for the garbage collector to delete the pods
Aug 27 00:23:23.571: INFO: Deleting DaemonSet.extensions daemon-set took: 22.96632ms
Aug 27 00:23:23.771: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.412813ms
Aug 27 00:23:28.188: INFO: Number of nodes with available pods: 0
Aug 27 00:23:28.189: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 00:23:28.206: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8792/daemonsets","resourceVersion":"47453"},"items":null}

Aug 27 00:23:28.223: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8792/pods","resourceVersion":"47453"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:23:28.306: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-8792" for this suite.
Aug 27 00:23:34.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:23:36.652: INFO: namespace daemonsets-8792 deletion completed in 8.315251218s

â€¢ [SLOW TEST:31.720 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:23:36.653: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:23:36.801: INFO: Creating deployment "test-recreate-deployment"
Aug 27 00:23:36.825: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 27 00:23:36.867: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 27 00:23:36.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-6566d46b4b\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:23:38.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462216, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:23:40.902: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 27 00:23:40.939: INFO: Updating deployment test-recreate-deployment
Aug 27 00:23:40.939: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 00:23:41.034: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8123,SelfLink:/apis/apps/v1/namespaces/deployment-8123/deployments/test-recreate-deployment,UID:e94e15dc-c860-11e9-8aff-12fef04b6120,ResourceVersion:47623,Generation:2,CreationTimestamp:2019-08-27 00:23:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-27 00:23:40 +0000 UTC 2019-08-27 00:23:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-27 00:23:41 +0000 UTC 2019-08-27 00:23:36 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 27 00:23:41.051: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-8123,SelfLink:/apis/apps/v1/namespaces/deployment-8123/replicasets/test-recreate-deployment-745fb9c84c,UID:ebc8a374-c860-11e9-8aff-12fef04b6120,ResourceVersion:47622,Generation:1,CreationTimestamp:2019-08-27 00:23:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e94e15dc-c860-11e9-8aff-12fef04b6120 0xc00307d107 0xc00307d108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 00:23:41.051: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 27 00:23:41.051: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-8123,SelfLink:/apis/apps/v1/namespaces/deployment-8123/replicasets/test-recreate-deployment-6566d46b4b,UID:e94f566f-c860-11e9-8aff-12fef04b6120,ResourceVersion:47611,Generation:2,CreationTimestamp:2019-08-27 00:23:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e94e15dc-c860-11e9-8aff-12fef04b6120 0xc00307d037 0xc00307d038}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 27 00:23:41.069: INFO: Pod "test-recreate-deployment-745fb9c84c-22pw2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-22pw2,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-8123,SelfLink:/api/v1/namespaces/deployment-8123/pods/test-recreate-deployment-745fb9c84c-22pw2,UID:ebca186e-c860-11e9-8aff-12fef04b6120,ResourceVersion:47624,Generation:0,CreationTimestamp:2019-08-27 00:23:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c ebc8a374-c860-11e9-8aff-12fef04b6120 0xc00307d9c7 0xc00307d9c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c95x7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c95x7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c95x7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-255.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-r858b}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00307da30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00307da50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:23:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:23:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:23:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:23:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.139.255,PodIP:,StartTime:2019-08-27 00:23:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:23:41.069: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-8123" for this suite.
Aug 27 00:23:47.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:23:49.322: INFO: namespace deployment-8123 deletion completed in 8.208752105s

â€¢ [SLOW TEST:12.669 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:23:49.322: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:23:49.455: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-f0d71372-c860-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:23:53.598: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-8701" for this suite.
Aug 27 00:24:17.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:24:19.867: INFO: namespace configmap-8701 deletion completed in 26.224188128s

â€¢ [SLOW TEST:30.545 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:24:19.867: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 00:24:20.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583" in namespace "projected-5943" to be "success or failure"
Aug 27 00:24:20.042: INFO: Pod "downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.134775ms
Aug 27 00:24:22.061: INFO: Pod "downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036683334s
Aug 27 00:24:24.078: INFO: Pod "downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054084696s
STEP: Saw pod success
Aug 27 00:24:24.078: INFO: Pod "downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:24:24.095: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 27 00:24:24.142: INFO: Waiting for pod downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:24:24.160: INFO: Pod downwardapi-volume-030a94f7-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:24:24.160: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-5943" for this suite.
Aug 27 00:24:30.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:24:32.419: INFO: namespace projected-5943 deletion completed in 8.21189131s

â€¢ [SLOW TEST:12.552 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:24:32.419: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Aug 27 00:24:33.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:24:35.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:24:37.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:24:39.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:24:41.408: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702462273, loc:(*time.Location)(0x8a23120)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 27 00:24:43.602: INFO: Waited 170.691419ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:24:45.728: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "aggregator-7705" for this suite.
Aug 27 00:24:51.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:24:54.080: INFO: namespace aggregator-7705 deletion completed in 8.293693331s

â€¢ [SLOW TEST:21.661 seconds]
[sig-api-machinery] Aggregator
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:24:54.080: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:24:54.228: INFO: (0) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 20.501506ms)
Aug 27 00:24:54.246: INFO: (1) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.886985ms)
Aug 27 00:24:54.264: INFO: (2) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.344649ms)
Aug 27 00:24:54.282: INFO: (3) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.00978ms)
Aug 27 00:24:54.301: INFO: (4) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.142001ms)
Aug 27 00:24:54.319: INFO: (5) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.073343ms)
Aug 27 00:24:54.348: INFO: (6) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 29.27367ms)
Aug 27 00:24:54.366: INFO: (7) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.366658ms)
Aug 27 00:24:54.385: INFO: (8) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.356413ms)
Aug 27 00:24:54.404: INFO: (9) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.981713ms)
Aug 27 00:24:54.422: INFO: (10) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.901289ms)
Aug 27 00:24:54.440: INFO: (11) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.874047ms)
Aug 27 00:24:54.458: INFO: (12) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.01424ms)
Aug 27 00:24:54.476: INFO: (13) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.990759ms)
Aug 27 00:24:54.494: INFO: (14) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.009564ms)
Aug 27 00:24:54.512: INFO: (15) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.299955ms)
Aug 27 00:24:54.530: INFO: (16) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.353964ms)
Aug 27 00:24:54.547: INFO: (17) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.008868ms)
Aug 27 00:24:54.566: INFO: (18) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 18.990961ms)
Aug 27 00:24:54.584: INFO: (19) /api/v1/nodes/ip-10-0-129-239.ec2.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a href="... (200; 17.870886ms)
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:24:54.584: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-5516" for this suite.
Aug 27 00:25:00.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:25:01.908: INFO: namespace proxy-5516 deletion completed in 7.306424312s

â€¢ [SLOW TEST:7.829 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:25:01.909: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-1c1a77a8-c861-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:25:02.092: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583" in namespace "configmap-6955" to be "success or failure"
Aug 27 00:25:02.112: INFO: Pod "pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 20.101956ms
Aug 27 00:25:04.130: INFO: Pod "pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037766104s
Aug 27 00:25:06.147: INFO: Pod "pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055571472s
STEP: Saw pod success
Aug 27 00:25:06.147: INFO: Pod "pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:25:06.164: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:25:06.212: INFO: Waiting for pod pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:25:06.229: INFO: Pod pod-configmaps-1c1deb6e-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:25:06.229: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-6955" for this suite.
Aug 27 00:25:12.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:25:14.592: INFO: namespace configmap-6955 deletion completed in 8.315574571s

â€¢ [SLOW TEST:12.683 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:25:14.592: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:25:14.829: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 27 00:25:14.866: INFO: Number of nodes with available pods: 0
Aug 27 00:25:14.866: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 27 00:25:14.940: INFO: Number of nodes with available pods: 0
Aug 27 00:25:14.940: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:15.957: INFO: Number of nodes with available pods: 0
Aug 27 00:25:15.957: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:16.958: INFO: Number of nodes with available pods: 0
Aug 27 00:25:16.958: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:17.958: INFO: Number of nodes with available pods: 1
Aug 27 00:25:17.958: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 27 00:25:18.037: INFO: Number of nodes with available pods: 0
Aug 27 00:25:18.037: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 27 00:25:18.078: INFO: Number of nodes with available pods: 0
Aug 27 00:25:18.079: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:19.097: INFO: Number of nodes with available pods: 0
Aug 27 00:25:19.097: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:20.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:20.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:21.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:21.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:22.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:22.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:23.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:23.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:24.097: INFO: Number of nodes with available pods: 0
Aug 27 00:25:24.097: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:25.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:25.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:26.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:26.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:27.097: INFO: Number of nodes with available pods: 0
Aug 27 00:25:27.097: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:28.097: INFO: Number of nodes with available pods: 0
Aug 27 00:25:28.097: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:29.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:29.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:30.096: INFO: Number of nodes with available pods: 0
Aug 27 00:25:30.096: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:25:31.096: INFO: Number of nodes with available pods: 1
Aug 27 00:25:31.096: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5400, will wait for the garbage collector to delete the pods
Aug 27 00:25:31.222: INFO: Deleting DaemonSet.extensions daemon-set took: 24.690012ms
Aug 27 00:25:31.422: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.325268ms
Aug 27 00:25:34.539: INFO: Number of nodes with available pods: 0
Aug 27 00:25:34.539: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 00:25:34.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5400/daemonsets","resourceVersion":"48684"},"items":null}

Aug 27 00:25:34.572: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5400/pods","resourceVersion":"48684"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:25:34.677: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-5400" for this suite.
Aug 27 00:25:40.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:25:42.919: INFO: namespace daemonsets-5400 deletion completed in 8.211329625s

â€¢ [SLOW TEST:28.328 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:25:42.919: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-348babe6-c861-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:25:43.094: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583" in namespace "projected-286" to be "success or failure"
Aug 27 00:25:43.110: INFO: Pod "pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.70243ms
Aug 27 00:25:45.129: INFO: Pod "pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035011309s
Aug 27 00:25:47.146: INFO: Pod "pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052469706s
STEP: Saw pod success
Aug 27 00:25:47.146: INFO: Pod "pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:25:47.164: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:25:47.211: INFO: Waiting for pod pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:25:47.228: INFO: Pod pod-projected-configmaps-348e6c4c-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:25:47.228: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-286" for this suite.
Aug 27 00:25:53.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:25:55.500: INFO: namespace projected-286 deletion completed in 8.22674897s

â€¢ [SLOW TEST:12.581 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:25:55.500: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Aug 27 00:25:55.611: INFO: namespace kubectl-9865
Aug 27 00:25:55.611: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-9865'
Aug 27 00:25:56.347: INFO: stderr: ""
Aug 27 00:25:56.347: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 27 00:25:57.365: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:25:57.365: INFO: Found 0 / 1
Aug 27 00:25:58.366: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:25:58.366: INFO: Found 0 / 1
Aug 27 00:25:59.365: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:25:59.365: INFO: Found 1 / 1
Aug 27 00:25:59.365: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 27 00:25:59.382: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:25:59.382: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 27 00:25:59.382: INFO: wait on redis-master startup in kubectl-9865 
Aug 27 00:25:59.382: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-cx5zr redis-master --namespace=kubectl-9865'
Aug 27 00:25:59.560: INFO: stderr: ""
Aug 27 00:25:59.560: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Aug 00:25:58.324 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Aug 00:25:58.324 # Server started, Redis version 3.2.12\n1:M 27 Aug 00:25:58.324 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Aug 00:25:58.324 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 27 00:25:59.560: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9865'
Aug 27 00:25:59.783: INFO: stderr: ""
Aug 27 00:25:59.783: INFO: stdout: "service/rm2 exposed\n"
Aug 27 00:25:59.801: INFO: Service rm2 in namespace kubectl-9865 found.
STEP: exposing service
Aug 27 00:26:01.836: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9865'
Aug 27 00:26:02.033: INFO: stderr: ""
Aug 27 00:26:02.033: INFO: stdout: "service/rm3 exposed\n"
Aug 27 00:26:02.050: INFO: Service rm3 in namespace kubectl-9865 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:26:04.090: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-9865" for this suite.
Aug 27 00:26:28.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:26:30.352: INFO: namespace kubectl-9865 deletion completed in 26.211423422s

â€¢ [SLOW TEST:34.851 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:26:30.352: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-50d22279-c861-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:26:30.534: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583" in namespace "projected-8363" to be "success or failure"
Aug 27 00:26:30.553: INFO: Pod "pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.223326ms
Aug 27 00:26:32.571: INFO: Pod "pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036502136s
Aug 27 00:26:34.589: INFO: Pod "pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054212992s
STEP: Saw pod success
Aug 27 00:26:34.589: INFO: Pod "pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:26:34.605: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:26:34.652: INFO: Waiting for pod pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:26:34.669: INFO: Pod pod-projected-secrets-50d53f12-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:26:34.669: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-8363" for this suite.
Aug 27 00:26:40.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:26:42.960: INFO: namespace projected-8363 deletion completed in 8.246650678s

â€¢ [SLOW TEST:12.609 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:26:42.961: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7904
Aug 27 00:26:49.154: INFO: Started pod liveness-http in namespace container-probe-7904
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 00:26:49.171: INFO: Initial restart count of pod liveness-http is 0
Aug 27 00:27:05.331: INFO: Restart count of pod container-probe-7904/liveness-http is now 1 (16.160260567s elapsed)
Aug 27 00:27:25.515: INFO: Restart count of pod container-probe-7904/liveness-http is now 2 (36.344765179s elapsed)
Aug 27 00:27:45.696: INFO: Restart count of pod container-probe-7904/liveness-http is now 3 (56.525286923s elapsed)
Aug 27 00:28:03.863: INFO: Restart count of pod container-probe-7904/liveness-http is now 4 (1m14.692211879s elapsed)
Aug 27 00:29:04.415: INFO: Restart count of pod container-probe-7904/liveness-http is now 5 (2m15.244098381s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:29:04.441: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-7904" for this suite.
Aug 27 00:29:10.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:29:12.738: INFO: namespace container-probe-7904 deletion completed in 8.250694639s

â€¢ [SLOW TEST:149.777 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:29:12.738: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-m5mz
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 00:29:12.960: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-m5mz" in namespace "subpath-6419" to be "success or failure"
Aug 27 00:29:12.978: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Pending", Reason="", readiness=false. Elapsed: 17.689557ms
Aug 27 00:29:14.997: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036623846s
Aug 27 00:29:17.015: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 4.054550538s
Aug 27 00:29:19.032: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 6.072162108s
Aug 27 00:29:21.050: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 8.090029432s
Aug 27 00:29:23.068: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 10.107699609s
Aug 27 00:29:25.087: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 12.12653825s
Aug 27 00:29:27.105: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 14.144575602s
Aug 27 00:29:29.122: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 16.161815153s
Aug 27 00:29:31.139: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 18.17929543s
Aug 27 00:29:33.157: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 20.19717281s
Aug 27 00:29:35.176: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Running", Reason="", readiness=true. Elapsed: 22.216244936s
Aug 27 00:29:37.198: INFO: Pod "pod-subpath-test-configmap-m5mz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.237998833s
STEP: Saw pod success
Aug 27 00:29:37.198: INFO: Pod "pod-subpath-test-configmap-m5mz" satisfied condition "success or failure"
Aug 27 00:29:37.215: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-subpath-test-configmap-m5mz container test-container-subpath-configmap-m5mz: <nil>
STEP: delete the pod
Aug 27 00:29:37.265: INFO: Waiting for pod pod-subpath-test-configmap-m5mz to disappear
Aug 27 00:29:37.281: INFO: Pod pod-subpath-test-configmap-m5mz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-m5mz
Aug 27 00:29:37.281: INFO: Deleting pod "pod-subpath-test-configmap-m5mz" in namespace "subpath-6419"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:29:37.298: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-6419" for this suite.
Aug 27 00:29:43.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:29:45.565: INFO: namespace subpath-6419 deletion completed in 8.220218592s

â€¢ [SLOW TEST:32.827 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:29:45.565: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 27 00:29:45.720: INFO: Waiting up to 5m0s for pod "pod-c52c6c6e-c861-11e9-974e-0a58ac107583" in namespace "emptydir-6394" to be "success or failure"
Aug 27 00:29:45.737: INFO: Pod "pod-c52c6c6e-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.621526ms
Aug 27 00:29:47.755: INFO: Pod "pod-c52c6c6e-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034577995s
Aug 27 00:29:49.773: INFO: Pod "pod-c52c6c6e-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052479511s
STEP: Saw pod success
Aug 27 00:29:49.773: INFO: Pod "pod-c52c6c6e-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:29:49.789: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-c52c6c6e-c861-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:29:49.837: INFO: Waiting for pod pod-c52c6c6e-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:29:49.853: INFO: Pod pod-c52c6c6e-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:29:49.853: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6394" for this suite.
Aug 27 00:29:55.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:29:58.129: INFO: namespace emptydir-6394 deletion completed in 8.2299846s

â€¢ [SLOW TEST:12.564 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:29:58.129: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-fdmz
STEP: Creating a pod to test atomic-volume-subpath
Aug 27 00:29:58.420: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fdmz" in namespace "subpath-8293" to be "success or failure"
Aug 27 00:29:58.439: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Pending", Reason="", readiness=false. Elapsed: 18.864482ms
Aug 27 00:30:00.457: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03677371s
Aug 27 00:30:02.476: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 4.05524236s
Aug 27 00:30:04.494: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 6.073605308s
Aug 27 00:30:06.512: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 8.092123679s
Aug 27 00:30:08.531: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 10.110400785s
Aug 27 00:30:10.548: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 12.128000823s
Aug 27 00:30:12.566: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 14.145758511s
Aug 27 00:30:14.584: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 16.164069193s
Aug 27 00:30:16.602: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 18.181429371s
Aug 27 00:30:18.620: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 20.199469945s
Aug 27 00:30:20.638: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Running", Reason="", readiness=true. Elapsed: 22.217220964s
Aug 27 00:30:22.656: INFO: Pod "pod-subpath-test-downwardapi-fdmz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.235274494s
STEP: Saw pod success
Aug 27 00:30:22.656: INFO: Pod "pod-subpath-test-downwardapi-fdmz" satisfied condition "success or failure"
Aug 27 00:30:22.672: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-subpath-test-downwardapi-fdmz container test-container-subpath-downwardapi-fdmz: <nil>
STEP: delete the pod
Aug 27 00:30:22.721: INFO: Waiting for pod pod-subpath-test-downwardapi-fdmz to disappear
Aug 27 00:30:22.738: INFO: Pod pod-subpath-test-downwardapi-fdmz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fdmz
Aug 27 00:30:22.738: INFO: Deleting pod "pod-subpath-test-downwardapi-fdmz" in namespace "subpath-8293"
[AfterEach] [sig-storage] Subpath
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:30:22.755: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "subpath-8293" for this suite.
Aug 27 00:30:28.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:30:31.026: INFO: namespace subpath-8293 deletion completed in 8.225218312s

â€¢ [SLOW TEST:32.897 seconds]
[sig-storage] Subpath
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:30:31.026: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 27 00:30:31.281: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1310,SelfLink:/api/v1/namespaces/watch-1310/configmaps/e2e-watch-test-resource-version,UID:e046a40a-c861-11e9-8aff-12fef04b6120,ResourceVersion:50636,Generation:0,CreationTimestamp:2019-08-27 00:30:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 27 00:30:31.281: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1310,SelfLink:/api/v1/namespaces/watch-1310/configmaps/e2e-watch-test-resource-version,UID:e046a40a-c861-11e9-8aff-12fef04b6120,ResourceVersion:50637,Generation:0,CreationTimestamp:2019-08-27 00:30:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:30:31.282: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "watch-1310" for this suite.
Aug 27 00:30:37.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:30:39.560: INFO: namespace watch-1310 deletion completed in 8.253280134s

â€¢ [SLOW TEST:8.535 seconds]
[sig-api-machinery] Watchers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:30:39.561: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e55c9aa4-c861-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:30:39.751: INFO: Waiting up to 5m0s for pod "pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583" in namespace "secrets-9834" to be "success or failure"
Aug 27 00:30:39.768: INFO: Pod "pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.228714ms
Aug 27 00:30:41.786: INFO: Pod "pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034857979s
Aug 27 00:30:43.803: INFO: Pod "pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052280681s
STEP: Saw pod success
Aug 27 00:30:43.803: INFO: Pod "pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:30:43.820: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:30:43.868: INFO: Waiting for pod pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:30:43.884: INFO: Pod pod-secrets-e55f6c16-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:30:43.884: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-9834" for this suite.
Aug 27 00:30:49.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:30:52.157: INFO: namespace secrets-9834 deletion completed in 8.227537939s

â€¢ [SLOW TEST:12.596 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:30:52.157: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2240.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2240.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 187.181.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.181.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.181.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.181.187_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2240.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2240.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 187.181.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.181.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.181.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.181.187_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 27 00:31:08.441: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc.cluster.local from pod dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583: the server could not find the requested resource (get pods dns-test-ece5e541-c861-11e9-974e-0a58ac107583)
Aug 27 00:31:08.459: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc.cluster.local from pod dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583: the server could not find the requested resource (get pods dns-test-ece5e541-c861-11e9-974e-0a58ac107583)
Aug 27 00:31:08.496: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local from pod dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583: the server could not find the requested resource (get pods dns-test-ece5e541-c861-11e9-974e-0a58ac107583)
Aug 27 00:31:08.654: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local from pod dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583: the server could not find the requested resource (get pods dns-test-ece5e541-c861-11e9-974e-0a58ac107583)
Aug 27 00:31:08.671: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local from pod dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583: the server could not find the requested resource (get pods dns-test-ece5e541-c861-11e9-974e-0a58ac107583)
Aug 27 00:31:08.777: INFO: Lookups using dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583 failed for: [wheezy_udp@dns-test-service.dns-2240.svc.cluster.local wheezy_tcp@dns-test-service.dns-2240.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc.cluster.local]

Aug 27 00:31:14.132: INFO: DNS probes using dns-2240/dns-test-ece5e541-c861-11e9-974e-0a58ac107583 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:31:14.361: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-2240" for this suite.
Aug 27 00:31:20.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:31:22.617: INFO: namespace dns-2240 deletion completed in 8.223901864s

â€¢ [SLOW TEST:30.460 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:31:22.617: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ff05fb41-c861-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:31:22.800: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583" in namespace "configmap-2994" to be "success or failure"
Aug 27 00:31:22.821: INFO: Pod "pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 20.700745ms
Aug 27 00:31:24.839: INFO: Pod "pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038745146s
Aug 27 00:31:26.857: INFO: Pod "pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056488863s
STEP: Saw pod success
Aug 27 00:31:26.857: INFO: Pod "pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:31:26.873: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:31:26.921: INFO: Waiting for pod pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583 to disappear
Aug 27 00:31:26.945: INFO: Pod pod-configmaps-ff091864-c861-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:31:26.945: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-2994" for this suite.
Aug 27 00:31:33.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:31:35.188: INFO: namespace configmap-2994 deletion completed in 8.197399343s

â€¢ [SLOW TEST:12.571 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:31:35.189: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 27 00:31:35.344: INFO: Waiting up to 5m0s for pod "pod-06826c3a-c862-11e9-974e-0a58ac107583" in namespace "emptydir-2719" to be "success or failure"
Aug 27 00:31:35.366: INFO: Pod "pod-06826c3a-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 22.55784ms
Aug 27 00:31:37.384: INFO: Pod "pod-06826c3a-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040237421s
Aug 27 00:31:39.403: INFO: Pod "pod-06826c3a-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059033833s
STEP: Saw pod success
Aug 27 00:31:39.403: INFO: Pod "pod-06826c3a-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:31:39.420: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-06826c3a-c862-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:31:39.470: INFO: Waiting for pod pod-06826c3a-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:31:39.486: INFO: Pod pod-06826c3a-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:31:39.486: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2719" for this suite.
Aug 27 00:31:45.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:31:47.771: INFO: namespace emptydir-2719 deletion completed in 8.239986738s

â€¢ [SLOW TEST:12.583 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:31:47.772: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:31:47.891: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig version'
Aug 27 00:31:48.104: INFO: stderr: ""
Aug 27 00:31:48.104: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14+\", GitVersion:\"v1.14.7-beta.0.14+3fe98d8e0f1966\", GitCommit:\"3fe98d8e0f19668c04a35b2cd74b9f55bf4a991a\", GitTreeState:\"clean\", BuildDate:\"2019-08-26T23:09:45Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14+\", GitVersion:\"v1.14.0+204cc07\", GitCommit:\"204cc07\", GitTreeState:\"clean\", BuildDate:\"2019-08-26T22:22:27Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:31:48.104: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4105" for this suite.
Aug 27 00:31:54.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:31:56.373: INFO: namespace kubectl-4105 deletion completed in 8.2352111s

â€¢ [SLOW TEST:8.601 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:31:56.374: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:31:56.543: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 27 00:32:00.582: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 27 00:32:04.725: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-960,SelfLink:/apis/apps/v1/namespaces/deployment-960/deployments/test-cleanup-deployment,UID:15994f99-c862-11e9-8aff-12fef04b6120,ResourceVersion:51505,Generation:1,CreationTimestamp:2019-08-27 00:32:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-27 00:32:00 +0000 UTC 2019-08-27 00:32:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-27 00:32:03 +0000 UTC 2019-08-27 00:32:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6865c98b76" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 27 00:32:04.743: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-960,SelfLink:/apis/apps/v1/namespaces/deployment-960/replicasets/test-cleanup-deployment-6865c98b76,UID:159ba75c-c862-11e9-8aff-12fef04b6120,ResourceVersion:51494,Generation:1,CreationTimestamp:2019-08-27 00:32:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 15994f99-c862-11e9-8aff-12fef04b6120 0xc002fca677 0xc002fca678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 27 00:32:04.760: INFO: Pod "test-cleanup-deployment-6865c98b76-vgkh5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-vgkh5,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-960,SelfLink:/api/v1/namespaces/deployment-960/pods/test-cleanup-deployment-6865c98b76-vgkh5,UID:159d6b41-c862-11e9-8aff-12fef04b6120,ResourceVersion:51493,Generation:0,CreationTimestamp:2019-08-27 00:32:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{openshift.io/scc: privileged,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 159ba75c-c862-11e9-8aff-12fef04b6120 0xc002fcac67 0xc002fcac68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-knzgv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-knzgv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-knzgv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-155-122.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[{default-dockercfg-xcg9t}],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fcacd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fcacf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:32:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:32:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:32:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-27 00:32:00 +0000 UTC  }],Message:,Reason:,HostIP:10.0.155.122,PodIP:10.128.2.91,StartTime:2019-08-27 00:32:00 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-27 00:32:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://4e53d50a43a7ed7ad69e6e7851a5ec8ddae9d11edbe4001d1e089c4a3b7dcfd5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:32:04.760: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "deployment-960" for this suite.
Aug 27 00:32:10.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:32:13.000: INFO: namespace deployment-960 deletion completed in 8.194455867s

â€¢ [SLOW TEST:16.626 seconds]
[sig-apps] Deployment
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:32:13.000: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 00:32:13.155: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583" in namespace "downward-api-6794" to be "success or failure"
Aug 27 00:32:13.172: INFO: Pod "downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.922039ms
Aug 27 00:32:15.190: INFO: Pod "downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035090048s
Aug 27 00:32:17.208: INFO: Pod "downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052713594s
STEP: Saw pod success
Aug 27 00:32:17.208: INFO: Pod "downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:32:17.227: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 27 00:32:17.273: INFO: Waiting for pod downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:32:17.289: INFO: Pod downwardapi-volume-1d0cb4ae-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:32:17.289: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-6794" for this suite.
Aug 27 00:32:23.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:32:25.538: INFO: namespace downward-api-6794 deletion completed in 8.203584502s

â€¢ [SLOW TEST:12.538 seconds]
[sig-storage] Downward API volume
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:32:25.538: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 27 00:32:25.708: INFO: Waiting up to 5m0s for pod "pod-24863de4-c862-11e9-974e-0a58ac107583" in namespace "emptydir-6622" to be "success or failure"
Aug 27 00:32:25.726: INFO: Pod "pod-24863de4-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.928306ms
Aug 27 00:32:27.744: INFO: Pod "pod-24863de4-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036255291s
Aug 27 00:32:29.762: INFO: Pod "pod-24863de4-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053898792s
STEP: Saw pod success
Aug 27 00:32:29.762: INFO: Pod "pod-24863de4-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:32:29.778: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-24863de4-c862-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:32:29.826: INFO: Waiting for pod pod-24863de4-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:32:29.843: INFO: Pod pod-24863de4-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:32:29.843: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-6622" for this suite.
Aug 27 00:32:35.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:32:38.096: INFO: namespace emptydir-6622 deletion completed in 8.207927063s

â€¢ [SLOW TEST:12.558 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:32:38.097: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-2c01a673-c862-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:32:38.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583" in namespace "configmap-448" to be "success or failure"
Aug 27 00:32:38.310: INFO: Pod "pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 25.270479ms
Aug 27 00:32:40.328: INFO: Pod "pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04369312s
Aug 27 00:32:42.346: INFO: Pod "pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061803491s
STEP: Saw pod success
Aug 27 00:32:42.346: INFO: Pod "pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:32:42.364: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:32:42.414: INFO: Waiting for pod pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:32:42.430: INFO: Pod pod-configmaps-2c05740e-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:32:42.430: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-448" for this suite.
Aug 27 00:32:48.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:32:50.696: INFO: namespace configmap-448 deletion completed in 8.220626469s

â€¢ [SLOW TEST:12.599 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:32:50.696: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:32:57.150: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-9228" for this suite.
Aug 27 00:33:03.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:33:05.398: INFO: namespace namespaces-9228 deletion completed in 8.203317501s
STEP: Destroying namespace "nsdeletetest-2611" for this suite.
Aug 27 00:33:05.414: INFO: Namespace nsdeletetest-2611 was already deleted
STEP: Destroying namespace "nsdeletetest-6767" for this suite.
Aug 27 00:33:11.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:33:13.635: INFO: namespace nsdeletetest-6767 deletion completed in 8.220236645s

â€¢ [SLOW TEST:22.940 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:33:13.636: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4352
Aug 27 00:33:17.833: INFO: Started pod liveness-exec in namespace container-probe-4352
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 00:33:17.850: INFO: Initial restart count of pod liveness-exec is 0
Aug 27 00:34:08.324: INFO: Restart count of pod container-probe-4352/liveness-exec is now 1 (50.473694248s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:34:08.350: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-4352" for this suite.
Aug 27 00:34:14.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:34:16.634: INFO: namespace container-probe-4352 deletion completed in 8.238336881s

â€¢ [SLOW TEST:62.998 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:34:16.634: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3108.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3108.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3108.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3108.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3108.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3108.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 27 00:34:20.988: INFO: DNS probes using dns-3108/dns-test-66be5f36-c862-11e9-974e-0a58ac107583 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:34:21.018: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "dns-3108" for this suite.
Aug 27 00:34:27.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:34:29.323: INFO: namespace dns-3108 deletion completed in 8.259887972s

â€¢ [SLOW TEST:12.689 seconds]
[sig-network] DNS
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:34:29.324: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 27 00:34:29.473: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Aug 27 00:34:29.545: INFO: Waiting for terminating namespaces to be deleted...
Aug 27 00:34:29.563: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-239.ec2.internal before test
Aug 27 00:34:29.614: INFO: ovs-qnv4t from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container openvswitch ready: true, restart count 0
Aug 27 00:34:29.614: INFO: node-exporter-nrtz2 from openshift-monitoring started at 2019-08-26 23:00:41 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container node-exporter ready: true, restart count 0
Aug 27 00:34:29.614: INFO: machine-config-daemon-qtvl9 from openshift-machine-config-operator started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 27 00:34:29.614: INFO: image-registry-78bd5f6f67-49xzm from openshift-image-registry started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container registry ready: true, restart count 0
Aug 27 00:34:29.614: INFO: node-ca-jsqtx from openshift-image-registry started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container node-ca ready: true, restart count 0
Aug 27 00:34:29.614: INFO: redhat-operators-67f45f9754-t96bh from openshift-marketplace started at 2019-08-26 23:01:47 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 27 00:34:29.614: INFO: certified-operators-575dfbb7d8-frm4v from openshift-marketplace started at 2019-08-26 23:01:47 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container certified-operators ready: true, restart count 0
Aug 27 00:34:29.614: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-08-26 23:03:08 +0000 UTC (3 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container alertmanager ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container config-reloader ready: true, restart count 0
Aug 27 00:34:29.614: INFO: sdn-xnf2w from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container sdn ready: true, restart count 0
Aug 27 00:34:29.614: INFO: tuned-p49z9 from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container tuned ready: true, restart count 0
Aug 27 00:34:29.614: INFO: community-operators-848c796fb7-94fds from openshift-marketplace started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container community-operators ready: true, restart count 0
Aug 27 00:34:29.614: INFO: openshift-state-metrics-6f45d6c785-jnjkl from openshift-monitoring started at 2019-08-26 23:01:43 +0000 UTC (3 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 27 00:34:29.614: INFO: multus-mj66c from openshift-multus started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container kube-multus ready: true, restart count 0
Aug 27 00:34:29.614: INFO: kube-state-metrics-5f57cc8b6f-p56kl from openshift-monitoring started at 2019-08-26 23:01:41 +0000 UTC (3 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 27 00:34:29.614: INFO: router-default-7879f59486-qp8d2 from openshift-ingress started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container router ready: true, restart count 0
Aug 27 00:34:29.614: INFO: dns-default-fnqls from openshift-dns started at 2019-08-26 23:00:41 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.614: INFO: 	Container dns ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 27 00:34:29.614: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-139-255.ec2.internal before test
Aug 27 00:34:29.664: INFO: tuned-4w8vz from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container tuned ready: true, restart count 0
Aug 27 00:34:29.664: INFO: machine-config-daemon-csqcg from openshift-machine-config-operator started at 2019-08-26 23:01:53 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 27 00:34:29.664: INFO: node-exporter-mhdpr from openshift-monitoring started at 2019-08-26 23:00:44 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container node-exporter ready: true, restart count 0
Aug 27 00:34:29.664: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-08-26 23:03:00 +0000 UTC (6 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container prometheus ready: true, restart count 1
Aug 27 00:34:29.664: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 27 00:34:29.664: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-08-26 23:03:20 +0000 UTC (3 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container alertmanager ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container config-reloader ready: true, restart count 0
Aug 27 00:34:29.664: INFO: ovs-7mgp8 from openshift-sdn started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container openvswitch ready: true, restart count 0
Aug 27 00:34:29.664: INFO: grafana-79767c7d48-clxjd from openshift-monitoring started at 2019-08-26 23:02:16 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container grafana ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 27 00:34:29.664: INFO: downloads-67cbd588c5-qgc4d from openshift-console started at 2019-08-26 23:02:16 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container download-server ready: true, restart count 0
Aug 27 00:34:29.664: INFO: prometheus-adapter-664f76c4d5-98pc9 from openshift-monitoring started at 2019-08-26 23:02:58 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 27 00:34:29.664: INFO: multus-qtzdx from openshift-multus started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container kube-multus ready: true, restart count 0
Aug 27 00:34:29.664: INFO: sdn-lpp6w from openshift-sdn started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container sdn ready: true, restart count 0
Aug 27 00:34:29.664: INFO: node-ca-4p4gt from openshift-image-registry started at 2019-08-26 23:01:53 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container node-ca ready: true, restart count 0
Aug 27 00:34:29.664: INFO: dns-default-x64xg from openshift-dns started at 2019-08-26 23:00:44 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.664: INFO: 	Container dns ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 27 00:34:29.664: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-155-122.ec2.internal before test
Aug 27 00:34:29.714: INFO: ovs-hq6b7 from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container openvswitch ready: true, restart count 0
Aug 27 00:34:29.714: INFO: multus-gmgxj from openshift-multus started at 2019-08-26 23:00:42 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container kube-multus ready: true, restart count 0
Aug 27 00:34:29.714: INFO: tuned-rwhck from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:42 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container tuned ready: true, restart count 0
Aug 27 00:34:29.714: INFO: machine-config-daemon-jtl47 from openshift-machine-config-operator started at 2019-08-26 23:01:50 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 27 00:34:29.714: INFO: node-exporter-7xwt8 from openshift-monitoring started at 2019-08-26 23:00:42 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container node-exporter ready: true, restart count 0
Aug 27 00:34:29.714: INFO: router-default-7879f59486-d98jc from openshift-ingress started at 2019-08-26 23:01:52 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container router ready: true, restart count 0
Aug 27 00:34:29.714: INFO: prometheus-adapter-664f76c4d5-5l7g7 from openshift-monitoring started at 2019-08-26 23:02:58 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 27 00:34:29.714: INFO: sdn-lvs9b from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container sdn ready: true, restart count 0
Aug 27 00:34:29.714: INFO: node-ca-ncxgw from openshift-image-registry started at 2019-08-26 23:01:51 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container node-ca ready: true, restart count 0
Aug 27 00:34:29.714: INFO: telemeter-client-9c775b868-4wbnh from openshift-monitoring started at 2019-08-26 23:02:56 +0000 UTC (3 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container reload ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 27 00:34:29.714: INFO: dns-default-gggf8 from openshift-dns started at 2019-08-26 23:00:42 +0000 UTC (2 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container dns ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 27 00:34:29.714: INFO: downloads-67cbd588c5-7fz6d from openshift-console started at 2019-08-26 23:02:16 +0000 UTC (1 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container download-server ready: true, restart count 0
Aug 27 00:34:29.714: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-08-26 23:03:28 +0000 UTC (3 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container alertmanager ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container config-reloader ready: true, restart count 0
Aug 27 00:34:29.714: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-08-26 23:03:00 +0000 UTC (6 container statuses recorded)
Aug 27 00:34:29.714: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container prometheus ready: true, restart count 1
Aug 27 00:34:29.714: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 27 00:34:29.714: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-70ea6a45-c862-11e9-974e-0a58ac107583 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-70ea6a45-c862-11e9-974e-0a58ac107583 off the node ip-10-0-155-122.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-70ea6a45-c862-11e9-974e-0a58ac107583
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:34:38.003: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-9927" for this suite.
Aug 27 00:34:50.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:34:52.250: INFO: namespace sched-pred-9927 deletion completed in 14.215993177s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:22.926 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:34:52.251: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-6867
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6867 to expose endpoints map[]
Aug 27 00:34:52.427: INFO: successfully validated that service multi-endpoint-test in namespace services-6867 exposes endpoints map[] (16.710305ms elapsed)
STEP: Creating pod pod1 in namespace services-6867
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6867 to expose endpoints map[pod1:[100]]
Aug 27 00:34:55.602: INFO: successfully validated that service multi-endpoint-test in namespace services-6867 exposes endpoints map[pod1:[100]] (3.143534499s elapsed)
STEP: Creating pod pod2 in namespace services-6867
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6867 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 27 00:34:58.837: INFO: successfully validated that service multi-endpoint-test in namespace services-6867 exposes endpoints map[pod1:[100] pod2:[101]] (3.209199973s elapsed)
STEP: Deleting pod pod1 in namespace services-6867
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6867 to expose endpoints map[pod2:[101]]
Aug 27 00:34:58.913: INFO: successfully validated that service multi-endpoint-test in namespace services-6867 exposes endpoints map[pod2:[101]] (48.900806ms elapsed)
STEP: Deleting pod pod2 in namespace services-6867
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6867 to expose endpoints map[]
Aug 27 00:34:58.959: INFO: successfully validated that service multi-endpoint-test in namespace services-6867 exposes endpoints map[] (22.429417ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:34:59.017: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-6867" for this suite.
Aug 27 00:35:23.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:35:25.856: INFO: namespace services-6867 deletion completed in 26.79305013s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:33.606 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:35:25.857: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 27 00:35:34.192: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:34.209: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:36.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:36.228: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:38.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:38.228: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:40.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:40.228: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:42.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:42.228: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:44.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:44.227: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:46.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:46.227: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:48.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:48.228: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:50.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:50.227: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:52.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:52.227: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 27 00:35:54.210: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 27 00:35:54.228: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:35:54.250: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8924" for this suite.
Aug 27 00:36:18.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:36:20.536: INFO: namespace container-lifecycle-hook-8924 deletion completed in 26.241056129s

â€¢ [SLOW TEST:54.679 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:36:20.537: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:36:20.769: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 27 00:36:20.822: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:20.822: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:20.822: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:20.845: INFO: Number of nodes with available pods: 0
Aug 27 00:36:20.845: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:36:21.892: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:21.892: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:21.892: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:21.910: INFO: Number of nodes with available pods: 0
Aug 27 00:36:21.910: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:36:22.893: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:22.893: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:22.893: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:22.911: INFO: Number of nodes with available pods: 0
Aug 27 00:36:22.911: INFO: Node ip-10-0-129-239.ec2.internal is running more than one daemon pod
Aug 27 00:36:23.905: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:23.905: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:23.905: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:23.926: INFO: Number of nodes with available pods: 3
Aug 27 00:36:23.926: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 27 00:36:24.093: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:24.093: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:24.093: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:24.119: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:24.119: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:24.119: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:25.144: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:25.144: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:25.144: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:25.179: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:25.179: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:25.179: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:26.147: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:26.147: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:26.147: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:26.186: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:26.186: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:26.186: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:27.144: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:27.144: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:27.144: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:27.144: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:27.205: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:27.205: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:27.205: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:28.144: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:28.144: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:28.144: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:28.144: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:28.194: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:28.194: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:28.194: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:29.136: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:29.136: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:29.136: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:29.136: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:29.167: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:29.167: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:29.168: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:30.137: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:30.137: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:30.137: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:30.137: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:30.170: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:30.170: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:30.170: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:31.139: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:31.139: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:31.139: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:31.139: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:31.180: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:31.180: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:31.180: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:32.154: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:32.154: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:32.154: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:32.154: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:32.190: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:32.191: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:32.191: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:33.137: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:33.137: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:33.137: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:33.137: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:33.174: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:33.174: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:33.174: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:34.138: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:34.138: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:34.138: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:34.138: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:34.170: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:34.171: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:34.171: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:35.150: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:35.150: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:35.150: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:35.150: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:35.194: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:35.195: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:35.195: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:36.139: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:36.139: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:36.139: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:36.139: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:36.176: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:36.177: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:36.177: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:37.141: INFO: Wrong image for pod: daemon-set-lcbwr. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:37.141: INFO: Pod daemon-set-lcbwr is not available
Aug 27 00:36:37.141: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:37.141: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:37.177: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:37.178: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:37.178: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:38.154: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:38.154: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:38.236: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:38.237: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:38.237: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:39.150: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:39.150: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:39.150: INFO: Pod daemon-set-vk8nl is not available
Aug 27 00:36:39.186: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:39.187: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:39.187: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:40.148: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:40.148: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:40.148: INFO: Pod daemon-set-vk8nl is not available
Aug 27 00:36:40.188: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:40.188: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:40.188: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:41.149: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:41.149: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:41.181: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:41.182: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:41.182: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:42.137: INFO: Wrong image for pod: daemon-set-m2m6m. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:42.137: INFO: Pod daemon-set-m2m6m is not available
Aug 27 00:36:42.137: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:42.189: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:42.189: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:42.189: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:43.141: INFO: Pod daemon-set-9srqf is not available
Aug 27 00:36:43.141: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:43.178: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:43.178: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:43.178: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:44.148: INFO: Pod daemon-set-9srqf is not available
Aug 27 00:36:44.148: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:44.181: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:44.181: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:44.181: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:45.137: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:45.137: INFO: Pod daemon-set-qdnk9 is not available
Aug 27 00:36:45.170: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:45.170: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:45.170: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:46.136: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:46.136: INFO: Pod daemon-set-qdnk9 is not available
Aug 27 00:36:46.170: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:46.170: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:46.170: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:47.137: INFO: Wrong image for pod: daemon-set-qdnk9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 27 00:36:47.137: INFO: Pod daemon-set-qdnk9 is not available
Aug 27 00:36:47.168: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:47.168: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:47.168: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:48.183: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:48.183: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:48.183: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:49.136: INFO: Pod daemon-set-v5dqv is not available
Aug 27 00:36:49.168: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:49.168: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:49.168: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 27 00:36:49.200: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:49.200: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:49.200: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:49.218: INFO: Number of nodes with available pods: 2
Aug 27 00:36:49.218: INFO: Node ip-10-0-155-122.ec2.internal is running more than one daemon pod
Aug 27 00:36:50.263: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:50.263: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:50.263: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:50.280: INFO: Number of nodes with available pods: 2
Aug 27 00:36:50.280: INFO: Node ip-10-0-155-122.ec2.internal is running more than one daemon pod
Aug 27 00:36:51.264: INFO: DaemonSet pods can't tolerate node ip-10-0-137-129.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:45 +0000 UTC}], skip checking this node
Aug 27 00:36:51.264: INFO: DaemonSet pods can't tolerate node ip-10-0-143-134.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:51.264: INFO: DaemonSet pods can't tolerate node ip-10-0-159-3.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:2019-08-26 23:08:46 +0000 UTC}], skip checking this node
Aug 27 00:36:51.281: INFO: Number of nodes with available pods: 3
Aug 27 00:36:51.281: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9348, will wait for the garbage collector to delete the pods
Aug 27 00:36:51.462: INFO: Deleting DaemonSet.extensions daemon-set took: 21.489349ms
Aug 27 00:36:51.662: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.372549ms
Aug 27 00:37:00.080: INFO: Number of nodes with available pods: 0
Aug 27 00:37:00.080: INFO: Number of running nodes: 0, number of available pods: 0
Aug 27 00:37:00.098: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9348/daemonsets","resourceVersion":"53811"},"items":null}

Aug 27 00:37:00.115: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9348/pods","resourceVersion":"53811"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:37:00.198: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "daemonsets-9348" for this suite.
Aug 27 00:37:06.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:37:08.455: INFO: namespace daemonsets-9348 deletion completed in 8.225822904s

â€¢ [SLOW TEST:47.919 seconds]
[sig-apps] Daemon set [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:37:08.456: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 27 00:37:08.649: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:37:18.150: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3669" for this suite.
Aug 27 00:37:24.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:37:26.406: INFO: namespace pods-3669 deletion completed in 8.224517667s

â€¢ [SLOW TEST:17.951 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:37:26.407: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-d7dadfce-c862-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:37:26.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583" in namespace "configmap-3131" to be "success or failure"
Aug 27 00:37:26.598: INFO: Pod "pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.480689ms
Aug 27 00:37:28.616: INFO: Pod "pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035511022s
Aug 27 00:37:30.635: INFO: Pod "pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053889437s
STEP: Saw pod success
Aug 27 00:37:30.635: INFO: Pod "pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:37:30.652: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:37:30.701: INFO: Waiting for pod pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:37:30.719: INFO: Pod pod-configmaps-d7ddf089-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:37:30.719: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-3131" for this suite.
Aug 27 00:37:36.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:37:39.001: INFO: namespace configmap-3131 deletion completed in 8.23373061s

â€¢ [SLOW TEST:12.594 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:37:39.001: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 27 00:37:39.122: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:37:46.736: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-4253" for this suite.
Aug 27 00:38:10.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:38:12.998: INFO: namespace init-container-4253 deletion completed in 26.217468666s

â€¢ [SLOW TEST:33.997 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:38:12.999: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-f3a245a4-c862-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:38:13.182: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583" in namespace "projected-766" to be "success or failure"
Aug 27 00:38:13.199: INFO: Pod "pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.334119ms
Aug 27 00:38:15.216: INFO: Pod "pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033889484s
Aug 27 00:38:17.234: INFO: Pod "pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051459625s
STEP: Saw pod success
Aug 27 00:38:17.234: INFO: Pod "pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:38:17.251: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:38:17.299: INFO: Waiting for pod pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:38:17.318: INFO: Pod pod-projected-configmaps-f3a5121f-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:38:17.318: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-766" for this suite.
Aug 27 00:38:23.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:38:25.572: INFO: namespace projected-766 deletion completed in 8.206036511s

â€¢ [SLOW TEST:12.574 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:38:25.572: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 27 00:38:25.753: INFO: Waiting up to 5m0s for pod "downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583" in namespace "downward-api-3249" to be "success or failure"
Aug 27 00:38:25.770: INFO: Pod "downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.149966ms
Aug 27 00:38:27.788: INFO: Pod "downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035377258s
Aug 27 00:38:29.806: INFO: Pod "downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053076588s
STEP: Saw pod success
Aug 27 00:38:29.806: INFO: Pod "downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:38:29.823: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583 container dapi-container: <nil>
STEP: delete the pod
Aug 27 00:38:29.872: INFO: Waiting for pod downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583 to disappear
Aug 27 00:38:29.888: INFO: Pod downward-api-fb22d4f6-c862-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:38:29.888: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "downward-api-3249" for this suite.
Aug 27 00:38:35.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:38:38.138: INFO: namespace downward-api-3249 deletion completed in 8.204458338s

â€¢ [SLOW TEST:12.566 seconds]
[sig-node] Downward API
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:38:38.138: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Aug 27 00:38:38.335: INFO: Waiting up to 5m0s for pod "client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583" in namespace "containers-377" to be "success or failure"
Aug 27 00:38:38.352: INFO: Pod "client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.504452ms
Aug 27 00:38:40.369: INFO: Pod "client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033708109s
Aug 27 00:38:42.387: INFO: Pod "client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05156152s
STEP: Saw pod success
Aug 27 00:38:42.387: INFO: Pod "client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:38:42.404: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:38:42.452: INFO: Waiting for pod client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:38:42.469: INFO: Pod client-containers-02a2ba9c-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:38:42.469: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "containers-377" for this suite.
Aug 27 00:38:48.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:38:50.745: INFO: namespace containers-377 deletion completed in 8.229977205s

â€¢ [SLOW TEST:12.607 seconds]
[k8s.io] Docker Containers
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:38:50.745: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:38:55.104: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-221" for this suite.
Aug 27 00:39:01.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:39:03.383: INFO: namespace emptydir-wrapper-221 deletion completed in 8.233238392s

â€¢ [SLOW TEST:12.638 seconds]
[sig-storage] EmptyDir wrapper volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:39:03.383: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 27 00:39:03.517: INFO: Waiting up to 1m0s for all (but 3) nodes to be ready
Aug 27 00:39:03.567: INFO: Waiting for terminating namespaces to be deleted...
Aug 27 00:39:03.586: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-129-239.ec2.internal before test
Aug 27 00:39:03.623: INFO: ovs-qnv4t from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container openvswitch ready: true, restart count 0
Aug 27 00:39:03.623: INFO: node-ca-jsqtx from openshift-image-registry started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container node-ca ready: true, restart count 0
Aug 27 00:39:03.623: INFO: redhat-operators-67f45f9754-t96bh from openshift-marketplace started at 2019-08-26 23:01:47 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container redhat-operators ready: true, restart count 0
Aug 27 00:39:03.623: INFO: certified-operators-575dfbb7d8-frm4v from openshift-marketplace started at 2019-08-26 23:01:47 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container certified-operators ready: true, restart count 0
Aug 27 00:39:03.623: INFO: alertmanager-main-2 from openshift-monitoring started at 2019-08-26 23:03:08 +0000 UTC (3 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container alertmanager ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container config-reloader ready: true, restart count 0
Aug 27 00:39:03.623: INFO: sdn-xnf2w from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container sdn ready: true, restart count 0
Aug 27 00:39:03.623: INFO: node-exporter-nrtz2 from openshift-monitoring started at 2019-08-26 23:00:41 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container node-exporter ready: true, restart count 0
Aug 27 00:39:03.623: INFO: machine-config-daemon-qtvl9 from openshift-machine-config-operator started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 27 00:39:03.623: INFO: image-registry-78bd5f6f67-49xzm from openshift-image-registry started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container registry ready: true, restart count 0
Aug 27 00:39:03.623: INFO: multus-mj66c from openshift-multus started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container kube-multus ready: true, restart count 0
Aug 27 00:39:03.623: INFO: tuned-p49z9 from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container tuned ready: true, restart count 0
Aug 27 00:39:03.623: INFO: community-operators-848c796fb7-94fds from openshift-marketplace started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container community-operators ready: true, restart count 0
Aug 27 00:39:03.623: INFO: openshift-state-metrics-6f45d6c785-jnjkl from openshift-monitoring started at 2019-08-26 23:01:43 +0000 UTC (3 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 27 00:39:03.623: INFO: dns-default-fnqls from openshift-dns started at 2019-08-26 23:00:41 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container dns ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 27 00:39:03.623: INFO: kube-state-metrics-5f57cc8b6f-p56kl from openshift-monitoring started at 2019-08-26 23:01:41 +0000 UTC (3 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 27 00:39:03.623: INFO: router-default-7879f59486-qp8d2 from openshift-ingress started at 2019-08-26 23:01:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.623: INFO: 	Container router ready: true, restart count 0
Aug 27 00:39:03.623: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-139-255.ec2.internal before test
Aug 27 00:39:03.659: INFO: node-exporter-mhdpr from openshift-monitoring started at 2019-08-26 23:00:44 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container node-exporter ready: true, restart count 0
Aug 27 00:39:03.659: INFO: tuned-4w8vz from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container tuned ready: true, restart count 0
Aug 27 00:39:03.659: INFO: machine-config-daemon-csqcg from openshift-machine-config-operator started at 2019-08-26 23:01:53 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 27 00:39:03.659: INFO: alertmanager-main-1 from openshift-monitoring started at 2019-08-26 23:03:20 +0000 UTC (3 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container alertmanager ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container config-reloader ready: true, restart count 0
Aug 27 00:39:03.659: INFO: prometheus-k8s-0 from openshift-monitoring started at 2019-08-26 23:03:00 +0000 UTC (6 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container prometheus ready: true, restart count 1
Aug 27 00:39:03.659: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 27 00:39:03.659: INFO: prometheus-adapter-664f76c4d5-98pc9 from openshift-monitoring started at 2019-08-26 23:02:58 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 27 00:39:03.659: INFO: multus-qtzdx from openshift-multus started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container kube-multus ready: true, restart count 0
Aug 27 00:39:03.659: INFO: ovs-7mgp8 from openshift-sdn started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container openvswitch ready: true, restart count 0
Aug 27 00:39:03.659: INFO: grafana-79767c7d48-clxjd from openshift-monitoring started at 2019-08-26 23:02:16 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container grafana ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container grafana-proxy ready: true, restart count 0
Aug 27 00:39:03.659: INFO: downloads-67cbd588c5-qgc4d from openshift-console started at 2019-08-26 23:02:16 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container download-server ready: true, restart count 0
Aug 27 00:39:03.659: INFO: dns-default-x64xg from openshift-dns started at 2019-08-26 23:00:44 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container dns ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 27 00:39:03.659: INFO: sdn-lpp6w from openshift-sdn started at 2019-08-26 23:00:44 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container sdn ready: true, restart count 0
Aug 27 00:39:03.659: INFO: node-ca-4p4gt from openshift-image-registry started at 2019-08-26 23:01:53 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.659: INFO: 	Container node-ca ready: true, restart count 0
Aug 27 00:39:03.659: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-155-122.ec2.internal before test
Aug 27 00:39:03.698: INFO: downloads-67cbd588c5-7fz6d from openshift-console started at 2019-08-26 23:02:16 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container download-server ready: true, restart count 0
Aug 27 00:39:03.698: INFO: dns-default-gggf8 from openshift-dns started at 2019-08-26 23:00:42 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container dns ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 27 00:39:03.698: INFO: alertmanager-main-0 from openshift-monitoring started at 2019-08-26 23:03:28 +0000 UTC (3 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container alertmanager ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container config-reloader ready: true, restart count 0
Aug 27 00:39:03.698: INFO: prometheus-k8s-1 from openshift-monitoring started at 2019-08-26 23:03:00 +0000 UTC (6 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container prometheus ready: true, restart count 1
Aug 27 00:39:03.698: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 27 00:39:03.698: INFO: multus-gmgxj from openshift-multus started at 2019-08-26 23:00:42 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container kube-multus ready: true, restart count 0
Aug 27 00:39:03.698: INFO: tuned-rwhck from openshift-cluster-node-tuning-operator started at 2019-08-26 23:00:42 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container tuned ready: true, restart count 0
Aug 27 00:39:03.698: INFO: machine-config-daemon-jtl47 from openshift-machine-config-operator started at 2019-08-26 23:01:50 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container machine-config-daemon ready: true, restart count 0
Aug 27 00:39:03.698: INFO: ovs-hq6b7 from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container openvswitch ready: true, restart count 0
Aug 27 00:39:03.698: INFO: node-exporter-7xwt8 from openshift-monitoring started at 2019-08-26 23:00:42 +0000 UTC (2 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container node-exporter ready: true, restart count 0
Aug 27 00:39:03.698: INFO: prometheus-adapter-664f76c4d5-5l7g7 from openshift-monitoring started at 2019-08-26 23:02:58 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 27 00:39:03.698: INFO: router-default-7879f59486-d98jc from openshift-ingress started at 2019-08-26 23:01:52 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container router ready: true, restart count 0
Aug 27 00:39:03.698: INFO: node-ca-ncxgw from openshift-image-registry started at 2019-08-26 23:01:51 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container node-ca ready: true, restart count 0
Aug 27 00:39:03.698: INFO: sdn-lvs9b from openshift-sdn started at 2019-08-26 23:00:41 +0000 UTC (1 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container sdn ready: true, restart count 0
Aug 27 00:39:03.698: INFO: telemeter-client-9c775b868-4wbnh from openshift-monitoring started at 2019-08-26 23:02:56 +0000 UTC (3 container statuses recorded)
Aug 27 00:39:03.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container reload ready: true, restart count 0
Aug 27 00:39:03.698: INFO: 	Container telemeter-client ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15bea0a35f5dc9c9], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match node selector, 3 node(s) were unschedulable.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:39:04.937: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "sched-pred-6086" for this suite.
Aug 27 00:39:11.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:39:13.220: INFO: namespace sched-pred-6086 deletion completed in 8.237632109s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.838 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:39:13.221: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 00:39:13.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583" in namespace "projected-2507" to be "success or failure"
Aug 27 00:39:13.396: INFO: Pod "downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.430829ms
Aug 27 00:39:15.414: INFO: Pod "downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036160862s
Aug 27 00:39:17.432: INFO: Pod "downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054179549s
STEP: Saw pod success
Aug 27 00:39:17.432: INFO: Pod "downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:39:17.449: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 27 00:39:17.495: INFO: Waiting for pod downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:39:17.512: INFO: Pod downwardapi-volume-17860111-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:39:17.512: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2507" for this suite.
Aug 27 00:39:23.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:39:25.762: INFO: namespace projected-2507 deletion completed in 8.204979527s

â€¢ [SLOW TEST:12.542 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:39:25.763: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Aug 27 00:39:25.892: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-4340'
Aug 27 00:39:26.523: INFO: stderr: ""
Aug 27 00:39:26.523: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Aug 27 00:39:27.542: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:39:27.542: INFO: Found 0 / 1
Aug 27 00:39:28.541: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:39:28.541: INFO: Found 0 / 1
Aug 27 00:39:29.542: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:39:29.542: INFO: Found 1 / 1
Aug 27 00:39:29.542: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 27 00:39:29.561: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:39:29.561: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 27 00:39:29.561: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig logs redis-master-kmhrr redis-master --namespace=kubectl-4340'
Aug 27 00:39:29.763: INFO: stderr: ""
Aug 27 00:39:29.763: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Aug 00:39:29.096 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Aug 00:39:29.096 # Server started, Redis version 3.2.12\n1:M 27 Aug 00:39:29.096 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Aug 00:39:29.096 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 27 00:39:29.763: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-kmhrr redis-master --namespace=kubectl-4340 --tail=1'
Aug 27 00:39:29.986: INFO: stderr: ""
Aug 27 00:39:29.986: INFO: stdout: "1:M 27 Aug 00:39:29.096 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 27 00:39:29.986: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-kmhrr redis-master --namespace=kubectl-4340 --limit-bytes=1'
Aug 27 00:39:30.184: INFO: stderr: ""
Aug 27 00:39:30.185: INFO: stdout: " "
STEP: exposing timestamps
Aug 27 00:39:30.185: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-kmhrr redis-master --namespace=kubectl-4340 --tail=1 --timestamps'
Aug 27 00:39:30.368: INFO: stderr: ""
Aug 27 00:39:30.368: INFO: stdout: "2019-08-27T00:39:29.096875568Z 1:M 27 Aug 00:39:29.096 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 27 00:39:32.869: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-kmhrr redis-master --namespace=kubectl-4340 --since=1s'
Aug 27 00:39:33.042: INFO: stderr: ""
Aug 27 00:39:33.042: INFO: stdout: ""
Aug 27 00:39:33.042: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig log redis-master-kmhrr redis-master --namespace=kubectl-4340 --since=24h'
Aug 27 00:39:33.228: INFO: stderr: ""
Aug 27 00:39:33.228: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Aug 00:39:29.096 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Aug 00:39:29.096 # Server started, Redis version 3.2.12\n1:M 27 Aug 00:39:29.096 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Aug 00:39:29.096 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Aug 27 00:39:33.229: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete --grace-period=0 --force -f - --namespace=kubectl-4340'
Aug 27 00:39:33.412: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 27 00:39:33.412: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 27 00:39:33.412: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get rc,svc -l name=nginx --no-headers --namespace=kubectl-4340'
Aug 27 00:39:33.589: INFO: stderr: "No resources found.\n"
Aug 27 00:39:33.589: INFO: stdout: ""
Aug 27 00:39:33.589: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig get pods -l name=nginx --namespace=kubectl-4340 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 27 00:39:33.739: INFO: stderr: ""
Aug 27 00:39:33.739: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:39:33.740: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4340" for this suite.
Aug 27 00:39:57.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:40:00.008: INFO: namespace kubectl-4340 deletion completed in 26.223705499s

â€¢ [SLOW TEST:34.246 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:40:00.009: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 27 00:40:00.183: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:40:01.288: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-4235" for this suite.
Aug 27 00:40:07.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:40:09.545: INFO: namespace replication-controller-4235 deletion completed in 8.209680262s

â€¢ [SLOW TEST:9.536 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:40:09.545: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-3918482c-c863-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:40:09.723: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583" in namespace "projected-3774" to be "success or failure"
Aug 27 00:40:09.740: INFO: Pod "pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.46205ms
Aug 27 00:40:11.758: INFO: Pod "pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035317863s
Aug 27 00:40:13.776: INFO: Pod "pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053425256s
STEP: Saw pod success
Aug 27 00:40:13.776: INFO: Pod "pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:40:13.794: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:40:13.844: INFO: Waiting for pod pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:40:13.862: INFO: Pod pod-projected-secrets-391b5bea-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:40:13.862: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-3774" for this suite.
Aug 27 00:40:19.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:40:22.120: INFO: namespace projected-3774 deletion completed in 8.209929585s

â€¢ [SLOW TEST:12.575 seconds]
[sig-storage] Projected secret
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:40:22.120: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 27 00:40:22.293: INFO: Waiting up to 5m0s for pod "pod-40976f0f-c863-11e9-974e-0a58ac107583" in namespace "emptydir-2328" to be "success or failure"
Aug 27 00:40:22.314: INFO: Pod "pod-40976f0f-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 21.791175ms
Aug 27 00:40:24.332: INFO: Pod "pod-40976f0f-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039303219s
Aug 27 00:40:26.349: INFO: Pod "pod-40976f0f-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056706797s
STEP: Saw pod success
Aug 27 00:40:26.349: INFO: Pod "pod-40976f0f-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:40:26.366: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-40976f0f-c863-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:40:26.413: INFO: Waiting for pod pod-40976f0f-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:40:26.431: INFO: Pod pod-40976f0f-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:40:26.431: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-2328" for this suite.
Aug 27 00:40:32.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:40:34.689: INFO: namespace emptydir-2328 deletion completed in 8.213057897s

â€¢ [SLOW TEST:12.569 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:40:34.689: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-481bef94-c863-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:40:34.910: INFO: Waiting up to 5m0s for pod "pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583" in namespace "secrets-2680" to be "success or failure"
Aug 27 00:40:34.927: INFO: Pod "pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.87191ms
Aug 27 00:40:36.945: INFO: Pod "pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035237182s
Aug 27 00:40:38.963: INFO: Pod "pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052900182s
STEP: Saw pod success
Aug 27 00:40:38.963: INFO: Pod "pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:40:38.979: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:40:39.026: INFO: Waiting for pod pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:40:39.045: INFO: Pod pod-secrets-481ec3dc-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:40:39.045: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-2680" for this suite.
Aug 27 00:40:45.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:40:47.354: INFO: namespace secrets-2680 deletion completed in 8.264165623s

â€¢ [SLOW TEST:12.665 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:40:47.355: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-5103/secret-test-4fa3e904-c863-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:40:47.547: INFO: Waiting up to 5m0s for pod "pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583" in namespace "secrets-5103" to be "success or failure"
Aug 27 00:40:47.593: INFO: Pod "pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 45.778944ms
Aug 27 00:40:49.613: INFO: Pod "pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065541053s
Aug 27 00:40:51.631: INFO: Pod "pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083259863s
STEP: Saw pod success
Aug 27 00:40:51.631: INFO: Pod "pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:40:51.647: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583 container env-test: <nil>
STEP: delete the pod
Aug 27 00:40:51.696: INFO: Waiting for pod pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:40:51.713: INFO: Pod pod-configmaps-4fa6ff2a-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:40:51.713: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-5103" for this suite.
Aug 27 00:40:57.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:40:59.989: INFO: namespace secrets-5103 deletion completed in 8.227773332s

â€¢ [SLOW TEST:12.635 seconds]
[sig-api-machinery] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:40:59.990: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-99lgw in namespace proxy-2603
I0827 00:41:00.164113     729 runners.go:184] Created replication controller with name: proxy-service-99lgw, namespace: proxy-2603, replica count: 1
I0827 00:41:01.214821     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 00:41:02.215131     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 00:41:03.215360     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 00:41:04.215620     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 00:41:05.215932     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:06.216192     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:07.216496     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:08.216774     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:09.217146     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:10.217433     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:11.218218     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:12.220357     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0827 00:41:13.220577     729 runners.go:184] proxy-service-99lgw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 27 00:41:13.238: INFO: setup took 13.125271262s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 27 00:41:13.260: INFO: (0) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 21.330129ms)
Aug 27 00:41:13.260: INFO: (0) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 21.235193ms)
Aug 27 00:41:13.260: INFO: (0) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 21.192849ms)
Aug 27 00:41:13.267: INFO: (0) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 28.276807ms)
Aug 27 00:41:13.267: INFO: (0) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 28.613767ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 35.569952ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 36.610311ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 36.325306ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 35.838223ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 35.808049ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 35.859272ms)
Aug 27 00:41:13.275: INFO: (0) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 35.965198ms)
Aug 27 00:41:13.277: INFO: (0) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 38.501697ms)
Aug 27 00:41:13.277: INFO: (0) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 38.356481ms)
Aug 27 00:41:13.277: INFO: (0) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 38.607569ms)
Aug 27 00:41:13.279: INFO: (0) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 39.577014ms)
Aug 27 00:41:13.298: INFO: (1) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.07371ms)
Aug 27 00:41:13.300: INFO: (1) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 20.320867ms)
Aug 27 00:41:13.300: INFO: (1) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 20.61461ms)
Aug 27 00:41:13.300: INFO: (1) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 20.491485ms)
Aug 27 00:41:13.300: INFO: (1) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 20.583919ms)
Aug 27 00:41:13.300: INFO: (1) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 20.459552ms)
Aug 27 00:41:13.301: INFO: (1) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 21.663016ms)
Aug 27 00:41:13.317: INFO: (1) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 37.089006ms)
Aug 27 00:41:13.317: INFO: (1) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 37.382511ms)
Aug 27 00:41:13.317: INFO: (1) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 37.080716ms)
Aug 27 00:41:13.317: INFO: (1) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 37.161183ms)
Aug 27 00:41:13.340: INFO: (1) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 60.276159ms)
Aug 27 00:41:13.340: INFO: (1) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 60.389752ms)
Aug 27 00:41:13.340: INFO: (1) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 60.179761ms)
Aug 27 00:41:13.340: INFO: (1) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 60.162074ms)
Aug 27 00:41:13.340: INFO: (1) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 60.53212ms)
Aug 27 00:41:13.358: INFO: (2) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 17.591138ms)
Aug 27 00:41:13.363: INFO: (2) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 22.118648ms)
Aug 27 00:41:13.363: INFO: (2) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 22.769445ms)
Aug 27 00:41:13.363: INFO: (2) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 22.518378ms)
Aug 27 00:41:13.363: INFO: (2) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 22.387858ms)
Aug 27 00:41:13.363: INFO: (2) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 22.574445ms)
Aug 27 00:41:13.363: INFO: (2) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 22.422998ms)
Aug 27 00:41:13.373: INFO: (2) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 32.717427ms)
Aug 27 00:41:13.374: INFO: (2) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 32.848631ms)
Aug 27 00:41:13.375: INFO: (2) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 33.570191ms)
Aug 27 00:41:13.375: INFO: (2) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 33.56912ms)
Aug 27 00:41:13.375: INFO: (2) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 33.540291ms)
Aug 27 00:41:13.375: INFO: (2) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 33.837351ms)
Aug 27 00:41:13.378: INFO: (2) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 36.851813ms)
Aug 27 00:41:13.385: INFO: (2) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 43.74358ms)
Aug 27 00:41:13.385: INFO: (2) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 43.722272ms)
Aug 27 00:41:13.404: INFO: (3) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 18.653258ms)
Aug 27 00:41:13.404: INFO: (3) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 19.222489ms)
Aug 27 00:41:13.404: INFO: (3) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 18.87049ms)
Aug 27 00:41:13.405: INFO: (3) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 19.688006ms)
Aug 27 00:41:13.405: INFO: (3) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.36242ms)
Aug 27 00:41:13.406: INFO: (3) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 20.773538ms)
Aug 27 00:41:13.406: INFO: (3) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 20.662859ms)
Aug 27 00:41:13.416: INFO: (3) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 30.737712ms)
Aug 27 00:41:13.417: INFO: (3) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 31.069123ms)
Aug 27 00:41:13.417: INFO: (3) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.344926ms)
Aug 27 00:41:13.417: INFO: (3) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 31.924452ms)
Aug 27 00:41:13.417: INFO: (3) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 31.930538ms)
Aug 27 00:41:13.417: INFO: (3) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 31.715922ms)
Aug 27 00:41:13.418: INFO: (3) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 32.59482ms)
Aug 27 00:41:13.420: INFO: (3) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 34.622981ms)
Aug 27 00:41:13.422: INFO: (3) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 35.810532ms)
Aug 27 00:41:13.441: INFO: (4) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 18.760262ms)
Aug 27 00:41:13.441: INFO: (4) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 18.633713ms)
Aug 27 00:41:13.441: INFO: (4) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 19.072383ms)
Aug 27 00:41:13.441: INFO: (4) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 18.825026ms)
Aug 27 00:41:13.441: INFO: (4) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 19.188907ms)
Aug 27 00:41:13.443: INFO: (4) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 20.596657ms)
Aug 27 00:41:13.444: INFO: (4) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 21.575809ms)
Aug 27 00:41:13.452: INFO: (4) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 29.712925ms)
Aug 27 00:41:13.454: INFO: (4) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 31.476486ms)
Aug 27 00:41:13.454: INFO: (4) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.33494ms)
Aug 27 00:41:13.454: INFO: (4) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 31.413171ms)
Aug 27 00:41:13.454: INFO: (4) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 31.856562ms)
Aug 27 00:41:13.454: INFO: (4) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 31.520085ms)
Aug 27 00:41:13.455: INFO: (4) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 32.202424ms)
Aug 27 00:41:13.456: INFO: (4) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 34.002204ms)
Aug 27 00:41:13.459: INFO: (4) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 36.4888ms)
Aug 27 00:41:13.477: INFO: (5) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 17.922152ms)
Aug 27 00:41:13.478: INFO: (5) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 18.727761ms)
Aug 27 00:41:13.479: INFO: (5) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 20.085002ms)
Aug 27 00:41:13.479: INFO: (5) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 19.8597ms)
Aug 27 00:41:13.479: INFO: (5) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 19.80609ms)
Aug 27 00:41:13.480: INFO: (5) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 21.004489ms)
Aug 27 00:41:13.481: INFO: (5) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 21.663841ms)
Aug 27 00:41:13.490: INFO: (5) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 30.374191ms)
Aug 27 00:41:13.491: INFO: (5) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 31.239525ms)
Aug 27 00:41:13.491: INFO: (5) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.200525ms)
Aug 27 00:41:13.491: INFO: (5) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 31.272059ms)
Aug 27 00:41:13.492: INFO: (5) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 32.30419ms)
Aug 27 00:41:13.492: INFO: (5) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 32.249446ms)
Aug 27 00:41:13.493: INFO: (5) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 33.325561ms)
Aug 27 00:41:13.494: INFO: (5) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 34.169655ms)
Aug 27 00:41:13.494: INFO: (5) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 34.652175ms)
Aug 27 00:41:13.517: INFO: (6) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 21.977346ms)
Aug 27 00:41:13.517: INFO: (6) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 22.196053ms)
Aug 27 00:41:13.517: INFO: (6) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 22.056598ms)
Aug 27 00:41:13.517: INFO: (6) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 22.2825ms)
Aug 27 00:41:13.517: INFO: (6) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 22.070725ms)
Aug 27 00:41:13.517: INFO: (6) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 22.244288ms)
Aug 27 00:41:13.518: INFO: (6) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 23.364113ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 37.340694ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 37.221317ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 37.203058ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 37.247658ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 37.130532ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 37.222548ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 37.265517ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 37.234689ms)
Aug 27 00:41:13.532: INFO: (6) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 37.27038ms)
Aug 27 00:41:13.550: INFO: (7) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 17.199855ms)
Aug 27 00:41:13.551: INFO: (7) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 18.051591ms)
Aug 27 00:41:13.551: INFO: (7) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 17.91135ms)
Aug 27 00:41:13.551: INFO: (7) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 18.059654ms)
Aug 27 00:41:13.552: INFO: (7) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 19.901328ms)
Aug 27 00:41:13.553: INFO: (7) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 20.624348ms)
Aug 27 00:41:13.555: INFO: (7) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 21.777809ms)
Aug 27 00:41:13.563: INFO: (7) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 29.969514ms)
Aug 27 00:41:13.563: INFO: (7) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 30.179216ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 36.872337ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 36.650465ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 36.730853ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 36.657437ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 36.831281ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 36.998279ms)
Aug 27 00:41:13.570: INFO: (7) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 36.928368ms)
Aug 27 00:41:13.588: INFO: (8) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 17.723516ms)
Aug 27 00:41:13.589: INFO: (8) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 17.886575ms)
Aug 27 00:41:13.589: INFO: (8) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 18.03727ms)
Aug 27 00:41:13.590: INFO: (8) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 18.951583ms)
Aug 27 00:41:13.592: INFO: (8) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 21.756996ms)
Aug 27 00:41:13.592: INFO: (8) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 21.40938ms)
Aug 27 00:41:13.599: INFO: (8) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 28.683251ms)
Aug 27 00:41:13.602: INFO: (8) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 31.356278ms)
Aug 27 00:41:13.603: INFO: (8) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 31.570759ms)
Aug 27 00:41:13.603: INFO: (8) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 31.927624ms)
Aug 27 00:41:13.603: INFO: (8) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.926481ms)
Aug 27 00:41:13.603: INFO: (8) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 32.081566ms)
Aug 27 00:41:13.603: INFO: (8) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 31.989631ms)
Aug 27 00:41:13.613: INFO: (8) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 41.535844ms)
Aug 27 00:41:13.613: INFO: (8) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 41.706015ms)
Aug 27 00:41:13.613: INFO: (8) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 41.681794ms)
Aug 27 00:41:13.635: INFO: (9) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 21.872357ms)
Aug 27 00:41:13.635: INFO: (9) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 21.48884ms)
Aug 27 00:41:13.641: INFO: (9) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 27.649612ms)
Aug 27 00:41:13.641: INFO: (9) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 27.37673ms)
Aug 27 00:41:13.641: INFO: (9) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 27.65372ms)
Aug 27 00:41:13.641: INFO: (9) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 27.603034ms)
Aug 27 00:41:13.641: INFO: (9) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 27.860142ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 41.905262ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 42.380481ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 42.157896ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 42.236734ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 42.158975ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 42.425308ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 42.399051ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 42.170269ms)
Aug 27 00:41:13.656: INFO: (9) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 42.40162ms)
Aug 27 00:41:13.676: INFO: (10) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.308141ms)
Aug 27 00:41:13.676: INFO: (10) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 19.752294ms)
Aug 27 00:41:13.676: INFO: (10) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 19.293842ms)
Aug 27 00:41:13.676: INFO: (10) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.348557ms)
Aug 27 00:41:13.676: INFO: (10) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 19.767877ms)
Aug 27 00:41:13.684: INFO: (10) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 27.311007ms)
Aug 27 00:41:13.684: INFO: (10) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 27.234159ms)
Aug 27 00:41:13.687: INFO: (10) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 29.959854ms)
Aug 27 00:41:13.689: INFO: (10) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 31.172778ms)
Aug 27 00:41:13.689: INFO: (10) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 31.51417ms)
Aug 27 00:41:13.689: INFO: (10) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 31.595256ms)
Aug 27 00:41:13.689: INFO: (10) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 31.603483ms)
Aug 27 00:41:13.689: INFO: (10) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.720871ms)
Aug 27 00:41:13.697: INFO: (10) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 39.512755ms)
Aug 27 00:41:13.697: INFO: (10) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 39.466998ms)
Aug 27 00:41:13.697: INFO: (10) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 39.54192ms)
Aug 27 00:41:13.715: INFO: (11) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 18.030087ms)
Aug 27 00:41:13.715: INFO: (11) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 17.769177ms)
Aug 27 00:41:13.715: INFO: (11) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 18.221411ms)
Aug 27 00:41:13.720: INFO: (11) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 23.230271ms)
Aug 27 00:41:13.720: INFO: (11) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 23.362818ms)
Aug 27 00:41:13.721: INFO: (11) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 23.251095ms)
Aug 27 00:41:13.721: INFO: (11) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 23.19556ms)
Aug 27 00:41:13.729: INFO: (11) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 31.324602ms)
Aug 27 00:41:13.729: INFO: (11) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.318976ms)
Aug 27 00:41:13.729: INFO: (11) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 31.275467ms)
Aug 27 00:41:13.729: INFO: (11) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 31.69311ms)
Aug 27 00:41:13.741: INFO: (11) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 43.146751ms)
Aug 27 00:41:13.741: INFO: (11) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 43.153053ms)
Aug 27 00:41:13.741: INFO: (11) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 43.109893ms)
Aug 27 00:41:13.741: INFO: (11) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 43.297522ms)
Aug 27 00:41:13.741: INFO: (11) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 43.380512ms)
Aug 27 00:41:13.759: INFO: (12) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 17.919708ms)
Aug 27 00:41:13.761: INFO: (12) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 19.051486ms)
Aug 27 00:41:13.761: INFO: (12) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 19.765294ms)
Aug 27 00:41:13.761: INFO: (12) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.834274ms)
Aug 27 00:41:13.761: INFO: (12) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 19.735191ms)
Aug 27 00:41:13.761: INFO: (12) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 19.97768ms)
Aug 27 00:41:13.761: INFO: (12) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 19.846768ms)
Aug 27 00:41:13.776: INFO: (12) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 33.7324ms)
Aug 27 00:41:13.776: INFO: (12) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 33.883381ms)
Aug 27 00:41:13.776: INFO: (12) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 33.645503ms)
Aug 27 00:41:13.776: INFO: (12) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 33.757368ms)
Aug 27 00:41:13.776: INFO: (12) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 33.746673ms)
Aug 27 00:41:13.777: INFO: (12) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 34.566751ms)
Aug 27 00:41:13.779: INFO: (12) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 36.782528ms)
Aug 27 00:41:13.779: INFO: (12) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 36.914021ms)
Aug 27 00:41:13.779: INFO: (12) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 36.672146ms)
Aug 27 00:41:13.798: INFO: (13) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 18.754245ms)
Aug 27 00:41:13.798: INFO: (13) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 18.911708ms)
Aug 27 00:41:13.798: INFO: (13) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 18.549821ms)
Aug 27 00:41:13.798: INFO: (13) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 18.642369ms)
Aug 27 00:41:13.807: INFO: (13) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 27.651868ms)
Aug 27 00:41:13.807: INFO: (13) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 28.080897ms)
Aug 27 00:41:13.807: INFO: (13) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 27.666243ms)
Aug 27 00:41:13.810: INFO: (13) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 30.143675ms)
Aug 27 00:41:13.811: INFO: (13) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 30.905377ms)
Aug 27 00:41:13.811: INFO: (13) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.174506ms)
Aug 27 00:41:13.811: INFO: (13) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 31.362402ms)
Aug 27 00:41:13.811: INFO: (13) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 31.818312ms)
Aug 27 00:41:13.812: INFO: (13) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 31.773035ms)
Aug 27 00:41:13.815: INFO: (13) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 35.329683ms)
Aug 27 00:41:13.815: INFO: (13) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 35.42278ms)
Aug 27 00:41:13.816: INFO: (13) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 36.267942ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 21.418471ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 21.737572ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 21.99521ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 21.71962ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 21.622382ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 21.912408ms)
Aug 27 00:41:13.838: INFO: (14) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 21.72852ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 37.285112ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 37.38539ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 37.335336ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 37.504984ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 37.492681ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 37.677427ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 37.751814ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 37.680014ms)
Aug 27 00:41:13.854: INFO: (14) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 37.666129ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 21.120348ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 21.327854ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 21.05157ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 21.365409ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 21.651296ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 21.353353ms)
Aug 27 00:41:13.876: INFO: (15) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 21.303848ms)
Aug 27 00:41:13.886: INFO: (15) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 30.557269ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 37.770107ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 37.63703ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 37.81035ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 37.640611ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 38.034997ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 37.784988ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 37.856748ms)
Aug 27 00:41:13.893: INFO: (15) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 38.024338ms)
Aug 27 00:41:13.918: INFO: (16) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 24.428667ms)
Aug 27 00:41:13.918: INFO: (16) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 24.707969ms)
Aug 27 00:41:13.918: INFO: (16) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 24.186886ms)
Aug 27 00:41:13.918: INFO: (16) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 24.135479ms)
Aug 27 00:41:13.918: INFO: (16) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 24.306593ms)
Aug 27 00:41:13.920: INFO: (16) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 26.146784ms)
Aug 27 00:41:13.920: INFO: (16) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 26.272693ms)
Aug 27 00:41:13.927: INFO: (16) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 32.665141ms)
Aug 27 00:41:13.927: INFO: (16) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 33.095459ms)
Aug 27 00:41:13.928: INFO: (16) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 33.294455ms)
Aug 27 00:41:13.928: INFO: (16) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 33.511801ms)
Aug 27 00:41:13.928: INFO: (16) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 33.912494ms)
Aug 27 00:41:13.929: INFO: (16) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 34.471993ms)
Aug 27 00:41:13.931: INFO: (16) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 37.015339ms)
Aug 27 00:41:13.931: INFO: (16) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 37.279347ms)
Aug 27 00:41:13.935: INFO: (16) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 40.663515ms)
Aug 27 00:41:13.953: INFO: (17) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 17.843771ms)
Aug 27 00:41:13.954: INFO: (17) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 18.723629ms)
Aug 27 00:41:13.954: INFO: (17) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 18.831612ms)
Aug 27 00:41:13.954: INFO: (17) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 18.925706ms)
Aug 27 00:41:13.955: INFO: (17) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 19.582262ms)
Aug 27 00:41:13.956: INFO: (17) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 20.89816ms)
Aug 27 00:41:13.957: INFO: (17) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 21.828028ms)
Aug 27 00:41:13.967: INFO: (17) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 31.435926ms)
Aug 27 00:41:13.967: INFO: (17) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 31.577837ms)
Aug 27 00:41:13.967: INFO: (17) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 31.544464ms)
Aug 27 00:41:13.967: INFO: (17) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 31.387693ms)
Aug 27 00:41:13.967: INFO: (17) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 31.459729ms)
Aug 27 00:41:13.967: INFO: (17) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 31.611518ms)
Aug 27 00:41:13.982: INFO: (17) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 46.269488ms)
Aug 27 00:41:13.982: INFO: (17) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 46.562256ms)
Aug 27 00:41:13.982: INFO: (17) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 46.199584ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 23.381473ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 23.626497ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 23.323952ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 23.839814ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 23.449034ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 23.64844ms)
Aug 27 00:41:14.006: INFO: (18) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 24.120826ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 35.264369ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 34.976562ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 34.959029ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 35.212638ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 35.103095ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 35.173824ms)
Aug 27 00:41:14.018: INFO: (18) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 35.402155ms)
Aug 27 00:41:14.022: INFO: (18) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 38.81395ms)
Aug 27 00:41:14.022: INFO: (18) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 38.774583ms)
Aug 27 00:41:14.042: INFO: (19) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname2/proxy/: bar (200; 19.912446ms)
Aug 27 00:41:14.042: INFO: (19) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 20.172865ms)
Aug 27 00:41:14.042: INFO: (19) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.485684ms)
Aug 27 00:41:14.042: INFO: (19) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:160/proxy/: foo (200; 19.834816ms)
Aug 27 00:41:14.042: INFO: (19) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">test<... (200; 19.711536ms)
Aug 27 00:41:14.049: INFO: (19) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname1/proxy/: tls baz (200; 26.880133ms)
Aug 27 00:41:14.049: INFO: (19) /api/v1/namespaces/proxy-2603/services/proxy-service-99lgw:portname1/proxy/: foo (200; 26.782089ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/services/https:proxy-service-99lgw:tlsportname2/proxy/: tls qux (200; 32.977394ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:460/proxy/: tls baz (200; 33.31471ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr/proxy/rewriteme">test</a> (200; 33.017119ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:443/proxy/tlsrewritem... (200; 33.265995ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/pods/proxy-service-99lgw-dg4nr:162/proxy/: bar (200; 33.433671ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2603/pods/http:proxy-service-99lgw-dg4nr:1080/proxy/rewriteme">... (200; 33.362393ms)
Aug 27 00:41:14.056: INFO: (19) /api/v1/namespaces/proxy-2603/pods/https:proxy-service-99lgw-dg4nr:462/proxy/: tls qux (200; 33.291809ms)
Aug 27 00:41:14.065: INFO: (19) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname1/proxy/: foo (200; 42.258175ms)
Aug 27 00:41:14.065: INFO: (19) /api/v1/namespaces/proxy-2603/services/http:proxy-service-99lgw:portname2/proxy/: bar (200; 42.324433ms)
STEP: deleting ReplicationController proxy-service-99lgw in namespace proxy-2603, will wait for the garbage collector to delete the pods
Aug 27 00:41:14.280: INFO: Deleting ReplicationController proxy-service-99lgw took: 141.958194ms
Aug 27 00:41:14.386: INFO: Terminating ReplicationController proxy-service-99lgw pods took: 105.280484ms
[AfterEach] version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:41:18.189: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "proxy-2603" for this suite.
Aug 27 00:41:24.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:41:25.697: INFO: namespace proxy-2603 deletion completed in 7.444658659s

â€¢ [SLOW TEST:25.707 seconds]
[sig-network] Proxy
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:41:25.698: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583
Aug 27 00:41:25.877: INFO: Pod name my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583: Found 1 pods out of 1
Aug 27 00:41:25.877: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583" are running
Aug 27 00:41:29.927: INFO: Pod "my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583-4fdx7" is running (conditions: [])
Aug 27 00:41:29.927: INFO: Trying to dial the pod
Aug 27 00:41:34.985: INFO: Controller my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583: Got expected result from replica 1 [my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583-4fdx7]: "my-hostname-basic-667d230d-c863-11e9-974e-0a58ac107583-4fdx7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:41:34.986: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replication-controller-5559" for this suite.
Aug 27 00:41:41.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:41:43.253: INFO: namespace replication-controller-5559 deletion completed in 8.217840706s

â€¢ [SLOW TEST:17.555 seconds]
[sig-apps] ReplicationController
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:41:43.253: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 27 00:41:43.402: INFO: Waiting up to 5m0s for pod "pod-70f0a270-c863-11e9-974e-0a58ac107583" in namespace "emptydir-641" to be "success or failure"
Aug 27 00:41:43.421: INFO: Pod "pod-70f0a270-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.924707ms
Aug 27 00:41:45.439: INFO: Pod "pod-70f0a270-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036633758s
Aug 27 00:41:47.457: INFO: Pod "pod-70f0a270-c863-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054596971s
Aug 27 00:41:49.476: INFO: Pod "pod-70f0a270-c863-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073824878s
STEP: Saw pod success
Aug 27 00:41:49.476: INFO: Pod "pod-70f0a270-c863-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:41:49.493: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-70f0a270-c863-11e9-974e-0a58ac107583 container test-container: <nil>
STEP: delete the pod
Aug 27 00:41:49.542: INFO: Waiting for pod pod-70f0a270-c863-11e9-974e-0a58ac107583 to disappear
Aug 27 00:41:49.558: INFO: Pod pod-70f0a270-c863-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:41:49.558: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "emptydir-641" for this suite.
Aug 27 00:41:55.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:41:57.810: INFO: namespace emptydir-641 deletion completed in 8.207031862s

â€¢ [SLOW TEST:14.556 seconds]
[sig-storage] EmptyDir volumes
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:41:57.810: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:41:57.960: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "services-7903" for this suite.
Aug 27 00:42:04.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:42:06.183: INFO: namespace services-7903 deletion completed in 8.191392808s
[AfterEach] [sig-network] Services
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:8.373 seconds]
[sig-network] Services
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:42:06.183: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:42:10.414: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubelet-test-1" for this suite.
Aug 27 00:43:00.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:43:02.707: INFO: namespace kubelet-test-1 deletion completed in 52.23499824s

â€¢ [SLOW TEST:56.524 seconds]
[k8s.io] Kubelet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:43:02.707: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 27 00:43:02.844: INFO: PodSpec: initContainers in spec.initContainers
Aug 27 00:43:57.266: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a0504a94-c863-11e9-974e-0a58ac107583", GenerateName:"", Namespace:"init-container-8700", SelfLink:"/api/v1/namespaces/init-container-8700/pods/pod-init-a0504a94-c863-11e9-974e-0a58ac107583", UID:"a052bfbb-c863-11e9-8aff-12fef04b6120", ResourceVersion:"57153", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63702463382, loc:(*time.Location)(0x8a23120)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"844543995"}, Annotations:map[string]string{"openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rg5tf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002225340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rg5tf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0021b60f0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rg5tf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0021b6190), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rg5tf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0021b6050), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00368b3a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-155-122.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0026bfb00), ImagePullSecrets:[]v1.LocalObjectReference{v1.LocalObjectReference{Name:"default-dockercfg-bs7cr"}}, Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00368b450)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00368b470)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00368b48c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00368b490)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702463382, loc:(*time.Location)(0x8a23120)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702463382, loc:(*time.Location)(0x8a23120)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702463382, loc:(*time.Location)(0x8a23120)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63702463382, loc:(*time.Location)(0x8a23120)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.155.122", PodIP:"10.128.2.106", StartTime:(*v1.Time)(0xc0027aa520), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000344b60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000344bd0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"cri-o://dec2a0897dbb6c1444191ff1f8205acd50211370be60ee0a7f55514698ec40bc"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027aa560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027aa540), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Burstable"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:43:57.268: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "init-container-8700" for this suite.
Aug 27 00:44:21.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:44:23.538: INFO: namespace init-container-8700 deletion completed in 26.22503779s

â€¢ [SLOW TEST:80.831 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:44:23.538: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:44:51.036: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "namespaces-7790" for this suite.
Aug 27 00:44:57.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:44:59.298: INFO: namespace namespaces-7790 deletion completed in 8.212132341s
STEP: Destroying namespace "nsdeletetest-3322" for this suite.
Aug 27 00:44:59.317: INFO: Namespace nsdeletetest-3322 was already deleted
STEP: Destroying namespace "nsdeletetest-6647" for this suite.
Aug 27 00:45:05.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:45:07.561: INFO: namespace nsdeletetest-6647 deletion completed in 8.243843979s

â€¢ [SLOW TEST:44.023 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:45:07.561: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7797
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 27 00:45:07.679: INFO: Waiting up to 10m0s for all (but 3) nodes to be schedulable
STEP: Creating test pods
Aug 27 00:45:32.130: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.108:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7797 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 00:45:32.130: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 27 00:45:32.366: INFO: Found all expected endpoints: [netserver-0]
Aug 27 00:45:32.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.104:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7797 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 00:45:32.385: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 27 00:45:32.584: INFO: Found all expected endpoints: [netserver-1]
Aug 27 00:45:32.601: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.0.125:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7797 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 27 00:45:32.601: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
Aug 27 00:45:32.784: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:45:32.784: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pod-network-test-7797" for this suite.
Aug 27 00:45:56.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:45:59.040: INFO: namespace pod-network-test-7797 deletion completed in 26.211151484s

â€¢ [SLOW TEST:51.479 seconds]
[sig-network] Networking
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:45:59.040: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0827 00:45:59.874572     729 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 00:45:59.874: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:45:59.874: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1314" for this suite.
Aug 27 00:46:05.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:46:08.130: INFO: namespace gc-1314 deletion completed in 8.224596124s

â€¢ [SLOW TEST:9.090 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:46:08.131: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0827 00:46:38.429456     729 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 00:46:38.429: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:46:38.429: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-2526" for this suite.
Aug 27 00:46:44.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:46:46.672: INFO: namespace gc-2526 deletion completed in 8.211568264s

â€¢ [SLOW TEST:38.542 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:46:46.672: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 27 00:46:54.991: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:46:55.008: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:46:57.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:46:57.027: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:46:59.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:46:59.026: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:47:01.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:47:01.026: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:47:03.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:47:03.026: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:47:05.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:47:05.025: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:47:07.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:47:07.029: INFO: Pod pod-with-poststart-http-hook still exists
Aug 27 00:47:09.008: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 27 00:47:09.026: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:47:09.026: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3563" for this suite.
Aug 27 00:47:33.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:47:35.370: INFO: namespace container-lifecycle-hook-3563 deletion completed in 26.298896122s

â€¢ [SLOW TEST:48.698 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:47:35.370: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5618
Aug 27 00:47:37.611: INFO: Started pod liveness-exec in namespace container-probe-5618
STEP: checking the pod's current state and verifying that restartCount is present
Aug 27 00:47:37.628: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:51:37.825: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-probe-5618" for this suite.
Aug 27 00:51:43.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:51:46.081: INFO: namespace container-probe-5618 deletion completed in 8.210991095s

â€¢ [SLOW TEST:250.711 seconds]
[k8s.io] Probing container
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:51:46.081: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d8435505-c864-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:51:46.420: INFO: Waiting up to 5m0s for pod "pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583" in namespace "secrets-6212" to be "success or failure"
Aug 27 00:51:46.437: INFO: Pod "pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.835211ms
Aug 27 00:51:48.455: INFO: Pod "pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035181571s
Aug 27 00:51:50.472: INFO: Pod "pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052436494s
STEP: Saw pod success
Aug 27 00:51:50.472: INFO: Pod "pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:51:50.489: INFO: Trying to get logs from node ip-10-0-129-239.ec2.internal pod pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:51:50.535: INFO: Waiting for pod pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583 to disappear
Aug 27 00:51:50.551: INFO: Pod pod-secrets-d85f1069-c864-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:51:50.551: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-6212" for this suite.
Aug 27 00:51:56.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:51:58.792: INFO: namespace secrets-6212 deletion completed in 8.19610493s
STEP: Destroying namespace "secret-namespace-7949" for this suite.
Aug 27 00:52:04.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:52:07.001: INFO: namespace secret-namespace-7949 deletion completed in 8.20882643s

â€¢ [SLOW TEST:20.920 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:52:07.001: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 27 00:52:07.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583" in namespace "projected-683" to be "success or failure"
Aug 27 00:52:07.211: INFO: Pod "downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.981548ms
Aug 27 00:52:09.230: INFO: Pod "downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037008484s
Aug 27 00:52:11.248: INFO: Pod "downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055090361s
STEP: Saw pod success
Aug 27 00:52:11.249: INFO: Pod "downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:52:11.266: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583 container client-container: <nil>
STEP: delete the pod
Aug 27 00:52:11.316: INFO: Waiting for pod downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583 to disappear
Aug 27 00:52:11.332: INFO: Pod downwardapi-volume-e4bf91ee-c864-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:52:11.332: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-683" for this suite.
Aug 27 00:52:17.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:52:19.583: INFO: namespace projected-683 deletion completed in 8.205860573s

â€¢ [SLOW TEST:12.582 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:52:19.584: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:52:19.743: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-ec4050c8-c864-11e9-974e-0a58ac107583
STEP: Creating secret with name s-test-opt-upd-ec405141-c864-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ec4050c8-c864-11e9-974e-0a58ac107583
STEP: Updating secret s-test-opt-upd-ec405141-c864-11e9-974e-0a58ac107583
STEP: Creating secret with name s-test-opt-create-ec40516a-c864-11e9-974e-0a58ac107583
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:53:46.928: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-4493" for this suite.
Aug 27 00:54:11.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:54:13.157: INFO: namespace secrets-4493 deletion completed in 26.197320362s

â€¢ [SLOW TEST:113.573 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:54:13.158: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0827 00:54:23.601107     729 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 00:54:23.601: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:54:23.601: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-5465" for this suite.
Aug 27 00:54:29.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:54:31.832: INFO: namespace gc-5465 deletion completed in 8.199050506s

â€¢ [SLOW TEST:18.674 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:54:31.832: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-3b0f2966-c865-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:54:32.015: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583" in namespace "projected-7710" to be "success or failure"
Aug 27 00:54:32.034: INFO: Pod "pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 18.509171ms
Aug 27 00:54:34.052: INFO: Pod "pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037194794s
Aug 27 00:54:36.069: INFO: Pod "pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054454376s
STEP: Saw pod success
Aug 27 00:54:36.070: INFO: Pod "pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:54:36.087: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:54:36.132: INFO: Waiting for pod pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583 to disappear
Aug 27 00:54:36.149: INFO: Pod pod-projected-configmaps-3b12da62-c865-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:54:36.149: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-7710" for this suite.
Aug 27 00:54:42.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:54:44.578: INFO: namespace projected-7710 deletion completed in 8.384201485s

â€¢ [SLOW TEST:12.746 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:54:44.578: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 27 00:54:52.923: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:54:52.939: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:54:54.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:54:54.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:54:56.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:54:56.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:54:58.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:54:58.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:00.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:00.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:02.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:02.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:04.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:04.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:06.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:06.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:08.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:08.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:10.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:10.958: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:12.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:12.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:14.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:14.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:16.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:16.957: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 27 00:55:18.940: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 27 00:55:18.957: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:55:18.958: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5280" for this suite.
Aug 27 00:55:43.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:55:45.363: INFO: namespace container-lifecycle-hook-5280 deletion completed in 26.358358286s

â€¢ [SLOW TEST:60.785 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:55:45.363: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-66e31a6c-c865-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:55:45.550: INFO: Waiting up to 5m0s for pod "pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583" in namespace "configmap-9577" to be "success or failure"
Aug 27 00:55:45.567: INFO: Pod "pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.947077ms
Aug 27 00:55:47.584: INFO: Pod "pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034796329s
Aug 27 00:55:49.602: INFO: Pod "pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052542498s
STEP: Saw pod success
Aug 27 00:55:49.602: INFO: Pod "pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:55:49.620: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:55:49.672: INFO: Waiting for pod pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583 to disappear
Aug 27 00:55:49.689: INFO: Pod pod-configmaps-66e617c2-c865-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:55:49.689: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-9577" for this suite.
Aug 27 00:55:55.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:55:57.990: INFO: namespace configmap-9577 deletion completed in 8.256363389s

â€¢ [SLOW TEST:12.627 seconds]
[sig-storage] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:55:57.991: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0827 00:56:04.280579     729 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 27 00:56:04.280: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:56:04.280: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "gc-1871" for this suite.
Aug 27 00:56:12.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:56:14.835: INFO: namespace gc-1871 deletion completed in 10.521567846s

â€¢ [SLOW TEST:16.844 seconds]
[sig-api-machinery] Garbage collector
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:56:14.835: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6337
I0827 00:56:15.002291     729 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6337, replica count: 1
I0827 00:56:16.053091     729 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 00:56:17.053391     729 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0827 00:56:18.053777     729 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 27 00:56:18.180: INFO: Created: latency-svc-z2fqx
Aug 27 00:56:18.187: INFO: Got endpoints: latency-svc-z2fqx [33.540087ms]
Aug 27 00:56:18.215: INFO: Created: latency-svc-gmfq6
Aug 27 00:56:18.223: INFO: Created: latency-svc-xcpd5
Aug 27 00:56:18.230: INFO: Got endpoints: latency-svc-gmfq6 [42.142009ms]
Aug 27 00:56:18.232: INFO: Got endpoints: latency-svc-xcpd5 [44.252547ms]
Aug 27 00:56:18.233: INFO: Created: latency-svc-vcwmd
Aug 27 00:56:18.240: INFO: Got endpoints: latency-svc-vcwmd [52.278865ms]
Aug 27 00:56:18.242: INFO: Created: latency-svc-k2f98
Aug 27 00:56:18.252: INFO: Got endpoints: latency-svc-k2f98 [64.806471ms]
Aug 27 00:56:18.253: INFO: Created: latency-svc-82ss4
Aug 27 00:56:18.264: INFO: Got endpoints: latency-svc-82ss4 [76.132848ms]
Aug 27 00:56:18.268: INFO: Created: latency-svc-wmjk5
Aug 27 00:56:18.275: INFO: Got endpoints: latency-svc-wmjk5 [87.231998ms]
Aug 27 00:56:18.277: INFO: Created: latency-svc-srcf6
Aug 27 00:56:18.287: INFO: Created: latency-svc-nsvzn
Aug 27 00:56:18.297: INFO: Got endpoints: latency-svc-srcf6 [57.448803ms]
Aug 27 00:56:18.302: INFO: Created: latency-svc-tvc2c
Aug 27 00:56:18.311: INFO: Created: latency-svc-j7kbz
Aug 27 00:56:18.312: INFO: Got endpoints: latency-svc-tvc2c [124.374783ms]
Aug 27 00:56:18.312: INFO: Got endpoints: latency-svc-nsvzn [124.318134ms]
Aug 27 00:56:18.317: INFO: Got endpoints: latency-svc-j7kbz [128.725952ms]
Aug 27 00:56:18.332: INFO: Created: latency-svc-fl5wr
Aug 27 00:56:18.338: INFO: Created: latency-svc-fjrxz
Aug 27 00:56:18.342: INFO: Got endpoints: latency-svc-fl5wr [154.740459ms]
Aug 27 00:56:18.346: INFO: Got endpoints: latency-svc-fjrxz [158.692168ms]
Aug 27 00:56:18.357: INFO: Created: latency-svc-ncwgz
Aug 27 00:56:18.366: INFO: Got endpoints: latency-svc-ncwgz [178.352861ms]
Aug 27 00:56:18.368: INFO: Created: latency-svc-69jxr
Aug 27 00:56:18.375: INFO: Got endpoints: latency-svc-69jxr [187.191084ms]
Aug 27 00:56:18.377: INFO: Created: latency-svc-dkj69
Aug 27 00:56:18.384: INFO: Got endpoints: latency-svc-dkj69 [195.854547ms]
Aug 27 00:56:18.389: INFO: Created: latency-svc-5kmjx
Aug 27 00:56:18.395: INFO: Created: latency-svc-rppx9
Aug 27 00:56:18.395: INFO: Got endpoints: latency-svc-5kmjx [207.685839ms]
Aug 27 00:56:18.404: INFO: Got endpoints: latency-svc-rppx9 [174.823986ms]
Aug 27 00:56:18.408: INFO: Created: latency-svc-gmmws
Aug 27 00:56:18.418: INFO: Created: latency-svc-sfjnq
Aug 27 00:56:18.419: INFO: Got endpoints: latency-svc-gmmws [187.165768ms]
Aug 27 00:56:18.427: INFO: Created: latency-svc-hx5vq
Aug 27 00:56:18.434: INFO: Got endpoints: latency-svc-sfjnq [181.136137ms]
Aug 27 00:56:18.437: INFO: Got endpoints: latency-svc-hx5vq [173.331085ms]
Aug 27 00:56:18.446: INFO: Created: latency-svc-x7d4l
Aug 27 00:56:18.450: INFO: Created: latency-svc-7jvkn
Aug 27 00:56:18.457: INFO: Got endpoints: latency-svc-x7d4l [181.930638ms]
Aug 27 00:56:18.461: INFO: Created: latency-svc-cpscd
Aug 27 00:56:18.464: INFO: Got endpoints: latency-svc-7jvkn [166.149747ms]
Aug 27 00:56:18.471: INFO: Created: latency-svc-pp2jf
Aug 27 00:56:18.472: INFO: Got endpoints: latency-svc-cpscd [159.776715ms]
Aug 27 00:56:18.481: INFO: Created: latency-svc-f8zln
Aug 27 00:56:18.481: INFO: Got endpoints: latency-svc-pp2jf [169.107185ms]
Aug 27 00:56:18.489: INFO: Got endpoints: latency-svc-f8zln [172.288721ms]
Aug 27 00:56:18.494: INFO: Created: latency-svc-7ttgs
Aug 27 00:56:18.501: INFO: Got endpoints: latency-svc-7ttgs [158.537298ms]
Aug 27 00:56:18.508: INFO: Created: latency-svc-hwkf7
Aug 27 00:56:18.513: INFO: Created: latency-svc-ffc4x
Aug 27 00:56:18.516: INFO: Got endpoints: latency-svc-hwkf7 [169.196777ms]
Aug 27 00:56:18.519: INFO: Got endpoints: latency-svc-ffc4x [153.223974ms]
Aug 27 00:56:18.526: INFO: Created: latency-svc-mp57c
Aug 27 00:56:18.533: INFO: Got endpoints: latency-svc-mp57c [158.122727ms]
Aug 27 00:56:18.544: INFO: Created: latency-svc-9r8w9
Aug 27 00:56:18.556: INFO: Got endpoints: latency-svc-9r8w9 [172.101919ms]
Aug 27 00:56:18.558: INFO: Created: latency-svc-knm5b
Aug 27 00:56:18.563: INFO: Created: latency-svc-l5gw9
Aug 27 00:56:18.567: INFO: Got endpoints: latency-svc-knm5b [171.417594ms]
Aug 27 00:56:18.573: INFO: Got endpoints: latency-svc-l5gw9 [168.65154ms]
Aug 27 00:56:18.574: INFO: Created: latency-svc-rn2xp
Aug 27 00:56:18.591: INFO: Got endpoints: latency-svc-rn2xp [172.117631ms]
Aug 27 00:56:18.592: INFO: Created: latency-svc-4n8tn
Aug 27 00:56:18.604: INFO: Created: latency-svc-gjhsx
Aug 27 00:56:18.612: INFO: Got endpoints: latency-svc-4n8tn [177.833109ms]
Aug 27 00:56:18.616: INFO: Created: latency-svc-84qgr
Aug 27 00:56:18.623: INFO: Got endpoints: latency-svc-84qgr [166.341226ms]
Aug 27 00:56:18.623: INFO: Got endpoints: latency-svc-gjhsx [186.22969ms]
Aug 27 00:56:18.625: INFO: Created: latency-svc-7b7rj
Aug 27 00:56:18.634: INFO: Created: latency-svc-mcjbh
Aug 27 00:56:18.636: INFO: Got endpoints: latency-svc-7b7rj [172.120712ms]
Aug 27 00:56:18.642: INFO: Created: latency-svc-6jtpr
Aug 27 00:56:18.642: INFO: Got endpoints: latency-svc-mcjbh [170.500626ms]
Aug 27 00:56:18.651: INFO: Got endpoints: latency-svc-6jtpr [170.054441ms]
Aug 27 00:56:18.652: INFO: Created: latency-svc-65x8w
Aug 27 00:56:18.665: INFO: Got endpoints: latency-svc-65x8w [176.348019ms]
Aug 27 00:56:18.668: INFO: Created: latency-svc-s54t5
Aug 27 00:56:18.675: INFO: Got endpoints: latency-svc-s54t5 [174.239351ms]
Aug 27 00:56:18.683: INFO: Created: latency-svc-4dsl9
Aug 27 00:56:18.691: INFO: Created: latency-svc-4nf75
Aug 27 00:56:18.693: INFO: Got endpoints: latency-svc-4dsl9 [177.055663ms]
Aug 27 00:56:18.700: INFO: Got endpoints: latency-svc-4nf75 [180.283503ms]
Aug 27 00:56:18.700: INFO: Created: latency-svc-jgbsf
Aug 27 00:56:18.709: INFO: Got endpoints: latency-svc-jgbsf [175.57327ms]
Aug 27 00:56:18.711: INFO: Created: latency-svc-bvtf7
Aug 27 00:56:18.721: INFO: Got endpoints: latency-svc-bvtf7 [164.85653ms]
Aug 27 00:56:18.723: INFO: Created: latency-svc-7mnbd
Aug 27 00:56:18.728: INFO: Created: latency-svc-d7gkr
Aug 27 00:56:18.732: INFO: Got endpoints: latency-svc-7mnbd [164.697367ms]
Aug 27 00:56:18.740: INFO: Created: latency-svc-dm485
Aug 27 00:56:18.740: INFO: Got endpoints: latency-svc-d7gkr [166.959498ms]
Aug 27 00:56:18.748: INFO: Got endpoints: latency-svc-dm485 [156.874501ms]
Aug 27 00:56:18.749: INFO: Created: latency-svc-jxrwv
Aug 27 00:56:18.759: INFO: Got endpoints: latency-svc-jxrwv [147.638371ms]
Aug 27 00:56:18.760: INFO: Created: latency-svc-f7vsz
Aug 27 00:56:18.766: INFO: Got endpoints: latency-svc-f7vsz [142.907255ms]
Aug 27 00:56:18.769: INFO: Created: latency-svc-qkb6j
Aug 27 00:56:18.778: INFO: Created: latency-svc-4v2d4
Aug 27 00:56:18.781: INFO: Got endpoints: latency-svc-qkb6j [157.21698ms]
Aug 27 00:56:18.786: INFO: Got endpoints: latency-svc-4v2d4 [150.539805ms]
Aug 27 00:56:18.789: INFO: Created: latency-svc-4jrtj
Aug 27 00:56:18.799: INFO: Got endpoints: latency-svc-4jrtj [157.053577ms]
Aug 27 00:56:18.802: INFO: Created: latency-svc-mvwr9
Aug 27 00:56:18.804: INFO: Created: latency-svc-5zkkb
Aug 27 00:56:18.813: INFO: Got endpoints: latency-svc-mvwr9 [161.178434ms]
Aug 27 00:56:18.814: INFO: Created: latency-svc-bpgzk
Aug 27 00:56:18.817: INFO: Got endpoints: latency-svc-5zkkb [151.606946ms]
Aug 27 00:56:18.823: INFO: Created: latency-svc-crd7s
Aug 27 00:56:18.825: INFO: Got endpoints: latency-svc-bpgzk [150.141551ms]
Aug 27 00:56:18.831: INFO: Got endpoints: latency-svc-crd7s [138.079301ms]
Aug 27 00:56:18.837: INFO: Created: latency-svc-jvj68
Aug 27 00:56:18.845: INFO: Got endpoints: latency-svc-jvj68 [145.381645ms]
Aug 27 00:56:18.848: INFO: Created: latency-svc-4nnlk
Aug 27 00:56:18.858: INFO: Created: latency-svc-gscqz
Aug 27 00:56:18.868: INFO: Got endpoints: latency-svc-gscqz [147.06768ms]
Aug 27 00:56:18.868: INFO: Got endpoints: latency-svc-4nnlk [159.202069ms]
Aug 27 00:56:18.872: INFO: Created: latency-svc-7jpzj
Aug 27 00:56:18.876: INFO: Created: latency-svc-sf5h8
Aug 27 00:56:18.890: INFO: Got endpoints: latency-svc-sf5h8 [149.62926ms]
Aug 27 00:56:18.890: INFO: Got endpoints: latency-svc-7jpzj [158.161309ms]
Aug 27 00:56:18.891: INFO: Created: latency-svc-lk8zv
Aug 27 00:56:18.902: INFO: Got endpoints: latency-svc-lk8zv [154.191971ms]
Aug 27 00:56:18.908: INFO: Created: latency-svc-c5gpg
Aug 27 00:56:18.915: INFO: Created: latency-svc-hlwd8
Aug 27 00:56:18.917: INFO: Got endpoints: latency-svc-c5gpg [157.217483ms]
Aug 27 00:56:18.923: INFO: Got endpoints: latency-svc-hlwd8 [156.105747ms]
Aug 27 00:56:18.925: INFO: Created: latency-svc-fz4pq
Aug 27 00:56:18.931: INFO: Got endpoints: latency-svc-fz4pq [149.776429ms]
Aug 27 00:56:18.932: INFO: Created: latency-svc-k8d82
Aug 27 00:56:18.940: INFO: Got endpoints: latency-svc-k8d82 [153.146628ms]
Aug 27 00:56:18.941: INFO: Created: latency-svc-74m2r
Aug 27 00:56:18.949: INFO: Got endpoints: latency-svc-74m2r [149.42741ms]
Aug 27 00:56:18.953: INFO: Created: latency-svc-sc7cn
Aug 27 00:56:18.958: INFO: Got endpoints: latency-svc-sc7cn [145.652653ms]
Aug 27 00:56:18.967: INFO: Created: latency-svc-5x4xj
Aug 27 00:56:18.971: INFO: Got endpoints: latency-svc-5x4xj [153.859719ms]
Aug 27 00:56:18.974: INFO: Created: latency-svc-vbp5s
Aug 27 00:56:18.981: INFO: Got endpoints: latency-svc-vbp5s [155.851336ms]
Aug 27 00:56:18.985: INFO: Created: latency-svc-kvkk2
Aug 27 00:56:18.990: INFO: Got endpoints: latency-svc-kvkk2 [159.405236ms]
Aug 27 00:56:18.997: INFO: Created: latency-svc-95f8t
Aug 27 00:56:19.006: INFO: Created: latency-svc-fqtcx
Aug 27 00:56:19.015: INFO: Got endpoints: latency-svc-95f8t [169.418501ms]
Aug 27 00:56:19.017: INFO: Got endpoints: latency-svc-fqtcx [148.738996ms]
Aug 27 00:56:19.019: INFO: Created: latency-svc-m7gb4
Aug 27 00:56:19.027: INFO: Got endpoints: latency-svc-m7gb4 [159.06762ms]
Aug 27 00:56:19.043: INFO: Created: latency-svc-pxjwq
Aug 27 00:56:19.050: INFO: Created: latency-svc-dnfzv
Aug 27 00:56:19.053: INFO: Got endpoints: latency-svc-pxjwq [162.622138ms]
Aug 27 00:56:19.060: INFO: Created: latency-svc-gh26k
Aug 27 00:56:19.060: INFO: Got endpoints: latency-svc-dnfzv [170.326679ms]
Aug 27 00:56:19.066: INFO: Got endpoints: latency-svc-gh26k [163.322505ms]
Aug 27 00:56:19.069: INFO: Created: latency-svc-ddxq2
Aug 27 00:56:19.077: INFO: Got endpoints: latency-svc-ddxq2 [160.41281ms]
Aug 27 00:56:19.079: INFO: Created: latency-svc-tccck
Aug 27 00:56:19.091: INFO: Got endpoints: latency-svc-tccck [168.28665ms]
Aug 27 00:56:19.098: INFO: Created: latency-svc-z4rqv
Aug 27 00:56:19.102: INFO: Created: latency-svc-tvjfj
Aug 27 00:56:19.102: INFO: Got endpoints: latency-svc-z4rqv [171.620331ms]
Aug 27 00:56:19.110: INFO: Got endpoints: latency-svc-tvjfj [170.732145ms]
Aug 27 00:56:19.112: INFO: Created: latency-svc-n5mpq
Aug 27 00:56:19.122: INFO: Got endpoints: latency-svc-n5mpq [172.660304ms]
Aug 27 00:56:19.123: INFO: Created: latency-svc-tgrwp
Aug 27 00:56:19.134: INFO: Created: latency-svc-4bsld
Aug 27 00:56:19.135: INFO: Got endpoints: latency-svc-tgrwp [176.825349ms]
Aug 27 00:56:19.142: INFO: Got endpoints: latency-svc-4bsld [170.478174ms]
Aug 27 00:56:19.144: INFO: Created: latency-svc-h44t7
Aug 27 00:56:19.152: INFO: Got endpoints: latency-svc-h44t7 [170.505208ms]
Aug 27 00:56:19.155: INFO: Created: latency-svc-x8lb7
Aug 27 00:56:19.168: INFO: Created: latency-svc-g4nwk
Aug 27 00:56:19.171: INFO: Got endpoints: latency-svc-x8lb7 [180.111936ms]
Aug 27 00:56:19.179: INFO: Created: latency-svc-zkc48
Aug 27 00:56:19.179: INFO: Got endpoints: latency-svc-g4nwk [164.3063ms]
Aug 27 00:56:19.186: INFO: Created: latency-svc-q2chn
Aug 27 00:56:19.193: INFO: Got endpoints: latency-svc-zkc48 [176.179104ms]
Aug 27 00:56:19.196: INFO: Got endpoints: latency-svc-q2chn [168.974395ms]
Aug 27 00:56:19.200: INFO: Created: latency-svc-zz5jk
Aug 27 00:56:19.206: INFO: Got endpoints: latency-svc-zz5jk [153.375181ms]
Aug 27 00:56:19.209: INFO: Created: latency-svc-47w26
Aug 27 00:56:19.218: INFO: Got endpoints: latency-svc-47w26 [157.636543ms]
Aug 27 00:56:19.219: INFO: Created: latency-svc-d8fnr
Aug 27 00:56:19.234: INFO: Got endpoints: latency-svc-d8fnr [167.721878ms]
Aug 27 00:56:19.236: INFO: Created: latency-svc-q2gbz
Aug 27 00:56:19.244: INFO: Got endpoints: latency-svc-q2gbz [166.633562ms]
Aug 27 00:56:19.251: INFO: Created: latency-svc-dm4jn
Aug 27 00:56:19.257: INFO: Created: latency-svc-9kmgv
Aug 27 00:56:19.259: INFO: Got endpoints: latency-svc-dm4jn [168.379973ms]
Aug 27 00:56:19.265: INFO: Got endpoints: latency-svc-9kmgv [162.638864ms]
Aug 27 00:56:19.266: INFO: Created: latency-svc-zrr8g
Aug 27 00:56:19.274: INFO: Got endpoints: latency-svc-zrr8g [162.98342ms]
Aug 27 00:56:19.275: INFO: Created: latency-svc-n92wd
Aug 27 00:56:19.283: INFO: Got endpoints: latency-svc-n92wd [161.707397ms]
Aug 27 00:56:19.286: INFO: Created: latency-svc-gnc4n
Aug 27 00:56:19.293: INFO: Got endpoints: latency-svc-gnc4n [157.733043ms]
Aug 27 00:56:19.296: INFO: Created: latency-svc-mpzr5
Aug 27 00:56:19.304: INFO: Got endpoints: latency-svc-mpzr5 [162.44841ms]
Aug 27 00:56:19.306: INFO: Created: latency-svc-qx882
Aug 27 00:56:19.314: INFO: Created: latency-svc-94694
Aug 27 00:56:19.315: INFO: Got endpoints: latency-svc-qx882 [163.13326ms]
Aug 27 00:56:19.322: INFO: Created: latency-svc-x9vsb
Aug 27 00:56:19.325: INFO: Got endpoints: latency-svc-94694 [153.929544ms]
Aug 27 00:56:19.329: INFO: Got endpoints: latency-svc-x9vsb [149.501958ms]
Aug 27 00:56:19.333: INFO: Created: latency-svc-p4rcl
Aug 27 00:56:19.343: INFO: Got endpoints: latency-svc-p4rcl [149.717146ms]
Aug 27 00:56:19.344: INFO: Created: latency-svc-mkhzz
Aug 27 00:56:19.349: INFO: Created: latency-svc-2l59d
Aug 27 00:56:19.354: INFO: Got endpoints: latency-svc-mkhzz [158.164656ms]
Aug 27 00:56:19.362: INFO: Got endpoints: latency-svc-2l59d [156.300188ms]
Aug 27 00:56:19.368: INFO: Created: latency-svc-jk9h2
Aug 27 00:56:19.379: INFO: Got endpoints: latency-svc-jk9h2 [160.805395ms]
Aug 27 00:56:19.380: INFO: Created: latency-svc-bllnv
Aug 27 00:56:19.390: INFO: Got endpoints: latency-svc-bllnv [156.595966ms]
Aug 27 00:56:19.391: INFO: Created: latency-svc-5mzr2
Aug 27 00:56:19.402: INFO: Got endpoints: latency-svc-5mzr2 [157.759503ms]
Aug 27 00:56:19.404: INFO: Created: latency-svc-m99wl
Aug 27 00:56:19.411: INFO: Got endpoints: latency-svc-m99wl [151.449677ms]
Aug 27 00:56:19.414: INFO: Created: latency-svc-c4zhh
Aug 27 00:56:19.424: INFO: Got endpoints: latency-svc-c4zhh [158.409549ms]
Aug 27 00:56:19.427: INFO: Created: latency-svc-7lbct
Aug 27 00:56:19.443: INFO: Created: latency-svc-r2b4r
Aug 27 00:56:19.444: INFO: Got endpoints: latency-svc-7lbct [169.908141ms]
Aug 27 00:56:19.447: INFO: Created: latency-svc-vhj66
Aug 27 00:56:19.453: INFO: Got endpoints: latency-svc-vhj66 [160.24804ms]
Aug 27 00:56:19.453: INFO: Got endpoints: latency-svc-r2b4r [169.890703ms]
Aug 27 00:56:19.454: INFO: Created: latency-svc-zdxpf
Aug 27 00:56:19.464: INFO: Got endpoints: latency-svc-zdxpf [159.510582ms]
Aug 27 00:56:19.466: INFO: Created: latency-svc-khrb7
Aug 27 00:56:19.473: INFO: Got endpoints: latency-svc-khrb7 [157.486164ms]
Aug 27 00:56:19.475: INFO: Created: latency-svc-gkb9d
Aug 27 00:56:19.482: INFO: Got endpoints: latency-svc-gkb9d [157.257853ms]
Aug 27 00:56:19.485: INFO: Created: latency-svc-nxz4z
Aug 27 00:56:19.492: INFO: Got endpoints: latency-svc-nxz4z [163.703143ms]
Aug 27 00:56:19.494: INFO: Created: latency-svc-gsjpl
Aug 27 00:56:19.501: INFO: Got endpoints: latency-svc-gsjpl [158.04436ms]
Aug 27 00:56:19.503: INFO: Created: latency-svc-hcdvv
Aug 27 00:56:19.517: INFO: Got endpoints: latency-svc-hcdvv [162.675845ms]
Aug 27 00:56:19.520: INFO: Created: latency-svc-mchzl
Aug 27 00:56:19.528: INFO: Got endpoints: latency-svc-mchzl [165.812441ms]
Aug 27 00:56:19.532: INFO: Created: latency-svc-2gc7g
Aug 27 00:56:19.538: INFO: Created: latency-svc-ttlc7
Aug 27 00:56:19.540: INFO: Got endpoints: latency-svc-2gc7g [161.240562ms]
Aug 27 00:56:19.547: INFO: Got endpoints: latency-svc-ttlc7 [156.55841ms]
Aug 27 00:56:19.559: INFO: Created: latency-svc-ttt8p
Aug 27 00:56:19.567: INFO: Got endpoints: latency-svc-ttt8p [164.962081ms]
Aug 27 00:56:19.569: INFO: Created: latency-svc-7kqtq
Aug 27 00:56:19.578: INFO: Got endpoints: latency-svc-7kqtq [166.915014ms]
Aug 27 00:56:19.583: INFO: Created: latency-svc-hgp56
Aug 27 00:56:19.590: INFO: Created: latency-svc-l4kzv
Aug 27 00:56:19.591: INFO: Got endpoints: latency-svc-hgp56 [167.010981ms]
Aug 27 00:56:19.597: INFO: Got endpoints: latency-svc-l4kzv [153.636032ms]
Aug 27 00:56:19.599: INFO: Created: latency-svc-z4t5v
Aug 27 00:56:19.608: INFO: Got endpoints: latency-svc-z4t5v [154.66697ms]
Aug 27 00:56:19.610: INFO: Created: latency-svc-7vmfg
Aug 27 00:56:19.617: INFO: Created: latency-svc-z2624
Aug 27 00:56:19.620: INFO: Got endpoints: latency-svc-7vmfg [166.309221ms]
Aug 27 00:56:19.623: INFO: Created: latency-svc-v9mwl
Aug 27 00:56:19.627: INFO: Got endpoints: latency-svc-z2624 [163.109789ms]
Aug 27 00:56:19.637: INFO: Got endpoints: latency-svc-v9mwl [163.589133ms]
Aug 27 00:56:19.637: INFO: Created: latency-svc-bghv9
Aug 27 00:56:19.644: INFO: Created: latency-svc-mhtqh
Aug 27 00:56:19.650: INFO: Got endpoints: latency-svc-bghv9 [167.839762ms]
Aug 27 00:56:19.653: INFO: Got endpoints: latency-svc-mhtqh [160.307071ms]
Aug 27 00:56:19.654: INFO: Created: latency-svc-blls7
Aug 27 00:56:19.661: INFO: Got endpoints: latency-svc-blls7 [160.217648ms]
Aug 27 00:56:19.663: INFO: Created: latency-svc-w92bk
Aug 27 00:56:19.672: INFO: Got endpoints: latency-svc-w92bk [154.550156ms]
Aug 27 00:56:19.676: INFO: Created: latency-svc-dbx75
Aug 27 00:56:19.686: INFO: Got endpoints: latency-svc-dbx75 [157.233241ms]
Aug 27 00:56:19.687: INFO: Created: latency-svc-l7sz2
Aug 27 00:56:19.693: INFO: Got endpoints: latency-svc-l7sz2 [153.115597ms]
Aug 27 00:56:19.695: INFO: Created: latency-svc-vgv6h
Aug 27 00:56:19.703: INFO: Got endpoints: latency-svc-vgv6h [156.2708ms]
Aug 27 00:56:19.713: INFO: Created: latency-svc-44np8
Aug 27 00:56:19.721: INFO: Got endpoints: latency-svc-44np8 [154.448928ms]
Aug 27 00:56:19.727: INFO: Created: latency-svc-xb7nk
Aug 27 00:56:19.735: INFO: Created: latency-svc-627ch
Aug 27 00:56:19.740: INFO: Created: latency-svc-crzw8
Aug 27 00:56:19.744: INFO: Got endpoints: latency-svc-627ch [152.818592ms]
Aug 27 00:56:19.744: INFO: Got endpoints: latency-svc-xb7nk [165.685995ms]
Aug 27 00:56:19.748: INFO: Got endpoints: latency-svc-crzw8 [150.459602ms]
Aug 27 00:56:19.750: INFO: Created: latency-svc-g82bb
Aug 27 00:56:19.758: INFO: Created: latency-svc-nzrfr
Aug 27 00:56:19.758: INFO: Got endpoints: latency-svc-g82bb [150.074337ms]
Aug 27 00:56:19.769: INFO: Got endpoints: latency-svc-nzrfr [149.107541ms]
Aug 27 00:56:19.769: INFO: Created: latency-svc-5nn6d
Aug 27 00:56:19.775: INFO: Got endpoints: latency-svc-5nn6d [147.945708ms]
Aug 27 00:56:19.781: INFO: Created: latency-svc-w2h8j
Aug 27 00:56:19.786: INFO: Got endpoints: latency-svc-w2h8j [149.37836ms]
Aug 27 00:56:19.788: INFO: Created: latency-svc-2rlw9
Aug 27 00:56:19.798: INFO: Got endpoints: latency-svc-2rlw9 [147.863884ms]
Aug 27 00:56:19.801: INFO: Created: latency-svc-j2zh8
Aug 27 00:56:19.808: INFO: Got endpoints: latency-svc-j2zh8 [155.200881ms]
Aug 27 00:56:19.814: INFO: Created: latency-svc-zlgtp
Aug 27 00:56:19.818: INFO: Created: latency-svc-qqgl6
Aug 27 00:56:19.824: INFO: Got endpoints: latency-svc-zlgtp [162.707485ms]
Aug 27 00:56:19.828: INFO: Got endpoints: latency-svc-qqgl6 [155.853038ms]
Aug 27 00:56:19.830: INFO: Created: latency-svc-cdwds
Aug 27 00:56:19.839: INFO: Created: latency-svc-sptxt
Aug 27 00:56:19.847: INFO: Got endpoints: latency-svc-cdwds [161.652369ms]
Aug 27 00:56:19.853: INFO: Got endpoints: latency-svc-sptxt [159.38128ms]
Aug 27 00:56:19.853: INFO: Created: latency-svc-kn9vw
Aug 27 00:56:19.860: INFO: Got endpoints: latency-svc-kn9vw [157.062771ms]
Aug 27 00:56:19.861: INFO: Created: latency-svc-rnp94
Aug 27 00:56:19.868: INFO: Got endpoints: latency-svc-rnp94 [146.227421ms]
Aug 27 00:56:19.870: INFO: Created: latency-svc-2dmkz
Aug 27 00:56:19.877: INFO: Got endpoints: latency-svc-2dmkz [133.118938ms]
Aug 27 00:56:19.883: INFO: Created: latency-svc-756jm
Aug 27 00:56:19.888: INFO: Got endpoints: latency-svc-756jm [144.644888ms]
Aug 27 00:56:19.889: INFO: Created: latency-svc-l4gcs
Aug 27 00:56:19.897: INFO: Got endpoints: latency-svc-l4gcs [149.595309ms]
Aug 27 00:56:19.897: INFO: Created: latency-svc-7ql76
Aug 27 00:56:19.905: INFO: Got endpoints: latency-svc-7ql76 [146.708632ms]
Aug 27 00:56:19.907: INFO: Created: latency-svc-2rp79
Aug 27 00:56:19.914: INFO: Got endpoints: latency-svc-2rp79 [145.278489ms]
Aug 27 00:56:19.916: INFO: Created: latency-svc-tr7ts
Aug 27 00:56:19.922: INFO: Got endpoints: latency-svc-tr7ts [147.556364ms]
Aug 27 00:56:19.925: INFO: Created: latency-svc-r6ntm
Aug 27 00:56:19.933: INFO: Got endpoints: latency-svc-r6ntm [146.884655ms]
Aug 27 00:56:19.939: INFO: Created: latency-svc-vpwfb
Aug 27 00:56:19.947: INFO: Got endpoints: latency-svc-vpwfb [149.290326ms]
Aug 27 00:56:19.949: INFO: Created: latency-svc-shjsv
Aug 27 00:56:19.958: INFO: Got endpoints: latency-svc-shjsv [150.156023ms]
Aug 27 00:56:19.963: INFO: Created: latency-svc-lc6zh
Aug 27 00:56:19.968: INFO: Got endpoints: latency-svc-lc6zh [143.746395ms]
Aug 27 00:56:19.970: INFO: Created: latency-svc-jdbf8
Aug 27 00:56:19.977: INFO: Got endpoints: latency-svc-jdbf8 [149.155232ms]
Aug 27 00:56:19.979: INFO: Created: latency-svc-tqbxf
Aug 27 00:56:19.985: INFO: Got endpoints: latency-svc-tqbxf [137.350748ms]
Aug 27 00:56:19.987: INFO: Created: latency-svc-5bbcj
Aug 27 00:56:19.994: INFO: Got endpoints: latency-svc-5bbcj [140.779739ms]
Aug 27 00:56:20.006: INFO: Created: latency-svc-whvr7
Aug 27 00:56:20.017: INFO: Got endpoints: latency-svc-whvr7 [156.322731ms]
Aug 27 00:56:20.021: INFO: Created: latency-svc-ct57b
Aug 27 00:56:20.030: INFO: Got endpoints: latency-svc-ct57b [162.259825ms]
Aug 27 00:56:20.030: INFO: Created: latency-svc-4nmd6
Aug 27 00:56:20.038: INFO: Got endpoints: latency-svc-4nmd6 [161.360722ms]
Aug 27 00:56:20.042: INFO: Created: latency-svc-f5b68
Aug 27 00:56:20.051: INFO: Created: latency-svc-kklcl
Aug 27 00:56:20.057: INFO: Got endpoints: latency-svc-f5b68 [168.942454ms]
Aug 27 00:56:20.061: INFO: Got endpoints: latency-svc-kklcl [163.162539ms]
Aug 27 00:56:20.070: INFO: Created: latency-svc-44jh7
Aug 27 00:56:20.080: INFO: Got endpoints: latency-svc-44jh7 [174.731195ms]
Aug 27 00:56:20.084: INFO: Created: latency-svc-9zv69
Aug 27 00:56:20.093: INFO: Got endpoints: latency-svc-9zv69 [178.396386ms]
Aug 27 00:56:20.096: INFO: Created: latency-svc-zdkk7
Aug 27 00:56:20.102: INFO: Got endpoints: latency-svc-zdkk7 [179.184538ms]
Aug 27 00:56:20.104: INFO: Created: latency-svc-pkwjd
Aug 27 00:56:20.110: INFO: Got endpoints: latency-svc-pkwjd [177.419046ms]
Aug 27 00:56:20.114: INFO: Created: latency-svc-rm4fv
Aug 27 00:56:20.126: INFO: Created: latency-svc-v2t4c
Aug 27 00:56:20.127: INFO: Got endpoints: latency-svc-rm4fv [179.996367ms]
Aug 27 00:56:20.132: INFO: Created: latency-svc-9pdgt
Aug 27 00:56:20.143: INFO: Got endpoints: latency-svc-9pdgt [175.002075ms]
Aug 27 00:56:20.143: INFO: Got endpoints: latency-svc-v2t4c [184.482876ms]
Aug 27 00:56:20.143: INFO: Created: latency-svc-dzdjr
Aug 27 00:56:20.150: INFO: Got endpoints: latency-svc-dzdjr [173.021581ms]
Aug 27 00:56:20.153: INFO: Created: latency-svc-t28q5
Aug 27 00:56:20.161: INFO: Got endpoints: latency-svc-t28q5 [176.267626ms]
Aug 27 00:56:20.164: INFO: Created: latency-svc-dzmwl
Aug 27 00:56:20.171: INFO: Got endpoints: latency-svc-dzmwl [177.578892ms]
Aug 27 00:56:20.173: INFO: Created: latency-svc-q6p6g
Aug 27 00:56:20.180: INFO: Got endpoints: latency-svc-q6p6g [163.43617ms]
Aug 27 00:56:20.181: INFO: Created: latency-svc-hxfj2
Aug 27 00:56:20.189: INFO: Created: latency-svc-9r8j6
Aug 27 00:56:20.189: INFO: Got endpoints: latency-svc-hxfj2 [159.220264ms]
Aug 27 00:56:20.200: INFO: Created: latency-svc-nbzjn
Aug 27 00:56:20.202: INFO: Got endpoints: latency-svc-9r8j6 [163.581575ms]
Aug 27 00:56:20.207: INFO: Got endpoints: latency-svc-nbzjn [149.560144ms]
Aug 27 00:56:20.210: INFO: Created: latency-svc-z9mc6
Aug 27 00:56:20.217: INFO: Got endpoints: latency-svc-z9mc6 [156.684576ms]
Aug 27 00:56:20.220: INFO: Created: latency-svc-jzt6r
Aug 27 00:56:20.227: INFO: Got endpoints: latency-svc-jzt6r [146.889301ms]
Aug 27 00:56:20.231: INFO: Created: latency-svc-vnzvj
Aug 27 00:56:20.237: INFO: Got endpoints: latency-svc-vnzvj [144.494727ms]
Aug 27 00:56:20.244: INFO: Created: latency-svc-vvpk6
Aug 27 00:56:20.253: INFO: Created: latency-svc-9lwxt
Aug 27 00:56:20.254: INFO: Got endpoints: latency-svc-vvpk6 [152.265591ms]
Aug 27 00:56:20.263: INFO: Got endpoints: latency-svc-9lwxt [152.813726ms]
Aug 27 00:56:20.268: INFO: Created: latency-svc-4hmvv
Aug 27 00:56:20.274: INFO: Got endpoints: latency-svc-4hmvv [146.144261ms]
Aug 27 00:56:20.278: INFO: Created: latency-svc-4c7p9
Aug 27 00:56:20.283: INFO: Got endpoints: latency-svc-4c7p9 [140.255792ms]
Aug 27 00:56:20.290: INFO: Created: latency-svc-wclfx
Aug 27 00:56:20.298: INFO: Created: latency-svc-hh4qm
Aug 27 00:56:20.299: INFO: Got endpoints: latency-svc-wclfx [155.832037ms]
Aug 27 00:56:20.305: INFO: Created: latency-svc-48css
Aug 27 00:56:20.312: INFO: Got endpoints: latency-svc-48css [150.979006ms]
Aug 27 00:56:20.312: INFO: Got endpoints: latency-svc-hh4qm [162.042476ms]
Aug 27 00:56:20.317: INFO: Created: latency-svc-nhlrj
Aug 27 00:56:20.325: INFO: Got endpoints: latency-svc-nhlrj [153.744299ms]
Aug 27 00:56:20.328: INFO: Created: latency-svc-jbzcr
Aug 27 00:56:20.336: INFO: Got endpoints: latency-svc-jbzcr [155.990775ms]
Aug 27 00:56:20.337: INFO: Created: latency-svc-bc6z6
Aug 27 00:56:20.345: INFO: Got endpoints: latency-svc-bc6z6 [155.278807ms]
Aug 27 00:56:20.345: INFO: Created: latency-svc-9rjf7
Aug 27 00:56:20.350: INFO: Got endpoints: latency-svc-9rjf7 [147.727517ms]
Aug 27 00:56:20.352: INFO: Latencies: [42.142009ms 44.252547ms 52.278865ms 57.448803ms 64.806471ms 76.132848ms 87.231998ms 124.318134ms 124.374783ms 128.725952ms 133.118938ms 137.350748ms 138.079301ms 140.255792ms 140.779739ms 142.907255ms 143.746395ms 144.494727ms 144.644888ms 145.278489ms 145.381645ms 145.652653ms 146.144261ms 146.227421ms 146.708632ms 146.884655ms 146.889301ms 147.06768ms 147.556364ms 147.638371ms 147.727517ms 147.863884ms 147.945708ms 148.738996ms 149.107541ms 149.155232ms 149.290326ms 149.37836ms 149.42741ms 149.501958ms 149.560144ms 149.595309ms 149.62926ms 149.717146ms 149.776429ms 150.074337ms 150.141551ms 150.156023ms 150.459602ms 150.539805ms 150.979006ms 151.449677ms 151.606946ms 152.265591ms 152.813726ms 152.818592ms 153.115597ms 153.146628ms 153.223974ms 153.375181ms 153.636032ms 153.744299ms 153.859719ms 153.929544ms 154.191971ms 154.448928ms 154.550156ms 154.66697ms 154.740459ms 155.200881ms 155.278807ms 155.832037ms 155.851336ms 155.853038ms 155.990775ms 156.105747ms 156.2708ms 156.300188ms 156.322731ms 156.55841ms 156.595966ms 156.684576ms 156.874501ms 157.053577ms 157.062771ms 157.21698ms 157.217483ms 157.233241ms 157.257853ms 157.486164ms 157.636543ms 157.733043ms 157.759503ms 158.04436ms 158.122727ms 158.161309ms 158.164656ms 158.409549ms 158.537298ms 158.692168ms 159.06762ms 159.202069ms 159.220264ms 159.38128ms 159.405236ms 159.510582ms 159.776715ms 160.217648ms 160.24804ms 160.307071ms 160.41281ms 160.805395ms 161.178434ms 161.240562ms 161.360722ms 161.652369ms 161.707397ms 162.042476ms 162.259825ms 162.44841ms 162.622138ms 162.638864ms 162.675845ms 162.707485ms 162.98342ms 163.109789ms 163.13326ms 163.162539ms 163.322505ms 163.43617ms 163.581575ms 163.589133ms 163.703143ms 164.3063ms 164.697367ms 164.85653ms 164.962081ms 165.685995ms 165.812441ms 166.149747ms 166.309221ms 166.341226ms 166.633562ms 166.915014ms 166.959498ms 167.010981ms 167.721878ms 167.839762ms 168.28665ms 168.379973ms 168.65154ms 168.942454ms 168.974395ms 169.107185ms 169.196777ms 169.418501ms 169.890703ms 169.908141ms 170.054441ms 170.326679ms 170.478174ms 170.500626ms 170.505208ms 170.732145ms 171.417594ms 171.620331ms 172.101919ms 172.117631ms 172.120712ms 172.288721ms 172.660304ms 173.021581ms 173.331085ms 174.239351ms 174.731195ms 174.823986ms 175.002075ms 175.57327ms 176.179104ms 176.267626ms 176.348019ms 176.825349ms 177.055663ms 177.419046ms 177.578892ms 177.833109ms 178.352861ms 178.396386ms 179.184538ms 179.996367ms 180.111936ms 180.283503ms 181.136137ms 181.930638ms 184.482876ms 186.22969ms 187.165768ms 187.191084ms 195.854547ms 207.685839ms]
Aug 27 00:56:20.352: INFO: 50 %ile: 159.06762ms
Aug 27 00:56:20.352: INFO: 90 %ile: 176.348019ms
Aug 27 00:56:20.352: INFO: 99 %ile: 195.854547ms
Aug 27 00:56:20.352: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:56:20.352: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "svc-latency-6337" for this suite.
Aug 27 00:56:40.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:56:42.638: INFO: namespace svc-latency-6337 deletion completed in 22.239255459s

â€¢ [SLOW TEST:27.802 seconds]
[sig-network] Service endpoints latency
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:56:42.638: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4869/configmap-test-89078784-c865-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:56:42.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583" in namespace "configmap-4869" to be "success or failure"
Aug 27 00:56:42.844: INFO: Pod "pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.705364ms
Aug 27 00:56:44.861: INFO: Pod "pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033925001s
Aug 27 00:56:46.878: INFO: Pod "pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051153251s
STEP: Saw pod success
Aug 27 00:56:46.878: INFO: Pod "pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:56:46.895: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583 container env-test: <nil>
STEP: delete the pod
Aug 27 00:56:46.942: INFO: Waiting for pod pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583 to disappear
Aug 27 00:56:46.959: INFO: Pod pod-configmaps-890aa383-c865-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:56:46.959: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "configmap-4869" for this suite.
Aug 27 00:56:53.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:56:55.234: INFO: namespace configmap-4869 deletion completed in 8.229308389s

â€¢ [SLOW TEST:12.596 seconds]
[sig-node] ConfigMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:56:55.234: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:56:55.364: INFO: Creating ReplicaSet my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583
Aug 27 00:56:55.406: INFO: Pod name my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583: Found 1 pods out of 1
Aug 27 00:56:55.406: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583" is running
Aug 27 00:57:01.443: INFO: Pod "my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583-zq6rm" is running (conditions: [])
Aug 27 00:57:01.443: INFO: Trying to dial the pod
Aug 27 00:57:06.652: INFO: Controller my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583: Got expected result from replica 1 [my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583-zq6rm]: "my-hostname-basic-9088d028-c865-11e9-974e-0a58ac107583-zq6rm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:57:06.652: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "replicaset-5938" for this suite.
Aug 27 00:57:12.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:57:15.050: INFO: namespace replicaset-5938 deletion completed in 8.350205629s

â€¢ [SLOW TEST:19.817 seconds]
[sig-apps] ReplicaSet
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:57:15.051: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:57:15.166: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-5851'
Aug 27 00:57:15.825: INFO: stderr: ""
Aug 27 00:57:15.825: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 27 00:57:15.825: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig create -f - --namespace=kubectl-5851'
Aug 27 00:57:16.361: INFO: stderr: ""
Aug 27 00:57:16.361: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 27 00:57:17.378: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:57:17.378: INFO: Found 0 / 1
Aug 27 00:57:18.379: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:57:18.379: INFO: Found 1 / 1
Aug 27 00:57:18.379: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 27 00:57:18.396: INFO: Selector matched 1 pods for map[app:redis]
Aug 27 00:57:18.397: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 27 00:57:18.397: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe pod redis-master-92fx6 --namespace=kubectl-5851'
Aug 27 00:57:18.607: INFO: stderr: ""
Aug 27 00:57:18.607: INFO: stdout: "Name:               redis-master-92fx6\nNamespace:          kubectl-5851\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-10-0-129-239.ec2.internal/10.0.129.239\nStart Time:         Tue, 27 Aug 2019 00:57:15 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        openshift.io/scc: privileged\nStatus:             Running\nIP:                 10.131.0.137\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://78804a16d7c490b9c38c683b2f87e5d4850d91063530c34498d350aaa35440ba\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 27 Aug 2019 00:57:18 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ljcx2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ljcx2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ljcx2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                   Message\n  ----    ------     ----  ----                                   -------\n  Normal  Scheduled  3s    default-scheduler                      Successfully assigned kubectl-5851/redis-master-92fx6 to ip-10-0-129-239.ec2.internal\n  Normal  Pulled     1s    kubelet, ip-10-0-129-239.ec2.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    0s    kubelet, ip-10-0-129-239.ec2.internal  Created container redis-master\n  Normal  Started    0s    kubelet, ip-10-0-129-239.ec2.internal  Started container redis-master\n"
Aug 27 00:57:18.607: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe rc redis-master --namespace=kubectl-5851'
Aug 27 00:57:18.846: INFO: stderr: ""
Aug 27 00:57:18.846: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5851\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-92fx6\n"
Aug 27 00:57:18.846: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe service redis-master --namespace=kubectl-5851'
Aug 27 00:57:19.085: INFO: stderr: ""
Aug 27 00:57:19.085: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5851\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.30.156.70\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.131.0.137:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 27 00:57:19.131: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe node ip-10-0-129-239.ec2.internal'
Aug 27 00:57:19.425: INFO: stderr: ""
Aug 27 00:57:19.425: INFO: stdout: "Name:               ip-10-0-129-239.ec2.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m4.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-129-239\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.openshift.io/os_id=rhcos\nAnnotations:        machine.openshift.io/machine: openshift-machine-api/ci-op-dpgbhtjj-1ac2a-7h8pv-worker-us-east-1a-qfwzx\n                    machineconfiguration.openshift.io/currentConfig: rendered-worker-92ec066795c6e93198884c2d6dee3876\n                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-92ec066795c6e93198884c2d6dee3876\n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 26 Aug 2019 23:00:40 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 27 Aug 2019 00:56:45 +0000   Mon, 26 Aug 2019 23:00:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 27 Aug 2019 00:56:45 +0000   Mon, 26 Aug 2019 23:00:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 27 Aug 2019 00:56:45 +0000   Mon, 26 Aug 2019 23:00:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 27 Aug 2019 00:56:45 +0000   Mon, 26 Aug 2019 23:01:41 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.129.239\n  InternalDNS:  ip-10-0-129-239.ec2.internal\n  Hostname:     ip-10-0-129-239.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         4\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      16420624Ki\n pods:                        250\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         3500m\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      15806224Ki\n pods:                        250\nSystem Info:\n Machine ID:                              36996e1937b4497fa1e709e4c8814481\n System UUID:                             ec26c04e-289f-40d8-76ba-d4aaaae0ada3\n Boot ID:                                 7384200a-8cbe-4269-8cbd-a51bfb83d2b0\n Kernel Version:                          4.18.0-80.7.2.el8_0.x86_64\n OS Image:                                Red Hat Enterprise Linux CoreOS 42.80.20190826.1 (Ootpa)\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.14.10-0.8.dev.rhaos4.2.gitaf00350.el8\n Kubelet Version:                         v1.14.0+204cc07\n Kube-Proxy Version:                      v1.14.0+204cc07\nProviderID:                               aws:///us-east-1a/i-02f982c861e1a2745\nNon-terminated Pods:                      (17 in total)\n  Namespace                               Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                        ------------  ----------  ---------------  -------------  ---\n  kubectl-5851                            redis-master-92fx6                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator  tuned-p49z9                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         116m\n  openshift-dns                           dns-default-fnqls                           110m (3%)     0 (0%)      70Mi (0%)        512Mi (3%)     116m\n  openshift-image-registry                image-registry-78bd5f6f67-49xzm             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         116m\n  openshift-image-registry                node-ca-jsqtx                               10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         115m\n  openshift-ingress                       router-default-7879f59486-qp8d2             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         116m\n  openshift-machine-config-operator       machine-config-daemon-qtvl9                 20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-marketplace                   certified-operators-575dfbb7d8-frm4v        0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  openshift-marketplace                   community-operators-848c796fb7-94fds        0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  openshift-marketplace                   redhat-operators-67f45f9754-t96bh           0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  openshift-monitoring                    alertmanager-main-2                         100m (2%)     100m (2%)   225Mi (1%)       25Mi (0%)      114m\n  openshift-monitoring                    kube-state-metrics-5f57cc8b6f-p56kl         30m (0%)      0 (0%)      120Mi (0%)       0 (0%)         116m\n  openshift-monitoring                    node-exporter-nrtz2                         10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         116m\n  openshift-monitoring                    openshift-state-metrics-6f45d6c785-jnjkl    120m (3%)     0 (0%)      190Mi (1%)       0 (0%)         116m\n  openshift-multus                        multus-mj66c                                10m (0%)      0 (0%)      150Mi (0%)       0 (0%)         116m\n  openshift-sdn                           ovs-qnv4t                                   200m (5%)     0 (0%)      400Mi (2%)       0 (0%)         116m\n  openshift-sdn                           sdn-xnf2w                                   100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         116m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         920m (26%)    100m (2%)\n  memory                      1997Mi (12%)  537Mi (3%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type    Reason                   Age                  From                                   Message\n  ----    ------                   ----                 ----                                   -------\n  Normal  NodeHasSufficientMemory  116m (x8 over 116m)  kubelet, ip-10-0-129-239.ec2.internal  Node ip-10-0-129-239.ec2.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    116m (x8 over 116m)  kubelet, ip-10-0-129-239.ec2.internal  Node ip-10-0-129-239.ec2.internal status is now: NodeHasNoDiskPressure\n"
Aug 27 00:57:19.425: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig describe namespace kubectl-5851'
Aug 27 00:57:19.632: INFO: stderr: ""
Aug 27 00:57:19.632: INFO: stdout: "Name:         kubectl-5851\nLabels:       e2e-framework=kubectl\n              e2e-run=5ce0c26b-c857-11e9-974e-0a58ac107583\nAnnotations:  openshift.io/sa.scc.mcs: s0:c50,c25\n              openshift.io/sa.scc.supplemental-groups: 1002500000/10000\n              openshift.io/sa.scc.uid-range: 1002500000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:57:19.632: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-5851" for this suite.
Aug 27 00:57:43.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:57:45.885: INFO: namespace kubectl-5851 deletion completed in 26.20600316s

â€¢ [SLOW TEST:30.834 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:57:45.885: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-aeb5ec6d-c865-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:57:46.041: INFO: Waiting up to 5m0s for pod "pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583" in namespace "secrets-1269" to be "success or failure"
Aug 27 00:57:46.059: INFO: Pod "pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.248705ms
Aug 27 00:57:48.077: INFO: Pod "pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035285139s
Aug 27 00:57:50.094: INFO: Pod "pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052739964s
STEP: Saw pod success
Aug 27 00:57:50.094: INFO: Pod "pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:57:50.112: INFO: Trying to get logs from node ip-10-0-155-122.ec2.internal pod pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:57:50.166: INFO: Waiting for pod pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583 to disappear
Aug 27 00:57:50.182: INFO: Pod pod-secrets-aeb92d94-c865-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:57:50.182: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-1269" for this suite.
Aug 27 00:57:56.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:57:58.433: INFO: namespace secrets-1269 deletion completed in 8.206261478s

â€¢ [SLOW TEST:12.549 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:57:58.434: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 00:57:58.558: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:58:02.783: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "pods-3777" for this suite.
Aug 27 00:58:42.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:58:45.248: INFO: namespace pods-3777 deletion completed in 42.419111198s

â€¢ [SLOW TEST:46.814 seconds]
[k8s.io] Pods
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:58:45.249: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 27 00:58:45.451: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4290'
Aug 27 00:58:45.734: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 27 00:58:45.734: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Aug 27 00:58:45.760: INFO: Running '/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/bin/linux/amd64/kubectl --kubeconfig=/tmp/admin.kubeconfig delete jobs e2e-test-nginx-job --namespace=kubectl-4290'
Aug 27 00:58:46.030: INFO: stderr: ""
Aug 27 00:58:46.030: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:58:46.030: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "kubectl-4290" for this suite.
Aug 27 00:59:10.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:59:12.295: INFO: namespace kubectl-4290 deletion completed in 26.217430577s

â€¢ [SLOW TEST:27.046 seconds]
[sig-cli] Kubectl client
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:59:12.295: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-e23a6f84-c865-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume secrets
Aug 27 00:59:12.475: INFO: Waiting up to 5m0s for pod "pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583" in namespace "secrets-3898" to be "success or failure"
Aug 27 00:59:12.492: INFO: Pod "pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 16.770899ms
Aug 27 00:59:14.510: INFO: Pod "pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035093098s
Aug 27 00:59:16.528: INFO: Pod "pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05283158s
Aug 27 00:59:18.545: INFO: Pod "pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070294606s
STEP: Saw pod success
Aug 27 00:59:18.545: INFO: Pod "pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:59:18.562: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583 container secret-volume-test: <nil>
STEP: delete the pod
Aug 27 00:59:18.608: INFO: Waiting for pod pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583 to disappear
Aug 27 00:59:18.624: INFO: Pod pod-secrets-e23dcea7-c865-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:59:18.624: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "secrets-3898" for this suite.
Aug 27 00:59:24.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:59:26.876: INFO: namespace secrets-3898 deletion completed in 8.207064298s

â€¢ [SLOW TEST:14.581 seconds]
[sig-storage] Secrets
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:59:26.876: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-eaed3feb-c865-11e9-974e-0a58ac107583
STEP: Creating a pod to test consume configMaps
Aug 27 00:59:27.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583" in namespace "projected-6740" to be "success or failure"
Aug 27 00:59:27.087: INFO: Pod "pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 17.057081ms
Aug 27 00:59:29.105: INFO: Pod "pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03472239s
Aug 27 00:59:31.123: INFO: Pod "pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05272519s
STEP: Saw pod success
Aug 27 00:59:31.123: INFO: Pod "pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583" satisfied condition "success or failure"
Aug 27 00:59:31.140: INFO: Trying to get logs from node ip-10-0-139-255.ec2.internal pod pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 27 00:59:31.187: INFO: Waiting for pod pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583 to disappear
Aug 27 00:59:31.206: INFO: Pod pod-projected-configmaps-eaf03a5e-c865-11e9-974e-0a58ac107583 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:59:31.206: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6740" for this suite.
Aug 27 00:59:37.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 00:59:39.478: INFO: namespace projected-6740 deletion completed in 8.2271555s

â€¢ [SLOW TEST:12.602 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 00:59:39.479: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 27 00:59:44.295: INFO: Successfully updated pod "annotationupdatef26df3b5-c865-11e9-974e-0a58ac107583"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 00:59:46.337: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-6791" for this suite.
Aug 27 01:00:10.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 01:00:12.623: INFO: namespace projected-6791 deletion completed in 26.240826314s

â€¢ [SLOW TEST:33.145 seconds]
[sig-storage] Projected downwardAPI
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 27 01:00:12.623: INFO: >>> kubeConfig: /tmp/admin.kubeconfig
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 27 01:00:12.792: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-0635e6c9-c866-11e9-974e-0a58ac107583
STEP: Creating configMap with name cm-test-opt-upd-0635e73b-c866-11e9-974e-0a58ac107583
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0635e6c9-c866-11e9-974e-0a58ac107583
STEP: Updating configmap cm-test-opt-upd-0635e73b-c866-11e9-974e-0a58ac107583
STEP: Creating configMap with name cm-test-opt-create-0635e774-c866-11e9-974e-0a58ac107583
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 27 01:01:27.836: INFO: Waiting up to 3m0s for all (but 3) nodes to be ready
STEP: Destroying namespace "projected-2133" for this suite.
Aug 27 01:01:51.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 27 01:01:54.079: INFO: namespace projected-2133 deletion completed in 26.210406117s

â€¢ [SLOW TEST:101.456 seconds]
[sig-storage] Projected configMap
/go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/github.com/openshift/origin/_output/components/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSAug 27 01:01:54.079: INFO: Running AfterSuite actions on all nodes
Aug 27 01:01:54.079: INFO: Running AfterSuite actions on node 1
Aug 27 01:01:54.079: INFO: Dumping logs locally to: /tmp/artifacts
Aug 27 01:01:54.080: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec ../../cluster/log-dump/log-dump.sh: no such file or directory

Ran 204 of 3586 Specs in 6397.257 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS
