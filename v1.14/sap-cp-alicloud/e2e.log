Conformance test: not doing test setup.
I1203 14:48:29.443069    5087 e2e.go:242] Starting e2e run "f7878873-15db-11ea-9142-f2314dba9465" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575384508 - Will randomize all specs
Will run 204 of 3586 specs

Dec  3 14:48:55.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Deleting namespaces
I1203 14:48:55.652723    5087 e2e.go:97] Waiting for deletion of the following namespaces: []
STEP: Waiting for namespaces to vanish
Dec  3 14:48:57.656: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  3 14:48:57.669: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  3 14:48:57.712: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  3 14:48:57.712: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Dec  3 14:48:57.712: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  3 14:48:57.723: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  3 14:48:57.723: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-disk-plugin-alicloud' (0 seconds elapsed)
Dec  3 14:48:57.723: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  3 14:48:57.723: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec  3 14:48:57.723: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Dec  3 14:48:57.723: INFO: e2e test version: v1.14.9
Dec  3 14:48:57.726: INFO: kube-apiserver version: v1.14.9
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:48:57.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
Dec  3 14:48:57.765: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec  3 14:48:57.782: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:48:57.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465" in namespace "projected-2467" to be "success or failure"
Dec  3 14:48:57.908: INFO: Pod "downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.826582ms
Dec  3 14:48:59.912: INFO: Pod "downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008373486s
Dec  3 14:49:01.917: INFO: Pod "downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013349975s
STEP: Saw pod success
Dec  3 14:49:01.917: INFO: Pod "downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 14:49:01.921: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 14:49:02.080: INFO: Waiting for pod downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465 to disappear
Dec  3 14:49:02.084: INFO: Pod downwardapi-volume-092a1deb-15dc-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:49:02.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2467" for this suite.
Dec  3 14:49:08.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:08.237: INFO: namespace projected-2467 deletion completed in 6.147732776s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:49:08.238: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 14:49:08.393: INFO: Waiting up to 5m0s for pod "pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465" in namespace "emptydir-6181" to be "success or failure"
Dec  3 14:49:08.397: INFO: Pod "pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73012ms
Dec  3 14:49:10.402: INFO: Pod "pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008445409s
STEP: Saw pod success
Dec  3 14:49:10.402: INFO: Pod "pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 14:49:10.406: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 14:49:10.425: INFO: Waiting for pod pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465 to disappear
Dec  3 14:49:10.428: INFO: Pod pod-0f6ad5cf-15dc-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:49:10.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6181" for this suite.
Dec  3 14:49:16.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:16.576: INFO: namespace emptydir-6181 deletion completed in 6.142357989s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:49:16.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:49:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1367" for this suite.
Dec  3 14:49:47.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:48.092: INFO: namespace container-runtime-1367 deletion completed in 6.147863747s
•SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:49:48.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6917
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Dec  3 14:49:48.249: INFO: Found 0 stateful pods, waiting for 3
Dec  3 14:49:58.254: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:49:58.254: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:49:58.254: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 14:50:08.254: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:50:08.254: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:50:08.254: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:50:08.266: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6917 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 14:50:08.811: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 14:50:08.812: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 14:50:08.812: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  3 14:50:18.848: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  3 14:50:28.869: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6917 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:50:29.353: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 14:50:29.353: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 14:50:29.353: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 14:50:39.384: INFO: Waiting for StatefulSet statefulset-6917/ss2 to complete update
Dec  3 14:50:39.384: INFO: Waiting for Pod statefulset-6917/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 14:50:39.384: INFO: Waiting for Pod statefulset-6917/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 14:50:49.393: INFO: Waiting for StatefulSet statefulset-6917/ss2 to complete update
Dec  3 14:50:49.393: INFO: Waiting for Pod statefulset-6917/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 14:50:59.393: INFO: Waiting for StatefulSet statefulset-6917/ss2 to complete update
Dec  3 14:50:59.393: INFO: Waiting for Pod statefulset-6917/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec  3 14:51:09.393: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6917 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 14:51:09.903: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 14:51:09.903: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 14:51:09.903: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 14:51:19.938: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  3 14:51:29.959: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6917 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:51:30.507: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 14:51:30.507: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 14:51:30.507: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 14:51:40.532: INFO: Waiting for StatefulSet statefulset-6917/ss2 to complete update
Dec  3 14:51:40.532: INFO: Waiting for Pod statefulset-6917/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 14:51:40.532: INFO: Waiting for Pod statefulset-6917/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 14:51:50.541: INFO: Waiting for StatefulSet statefulset-6917/ss2 to complete update
Dec  3 14:51:50.541: INFO: Waiting for Pod statefulset-6917/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 14:51:50.541: INFO: Waiting for Pod statefulset-6917/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 14:52:00.541: INFO: Waiting for StatefulSet statefulset-6917/ss2 to complete update
Dec  3 14:52:00.541: INFO: Waiting for Pod statefulset-6917/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  3 14:52:10.541: INFO: Deleting all statefulset in ns statefulset-6917
Dec  3 14:52:10.545: INFO: Scaling statefulset ss2 to 0
Dec  3 14:52:40.562: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:52:40.566: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:52:40.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6917" for this suite.
Dec  3 14:52:46.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:46.722: INFO: namespace statefulset-6917 deletion completed in 6.138449685s
•SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:52:46.723: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6895
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 14:52:46.866: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 14:53:06.948: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6895 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:53:06.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:53:07.447: INFO: Found all expected endpoints: [netserver-0]
Dec  3 14:53:07.451: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.15:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6895 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:53:07.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:53:07.835: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:53:07.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6895" for this suite.
Dec  3 14:53:29.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:30.043: INFO: namespace pod-network-test-6895 deletion completed in 22.201431069s
•S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:53:30.044: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  3 14:53:30.201: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  3 14:53:35.206: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:53:35.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7780" for this suite.
Dec  3 14:53:41.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:41.375: INFO: namespace replication-controller-7780 deletion completed in 6.148008878s
•SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:53:41.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9135
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-9135
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9135
Dec  3 14:53:41.531: INFO: Found 0 stateful pods, waiting for 1
Dec  3 14:53:51.536: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  3 14:53:51.540: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 14:53:52.097: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 14:53:52.097: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 14:53:52.097: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 14:53:52.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 14:54:02.107: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:54:02.107: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:54:02.126: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:02.126: INFO: ss-0  izgw8jactz8ahkwwzt6d3gz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:41 +0000 UTC  }]
Dec  3 14:54:02.126: INFO: 
Dec  3 14:54:02.126: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  3 14:54:03.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995695685s
Dec  3 14:54:04.136: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990568156s
Dec  3 14:54:05.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985570099s
Dec  3 14:54:06.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980341597s
Dec  3 14:54:07.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975207838s
Dec  3 14:54:08.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9700128s
Dec  3 14:54:09.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964651303s
Dec  3 14:54:10.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959850561s
Dec  3 14:54:11.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.747553ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9135
Dec  3 14:54:12.177: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:54:12.773: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 14:54:12.773: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 14:54:12.773: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 14:54:12.773: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:54:13.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 14:54:13.383: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 14:54:13.383: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 14:54:13.384: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:54:13.915: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 14:54:13.915: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 14:54:13.915: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 14:54:13.920: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:54:13.920: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:54:13.920: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  3 14:54:13.924: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 14:54:14.481: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 14:54:14.481: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 14:54:14.481: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 14:54:14.481: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 14:54:15.131: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 14:54:15.131: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 14:54:15.131: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 14:54:15.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 14:54:15.777: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 14:54:15.777: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 14:54:15.777: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 14:54:15.777: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:54:15.782: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 14:54:25.792: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:54:25.792: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:54:25.792: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:54:25.804: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:25.804: INFO: ss-0  izgw8jactz8ahkwwzt6d3gz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:41 +0000 UTC  }]
Dec  3 14:54:25.804: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:25.804: INFO: ss-2  izgw8jactz8ahkwwzt6d3gz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:25.804: INFO: 
Dec  3 14:54:25.804: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 14:54:26.808: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:26.808: INFO: ss-0  izgw8jactz8ahkwwzt6d3gz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:41 +0000 UTC  }]
Dec  3 14:54:26.808: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:26.808: INFO: ss-2  izgw8jactz8ahkwwzt6d3gz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:26.808: INFO: 
Dec  3 14:54:26.808: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 14:54:27.814: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:27.814: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:27.814: INFO: 
Dec  3 14:54:27.814: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:28.819: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:28.819: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:28.819: INFO: 
Dec  3 14:54:28.819: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:29.823: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:29.824: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:29.824: INFO: 
Dec  3 14:54:29.824: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:30.828: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:30.828: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:30.828: INFO: 
Dec  3 14:54:30.828: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:31.833: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:31.833: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:31.833: INFO: 
Dec  3 14:54:31.833: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:32.838: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:32.838: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:32.838: INFO: 
Dec  3 14:54:32.838: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:33.843: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:33.843: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:33.843: INFO: 
Dec  3 14:54:33.843: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 14:54:34.848: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
Dec  3 14:54:34.848: INFO: ss-1  izgw8afzp8040eosj1udh4z  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:54:02 +0000 UTC  }]
Dec  3 14:54:34.848: INFO: 
Dec  3 14:54:34.848: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9135
Dec  3 14:54:35.853: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:54:36.252: INFO: rc: 1
Dec  3 14:54:36.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002480120 exit status 1 <nil> <nil> true [0xc000f2a728 0xc000f2a740 0xc000f2a758] [0xc000f2a728 0xc000f2a740 0xc000f2a758] [0xc000f2a738 0xc000f2a750] [0xb91810 0xb91810] 0xc00190d7a0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Dec  3 14:54:46.253: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:54:46.387: INFO: rc: 1
Dec  3 14:54:46.388: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0024806f0 exit status 1 <nil> <nil> true [0xc000f2a760 0xc000f2a778 0xc000f2a790] [0xc000f2a760 0xc000f2a778 0xc000f2a790] [0xc000f2a770 0xc000f2a788] [0xb91810 0xb91810] 0xc00190daa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:54:56.388: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:54:56.512: INFO: rc: 1
Dec  3 14:54:56.512: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002480cc0 exit status 1 <nil> <nil> true [0xc000f2a798 0xc000f2a7b0 0xc000f2a7c8] [0xc000f2a798 0xc000f2a7b0 0xc000f2a7c8] [0xc000f2a7a8 0xc000f2a7c0] [0xb91810 0xb91810] 0xc00190dda0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:55:06.512: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:55:06.672: INFO: rc: 1
Dec  3 14:55:06.672: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0025864b0 exit status 1 <nil> <nil> true [0xc0026c0838 0xc0026c0850 0xc0026c0868] [0xc0026c0838 0xc0026c0850 0xc0026c0868] [0xc0026c0848 0xc0026c0860] [0xb91810 0xb91810] 0xc001827d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:55:16.672: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:55:16.795: INFO: rc: 1
Dec  3 14:55:16.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db2600 exit status 1 <nil> <nil> true [0xc0018e40a8 0xc0018e4140 0xc0018e41b0] [0xc0018e40a8 0xc0018e4140 0xc0018e41b0] [0xc0018e4110 0xc0018e4190] [0xb91810 0xb91810] 0xc000d2c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:55:26.796: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:55:26.959: INFO: rc: 1
Dec  3 14:55:26.960: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db2c60 exit status 1 <nil> <nil> true [0xc0018e41c8 0xc0018e42c8 0xc0018e4338] [0xc0018e41c8 0xc0018e42c8 0xc0018e4338] [0xc0018e4290 0xc0018e4310] [0xb91810 0xb91810] 0xc000d2c6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:55:36.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:55:37.099: INFO: rc: 1
Dec  3 14:55:37.099: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00140a5d0 exit status 1 <nil> <nil> true [0xc000551168 0xc000551518 0xc000551570] [0xc000551168 0xc000551518 0xc000551570] [0xc000551408 0xc000551550] [0xb91810 0xb91810] 0xc0012cc5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:55:47.100: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:55:47.221: INFO: rc: 1
Dec  3 14:55:47.222: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001e3a720 exit status 1 <nil> <nil> true [0xc000178000 0xc0001788a8 0xc000178ff8] [0xc000178000 0xc0001788a8 0xc000178ff8] [0xc000178720 0xc000178e80] [0xb91810 0xb91810] 0xc001f6e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:55:57.222: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:55:57.295: INFO: rc: 1
Dec  3 14:55:57.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db3290 exit status 1 <nil> <nil> true [0xc0018e4388 0xc0018e4408 0xc0018e44d0] [0xc0018e4388 0xc0018e4408 0xc0018e44d0] [0xc0018e43c8 0xc0018e44a0] [0xb91810 0xb91810] 0xc000d2c9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:56:07.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:56:07.440: INFO: rc: 1
Dec  3 14:56:07.440: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00140ac00 exit status 1 <nil> <nil> true [0xc0005515b0 0xc0005517f0 0xc0005519f8] [0xc0005515b0 0xc0005517f0 0xc0005519f8] [0xc000551620 0xc0005518b8] [0xb91810 0xb91810] 0xc0012ccae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:56:17.440: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:56:17.510: INFO: rc: 1
Dec  3 14:56:17.510: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db3890 exit status 1 <nil> <nil> true [0xc0018e4530 0xc0018e45d0 0xc0018e4618] [0xc0018e4530 0xc0018e45d0 0xc0018e4618] [0xc0018e45a8 0xc0018e4610] [0xb91810 0xb91810] 0xc000d2d020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:56:27.510: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:56:27.683: INFO: rc: 1
Dec  3 14:56:27.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0008c2060 exit status 1 <nil> <nil> true [0xc000010068 0xc0000105b0 0xc0000107a0] [0xc000010068 0xc0000105b0 0xc0000107a0] [0xc000010468 0xc0000106b0] [0xb91810 0xb91810] 0xc002b9ca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:56:37.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:56:37.812: INFO: rc: 1
Dec  3 14:56:37.812: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0008c2660 exit status 1 <nil> <nil> true [0xc000010878 0xc000010cb0 0xc000010e10] [0xc000010878 0xc000010cb0 0xc000010e10] [0xc000010c80 0xc000010da8] [0xb91810 0xb91810] 0xc002b9cd80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:56:47.812: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:56:47.928: INFO: rc: 1
Dec  3 14:56:47.928: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001e3adb0 exit status 1 <nil> <nil> true [0xc000179258 0xc0001798a0 0xc000179f80] [0xc000179258 0xc0001798a0 0xc000179f80] [0xc000179528 0xc000179d80] [0xb91810 0xb91810] 0xc001f6e960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:56:57.928: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:56:57.999: INFO: rc: 1
Dec  3 14:56:57.999: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001e3b3b0 exit status 1 <nil> <nil> true [0xc000179f90 0xc00050e108 0xc00050e218] [0xc000179f90 0xc00050e108 0xc00050e218] [0xc00050e0d0 0xc00050e1b8] [0xb91810 0xb91810] 0xc001f6ef60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:57:07.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:57:08.119: INFO: rc: 1
Dec  3 14:57:08.119: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0008c2c60 exit status 1 <nil> <nil> true [0xc000010e98 0xc000010f90 0xc000011070] [0xc000010e98 0xc000010f90 0xc000011070] [0xc000010f50 0xc000010ff8] [0xb91810 0xb91810] 0xc002b9d080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:57:18.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:57:18.237: INFO: rc: 1
Dec  3 14:57:18.237: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0008c2030 exit status 1 <nil> <nil> true [0xc000178298 0xc000178b40 0xc000179258] [0xc000178298 0xc000178b40 0xc000179258] [0xc0001788a8 0xc000178ff8] [0xb91810 0xb91810] 0xc002b9ca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:57:28.237: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:57:28.354: INFO: rc: 1
Dec  3 14:57:28.354: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db2630 exit status 1 <nil> <nil> true [0xc000010068 0xc0000105b0 0xc0000107a0] [0xc000010068 0xc0000105b0 0xc0000107a0] [0xc000010468 0xc0000106b0] [0xb91810 0xb91810] 0xc0012cc5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:57:38.354: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:57:38.430: INFO: rc: 1
Dec  3 14:57:38.430: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001e3a780 exit status 1 <nil> <nil> true [0xc000551168 0xc000551518 0xc000551570] [0xc000551168 0xc000551518 0xc000551570] [0xc000551408 0xc000551550] [0xb91810 0xb91810] 0xc000d2c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:57:48.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:57:48.555: INFO: rc: 1
Dec  3 14:57:48.555: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db2cc0 exit status 1 <nil> <nil> true [0xc000010878 0xc000010cb0 0xc000010e10] [0xc000010878 0xc000010cb0 0xc000010e10] [0xc000010c80 0xc000010da8] [0xb91810 0xb91810] 0xc0012ccae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:57:58.555: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:57:58.672: INFO: rc: 1
Dec  3 14:57:58.672: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db32f0 exit status 1 <nil> <nil> true [0xc000010e98 0xc000010f90 0xc000011070] [0xc000010e98 0xc000010f90 0xc000011070] [0xc000010f50 0xc000010ff8] [0xb91810 0xb91810] 0xc0012cd020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:58:08.672: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:58:08.763: INFO: rc: 1
Dec  3 14:58:08.763: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db3920 exit status 1 <nil> <nil> true [0xc000011120 0xc000011240 0xc000011360] [0xc000011120 0xc000011240 0xc000011360] [0xc0000111e0 0xc000011308] [0xb91810 0xb91810] 0xc0012cd3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:58:18.763: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:58:18.881: INFO: rc: 1
Dec  3 14:58:18.881: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00140a660 exit status 1 <nil> <nil> true [0xc0018e4018 0xc0018e4110 0xc0018e4190] [0xc0018e4018 0xc0018e4110 0xc0018e4190] [0xc0018e40c0 0xc0018e4170] [0xb91810 0xb91810] 0xc001f6e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:58:28.881: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:58:28.997: INFO: rc: 1
Dec  3 14:58:28.997: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00140ac90 exit status 1 <nil> <nil> true [0xc0018e41b0 0xc0018e4290 0xc0018e4310] [0xc0018e41b0 0xc0018e4290 0xc0018e4310] [0xc0018e4220 0xc0018e42f0] [0xb91810 0xb91810] 0xc001f6e960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:58:38.997: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:58:39.074: INFO: rc: 1
Dec  3 14:58:39.074: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db3f80 exit status 1 <nil> <nil> true [0xc000011410 0xc000011590 0xc000011680] [0xc000011410 0xc000011590 0xc000011680] [0xc0000114e8 0xc0000115c0] [0xb91810 0xb91810] 0xc0012cd6e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:58:49.074: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:58:49.147: INFO: rc: 1
Dec  3 14:58:49.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0015ba5a0 exit status 1 <nil> <nil> true [0xc0000116f0 0xc000011868 0xc0000118f0] [0xc0000116f0 0xc000011868 0xc0000118f0] [0xc0000117d8 0xc0000118d0] [0xb91810 0xb91810] 0xc0012cd9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:58:59.147: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:58:59.263: INFO: rc: 1
Dec  3 14:58:59.263: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001e3ae10 exit status 1 <nil> <nil> true [0xc0005515b0 0xc0005517f0 0xc0005519f8] [0xc0005515b0 0xc0005517f0 0xc0005519f8] [0xc000551620 0xc0005518b8] [0xb91810 0xb91810] 0xc000d2c6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:59:09.264: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:59:09.351: INFO: rc: 1
Dec  3 14:59:09.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0015bac90 exit status 1 <nil> <nil> true [0xc000011938 0xc0000119a8 0xc000011a90] [0xc000011938 0xc0000119a8 0xc000011a90] [0xc000011988 0xc000011a10] [0xb91810 0xb91810] 0xc0012cdce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:59:19.351: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:59:19.434: INFO: rc: 1
Dec  3 14:59:19.434: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001db2600 exit status 1 <nil> <nil> true [0xc000178298 0xc000178b40 0xc000179258] [0xc000178298 0xc000178b40 0xc000179258] [0xc0001788a8 0xc000178ff8] [0xb91810 0xb91810] 0xc002b9ca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:59:29.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:59:29.552: INFO: rc: 1
Dec  3 14:59:29.552: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0008c25d0 exit status 1 <nil> <nil> true [0xc000551168 0xc000551518 0xc000551570] [0xc000551168 0xc000551518 0xc000551570] [0xc000551408 0xc000551550] [0xb91810 0xb91810] 0xc000d2c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Dec  3 14:59:39.552: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9135 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 14:59:39.628: INFO: rc: 1
Dec  3 14:59:39.628: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Dec  3 14:59:39.628: INFO: Scaling statefulset ss to 0
Dec  3 14:59:39.640: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  3 14:59:39.643: INFO: Deleting all statefulset in ns statefulset-9135
Dec  3 14:59:39.647: INFO: Scaling statefulset ss to 0
Dec  3 14:59:39.660: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:59:39.664: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:59:39.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9135" for this suite.
Dec  3 14:59:45.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:45.828: INFO: namespace statefulset-9135 deletion completed in 6.14706093s

• [SLOW TEST:364.453 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 14:59:45.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  3 14:59:45.973: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 14:59:45.981: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 14:59:45.985: INFO: 
Logging pods the kubelet thinks is on node izgw8afzp8040eosj1udh4z before test
Dec  3 14:59:46.016: INFO: kube-proxy-n9znm from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 14:59:46.016: INFO: node-exporter-lx8zq from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 14:59:46.016: INFO: calico-kube-controllers-54d8589c95-vcpcz from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 14:59:46.016: INFO: addons-nginx-ingress-controller-58b8956897-thc29 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 14:59:46.016: INFO: calico-typha-vertical-autoscaler-f947784fd-qx7gr from kube-system started at 2019-12-03 14:27:29 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container autoscaler ready: true, restart count 3
Dec  3 14:59:46.016: INFO: vpn-shoot-6b8f7bf8ff-29lv8 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 14:59:46.016: INFO: metrics-server-84dd8df646-cnl7w from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 14:59:46.016: INFO: csi-disk-plugin-alicloud-r68mx from kube-system started at 2019-12-03 14:27:28 +0000 UTC (2 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  3 14:59:46.016: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  3 14:59:46.016: INFO: blackbox-exporter-597fdccb7d-kh2w7 from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 14:59:46.016: INFO: node-problem-detector-cw56z from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 14:59:46.016: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-768dbf555c-kc8zf from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 14:59:46.016: INFO: coredns-647b89dbf-l75g2 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container coredns ready: true, restart count 0
Dec  3 14:59:46.016: INFO: calico-typha-horizontal-autoscaler-7567d6945d-h2gzq from kube-system started at 2019-12-03 14:27:29 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 14:59:46.016: INFO: calico-node-ldhgc from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 14:59:46.016: INFO: coredns-647b89dbf-fb7p4 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container coredns ready: true, restart count 0
Dec  3 14:59:46.016: INFO: addons-kubernetes-dashboard-9497cfc54-v5l9m from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.016: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 14:59:46.016: INFO: 
Logging pods the kubelet thinks is on node izgw8jactz8ahkwwzt6d3gz before test
Dec  3 14:59:46.045: INFO: node-exporter-9ctx5 from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.045: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 14:59:46.045: INFO: calico-node-bjs4g from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.045: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 14:59:46.045: INFO: calico-typha-deploy-f7f8bff5f-86h5q from kube-system started at 2019-12-03 14:30:22 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.045: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 14:59:46.045: INFO: kube-proxy-llwcl from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.045: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 14:59:46.045: INFO: node-problem-detector-j4bdz from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 14:59:46.045: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 14:59:46.045: INFO: csi-disk-plugin-alicloud-8p2lk from kube-system started at 2019-12-03 14:27:31 +0000 UTC (2 container statuses recorded)
Dec  3 14:59:46.045: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  3 14:59:46.046: INFO: 	Container driver-registrar ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8cb35375-15dd-11ea-9142-f2314dba9465 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8cb35375-15dd-11ea-9142-f2314dba9465 off the node izgw8jactz8ahkwwzt6d3gz
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8cb35375-15dd-11ea-9142-f2314dba9465
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 14:59:50.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1056" for this suite.
Dec  3 15:00:02.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:02.272: INFO: namespace sched-pred-1056 deletion completed in 12.15387755s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:00:02.272: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:00:02.418: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1232'
Dec  3 15:00:02.749: INFO: stderr: ""
Dec  3 15:00:02.749: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Dec  3 15:00:02.753: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-1232'
Dec  3 15:00:11.377: INFO: stderr: ""
Dec  3 15:00:11.377: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:00:11.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1232" for this suite.
Dec  3 15:00:17.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:17.575: INFO: namespace kubectl-1232 deletion completed in 6.191644152s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:00:17.575: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:00:17.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3691" for this suite.
Dec  3 15:00:23.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:23.932: INFO: namespace kubelet-test-3691 deletion completed in 6.188121486s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:00:23.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-a2291712-15dd-11ea-9142-f2314dba9465
STEP: Creating secret with name secret-projected-all-test-volume-a2291700-15dd-11ea-9142-f2314dba9465
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  3 15:00:24.093: INFO: Waiting up to 5m0s for pod "projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465" in namespace "projected-334" to be "success or failure"
Dec  3 15:00:24.097: INFO: Pod "projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154177ms
Dec  3 15:00:26.102: INFO: Pod "projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009267047s
STEP: Saw pod success
Dec  3 15:00:26.102: INFO: Pod "projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:00:26.106: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  3 15:00:26.127: INFO: Waiting for pod projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465 to disappear
Dec  3 15:00:26.131: INFO: Pod projected-volume-a22916d4-15dd-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:00:26.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-334" for this suite.
Dec  3 15:00:32.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:32.323: INFO: namespace projected-334 deletion completed in 6.185995134s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:00:32.323: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3176.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3176.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:00:53.111: INFO: DNS probes using dns-3176/dns-test-a7293f44-15dd-11ea-9142-f2314dba9465 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:00:53.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3176" for this suite.
Dec  3 15:00:59.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:59.267: INFO: namespace dns-3176 deletion completed in 6.142024641s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:00:59.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 15:00:59.420: INFO: Waiting up to 5m0s for pod "pod-b738e10b-15dd-11ea-9142-f2314dba9465" in namespace "emptydir-1749" to be "success or failure"
Dec  3 15:00:59.424: INFO: Pod "pod-b738e10b-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.754798ms
Dec  3 15:01:01.429: INFO: Pod "pod-b738e10b-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008654049s
Dec  3 15:01:03.433: INFO: Pod "pod-b738e10b-15dd-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013322463s
STEP: Saw pod success
Dec  3 15:01:03.434: INFO: Pod "pod-b738e10b-15dd-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:01:03.437: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-b738e10b-15dd-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:01:03.456: INFO: Waiting for pod pod-b738e10b-15dd-11ea-9142-f2314dba9465 to disappear
Dec  3 15:01:03.459: INFO: Pod pod-b738e10b-15dd-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:03.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1749" for this suite.
Dec  3 15:01:09.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:09.607: INFO: namespace emptydir-1749 deletion completed in 6.141976376s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:09.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-bd62610d-15dd-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:01:09.762: INFO: Waiting up to 5m0s for pod "pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465" in namespace "secrets-8484" to be "success or failure"
Dec  3 15:01:09.766: INFO: Pod "pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.589107ms
Dec  3 15:01:11.771: INFO: Pod "pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008704662s
STEP: Saw pod success
Dec  3 15:01:11.771: INFO: Pod "pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:01:11.775: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:01:11.805: INFO: Waiting for pod pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465 to disappear
Dec  3 15:01:11.809: INFO: Pod pod-secrets-bd6303fb-15dd-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:11.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8484" for this suite.
Dec  3 15:01:17.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:17.960: INFO: namespace secrets-8484 deletion completed in 6.144089457s
•SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:17.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-c25d0785-15dd-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:01:18.116: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465" in namespace "projected-493" to be "success or failure"
Dec  3 15:01:18.121: INFO: Pod "pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170063ms
Dec  3 15:01:20.126: INFO: Pod "pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009053623s
STEP: Saw pod success
Dec  3 15:01:20.126: INFO: Pod "pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:01:20.130: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:01:20.151: INFO: Waiting for pod pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465 to disappear
Dec  3 15:01:20.154: INFO: Pod pod-projected-configmaps-c25dbc33-15dd-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:20.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-493" for this suite.
Dec  3 15:01:26.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:26.347: INFO: namespace projected-493 deletion completed in 6.184172817s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:26.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8708
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8708.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8708.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8708.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8708.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8708.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8708.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:01:29.112: INFO: DNS probes using dns-8708/dns-test-c75f2500-15dd-11ea-9142-f2314dba9465 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:29.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8708" for this suite.
Dec  3 15:01:35.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:35.266: INFO: namespace dns-8708 deletion completed in 6.140665223s
•SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:35.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 15:01:35.416: INFO: Waiting up to 5m0s for pod "pod-ccad7cd8-15dd-11ea-9142-f2314dba9465" in namespace "emptydir-7539" to be "success or failure"
Dec  3 15:01:35.420: INFO: Pod "pod-ccad7cd8-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485283ms
Dec  3 15:01:37.425: INFO: Pod "pod-ccad7cd8-15dd-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008279066s
STEP: Saw pod success
Dec  3 15:01:37.425: INFO: Pod "pod-ccad7cd8-15dd-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:01:37.429: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-ccad7cd8-15dd-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:01:37.450: INFO: Waiting for pod pod-ccad7cd8-15dd-11ea-9142-f2314dba9465 to disappear
Dec  3 15:01:37.453: INFO: Pod pod-ccad7cd8-15dd-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:37.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7539" for this suite.
Dec  3 15:01:43.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:43.604: INFO: namespace emptydir-7539 deletion completed in 6.145199495s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:43.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  3 15:01:43.762: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9242,SelfLink:/api/v1/namespaces/watch-9242/configmaps/e2e-watch-test-watch-closed,UID:d1a65ed5-15dd-11ea-a7d5-563937197a2b,ResourceVersion:8451,Generation:0,CreationTimestamp:2019-12-03 15:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:01:43.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9242,SelfLink:/api/v1/namespaces/watch-9242/configmaps/e2e-watch-test-watch-closed,UID:d1a65ed5-15dd-11ea-a7d5-563937197a2b,ResourceVersion:8452,Generation:0,CreationTimestamp:2019-12-03 15:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  3 15:01:43.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9242,SelfLink:/api/v1/namespaces/watch-9242/configmaps/e2e-watch-test-watch-closed,UID:d1a65ed5-15dd-11ea-a7d5-563937197a2b,ResourceVersion:8453,Generation:0,CreationTimestamp:2019-12-03 15:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:01:43.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9242,SelfLink:/api/v1/namespaces/watch-9242/configmaps/e2e-watch-test-watch-closed,UID:d1a65ed5-15dd-11ea-a7d5-563937197a2b,ResourceVersion:8454,Generation:0,CreationTimestamp:2019-12-03 15:01:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:43.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9242" for this suite.
Dec  3 15:01:49.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:49.931: INFO: namespace watch-9242 deletion completed in 6.150149315s
•SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:49.931: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-d56b68ee-15dd-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:01:50.087: INFO: Waiting up to 5m0s for pod "pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465" in namespace "secrets-8741" to be "success or failure"
Dec  3 15:01:50.091: INFO: Pod "pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62591ms
Dec  3 15:01:52.096: INFO: Pod "pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009662451s
STEP: Saw pod success
Dec  3 15:01:52.096: INFO: Pod "pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:01:52.100: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:01:52.120: INFO: Waiting for pod pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465 to disappear
Dec  3 15:01:52.124: INFO: Pod pod-secrets-d56c0c60-15dd-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:01:52.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8741" for this suite.
Dec  3 15:01:58.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:58.275: INFO: namespace secrets-8741 deletion completed in 6.146067452s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:01:58.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  3 15:01:58.420: INFO: PodSpec: initContainers in spec.initContainers
Dec  3 15:02:45.295: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-da649dfa-15dd-11ea-9142-f2314dba9465", GenerateName:"", Namespace:"init-container-7749", SelfLink:"/api/v1/namespaces/init-container-7749/pods/pod-init-da649dfa-15dd-11ea-9142-f2314dba9465", UID:"da64fb5b-15dd-11ea-a7d5-563937197a2b", ResourceVersion:"8684", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63710982118, loc:(*time.Location)(0x8830100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"420432129"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.64.1.32/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-cd7zp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00292a840), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cd7zp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cd7zp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cd7zp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002a7a048), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"izgw8jactz8ahkwwzt6d3gz", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002cd6d20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a7a0c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a7a0e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002a7a0e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002a7a0ec)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982118, loc:(*time.Location)(0x8830100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982118, loc:(*time.Location)(0x8830100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982118, loc:(*time.Location)(0x8830100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982118, loc:(*time.Location)(0x8830100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.16.152", PodIP:"100.64.1.32", StartTime:(*v1.Time)(0xc002f77dc0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ce9490)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000ce9500)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://00dbdcfa97a7ac3bd7887d00a74e590a852b3ec3b91d1c9e30bd90434d21f378"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f77e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f77de0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:02:45.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7749" for this suite.
Dec  3 15:03:07.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:07.446: INFO: namespace init-container-7749 deletion completed in 22.144600044s
•SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:03:07.447: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2325
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2325
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2325
Dec  3 15:03:07.605: INFO: Found 0 stateful pods, waiting for 1
Dec  3 15:03:17.611: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  3 15:03:17.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:03:18.145: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:03:18.145: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:03:18.145: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:03:18.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 15:03:28.155: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:03:28.155: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:03:28.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999472s
Dec  3 15:03:29.175: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996032474s
Dec  3 15:03:30.180: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99146015s
Dec  3 15:03:31.185: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986310982s
Dec  3 15:03:32.190: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981349053s
Dec  3 15:03:33.195: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976412065s
Dec  3 15:03:34.200: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971471501s
Dec  3 15:03:35.206: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966476826s
Dec  3 15:03:36.210: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.96105343s
Dec  3 15:03:37.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.164755ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2325
Dec  3 15:03:38.221: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:03:38.797: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:03:38.797: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:03:38.797: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:03:38.802: INFO: Found 1 stateful pods, waiting for 3
Dec  3 15:03:48.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:03:48.808: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:03:48.808: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  3 15:03:48.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:03:49.323: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:03:49.323: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:03:49.323: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:03:49.323: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:03:49.857: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:03:49.857: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:03:49.857: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:03:49.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:03:50.345: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:03:50.345: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:03:50.345: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:03:50.345: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:03:50.350: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 15:04:00.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:04:00.358: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:04:00.358: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:04:00.372: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999524s
Dec  3 15:04:01.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995046841s
Dec  3 15:04:02.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989856992s
Dec  3 15:04:03.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984702981s
Dec  3 15:04:04.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975149913s
Dec  3 15:04:05.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970098695s
Dec  3 15:04:06.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964724854s
Dec  3 15:04:07.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959958448s
Dec  3 15:04:08.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955245325s
Dec  3 15:04:09.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.389393ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2325
Dec  3 15:04:10.427: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:04:10.941: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:04:10.941: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:04:10.941: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:04:10.941: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:04:11.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:04:11.595: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:04:11.595: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:04:11.595: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2325 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:04:12.188: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:04:12.189: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:04:12.189: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:04:12.189: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  3 15:04:22.207: INFO: Deleting all statefulset in ns statefulset-2325
Dec  3 15:04:22.211: INFO: Scaling statefulset ss to 0
Dec  3 15:04:22.223: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:04:22.227: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:04:22.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2325" for this suite.
Dec  3 15:04:28.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:28.432: INFO: namespace statefulset-2325 deletion completed in 6.1870282s
•SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:04:28.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 15:04:28.583: INFO: Waiting up to 5m0s for pod "pod-33e4ab6c-15de-11ea-9142-f2314dba9465" in namespace "emptydir-4125" to be "success or failure"
Dec  3 15:04:28.588: INFO: Pod "pod-33e4ab6c-15de-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542581ms
Dec  3 15:04:30.592: INFO: Pod "pod-33e4ab6c-15de-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008992278s
STEP: Saw pod success
Dec  3 15:04:30.592: INFO: Pod "pod-33e4ab6c-15de-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:04:30.596: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-33e4ab6c-15de-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:04:30.616: INFO: Waiting for pod pod-33e4ab6c-15de-11ea-9142-f2314dba9465 to disappear
Dec  3 15:04:30.619: INFO: Pod pod-33e4ab6c-15de-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:04:30.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4125" for this suite.
Dec  3 15:04:36.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:36.768: INFO: namespace emptydir-4125 deletion completed in 6.143278169s
•SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:04:36.768: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-wvj2
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:04:36.926: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wvj2" in namespace "subpath-6548" to be "success or failure"
Dec  3 15:04:36.930: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724001ms
Dec  3 15:04:38.934: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008253215s
Dec  3 15:04:40.939: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013100369s
Dec  3 15:04:42.944: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 6.018265964s
Dec  3 15:04:44.949: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 8.023524099s
Dec  3 15:04:46.955: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 10.028560181s
Dec  3 15:04:48.962: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 12.035874765s
Dec  3 15:04:50.967: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 14.040720192s
Dec  3 15:04:52.972: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 16.045644793s
Dec  3 15:04:54.976: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 18.050322953s
Dec  3 15:04:56.981: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Running", Reason="", readiness=true. Elapsed: 20.055193187s
Dec  3 15:04:58.986: INFO: Pod "pod-subpath-test-configmap-wvj2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059931182s
STEP: Saw pod success
Dec  3 15:04:58.986: INFO: Pod "pod-subpath-test-configmap-wvj2" satisfied condition "success or failure"
Dec  3 15:04:58.990: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-subpath-test-configmap-wvj2 container test-container-subpath-configmap-wvj2: <nil>
STEP: delete the pod
Dec  3 15:04:59.015: INFO: Waiting for pod pod-subpath-test-configmap-wvj2 to disappear
Dec  3 15:04:59.018: INFO: Pod pod-subpath-test-configmap-wvj2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wvj2
Dec  3 15:04:59.019: INFO: Deleting pod "pod-subpath-test-configmap-wvj2" in namespace "subpath-6548"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:04:59.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6548" for this suite.
Dec  3 15:05:05.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:05.181: INFO: namespace subpath-6548 deletion completed in 6.153658027s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:05:05.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-49cc5f1f-15de-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:05:05.338: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465" in namespace "projected-9991" to be "success or failure"
Dec  3 15:05:05.342: INFO: Pod "pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987945ms
Dec  3 15:05:07.347: INFO: Pod "pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008557249s
STEP: Saw pod success
Dec  3 15:05:07.347: INFO: Pod "pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:05:07.351: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:05:07.371: INFO: Waiting for pod pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465 to disappear
Dec  3 15:05:07.374: INFO: Pod pod-projected-secrets-49cd07ec-15de-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:05:07.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9991" for this suite.
Dec  3 15:05:13.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:13.567: INFO: namespace projected-9991 deletion completed in 6.187007198s
•SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:05:13.567: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2635.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2635.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 227.209.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.209.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.209.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.209.227_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2635.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2635.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2635.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2635.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2635.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 227.209.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.209.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.209.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.209.227_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:05:15.842: INFO: Unable to read wheezy_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:15.886: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:15.894: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:15.901: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:16.311: INFO: Unable to read jessie_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:16.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:16.325: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:16.332: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:16.693: INFO: Lookups using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 failed for: [wheezy_udp@dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_udp@dns-test-service.dns-2635.svc.cluster.local jessie_tcp@dns-test-service.dns-2635.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local]

Dec  3 15:05:21.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:21.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:21.714: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:21.720: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:22.130: INFO: Unable to read jessie_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:22.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:22.143: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:22.150: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:22.512: INFO: Lookups using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 failed for: [wheezy_udp@dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_udp@dns-test-service.dns-2635.svc.cluster.local jessie_tcp@dns-test-service.dns-2635.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local]

Dec  3 15:05:26.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:26.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:26.714: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:26.720: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:27.098: INFO: Unable to read jessie_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:27.105: INFO: Unable to read jessie_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:27.112: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:27.119: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:27.480: INFO: Lookups using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 failed for: [wheezy_udp@dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_udp@dns-test-service.dns-2635.svc.cluster.local jessie_tcp@dns-test-service.dns-2635.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local]

Dec  3 15:05:31.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:31.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:31.714: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:31.720: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:32.131: INFO: Unable to read jessie_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:32.138: INFO: Unable to read jessie_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:32.145: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:32.152: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:32.514: INFO: Lookups using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 failed for: [wheezy_udp@dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_udp@dns-test-service.dns-2635.svc.cluster.local jessie_tcp@dns-test-service.dns-2635.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local]

Dec  3 15:05:36.702: INFO: Unable to read wheezy_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:36.708: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:36.715: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:36.721: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:37.128: INFO: Unable to read jessie_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:37.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:37.141: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:37.148: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:37.550: INFO: Lookups using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 failed for: [wheezy_udp@dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_udp@dns-test-service.dns-2635.svc.cluster.local jessie_tcp@dns-test-service.dns-2635.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local]

Dec  3 15:05:41.701: INFO: Unable to read wheezy_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:41.747: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:41.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:41.762: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:42.211: INFO: Unable to read jessie_udp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:42.218: INFO: Unable to read jessie_tcp@dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:42.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:42.231: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local from pod dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465: the server could not find the requested resource (get pods dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465)
Dec  3 15:05:42.592: INFO: Lookups using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 failed for: [wheezy_udp@dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@dns-test-service.dns-2635.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_udp@dns-test-service.dns-2635.svc.cluster.local jessie_tcp@dns-test-service.dns-2635.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2635.svc.cluster.local]

Dec  3 15:05:47.951: INFO: DNS probes using dns-2635/dns-test-4ecdfe13-15de-11ea-9142-f2314dba9465 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:05:47.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2635" for this suite.
Dec  3 15:05:53.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:54.129: INFO: namespace dns-2635 deletion completed in 6.149516384s
•SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:05:54.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-shxwd in namespace proxy-1360
I1203 15:05:54.289366    5087 runners.go:184] Created replication controller with name: proxy-service-shxwd, namespace: proxy-1360, replica count: 1
I1203 15:05:55.339983    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:05:56.340602    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:05:57.340968    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:05:58.341356    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:05:59.341660    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:00.342022    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:01.342349    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:02.342633    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:03.342918    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:04.343289    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:05.343609    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:06.343858    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:06:07.344160    5087 runners.go:184] proxy-service-shxwd Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:06:07.348: INFO: setup took 13.071116765s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  3 15:06:07.400: INFO: (0) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 52.487009ms)
Dec  3 15:06:07.400: INFO: (0) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 52.362647ms)
Dec  3 15:06:07.401: INFO: (0) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 53.478008ms)
Dec  3 15:06:07.401: INFO: (0) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 53.656318ms)
Dec  3 15:06:07.401: INFO: (0) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 53.480639ms)
Dec  3 15:06:07.406: INFO: (0) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 58.566099ms)
Dec  3 15:06:07.406: INFO: (0) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 58.38339ms)
Dec  3 15:06:07.406: INFO: (0) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 58.425656ms)
Dec  3 15:06:07.406: INFO: (0) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 58.511464ms)
Dec  3 15:06:07.406: INFO: (0) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 58.531349ms)
Dec  3 15:06:07.406: INFO: (0) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 58.491471ms)
Dec  3 15:06:07.418: INFO: (0) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 69.946688ms)
Dec  3 15:06:07.422: INFO: (0) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 74.008346ms)
Dec  3 15:06:07.424: INFO: (0) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 76.082452ms)
Dec  3 15:06:07.426: INFO: (0) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 78.067636ms)
Dec  3 15:06:07.428: INFO: (0) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 80.135348ms)
Dec  3 15:06:07.435: INFO: (1) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 6.90693ms)
Dec  3 15:06:07.436: INFO: (1) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.315615ms)
Dec  3 15:06:07.436: INFO: (1) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.490779ms)
Dec  3 15:06:07.436: INFO: (1) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 7.396546ms)
Dec  3 15:06:07.436: INFO: (1) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.515943ms)
Dec  3 15:06:07.436: INFO: (1) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.378291ms)
Dec  3 15:06:07.436: INFO: (1) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.379843ms)
Dec  3 15:06:07.438: INFO: (1) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 9.563463ms)
Dec  3 15:06:07.438: INFO: (1) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 9.472188ms)
Dec  3 15:06:07.438: INFO: (1) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 9.501934ms)
Dec  3 15:06:07.438: INFO: (1) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 9.484607ms)
Dec  3 15:06:07.438: INFO: (1) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 9.522371ms)
Dec  3 15:06:07.440: INFO: (1) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 11.402482ms)
Dec  3 15:06:07.440: INFO: (1) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 11.391399ms)
Dec  3 15:06:07.442: INFO: (1) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 13.341859ms)
Dec  3 15:06:07.444: INFO: (1) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 15.486404ms)
Dec  3 15:06:07.451: INFO: (2) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.184703ms)
Dec  3 15:06:07.451: INFO: (2) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.35371ms)
Dec  3 15:06:07.451: INFO: (2) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 7.232949ms)
Dec  3 15:06:07.451: INFO: (2) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.210712ms)
Dec  3 15:06:07.451: INFO: (2) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 7.196546ms)
Dec  3 15:06:07.452: INFO: (2) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 7.880972ms)
Dec  3 15:06:07.452: INFO: (2) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.989451ms)
Dec  3 15:06:07.452: INFO: (2) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.98232ms)
Dec  3 15:06:07.452: INFO: (2) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.92827ms)
Dec  3 15:06:07.454: INFO: (2) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.092716ms)
Dec  3 15:06:07.454: INFO: (2) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 10.251311ms)
Dec  3 15:06:07.454: INFO: (2) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 10.305197ms)
Dec  3 15:06:07.454: INFO: (2) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.280065ms)
Dec  3 15:06:07.456: INFO: (2) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 12.151888ms)
Dec  3 15:06:07.456: INFO: (2) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 12.217943ms)
Dec  3 15:06:07.458: INFO: (2) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 14.100536ms)
Dec  3 15:06:07.465: INFO: (3) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 6.879701ms)
Dec  3 15:06:07.465: INFO: (3) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.008604ms)
Dec  3 15:06:07.465: INFO: (3) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 6.979697ms)
Dec  3 15:06:07.465: INFO: (3) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 6.886572ms)
Dec  3 15:06:07.465: INFO: (3) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 6.885485ms)
Dec  3 15:06:07.465: INFO: (3) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 7.050839ms)
Dec  3 15:06:07.466: INFO: (3) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.190244ms)
Dec  3 15:06:07.467: INFO: (3) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.16928ms)
Dec  3 15:06:07.467: INFO: (3) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.338391ms)
Dec  3 15:06:07.467: INFO: (3) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 8.425878ms)
Dec  3 15:06:07.467: INFO: (3) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.332283ms)
Dec  3 15:06:07.469: INFO: (3) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.332614ms)
Dec  3 15:06:07.471: INFO: (3) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 12.366922ms)
Dec  3 15:06:07.471: INFO: (3) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.339213ms)
Dec  3 15:06:07.471: INFO: (3) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.525101ms)
Dec  3 15:06:07.473: INFO: (3) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 14.482365ms)
Dec  3 15:06:07.479: INFO: (4) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 6.371996ms)
Dec  3 15:06:07.479: INFO: (4) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 6.314997ms)
Dec  3 15:06:07.480: INFO: (4) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.17422ms)
Dec  3 15:06:07.480: INFO: (4) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.156598ms)
Dec  3 15:06:07.480: INFO: (4) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.114651ms)
Dec  3 15:06:07.480: INFO: (4) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.168601ms)
Dec  3 15:06:07.480: INFO: (4) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.198234ms)
Dec  3 15:06:07.480: INFO: (4) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.125585ms)
Dec  3 15:06:07.481: INFO: (4) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 8.182169ms)
Dec  3 15:06:07.483: INFO: (4) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 10.40618ms)
Dec  3 15:06:07.483: INFO: (4) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.486549ms)
Dec  3 15:06:07.484: INFO: (4) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 10.5533ms)
Dec  3 15:06:07.484: INFO: (4) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 10.618901ms)
Dec  3 15:06:07.484: INFO: (4) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.477151ms)
Dec  3 15:06:07.485: INFO: (4) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.452333ms)
Dec  3 15:06:07.486: INFO: (4) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.709516ms)
Dec  3 15:06:07.493: INFO: (5) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.005383ms)
Dec  3 15:06:07.493: INFO: (5) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.016559ms)
Dec  3 15:06:07.493: INFO: (5) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.118421ms)
Dec  3 15:06:07.493: INFO: (5) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.075647ms)
Dec  3 15:06:07.493: INFO: (5) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.033082ms)
Dec  3 15:06:07.494: INFO: (5) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 8.333819ms)
Dec  3 15:06:07.494: INFO: (5) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 8.294497ms)
Dec  3 15:06:07.494: INFO: (5) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 8.510739ms)
Dec  3 15:06:07.496: INFO: (5) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 10.396422ms)
Dec  3 15:06:07.496: INFO: (5) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.411321ms)
Dec  3 15:06:07.496: INFO: (5) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 10.437472ms)
Dec  3 15:06:07.496: INFO: (5) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.481001ms)
Dec  3 15:06:07.496: INFO: (5) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 10.466117ms)
Dec  3 15:06:07.498: INFO: (5) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.466641ms)
Dec  3 15:06:07.498: INFO: (5) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 12.409299ms)
Dec  3 15:06:07.500: INFO: (5) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 14.402363ms)
Dec  3 15:06:07.508: INFO: (6) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.204389ms)
Dec  3 15:06:07.508: INFO: (6) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.398717ms)
Dec  3 15:06:07.508: INFO: (6) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.328196ms)
Dec  3 15:06:07.508: INFO: (6) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.198819ms)
Dec  3 15:06:07.508: INFO: (6) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.350715ms)
Dec  3 15:06:07.509: INFO: (6) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.420862ms)
Dec  3 15:06:07.509: INFO: (6) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 8.310803ms)
Dec  3 15:06:07.509: INFO: (6) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.380878ms)
Dec  3 15:06:07.509: INFO: (6) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.29795ms)
Dec  3 15:06:07.511: INFO: (6) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.373624ms)
Dec  3 15:06:07.511: INFO: (6) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 10.587043ms)
Dec  3 15:06:07.511: INFO: (6) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 10.586146ms)
Dec  3 15:06:07.511: INFO: (6) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.455349ms)
Dec  3 15:06:07.511: INFO: (6) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 10.742029ms)
Dec  3 15:06:07.513: INFO: (6) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.457666ms)
Dec  3 15:06:07.513: INFO: (6) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 12.398374ms)
Dec  3 15:06:07.520: INFO: (7) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.300698ms)
Dec  3 15:06:07.520: INFO: (7) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 7.447415ms)
Dec  3 15:06:07.520: INFO: (7) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.328ms)
Dec  3 15:06:07.520: INFO: (7) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.327917ms)
Dec  3 15:06:07.520: INFO: (7) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.432885ms)
Dec  3 15:06:07.521: INFO: (7) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.119812ms)
Dec  3 15:06:07.521: INFO: (7) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.106053ms)
Dec  3 15:06:07.524: INFO: (7) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 10.484386ms)
Dec  3 15:06:07.524: INFO: (7) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 10.391829ms)
Dec  3 15:06:07.524: INFO: (7) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 10.497593ms)
Dec  3 15:06:07.524: INFO: (7) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.46712ms)
Dec  3 15:06:07.524: INFO: (7) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 10.424061ms)
Dec  3 15:06:07.524: INFO: (7) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.414499ms)
Dec  3 15:06:07.526: INFO: (7) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.389401ms)
Dec  3 15:06:07.526: INFO: (7) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.368392ms)
Dec  3 15:06:07.528: INFO: (7) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 14.32119ms)
Dec  3 15:06:07.536: INFO: (8) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.404568ms)
Dec  3 15:06:07.536: INFO: (8) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 8.244134ms)
Dec  3 15:06:07.536: INFO: (8) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.212832ms)
Dec  3 15:06:07.538: INFO: (8) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 10.288429ms)
Dec  3 15:06:07.538: INFO: (8) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 10.124201ms)
Dec  3 15:06:07.538: INFO: (8) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 10.249413ms)
Dec  3 15:06:07.538: INFO: (8) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 10.229829ms)
Dec  3 15:06:07.541: INFO: (8) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 13.264524ms)
Dec  3 15:06:07.584: INFO: (8) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 56.663101ms)
Dec  3 15:06:07.584: INFO: (8) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 56.436461ms)
Dec  3 15:06:07.584: INFO: (8) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 56.482207ms)
Dec  3 15:06:07.584: INFO: (8) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 56.629714ms)
Dec  3 15:06:07.584: INFO: (8) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 56.49202ms)
Dec  3 15:06:07.584: INFO: (8) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 56.653213ms)
Dec  3 15:06:07.627: INFO: (8) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 99.575641ms)
Dec  3 15:06:07.627: INFO: (8) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 99.463448ms)
Dec  3 15:06:07.635: INFO: (9) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 7.664948ms)
Dec  3 15:06:07.636: INFO: (9) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.207706ms)
Dec  3 15:06:07.636: INFO: (9) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 8.202885ms)
Dec  3 15:06:07.636: INFO: (9) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.310836ms)
Dec  3 15:06:07.636: INFO: (9) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.183857ms)
Dec  3 15:06:07.636: INFO: (9) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 8.284463ms)
Dec  3 15:06:07.636: INFO: (9) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 8.272759ms)
Dec  3 15:06:07.637: INFO: (9) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 9.935317ms)
Dec  3 15:06:07.637: INFO: (9) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 10.047187ms)
Dec  3 15:06:07.637: INFO: (9) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.021044ms)
Dec  3 15:06:07.637: INFO: (9) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 9.938228ms)
Dec  3 15:06:07.637: INFO: (9) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 9.894583ms)
Dec  3 15:06:07.639: INFO: (9) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.055263ms)
Dec  3 15:06:07.642: INFO: (9) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 14.118457ms)
Dec  3 15:06:07.642: INFO: (9) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 14.140035ms)
Dec  3 15:06:07.644: INFO: (9) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 16.244155ms)
Dec  3 15:06:07.651: INFO: (10) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.187821ms)
Dec  3 15:06:07.651: INFO: (10) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.4028ms)
Dec  3 15:06:07.651: INFO: (10) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.330515ms)
Dec  3 15:06:07.651: INFO: (10) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 7.197052ms)
Dec  3 15:06:07.651: INFO: (10) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.208783ms)
Dec  3 15:06:07.651: INFO: (10) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.212606ms)
Dec  3 15:06:07.652: INFO: (10) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.306301ms)
Dec  3 15:06:07.652: INFO: (10) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 8.472948ms)
Dec  3 15:06:07.652: INFO: (10) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 8.450006ms)
Dec  3 15:06:07.653: INFO: (10) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 8.783793ms)
Dec  3 15:06:07.654: INFO: (10) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.523895ms)
Dec  3 15:06:07.654: INFO: (10) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 10.532567ms)
Dec  3 15:06:07.654: INFO: (10) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 10.538888ms)
Dec  3 15:06:07.655: INFO: (10) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.622314ms)
Dec  3 15:06:07.657: INFO: (10) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.718787ms)
Dec  3 15:06:07.658: INFO: (10) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 14.543191ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.99731ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.808956ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.791148ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.990127ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.846056ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.300521ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 8.268742ms)
Dec  3 15:06:07.667: INFO: (11) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 8.160485ms)
Dec  3 15:06:07.669: INFO: (11) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 10.171822ms)
Dec  3 15:06:07.669: INFO: (11) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.207646ms)
Dec  3 15:06:07.669: INFO: (11) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 10.181113ms)
Dec  3 15:06:07.669: INFO: (11) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.29907ms)
Dec  3 15:06:07.671: INFO: (11) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.456935ms)
Dec  3 15:06:07.671: INFO: (11) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 12.282042ms)
Dec  3 15:06:07.671: INFO: (11) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.376498ms)
Dec  3 15:06:07.675: INFO: (11) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 16.665056ms)
Dec  3 15:06:07.682: INFO: (12) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 6.648463ms)
Dec  3 15:06:07.682: INFO: (12) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 6.795028ms)
Dec  3 15:06:07.683: INFO: (12) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.719905ms)
Dec  3 15:06:07.683: INFO: (12) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.689955ms)
Dec  3 15:06:07.683: INFO: (12) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.845587ms)
Dec  3 15:06:07.683: INFO: (12) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.811664ms)
Dec  3 15:06:07.683: INFO: (12) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.928608ms)
Dec  3 15:06:07.684: INFO: (12) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 7.985422ms)
Dec  3 15:06:07.684: INFO: (12) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.466823ms)
Dec  3 15:06:07.684: INFO: (12) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.429684ms)
Dec  3 15:06:07.684: INFO: (12) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 8.469536ms)
Dec  3 15:06:07.686: INFO: (12) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.321959ms)
Dec  3 15:06:07.688: INFO: (12) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.668845ms)
Dec  3 15:06:07.688: INFO: (12) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.720119ms)
Dec  3 15:06:07.688: INFO: (12) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.672544ms)
Dec  3 15:06:07.690: INFO: (12) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 14.643456ms)
Dec  3 15:06:07.698: INFO: (13) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.66555ms)
Dec  3 15:06:07.698: INFO: (13) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.535741ms)
Dec  3 15:06:07.698: INFO: (13) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.522865ms)
Dec  3 15:06:07.698: INFO: (13) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 7.676926ms)
Dec  3 15:06:07.698: INFO: (13) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.579681ms)
Dec  3 15:06:07.699: INFO: (13) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.47362ms)
Dec  3 15:06:07.699: INFO: (13) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 8.771111ms)
Dec  3 15:06:07.699: INFO: (13) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.735298ms)
Dec  3 15:06:07.699: INFO: (13) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.722336ms)
Dec  3 15:06:07.699: INFO: (13) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 8.716408ms)
Dec  3 15:06:07.699: INFO: (13) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 8.941218ms)
Dec  3 15:06:07.701: INFO: (13) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 10.471542ms)
Dec  3 15:06:07.701: INFO: (13) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 10.590863ms)
Dec  3 15:06:07.703: INFO: (13) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 12.631569ms)
Dec  3 15:06:07.703: INFO: (13) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 12.579576ms)
Dec  3 15:06:07.705: INFO: (13) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 14.722487ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.016205ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 8.088802ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.132015ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 7.981216ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 8.080234ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.925758ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.985545ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 8.084567ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 8.03305ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 8.04717ms)
Dec  3 15:06:07.713: INFO: (14) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.103202ms)
Dec  3 15:06:07.715: INFO: (14) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 10.079877ms)
Dec  3 15:06:07.718: INFO: (14) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.241216ms)
Dec  3 15:06:07.718: INFO: (14) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 12.185095ms)
Dec  3 15:06:07.718: INFO: (14) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 12.190003ms)
Dec  3 15:06:07.720: INFO: (14) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 14.274909ms)
Dec  3 15:06:07.726: INFO: (15) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 6.491327ms)
Dec  3 15:06:07.726: INFO: (15) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 6.428847ms)
Dec  3 15:06:07.728: INFO: (15) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.411481ms)
Dec  3 15:06:07.728: INFO: (15) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 8.349681ms)
Dec  3 15:06:07.728: INFO: (15) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.396157ms)
Dec  3 15:06:07.728: INFO: (15) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 8.427517ms)
Dec  3 15:06:07.728: INFO: (15) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 8.402701ms)
Dec  3 15:06:07.730: INFO: (15) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 10.569572ms)
Dec  3 15:06:07.730: INFO: (15) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.686028ms)
Dec  3 15:06:07.730: INFO: (15) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 10.551444ms)
Dec  3 15:06:07.730: INFO: (15) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.633088ms)
Dec  3 15:06:07.730: INFO: (15) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 10.633348ms)
Dec  3 15:06:07.732: INFO: (15) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.467814ms)
Dec  3 15:06:07.732: INFO: (15) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 12.403134ms)
Dec  3 15:06:07.734: INFO: (15) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 14.489937ms)
Dec  3 15:06:07.734: INFO: (15) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 14.681692ms)
Dec  3 15:06:07.742: INFO: (16) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.275881ms)
Dec  3 15:06:07.742: INFO: (16) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 7.086788ms)
Dec  3 15:06:07.742: INFO: (16) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.459436ms)
Dec  3 15:06:07.742: INFO: (16) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.461574ms)
Dec  3 15:06:07.742: INFO: (16) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.431114ms)
Dec  3 15:06:07.743: INFO: (16) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 8.60491ms)
Dec  3 15:06:07.743: INFO: (16) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.642015ms)
Dec  3 15:06:07.743: INFO: (16) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.551913ms)
Dec  3 15:06:07.743: INFO: (16) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 8.650188ms)
Dec  3 15:06:07.745: INFO: (16) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 10.62244ms)
Dec  3 15:06:07.745: INFO: (16) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.702218ms)
Dec  3 15:06:07.745: INFO: (16) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 10.555292ms)
Dec  3 15:06:07.747: INFO: (16) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.545772ms)
Dec  3 15:06:07.747: INFO: (16) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.672935ms)
Dec  3 15:06:07.749: INFO: (16) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 14.594091ms)
Dec  3 15:06:07.749: INFO: (16) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 14.546109ms)
Dec  3 15:06:07.757: INFO: (17) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 7.305121ms)
Dec  3 15:06:07.757: INFO: (17) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.181111ms)
Dec  3 15:06:07.757: INFO: (17) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.274486ms)
Dec  3 15:06:07.757: INFO: (17) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 7.4507ms)
Dec  3 15:06:07.757: INFO: (17) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.150546ms)
Dec  3 15:06:07.757: INFO: (17) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.338818ms)
Dec  3 15:06:07.758: INFO: (17) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 8.463143ms)
Dec  3 15:06:07.758: INFO: (17) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 8.592103ms)
Dec  3 15:06:07.758: INFO: (17) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 8.449322ms)
Dec  3 15:06:07.758: INFO: (17) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 8.351643ms)
Dec  3 15:06:07.760: INFO: (17) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.522629ms)
Dec  3 15:06:07.760: INFO: (17) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.586634ms)
Dec  3 15:06:07.762: INFO: (17) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 12.358366ms)
Dec  3 15:06:07.762: INFO: (17) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 12.436041ms)
Dec  3 15:06:07.764: INFO: (17) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 14.435995ms)
Dec  3 15:06:07.764: INFO: (17) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 14.384955ms)
Dec  3 15:06:07.772: INFO: (18) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.591399ms)
Dec  3 15:06:07.772: INFO: (18) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 7.611165ms)
Dec  3 15:06:07.772: INFO: (18) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.728084ms)
Dec  3 15:06:07.772: INFO: (18) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.834496ms)
Dec  3 15:06:07.772: INFO: (18) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.884888ms)
Dec  3 15:06:07.773: INFO: (18) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 8.331058ms)
Dec  3 15:06:07.773: INFO: (18) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 8.370296ms)
Dec  3 15:06:07.775: INFO: (18) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 10.385604ms)
Dec  3 15:06:07.775: INFO: (18) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 10.384033ms)
Dec  3 15:06:07.775: INFO: (18) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 10.331822ms)
Dec  3 15:06:07.775: INFO: (18) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 10.409294ms)
Dec  3 15:06:07.775: INFO: (18) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 10.463607ms)
Dec  3 15:06:07.775: INFO: (18) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 10.409066ms)
Dec  3 15:06:07.777: INFO: (18) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 12.696568ms)
Dec  3 15:06:07.777: INFO: (18) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 12.667498ms)
Dec  3 15:06:07.781: INFO: (18) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 17.168456ms)
Dec  3 15:06:07.789: INFO: (19) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:460/proxy/: tls baz (200; 7.16293ms)
Dec  3 15:06:07.789: INFO: (19) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname1/proxy/: foo (200; 7.1807ms)
Dec  3 15:06:07.789: INFO: (19) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 7.133985ms)
Dec  3 15:06:07.789: INFO: (19) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">... (200; 7.196546ms)
Dec  3 15:06:07.789: INFO: (19) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw/proxy/rewriteme">test</a> (200; 7.226112ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 9.173958ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:443/proxy/tlsrewritem... (200; 9.276398ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:1080/proxy/rewriteme">test<... (200; 9.305065ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/pods/https:proxy-service-shxwd-6bwmw:462/proxy/: tls qux (200; 9.275392ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname1/proxy/: tls baz (200; 9.414356ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/services/https:proxy-service-shxwd:tlsportname2/proxy/: tls qux (200; 9.384776ms)
Dec  3 15:06:07.791: INFO: (19) /api/v1/namespaces/proxy-1360/pods/proxy-service-shxwd-6bwmw:162/proxy/: bar (200; 9.321586ms)
Dec  3 15:06:07.795: INFO: (19) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname1/proxy/: foo (200; 13.307181ms)
Dec  3 15:06:07.795: INFO: (19) /api/v1/namespaces/proxy-1360/services/proxy-service-shxwd:portname2/proxy/: bar (200; 13.345396ms)
Dec  3 15:06:07.795: INFO: (19) /api/v1/namespaces/proxy-1360/pods/http:proxy-service-shxwd-6bwmw:160/proxy/: foo (200; 13.528938ms)
Dec  3 15:06:07.797: INFO: (19) /api/v1/namespaces/proxy-1360/services/http:proxy-service-shxwd:portname2/proxy/: bar (200; 15.393168ms)
STEP: deleting ReplicationController proxy-service-shxwd in namespace proxy-1360, will wait for the garbage collector to delete the pods
Dec  3 15:06:07.857: INFO: Deleting ReplicationController proxy-service-shxwd took: 5.947418ms
Dec  3 15:06:08.357: INFO: Terminating ReplicationController proxy-service-shxwd pods took: 500.385322ms
[AfterEach] version v1
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:06:21.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1360" for this suite.
Dec  3 15:06:27.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:27.550: INFO: namespace proxy-1360 deletion completed in 6.186070494s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:06:27.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1203 15:06:57.731416    5087 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:06:57.731: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:06:57.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2176" for this suite.
Dec  3 15:07:03.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:03.878: INFO: namespace gc-2176 deletion completed in 6.143078031s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:07:03.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-908bb01e-15de-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:07:04.032: INFO: Waiting up to 5m0s for pod "pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465" in namespace "configmap-5635" to be "success or failure"
Dec  3 15:07:04.035: INFO: Pod "pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509905ms
Dec  3 15:07:06.040: INFO: Pod "pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008338055s
STEP: Saw pod success
Dec  3 15:07:06.040: INFO: Pod "pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:07:06.044: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:07:06.106: INFO: Waiting for pod pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465 to disappear
Dec  3 15:07:06.110: INFO: Pod pod-configmaps-908c4ec5-15de-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:07:06.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5635" for this suite.
Dec  3 15:07:12.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:12.269: INFO: namespace configmap-5635 deletion completed in 6.153324058s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:07:12.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  3 15:07:14.961: INFO: Successfully updated pod "labelsupdate958d0e71-15de-11ea-9142-f2314dba9465"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:07:16.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2493" for this suite.
Dec  3 15:07:37.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:37.135: INFO: namespace downward-api-2493 deletion completed in 20.14217627s
•
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:07:37.135: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1203 15:07:43.310771    5087 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:07:43.310: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:07:43.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8303" for this suite.
Dec  3 15:07:49.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:49.499: INFO: namespace gc-8303 deletion completed in 6.185454677s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:07:49.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1203 15:07:59.673780    5087 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:07:59.673: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:07:59.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7684" for this suite.
Dec  3 15:08:05.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:05.817: INFO: namespace gc-7684 deletion completed in 6.140142599s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:08:05.818: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:08:05.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465" in namespace "downward-api-8226" to be "success or failure"
Dec  3 15:08:05.972: INFO: Pod "downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.730504ms
Dec  3 15:08:07.977: INFO: Pod "downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008295079s
STEP: Saw pod success
Dec  3 15:08:07.977: INFO: Pod "downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:08:07.981: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:08:08.000: INFO: Waiting for pod downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465 to disappear
Dec  3 15:08:08.003: INFO: Pod downwardapi-volume-b577062a-15de-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:08:08.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8226" for this suite.
Dec  3 15:08:14.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:14.157: INFO: namespace downward-api-8226 deletion completed in 6.14796209s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:08:14.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:08:14.303: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9967'
Dec  3 15:08:14.511: INFO: stderr: ""
Dec  3 15:08:14.511: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  3 15:08:14.511: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9967'
Dec  3 15:08:14.673: INFO: stderr: ""
Dec  3 15:08:14.673: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 15:08:15.678: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:08:15.678: INFO: Found 0 / 1
Dec  3 15:08:16.678: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:08:16.678: INFO: Found 0 / 1
Dec  3 15:08:17.677: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:08:17.678: INFO: Found 0 / 1
Dec  3 15:08:18.678: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:08:18.678: INFO: Found 1 / 1
Dec  3 15:08:18.678: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 15:08:18.682: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:08:18.682: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 15:08:18.682: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-d9627 --namespace=kubectl-9967'
Dec  3 15:08:18.848: INFO: stderr: ""
Dec  3 15:08:18.848: INFO: stdout: "Name:               redis-master-d9627\nNamespace:          kubectl-9967\nPriority:           0\nPriorityClassName:  <none>\nNode:               izgw8jactz8ahkwwzt6d3gz/10.250.16.152\nStart Time:         Tue, 03 Dec 2019 15:08:14 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.64.1.54/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.64.1.54\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://17706b5fef36a75b0881c190deb48a4aab861e989b80fba6dc34ccccbe34c801\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 03 Dec 2019 15:08:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tk9l4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tk9l4:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tk9l4\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                              Message\n  ----    ------     ----  ----                              -------\n  Normal  Scheduled  4s    default-scheduler                 Successfully assigned kubectl-9967/redis-master-d9627 to izgw8jactz8ahkwwzt6d3gz\n  Normal  Pulling    3s    kubelet, izgw8jactz8ahkwwzt6d3gz  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, izgw8jactz8ahkwwzt6d3gz  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, izgw8jactz8ahkwwzt6d3gz  Created container redis-master\n  Normal  Started    2s    kubelet, izgw8jactz8ahkwwzt6d3gz  Started container redis-master\n"
Dec  3 15:08:18.849: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-9967'
Dec  3 15:08:18.951: INFO: stderr: ""
Dec  3 15:08:18.951: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9967\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-d9627\n"
Dec  3 15:08:18.951: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-9967'
Dec  3 15:08:19.046: INFO: stderr: ""
Dec  3 15:08:19.046: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9967\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.105.200.40\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.64.1.54:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  3 15:08:19.052: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node izgw8afzp8040eosj1udh4z'
Dec  3 15:08:19.269: INFO: stderr: ""
Dec  3 15:08:19.269: INFO: stdout: "Name:               izgw8afzp8040eosj1udh4z\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecs.sn2ne.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=izgw8afzp8040eosj1udh4z\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    topology.diskplugin.csi.alibabacloud.com/zone=eu-central-1b\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"diskplugin.csi.alibabacloud.com\":\"i-gw8afzp8040eosj1udh4\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.16.153/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 03 Dec 2019 14:26:48 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentKubeletRestart        False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Tue, 03 Dec 2019 15:07:51 +0000   Tue, 03 Dec 2019 14:29:32 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  NetworkUnavailable            False   Tue, 03 Dec 2019 14:27:52 +0000   Tue, 03 Dec 2019 14:27:52 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Tue, 03 Dec 2019 15:08:12 +0000   Tue, 03 Dec 2019 14:26:48 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 03 Dec 2019 15:08:12 +0000   Tue, 03 Dec 2019 14:26:48 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 03 Dec 2019 15:08:12 +0000   Tue, 03 Dec 2019 14:26:48 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 03 Dec 2019 15:08:12 +0000   Tue, 03 Dec 2019 14:27:28 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.16.153\nCapacity:\n attachable-volumes-csi-diskplugin.csi.alibabacloud.com:  15\n cpu:                                                     2\n ephemeral-storage:                                       33136428Ki\n hugepages-1Gi:                                           0\n hugepages-2Mi:                                           0\n memory:                                                  8169216Ki\n pods:                                                    110\nAllocatable:\n attachable-volumes-csi-diskplugin.csi.alibabacloud.com:  15\n cpu:                                                     1920m\n ephemeral-storage:                                       32235117134\n hugepages-1Gi:                                           0\n hugepages-2Mi:                                           0\n memory:                                                  6873271495\n pods:                                                    110\nSystem Info:\n Machine ID:                 fe45ce7863f3441ea7776827610c42b2\n System UUID:                fe45ce78-63f3-441e-a777-6827610c42b2\n Boot ID:                    525e2a34-df55-42c4-b7c1-2d86db3fb256\n Kernel Version:             4.19.66-coreos\n OS Image:                   Container Linux by CoreOS 2191.4.1 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.14.9\n Kube-Proxy Version:         v1.14.9\nPodCIDR:                     100.64.0.0/24\nProviderID:                  eu-central-1.i-gw8afzp8040eosj1udh4\nNon-terminated Pods:         (16 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kubernetes-dashboard-9497cfc54-v5l9m                        50m (2%)      100m (5%)   50Mi (0%)        256Mi (3%)     43m\n  kube-system                addons-nginx-ingress-controller-58b8956897-thc29                   100m (5%)     2 (104%)    100Mi (1%)       1Gi (15%)      43m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-768dbf555c-kc8zf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  kube-system                blackbox-exporter-597fdccb7d-kh2w7                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      43m\n  kube-system                calico-kube-controllers-54d8589c95-vcpcz                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  kube-system                calico-node-ldhgc                                                  100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)    41m\n  kube-system                calico-typha-horizontal-autoscaler-7567d6945d-h2gzq                10m (0%)      10m (0%)    0 (0%)           0 (0%)         43m\n  kube-system                calico-typha-vertical-autoscaler-f947784fd-qx7gr                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m\n  kube-system                coredns-647b89dbf-fb7p4                                            50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     43m\n  kube-system                coredns-647b89dbf-l75g2                                            50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     43m\n  kube-system                csi-disk-plugin-alicloud-r68mx                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\n  kube-system                kube-proxy-n9znm                                                   20m (1%)      0 (0%)      64Mi (0%)        0 (0%)         41m\n  kube-system                metrics-server-84dd8df646-cnl7w                                    20m (1%)      80m (4%)    100Mi (1%)       400Mi (6%)     43m\n  kube-system                node-exporter-lx8zq                                                5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     41m\n  kube-system                node-problem-detector-cw56z                                        20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)     41m\n  kube-system                vpn-shoot-6b8f7bf8ff-29lv8                                         100m (5%)     1 (52%)     100Mi (1%)       1000Mi (15%)   43m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                                                Requests    Limits\n  --------                                                --------    ------\n  cpu                                                     530m (27%)  4125m (214%)\n  memory                                                  579Mi (8%)  3815Mi (58%)\n  ephemeral-storage                                       0 (0%)      0 (0%)\n  attachable-volumes-csi-diskplugin.csi.alibabacloud.com  0           0\nEvents:\n  Type     Reason                   Age                From                                      Message\n  ----     ------                   ----               ----                                      -------\n  Normal   Starting                 41m                kubelet, izgw8afzp8040eosj1udh4z          Starting kubelet.\n  Normal   NodeHasSufficientMemory  41m                kubelet, izgw8afzp8040eosj1udh4z          Node izgw8afzp8040eosj1udh4z status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    41m                kubelet, izgw8afzp8040eosj1udh4z          Node izgw8afzp8040eosj1udh4z status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     41m                kubelet, izgw8afzp8040eosj1udh4z          Node izgw8afzp8040eosj1udh4z status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  41m                kubelet, izgw8afzp8040eosj1udh4z          Updated Node Allocatable limit across pods\n  Normal   Starting                 41m                kube-proxy, izgw8afzp8040eosj1udh4z       Starting kube-proxy.\n  Normal   NodeReady                40m                kubelet, izgw8afzp8040eosj1udh4z          Node izgw8afzp8040eosj1udh4z status is now: NodeReady\n  Warning  DockerStart              38m (x3 over 38m)  systemd-monitor, izgw8afzp8040eosj1udh4z  Starting Docker Application Container Engine...\n"
Dec  3 15:08:19.269: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-9967'
Dec  3 15:08:19.363: INFO: stderr: ""
Dec  3 15:08:19.363: INFO: stdout: "Name:         kubectl-9967\nLabels:       e2e-framework=kubectl\n              e2e-run=f7878873-15db-11ea-9142-f2314dba9465\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:08:19.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9967" for this suite.
Dec  3 15:08:41.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:41.518: INFO: namespace kubectl-9967 deletion completed in 22.148662323s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:08:41.519: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Dec  3 15:08:41.671: INFO: Waiting up to 5m0s for pod "var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465" in namespace "var-expansion-3141" to be "success or failure"
Dec  3 15:08:41.675: INFO: Pod "var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4332ms
Dec  3 15:08:43.679: INFO: Pod "var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008291638s
STEP: Saw pod success
Dec  3 15:08:43.680: INFO: Pod "var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:08:43.683: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:08:43.702: INFO: Waiting for pod var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465 to disappear
Dec  3 15:08:43.706: INFO: Pod var-expansion-cabed5ef-15de-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:08:43.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3141" for this suite.
Dec  3 15:08:49.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:49.898: INFO: namespace var-expansion-3141 deletion completed in 6.186683478s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:08:49.899: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-447
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-cfbe3d3d-15de-11ea-9142-f2314dba9465
STEP: Creating configMap with name cm-test-opt-upd-cfbe3d91-15de-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-cfbe3d3d-15de-11ea-9142-f2314dba9465
STEP: Updating configmap cm-test-opt-upd-cfbe3d91-15de-11ea-9142-f2314dba9465
STEP: Creating configMap with name cm-test-opt-create-cfbe3db2-15de-11ea-9142-f2314dba9465
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:08:56.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-447" for this suite.
Dec  3 15:09:18.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:18.613: INFO: namespace projected-447 deletion completed in 22.144444098s
•SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:09:18.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1344
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-e0da9b60-15de-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:09:18.767: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465" in namespace "projected-1344" to be "success or failure"
Dec  3 15:09:18.771: INFO: Pod "pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638864ms
Dec  3 15:09:20.775: INFO: Pod "pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008182446s
STEP: Saw pod success
Dec  3 15:09:20.775: INFO: Pod "pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:09:20.779: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:09:20.837: INFO: Waiting for pod pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465 to disappear
Dec  3 15:09:20.840: INFO: Pod pod-projected-configmaps-e0db44d0-15de-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:09:20.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1344" for this suite.
Dec  3 15:09:26.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:27.033: INFO: namespace projected-1344 deletion completed in 6.186599316s
•SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:09:27.033: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:09:29.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9349" for this suite.
Dec  3 15:09:35.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:35.371: INFO: namespace emptydir-wrapper-9349 deletion completed in 6.143192568s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:09:35.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6484
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3920
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:09:59.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2412" for this suite.
Dec  3 15:10:05.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:05.981: INFO: namespace namespaces-2412 deletion completed in 6.146476746s
STEP: Destroying namespace "nsdeletetest-6484" for this suite.
Dec  3 15:10:05.985: INFO: Namespace nsdeletetest-6484 was already deleted
STEP: Destroying namespace "nsdeletetest-3920" for this suite.
Dec  3 15:10:11.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:12.129: INFO: namespace nsdeletetest-3920 deletion completed in 6.144593652s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:10:12.130: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  3 15:10:14.822: INFO: Successfully updated pod "labelsupdate00c111b4-15df-11ea-9142-f2314dba9465"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:10:16.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5319" for this suite.
Dec  3 15:10:38.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:38.996: INFO: namespace projected-5319 deletion completed in 22.140431521s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:10:38.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:10:39.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465" in namespace "projected-155" to be "success or failure"
Dec  3 15:10:39.151: INFO: Pod "downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.329739ms
Dec  3 15:10:41.156: INFO: Pod "downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008389076s
STEP: Saw pod success
Dec  3 15:10:41.156: INFO: Pod "downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:10:41.161: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:10:41.180: INFO: Waiting for pod downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465 to disappear
Dec  3 15:10:41.184: INFO: Pod downwardapi-volume-10c46902-15df-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:10:41.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-155" for this suite.
Dec  3 15:10:47.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:47.384: INFO: namespace projected-155 deletion completed in 6.193459609s
•SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:10:47.384: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  3 15:10:50.061: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9622 pod-service-account-16130bcd-15df-11ea-9142-f2314dba9465 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  3 15:10:50.675: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9622 pod-service-account-16130bcd-15df-11ea-9142-f2314dba9465 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  3 15:10:51.199: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9622 pod-service-account-16130bcd-15df-11ea-9142-f2314dba9465 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:10:51.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9622" for this suite.
Dec  3 15:10:57.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:57.920: INFO: namespace svcaccounts-9622 deletion completed in 6.145447357s
•SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:10:57.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  3 15:10:58.077: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10899,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:10:58.077: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10899,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  3 15:11:08.088: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10929,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 15:11:08.088: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10929,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  3 15:11:18.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10960,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:11:18.097: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10960,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  3 15:11:28.103: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10991,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:11:28.104: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-a,UID:1c0d330c-15df-11ea-a7d5-563937197a2b,ResourceVersion:10991,Generation:0,CreationTimestamp:2019-12-03 15:10:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  3 15:11:38.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-b,UID:33e98333-15df-11ea-a7d5-563937197a2b,ResourceVersion:11023,Generation:0,CreationTimestamp:2019-12-03 15:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:11:38.110: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-b,UID:33e98333-15df-11ea-a7d5-563937197a2b,ResourceVersion:11023,Generation:0,CreationTimestamp:2019-12-03 15:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  3 15:11:48.117: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-b,UID:33e98333-15df-11ea-a7d5-563937197a2b,ResourceVersion:11053,Generation:0,CreationTimestamp:2019-12-03 15:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:11:48.117: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7364,SelfLink:/api/v1/namespaces/watch-7364/configmaps/e2e-watch-test-configmap-b,UID:33e98333-15df-11ea-a7d5-563937197a2b,ResourceVersion:11053,Generation:0,CreationTimestamp:2019-12-03 15:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:11:58.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7364" for this suite.
Dec  3 15:12:04.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:04.275: INFO: namespace watch-7364 deletion completed in 6.151368798s
•SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:12:04.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-4398fb5b-15df-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:12:04.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465" in namespace "configmap-2259" to be "success or failure"
Dec  3 15:12:04.435: INFO: Pod "pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63223ms
Dec  3 15:12:06.440: INFO: Pod "pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008620762s
STEP: Saw pod success
Dec  3 15:12:06.440: INFO: Pod "pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:12:06.445: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:12:06.465: INFO: Waiting for pod pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465 to disappear
Dec  3 15:12:06.468: INFO: Pod pod-configmaps-4399a648-15df-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:12:06.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2259" for this suite.
Dec  3 15:12:12.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:12.620: INFO: namespace configmap-2259 deletion completed in 6.145857572s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:12:12.620: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:12:12.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465" in namespace "projected-5309" to be "success or failure"
Dec  3 15:12:12.787: INFO: Pod "downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069436ms
Dec  3 15:12:14.792: INFO: Pod "downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009160225s
STEP: Saw pod success
Dec  3 15:12:14.792: INFO: Pod "downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:12:14.796: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:12:14.817: INFO: Waiting for pod downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465 to disappear
Dec  3 15:12:14.820: INFO: Pod downwardapi-volume-4893f2d3-15df-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:12:14.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5309" for this suite.
Dec  3 15:12:20.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:20.979: INFO: namespace projected-5309 deletion completed in 6.152524933s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:12:20.979: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:12:21.146: INFO: (0) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 10.751063ms)
Dec  3 15:12:21.191: INFO: (1) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 44.903525ms)
Dec  3 15:12:21.199: INFO: (2) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 7.243972ms)
Dec  3 15:12:21.205: INFO: (3) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.865291ms)
Dec  3 15:12:21.212: INFO: (4) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.596212ms)
Dec  3 15:12:21.219: INFO: (5) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.768717ms)
Dec  3 15:12:21.226: INFO: (6) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.55235ms)
Dec  3 15:12:21.232: INFO: (7) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.519721ms)
Dec  3 15:12:21.239: INFO: (8) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.496465ms)
Dec  3 15:12:21.245: INFO: (9) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.290065ms)
Dec  3 15:12:21.251: INFO: (10) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.190904ms)
Dec  3 15:12:21.258: INFO: (11) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.4811ms)
Dec  3 15:12:21.264: INFO: (12) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.453659ms)
Dec  3 15:12:21.271: INFO: (13) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.354466ms)
Dec  3 15:12:21.277: INFO: (14) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.367941ms)
Dec  3 15:12:21.284: INFO: (15) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.460424ms)
Dec  3 15:12:21.290: INFO: (16) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.48012ms)
Dec  3 15:12:21.297: INFO: (17) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.46499ms)
Dec  3 15:12:21.303: INFO: (18) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.401556ms)
Dec  3 15:12:21.310: INFO: (19) /api/v1/nodes/izgw8afzp8040eosj1udh4z/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.568545ms)
[AfterEach] version v1
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:12:21.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7382" for this suite.
Dec  3 15:12:27.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:27.454: INFO: namespace proxy-7382 deletion completed in 6.139652401s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:12:27.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Dec  3 15:12:27.599: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-2597'
Dec  3 15:12:27.991: INFO: stderr: ""
Dec  3 15:12:27.991: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Dec  3 15:12:28.995: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:12:28.995: INFO: Found 0 / 1
Dec  3 15:12:29.995: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:12:29.995: INFO: Found 1 / 1
Dec  3 15:12:29.995: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 15:12:29.999: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:12:29.999: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  3 15:12:30.000: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-fwzqj redis-master --namespace=kubectl-2597'
Dec  3 15:12:30.193: INFO: stderr: ""
Dec  3 15:12:30.193: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:12:28.894 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:12:28.894 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:12:28.894 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:12:28.894 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  3 15:12:30.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-fwzqj redis-master --namespace=kubectl-2597 --tail=1'
Dec  3 15:12:30.382: INFO: stderr: ""
Dec  3 15:12:30.383: INFO: stdout: "1:M 03 Dec 15:12:28.894 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  3 15:12:30.383: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-fwzqj redis-master --namespace=kubectl-2597 --limit-bytes=1'
Dec  3 15:12:30.479: INFO: stderr: ""
Dec  3 15:12:30.479: INFO: stdout: " "
STEP: exposing timestamps
Dec  3 15:12:30.479: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-fwzqj redis-master --namespace=kubectl-2597 --tail=1 --timestamps'
Dec  3 15:12:30.628: INFO: stderr: ""
Dec  3 15:12:30.628: INFO: stdout: "2019-12-03T15:12:28.895060796Z 1:M 03 Dec 15:12:28.894 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  3 15:12:33.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-fwzqj redis-master --namespace=kubectl-2597 --since=1s'
Dec  3 15:12:33.276: INFO: stderr: ""
Dec  3 15:12:33.276: INFO: stdout: ""
Dec  3 15:12:33.276: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-fwzqj redis-master --namespace=kubectl-2597 --since=24h'
Dec  3 15:12:33.372: INFO: stderr: ""
Dec  3 15:12:33.372: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:12:28.894 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:12:28.894 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:12:28.894 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:12:28.894 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Dec  3 15:12:33.372: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2597'
Dec  3 15:12:33.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:12:33.518: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  3 15:12:33.518: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-2597'
Dec  3 15:12:33.610: INFO: stderr: "No resources found.\n"
Dec  3 15:12:33.610: INFO: stdout: ""
Dec  3 15:12:33.610: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-2597 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:12:33.742: INFO: stderr: ""
Dec  3 15:12:33.742: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:12:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2597" for this suite.
Dec  3 15:12:55.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:55.889: INFO: namespace kubectl-2597 deletion completed in 22.140827113s
•
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:12:55.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:12:56.042: INFO: Waiting up to 5m0s for pod "pod-625ca9f3-15df-11ea-9142-f2314dba9465" in namespace "emptydir-2102" to be "success or failure"
Dec  3 15:12:56.045: INFO: Pod "pod-625ca9f3-15df-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.76212ms
Dec  3 15:12:58.050: INFO: Pod "pod-625ca9f3-15df-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008411355s
STEP: Saw pod success
Dec  3 15:12:58.050: INFO: Pod "pod-625ca9f3-15df-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:12:58.054: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-625ca9f3-15df-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:12:58.074: INFO: Waiting for pod pod-625ca9f3-15df-11ea-9142-f2314dba9465 to disappear
Dec  3 15:12:58.077: INFO: Pod pod-625ca9f3-15df-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:12:58.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2102" for this suite.
Dec  3 15:13:04.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:13:04.238: INFO: namespace emptydir-2102 deletion completed in 6.155119928s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:13:04.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9462
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  3 15:13:04.391: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  3 15:13:11.434: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:13:11.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9462" for this suite.
Dec  3 15:13:17.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:13:17.602: INFO: namespace pods-9462 deletion completed in 6.160016041s
•SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:13:17.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:14:17.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1068" for this suite.
Dec  3 15:14:39.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:14:39.951: INFO: namespace container-probe-1068 deletion completed in 22.182646581s
•SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:14:39.951: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4491
Dec  3 15:14:42.111: INFO: Started pod liveness-exec in namespace container-probe-4491
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:14:42.115: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:18:42.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4491" for this suite.
Dec  3 15:18:48.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:18:48.868: INFO: namespace container-probe-4491 deletion completed in 6.144876366s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:18:48.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-651
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:18:49.015: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:19:13.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.71:8080/dial?request=hostName&protocol=http&host=100.64.1.70&port=8080&tries=1'] Namespace:pod-network-test-651 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:19:13.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:19:13.589: INFO: Waiting for endpoints: map[]
Dec  3 15:19:13.594: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.71:8080/dial?request=hostName&protocol=http&host=100.64.0.24&port=8080&tries=1'] Namespace:pod-network-test-651 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:19:13.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:19:13.978: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:19:13.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-651" for this suite.
Dec  3 15:19:36.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:19:36.149: INFO: namespace pod-network-test-651 deletion completed in 22.161330188s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:19:36.149: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:19:36.318: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 15:19:36.332: INFO: Number of nodes with available pods: 0
Dec  3 15:19:36.332: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 15:19:37.343: INFO: Number of nodes with available pods: 0
Dec  3 15:19:37.343: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 15:19:38.343: INFO: Number of nodes with available pods: 2
Dec  3 15:19:38.343: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  3 15:19:38.377: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:38.377: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:39.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:39.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:40.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:40.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:41.385: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:41.385: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:41.385: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:42.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:42.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:42.386: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:43.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:43.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:43.386: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:44.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:44.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:44.386: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:45.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:45.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:45.386: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:46.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:46.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:46.386: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:47.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:47.386: INFO: Wrong image for pod: daemon-set-s9c9t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:47.386: INFO: Pod daemon-set-s9c9t is not available
Dec  3 15:19:48.385: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:48.385: INFO: Pod daemon-set-vf49r is not available
Dec  3 15:19:49.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:49.386: INFO: Pod daemon-set-vf49r is not available
Dec  3 15:19:50.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:50.386: INFO: Pod daemon-set-vf49r is not available
Dec  3 15:19:51.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:52.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:52.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:53.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:53.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:54.385: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:54.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:55.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:55.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:56.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:56.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:57.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:57.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:58.387: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:58.387: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:19:59.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:19:59.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:20:00.386: INFO: Wrong image for pod: daemon-set-cjrn5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:20:00.386: INFO: Pod daemon-set-cjrn5 is not available
Dec  3 15:20:01.385: INFO: Pod daemon-set-vbjp7 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  3 15:20:01.400: INFO: Number of nodes with available pods: 1
Dec  3 15:20:01.400: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:20:02.411: INFO: Number of nodes with available pods: 1
Dec  3 15:20:02.411: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:20:03.411: INFO: Number of nodes with available pods: 2
Dec  3 15:20:03.411: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4343, will wait for the garbage collector to delete the pods
Dec  3 15:20:03.490: INFO: Deleting DaemonSet.extensions daemon-set took: 5.814258ms
Dec  3 15:20:03.590: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.368675ms
Dec  3 15:20:11.394: INFO: Number of nodes with available pods: 0
Dec  3 15:20:11.394: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:20:11.398: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4343/daemonsets","resourceVersion":"12851"},"items":null}

Dec  3 15:20:11.402: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4343/pods","resourceVersion":"12851"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:20:11.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4343" for this suite.
Dec  3 15:20:17.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:17.587: INFO: namespace daemonsets-4343 deletion completed in 6.167032504s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:20:17.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-69a321d6-15e0-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:20:17.749: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465" in namespace "projected-5660" to be "success or failure"
Dec  3 15:20:17.752: INFO: Pod "pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.704638ms
Dec  3 15:20:19.758: INFO: Pod "pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009871991s
STEP: Saw pod success
Dec  3 15:20:19.759: INFO: Pod "pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:20:19.762: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:20:19.784: INFO: Waiting for pod pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465 to disappear
Dec  3 15:20:19.787: INFO: Pod pod-projected-configmaps-69a3d76f-15e0-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:20:19.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5660" for this suite.
Dec  3 15:20:25.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:25.938: INFO: namespace projected-5660 deletion completed in 6.145043703s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:20:25.939: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Dec  3 15:20:26.085: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:20:26.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4786" for this suite.
Dec  3 15:20:32.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:32.313: INFO: namespace kubectl-4786 deletion completed in 6.149338566s
•SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:20:32.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1203 15:20:42.524761    5087 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:20:42.524: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:20:42.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9697" for this suite.
Dec  3 15:20:48.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:48.671: INFO: namespace gc-9697 deletion completed in 6.143056089s
•S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:20:48.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-544
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-7c29db82-15e0-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7c29db82-15e0-11ea-9142-f2314dba9465
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:20:52.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-544" for this suite.
Dec  3 15:21:14.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:15.106: INFO: namespace configmap-544 deletion completed in 22.143399576s
•S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:21:15.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4123/configmap-test-8beb230e-15e0-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:21:15.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465" in namespace "configmap-4123" to be "success or failure"
Dec  3 15:21:15.266: INFO: Pod "pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430035ms
Dec  3 15:21:17.271: INFO: Pod "pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008359734s
STEP: Saw pod success
Dec  3 15:21:17.271: INFO: Pod "pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:21:17.275: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465 container env-test: <nil>
STEP: delete the pod
Dec  3 15:21:17.298: INFO: Waiting for pod pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465 to disappear
Dec  3 15:21:17.301: INFO: Pod pod-configmaps-8bebcb25-15e0-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:21:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4123" for this suite.
Dec  3 15:21:23.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:23.474: INFO: namespace configmap-4123 deletion completed in 6.165823357s
•S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:21:23.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5241
Dec  3 15:21:27.639: INFO: Started pod liveness-http in namespace container-probe-5241
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:21:27.643: INFO: Initial restart count of pod liveness-http is 0
Dec  3 15:21:41.679: INFO: Restart count of pod container-probe-5241/liveness-http is now 1 (14.036409265s elapsed)
Dec  3 15:22:01.728: INFO: Restart count of pod container-probe-5241/liveness-http is now 2 (34.084890249s elapsed)
Dec  3 15:22:21.776: INFO: Restart count of pod container-probe-5241/liveness-http is now 3 (54.133223471s elapsed)
Dec  3 15:22:41.823: INFO: Restart count of pod container-probe-5241/liveness-http is now 4 (1m14.180008543s elapsed)
Dec  3 15:23:43.975: INFO: Restart count of pod container-probe-5241/liveness-http is now 5 (2m16.332118572s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:23:43.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5241" for this suite.
Dec  3 15:23:50.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:50.136: INFO: namespace container-probe-5241 deletion completed in 6.146895748s
•SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:23:50.136: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:23:50.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465" in namespace "downward-api-2411" to be "success or failure"
Dec  3 15:23:50.292: INFO: Pod "downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813169ms
Dec  3 15:23:52.297: INFO: Pod "downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00865708s
STEP: Saw pod success
Dec  3 15:23:52.297: INFO: Pod "downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:23:52.301: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:23:52.322: INFO: Waiting for pod downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465 to disappear
Dec  3 15:23:52.325: INFO: Pod downwardapi-volume-e852adcb-15e0-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:23:52.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2411" for this suite.
Dec  3 15:23:58.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:58.474: INFO: namespace downward-api-2411 deletion completed in 6.142739238s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:23:58.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-ed4aea2b-15e0-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:23:58.630: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465" in namespace "projected-8780" to be "success or failure"
Dec  3 15:23:58.633: INFO: Pod "pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.690957ms
Dec  3 15:24:00.638: INFO: Pod "pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00874844s
STEP: Saw pod success
Dec  3 15:24:00.638: INFO: Pod "pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:24:00.643: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:24:00.662: INFO: Waiting for pod pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465 to disappear
Dec  3 15:24:00.665: INFO: Pod pod-projected-secrets-ed4b85a5-15e0-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:00.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8780" for this suite.
Dec  3 15:24:06.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:06.813: INFO: namespace projected-8780 deletion completed in 6.141922806s
•
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:24:06.813: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f2432ff7-15e0-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:24:06.967: INFO: Waiting up to 5m0s for pod "pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465" in namespace "secrets-5683" to be "success or failure"
Dec  3 15:24:06.971: INFO: Pod "pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.833591ms
Dec  3 15:24:08.976: INFO: Pod "pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009222102s
STEP: Saw pod success
Dec  3 15:24:08.976: INFO: Pod "pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:24:08.980: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465 container secret-env-test: <nil>
STEP: delete the pod
Dec  3 15:24:09.011: INFO: Waiting for pod pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465 to disappear
Dec  3 15:24:09.014: INFO: Pod pod-secrets-f243d967-15e0-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:09.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5683" for this suite.
Dec  3 15:24:15.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:15.162: INFO: namespace secrets-5683 deletion completed in 6.141590438s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:24:15.162: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  3 15:24:15.327: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2620,SelfLink:/api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-label-changed,UID:f73d9204-15e0-11ea-a7d5-563937197a2b,ResourceVersion:13887,Generation:0,CreationTimestamp:2019-12-03 15:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:24:15.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2620,SelfLink:/api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-label-changed,UID:f73d9204-15e0-11ea-a7d5-563937197a2b,ResourceVersion:13888,Generation:0,CreationTimestamp:2019-12-03 15:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 15:24:15.327: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2620,SelfLink:/api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-label-changed,UID:f73d9204-15e0-11ea-a7d5-563937197a2b,ResourceVersion:13889,Generation:0,CreationTimestamp:2019-12-03 15:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  3 15:24:25.355: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2620,SelfLink:/api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-label-changed,UID:f73d9204-15e0-11ea-a7d5-563937197a2b,ResourceVersion:13922,Generation:0,CreationTimestamp:2019-12-03 15:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:24:25.355: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2620,SelfLink:/api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-label-changed,UID:f73d9204-15e0-11ea-a7d5-563937197a2b,ResourceVersion:13923,Generation:0,CreationTimestamp:2019-12-03 15:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  3 15:24:25.356: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2620,SelfLink:/api/v1/namespaces/watch-2620/configmaps/e2e-watch-test-label-changed,UID:f73d9204-15e0-11ea-a7d5-563937197a2b,ResourceVersion:13924,Generation:0,CreationTimestamp:2019-12-03 15:24:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:25.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2620" for this suite.
Dec  3 15:24:31.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:31.503: INFO: namespace watch-2620 deletion completed in 6.141908759s
•SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:24:31.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1542
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Dec  3 15:24:32.172: INFO: created pod pod-service-account-defaultsa
Dec  3 15:24:32.172: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  3 15:24:32.177: INFO: created pod pod-service-account-mountsa
Dec  3 15:24:32.177: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  3 15:24:32.181: INFO: created pod pod-service-account-nomountsa
Dec  3 15:24:32.181: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  3 15:24:32.185: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  3 15:24:32.185: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  3 15:24:32.190: INFO: created pod pod-service-account-mountsa-mountspec
Dec  3 15:24:32.190: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  3 15:24:32.195: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  3 15:24:32.195: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  3 15:24:32.199: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  3 15:24:32.199: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  3 15:24:32.203: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  3 15:24:32.203: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  3 15:24:32.208: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  3 15:24:32.208: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:32.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1542" for this suite.
Dec  3 15:24:38.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:38.402: INFO: namespace svcaccounts-1542 deletion completed in 6.187874727s
•SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:24:38.402: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 15:24:38.551: INFO: Waiting up to 5m0s for pod "pod-05172e1e-15e1-11ea-9142-f2314dba9465" in namespace "emptydir-6138" to be "success or failure"
Dec  3 15:24:38.554: INFO: Pod "pod-05172e1e-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.24594ms
Dec  3 15:24:40.559: INFO: Pod "pod-05172e1e-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007932801s
STEP: Saw pod success
Dec  3 15:24:40.559: INFO: Pod "pod-05172e1e-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:24:40.563: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-05172e1e-15e1-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:24:40.583: INFO: Waiting for pod pod-05172e1e-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:24:40.586: INFO: Pod pod-05172e1e-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:40.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6138" for this suite.
Dec  3 15:24:46.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:46.775: INFO: namespace emptydir-6138 deletion completed in 6.183393047s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:24:46.776: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Dec  3 15:24:46.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Dec  3 15:24:47.310: INFO: stderr: ""
Dec  3 15:24:47.310: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:47.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9953" for this suite.
Dec  3 15:24:53.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:53.463: INFO: namespace kubectl-9953 deletion completed in 6.146274306s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:24:53.464: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-0e11bd4c-15e1-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:24:53.619: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465" in namespace "projected-4898" to be "success or failure"
Dec  3 15:24:53.623: INFO: Pod "pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837ms
Dec  3 15:24:55.628: INFO: Pod "pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009060741s
STEP: Saw pod success
Dec  3 15:24:55.628: INFO: Pod "pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:24:55.633: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:24:55.653: INFO: Waiting for pod pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:24:55.657: INFO: Pod pod-projected-configmaps-0e126476-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:24:55.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4898" for this suite.
Dec  3 15:25:01.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:01.826: INFO: namespace projected-4898 deletion completed in 6.163092835s
•
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:25:01.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-lvpr
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:25:01.989: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lvpr" in namespace "subpath-1383" to be "success or failure"
Dec  3 15:25:01.993: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.59839ms
Dec  3 15:25:03.997: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008283485s
Dec  3 15:25:06.003: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 4.013605623s
Dec  3 15:25:08.007: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 6.018248432s
Dec  3 15:25:10.012: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 8.023092s
Dec  3 15:25:12.017: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 10.028191008s
Dec  3 15:25:14.022: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 12.033217229s
Dec  3 15:25:16.027: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 14.03810247s
Dec  3 15:25:18.032: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 16.042965207s
Dec  3 15:25:20.037: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 18.047856361s
Dec  3 15:25:22.043: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Running", Reason="", readiness=true. Elapsed: 20.054014923s
Dec  3 15:25:24.048: INFO: Pod "pod-subpath-test-secret-lvpr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059051201s
STEP: Saw pod success
Dec  3 15:25:24.048: INFO: Pod "pod-subpath-test-secret-lvpr" satisfied condition "success or failure"
Dec  3 15:25:24.052: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-subpath-test-secret-lvpr container test-container-subpath-secret-lvpr: <nil>
STEP: delete the pod
Dec  3 15:25:24.077: INFO: Waiting for pod pod-subpath-test-secret-lvpr to disappear
Dec  3 15:25:24.081: INFO: Pod pod-subpath-test-secret-lvpr no longer exists
STEP: Deleting pod pod-subpath-test-secret-lvpr
Dec  3 15:25:24.081: INFO: Deleting pod "pod-subpath-test-secret-lvpr" in namespace "subpath-1383"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:25:24.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1383" for this suite.
Dec  3 15:25:30.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:30.235: INFO: namespace subpath-1383 deletion completed in 6.144103368s
•SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:25:30.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6236
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-23fd2bc9-15e1-11ea-9142-f2314dba9465
STEP: Creating configMap with name cm-test-opt-upd-23fd2c6c-15e1-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-23fd2bc9-15e1-11ea-9142-f2314dba9465
STEP: Updating configmap cm-test-opt-upd-23fd2c6c-15e1-11ea-9142-f2314dba9465
STEP: Creating configMap with name cm-test-opt-create-23fd2c8d-15e1-11ea-9142-f2314dba9465
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:25:36.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6236" for this suite.
Dec  3 15:25:58.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:59.000: INFO: namespace configmap-6236 deletion completed in 22.182609432s
•SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:25:59.000: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:25:59.145: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Dec  3 15:25:59.250: INFO: stderr: ""
Dec  3 15:25:59.250: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.9\", GitCommit:\"500f5aba80d71253cc01ac6a8622b8377f4a7ef9\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:21:43Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.9\", GitCommit:\"500f5aba80d71253cc01ac6a8622b8377f4a7ef9\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:13:04Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:25:59.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6538" for this suite.
Dec  3 15:26:05.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:05.402: INFO: namespace kubectl-6538 deletion completed in 6.147428776s
•SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:26:05.402: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1664
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2633
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:26:11.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4379" for this suite.
Dec  3 15:26:17.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:18.000: INFO: namespace namespaces-4379 deletion completed in 6.14318873s
STEP: Destroying namespace "nsdeletetest-1664" for this suite.
Dec  3 15:26:18.004: INFO: Namespace nsdeletetest-1664 was already deleted
STEP: Destroying namespace "nsdeletetest-2633" for this suite.
Dec  3 15:26:24.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:24.181: INFO: namespace nsdeletetest-2633 deletion completed in 6.177505559s
•SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:26:24.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  3 15:26:24.334: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:26:24.342: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:26:24.345: INFO: 
Logging pods the kubelet thinks is on node izgw8afzp8040eosj1udh4z before test
Dec  3 15:26:24.374: INFO: kube-proxy-n9znm from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.374: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:26:24.374: INFO: node-exporter-lx8zq from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:26:24.375: INFO: calico-kube-controllers-54d8589c95-vcpcz from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 15:26:24.375: INFO: addons-nginx-ingress-controller-58b8956897-thc29 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:26:24.375: INFO: calico-typha-vertical-autoscaler-f947784fd-qx7gr from kube-system started at 2019-12-03 14:27:29 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container autoscaler ready: true, restart count 3
Dec  3 15:26:24.375: INFO: vpn-shoot-6b8f7bf8ff-29lv8 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:26:24.375: INFO: metrics-server-84dd8df646-cnl7w from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:26:24.375: INFO: csi-disk-plugin-alicloud-r68mx from kube-system started at 2019-12-03 14:27:28 +0000 UTC (2 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  3 15:26:24.375: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  3 15:26:24.375: INFO: blackbox-exporter-597fdccb7d-kh2w7 from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:26:24.375: INFO: node-problem-detector-cw56z from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:26:24.375: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-768dbf555c-kc8zf from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:26:24.375: INFO: coredns-647b89dbf-l75g2 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:26:24.375: INFO: calico-typha-horizontal-autoscaler-7567d6945d-h2gzq from kube-system started at 2019-12-03 14:27:29 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:26:24.375: INFO: calico-node-ldhgc from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:26:24.375: INFO: coredns-647b89dbf-fb7p4 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:26:24.375: INFO: addons-kubernetes-dashboard-9497cfc54-v5l9m from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.375: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:26:24.375: INFO: 
Logging pods the kubelet thinks is on node izgw8jactz8ahkwwzt6d3gz before test
Dec  3 15:26:24.424: INFO: node-exporter-9ctx5 from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.424: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:26:24.424: INFO: kube-proxy-llwcl from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.424: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:26:24.424: INFO: node-problem-detector-j4bdz from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.424: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:26:24.424: INFO: csi-disk-plugin-alicloud-8p2lk from kube-system started at 2019-12-03 14:27:31 +0000 UTC (2 container statuses recorded)
Dec  3 15:26:24.424: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  3 15:26:24.424: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  3 15:26:24.424: INFO: calico-node-bjs4g from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.424: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:26:24.424: INFO: calico-typha-deploy-f7f8bff5f-86h5q from kube-system started at 2019-12-03 14:30:22 +0000 UTC (1 container statuses recorded)
Dec  3 15:26:24.424: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node izgw8afzp8040eosj1udh4z
STEP: verifying the node has the label node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod addons-kubernetes-dashboard-9497cfc54-v5l9m requesting resource cpu=50m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod addons-nginx-ingress-controller-58b8956897-thc29 requesting resource cpu=100m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-768dbf555c-kc8zf requesting resource cpu=0m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod blackbox-exporter-597fdccb7d-kh2w7 requesting resource cpu=5m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod calico-kube-controllers-54d8589c95-vcpcz requesting resource cpu=0m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod calico-node-bjs4g requesting resource cpu=100m on Node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod calico-node-ldhgc requesting resource cpu=100m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod calico-typha-deploy-f7f8bff5f-86h5q requesting resource cpu=0m on Node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod calico-typha-horizontal-autoscaler-7567d6945d-h2gzq requesting resource cpu=10m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod calico-typha-vertical-autoscaler-f947784fd-qx7gr requesting resource cpu=0m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod coredns-647b89dbf-fb7p4 requesting resource cpu=50m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod coredns-647b89dbf-l75g2 requesting resource cpu=50m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod csi-disk-plugin-alicloud-8p2lk requesting resource cpu=0m on Node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod csi-disk-plugin-alicloud-r68mx requesting resource cpu=0m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod kube-proxy-llwcl requesting resource cpu=20m on Node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod kube-proxy-n9znm requesting resource cpu=20m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod metrics-server-84dd8df646-cnl7w requesting resource cpu=20m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod node-exporter-9ctx5 requesting resource cpu=5m on Node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod node-exporter-lx8zq requesting resource cpu=5m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod node-problem-detector-cw56z requesting resource cpu=20m on Node izgw8afzp8040eosj1udh4z
Dec  3 15:26:24.462: INFO: Pod node-problem-detector-j4bdz requesting resource cpu=20m on Node izgw8jactz8ahkwwzt6d3gz
Dec  3 15:26:24.462: INFO: Pod vpn-shoot-6b8f7bf8ff-29lv8 requesting resource cpu=100m on Node izgw8afzp8040eosj1udh4z
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465.15dce5ef083d05fd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2752/filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465 to izgw8afzp8040eosj1udh4z]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465.15dce5ef353c6f9d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465.15dce5ef576cc50d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465.15dce5ef5a57140d], Reason = [Created], Message = [Created container filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465.15dce5ef64a230c9], Reason = [Started], Message = [Started container filler-pod-4438f4a0-15e1-11ea-9142-f2314dba9465]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465.15dce5ef08842fb1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2752/filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465 to izgw8jactz8ahkwwzt6d3gz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465.15dce5ef31819f17], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465.15dce5ef3429daa6], Reason = [Created], Message = [Created container filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465.15dce5ef3b1cbc87], Reason = [Started], Message = [Started container filler-pod-4439ecd5-15e1-11ea-9142-f2314dba9465]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dce5ef811865c6], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node izgw8afzp8040eosj1udh4z
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node izgw8jactz8ahkwwzt6d3gz
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:26:27.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2752" for this suite.
Dec  3 15:26:33.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:33.683: INFO: namespace sched-pred-2752 deletion completed in 6.147831931s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:26:33.684: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Dec  3 15:26:34.302: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  3 15:26:36.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:26:38.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:26:40.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983594, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:26:43.754: INFO: Waited 1.400712438s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:26:44.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1620" for this suite.
Dec  3 15:26:50.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:50.593: INFO: namespace aggregator-1620 deletion completed in 6.172976685s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:26:50.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Dec  3 15:26:50.740: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix643692489/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:26:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9971" for this suite.
Dec  3 15:26:56.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:56.945: INFO: namespace kubectl-9971 deletion completed in 6.145748576s
•S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:26:56.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7714
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-57ab6b33-15e1-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:26:57.101: INFO: Waiting up to 5m0s for pod "pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465" in namespace "secrets-7714" to be "success or failure"
Dec  3 15:26:57.105: INFO: Pod "pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042942ms
Dec  3 15:26:59.110: INFO: Pod "pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008888349s
STEP: Saw pod success
Dec  3 15:26:59.110: INFO: Pod "pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:26:59.114: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:26:59.133: INFO: Waiting for pod pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:26:59.137: INFO: Pod pod-secrets-57ac1af6-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:26:59.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7714" for this suite.
Dec  3 15:27:05.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:05.289: INFO: namespace secrets-7714 deletion completed in 6.144680265s
•SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:27:05.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:27:05.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465" in namespace "projected-4551" to be "success or failure"
Dec  3 15:27:05.444: INFO: Pod "downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006364ms
Dec  3 15:27:07.449: INFO: Pod "downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008971485s
STEP: Saw pod success
Dec  3 15:27:07.449: INFO: Pod "downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:27:07.453: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:27:07.472: INFO: Waiting for pod downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:27:07.476: INFO: Pod downwardapi-volume-5ca49672-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:27:07.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4551" for this suite.
Dec  3 15:27:13.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:13.627: INFO: namespace projected-4551 deletion completed in 6.144745918s
•SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:27:13.627: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Dec  3 15:27:13.786: INFO: Waiting up to 5m0s for pod "client-containers-619e0b04-15e1-11ea-9142-f2314dba9465" in namespace "containers-6458" to be "success or failure"
Dec  3 15:27:13.790: INFO: Pod "client-containers-619e0b04-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079896ms
Dec  3 15:27:15.795: INFO: Pod "client-containers-619e0b04-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009038596s
Dec  3 15:27:17.800: INFO: Pod "client-containers-619e0b04-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0137572s
STEP: Saw pod success
Dec  3 15:27:17.800: INFO: Pod "client-containers-619e0b04-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:27:17.803: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod client-containers-619e0b04-15e1-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:27:17.825: INFO: Waiting for pod client-containers-619e0b04-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:27:17.828: INFO: Pod client-containers-619e0b04-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:27:17.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6458" for this suite.
Dec  3 15:27:23.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:23.975: INFO: namespace containers-6458 deletion completed in 6.139957696s
•SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:27:23.975: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6646
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-67c8ac64-15e1-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:27:26.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6646" for this suite.
Dec  3 15:27:44.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:44.401: INFO: namespace configmap-6646 deletion completed in 18.144169028s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:27:44.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-73f48628-15e1-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:27:44.555: INFO: Waiting up to 5m0s for pod "pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465" in namespace "secrets-9398" to be "success or failure"
Dec  3 15:27:44.559: INFO: Pod "pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.714588ms
Dec  3 15:27:46.564: INFO: Pod "pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008748517s
STEP: Saw pod success
Dec  3 15:27:46.564: INFO: Pod "pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:27:46.568: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:27:46.588: INFO: Waiting for pod pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:27:46.591: INFO: Pod pod-secrets-73f5282a-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:27:46.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9398" for this suite.
Dec  3 15:27:52.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:52.738: INFO: namespace secrets-9398 deletion completed in 6.141325163s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:27:52.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:27:52.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465" in namespace "downward-api-4044" to be "success or failure"
Dec  3 15:27:52.893: INFO: Pod "downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603386ms
Dec  3 15:27:54.898: INFO: Pod "downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008592831s
STEP: Saw pod success
Dec  3 15:27:54.898: INFO: Pod "downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:27:54.902: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:27:54.923: INFO: Waiting for pod downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:27:54.926: INFO: Pod downwardapi-volume-78ecc7fa-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:27:54.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4044" for this suite.
Dec  3 15:28:00.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:01.117: INFO: namespace downward-api-4044 deletion completed in 6.184935025s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:28:01.117: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-8916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Dec  3 15:28:01.270: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8916" to be "success or failure"
Dec  3 15:28:01.275: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712401ms
Dec  3 15:28:03.279: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009417039s
STEP: Saw pod success
Dec  3 15:28:03.279: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  3 15:28:03.283: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  3 15:28:03.331: INFO: Waiting for pod pod-host-path-test to disappear
Dec  3 15:28:03.334: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:28:03.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8916" for this suite.
Dec  3 15:28:09.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:09.485: INFO: namespace hostpath-8916 deletion completed in 6.144795558s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:28:09.485: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-98
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 15:28:09.659: INFO: Number of nodes with available pods: 0
Dec  3 15:28:09.660: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 15:28:10.671: INFO: Number of nodes with available pods: 0
Dec  3 15:28:10.671: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 15:28:11.670: INFO: Number of nodes with available pods: 2
Dec  3 15:28:11.670: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  3 15:28:11.689: INFO: Number of nodes with available pods: 1
Dec  3 15:28:11.689: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:28:12.700: INFO: Number of nodes with available pods: 1
Dec  3 15:28:12.700: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:28:13.700: INFO: Number of nodes with available pods: 1
Dec  3 15:28:13.700: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:28:14.700: INFO: Number of nodes with available pods: 1
Dec  3 15:28:14.701: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:28:15.701: INFO: Number of nodes with available pods: 1
Dec  3 15:28:15.701: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:28:16.699: INFO: Number of nodes with available pods: 2
Dec  3 15:28:16.700: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-98, will wait for the garbage collector to delete the pods
Dec  3 15:28:16.764: INFO: Deleting DaemonSet.extensions daemon-set took: 6.831029ms
Dec  3 15:28:16.864: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.273733ms
Dec  3 15:28:31.368: INFO: Number of nodes with available pods: 0
Dec  3 15:28:31.368: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:28:31.372: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-98/daemonsets","resourceVersion":"15197"},"items":null}

Dec  3 15:28:31.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-98/pods","resourceVersion":"15197"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:28:31.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-98" for this suite.
Dec  3 15:28:37.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:37.537: INFO: namespace daemonsets-98 deletion completed in 6.143874642s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:28:37.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:28:41.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6060" for this suite.
Dec  3 15:28:47.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:47.850: INFO: namespace kubelet-test-6060 deletion completed in 6.147297866s
•SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:28:47.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  3 15:28:48.001: INFO: Waiting up to 5m0s for pod "downward-api-99c619fa-15e1-11ea-9142-f2314dba9465" in namespace "downward-api-6851" to be "success or failure"
Dec  3 15:28:48.004: INFO: Pod "downward-api-99c619fa-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477704ms
Dec  3 15:28:50.009: INFO: Pod "downward-api-99c619fa-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008361707s
STEP: Saw pod success
Dec  3 15:28:50.009: INFO: Pod "downward-api-99c619fa-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:28:50.013: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downward-api-99c619fa-15e1-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:28:50.034: INFO: Waiting for pod downward-api-99c619fa-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:28:50.038: INFO: Pod downward-api-99c619fa-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:28:50.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6851" for this suite.
Dec  3 15:28:56.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:56.194: INFO: namespace downward-api-6851 deletion completed in 6.1439081s
•SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:28:56.194: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 15:28:56.345: INFO: Waiting up to 5m0s for pod "pod-9ebf43f7-15e1-11ea-9142-f2314dba9465" in namespace "emptydir-6502" to be "success or failure"
Dec  3 15:28:56.348: INFO: Pod "pod-9ebf43f7-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803009ms
Dec  3 15:28:58.353: INFO: Pod "pod-9ebf43f7-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008708733s
STEP: Saw pod success
Dec  3 15:28:58.353: INFO: Pod "pod-9ebf43f7-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:28:58.357: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-9ebf43f7-15e1-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:28:58.377: INFO: Waiting for pod pod-9ebf43f7-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:28:58.380: INFO: Pod pod-9ebf43f7-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:28:58.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6502" for this suite.
Dec  3 15:29:04.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:04.531: INFO: namespace emptydir-6502 deletion completed in 6.145263003s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:29:04.531: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:29:04.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465" in namespace "projected-782" to be "success or failure"
Dec  3 15:29:04.686: INFO: Pod "downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.775269ms
Dec  3 15:29:06.691: INFO: Pod "downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008885337s
STEP: Saw pod success
Dec  3 15:29:06.691: INFO: Pod "downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:29:06.695: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:29:06.715: INFO: Waiting for pod downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:29:06.718: INFO: Pod downwardapi-volume-a3b76d7d-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:29:06.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-782" for this suite.
Dec  3 15:29:12.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:12.875: INFO: namespace projected-782 deletion completed in 6.150232773s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:29:12.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1925
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Dec  3 15:29:13.031: INFO: Found 0 stateful pods, waiting for 3
Dec  3 15:29:23.036: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:29:23.036: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:29:23.036: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  3 15:29:23.066: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  3 15:29:33.101: INFO: Updating stateful set ss2
Dec  3 15:29:33.109: INFO: Waiting for Pod statefulset-1925/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec  3 15:29:43.136: INFO: Found 2 stateful pods, waiting for 3
Dec  3 15:29:53.141: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:29:53.141: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:29:53.141: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  3 15:29:53.167: INFO: Updating stateful set ss2
Dec  3 15:29:53.175: INFO: Waiting for Pod statefulset-1925/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:30:03.203: INFO: Updating stateful set ss2
Dec  3 15:30:03.214: INFO: Waiting for StatefulSet statefulset-1925/ss2 to complete update
Dec  3 15:30:03.214: INFO: Waiting for Pod statefulset-1925/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:30:13.224: INFO: Waiting for StatefulSet statefulset-1925/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  3 15:30:23.224: INFO: Deleting all statefulset in ns statefulset-1925
Dec  3 15:30:23.231: INFO: Scaling statefulset ss2 to 0
Dec  3 15:30:53.249: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:30:53.253: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:30:53.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1925" for this suite.
Dec  3 15:30:59.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:59.411: INFO: namespace statefulset-1925 deletion completed in 6.139752688s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:30:59.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-77
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 15:30:59.563: INFO: Waiting up to 5m0s for pod "pod-e830d666-15e1-11ea-9142-f2314dba9465" in namespace "emptydir-77" to be "success or failure"
Dec  3 15:30:59.566: INFO: Pod "pod-e830d666-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.540718ms
Dec  3 15:31:01.571: INFO: Pod "pod-e830d666-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00843803s
STEP: Saw pod success
Dec  3 15:31:01.571: INFO: Pod "pod-e830d666-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:31:01.575: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-e830d666-15e1-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:31:01.598: INFO: Waiting for pod pod-e830d666-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:31:01.601: INFO: Pod pod-e830d666-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:31:01.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-77" for this suite.
Dec  3 15:31:07.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:07.750: INFO: namespace emptydir-77 deletion completed in 6.141661415s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:31:07.750: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ed2957fb-15e1-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:31:07.906: INFO: Waiting up to 5m0s for pod "pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465" in namespace "secrets-9076" to be "success or failure"
Dec  3 15:31:07.909: INFO: Pod "pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477071ms
Dec  3 15:31:09.915: INFO: Pod "pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008595158s
STEP: Saw pod success
Dec  3 15:31:09.915: INFO: Pod "pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:31:09.919: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:31:09.938: INFO: Waiting for pod pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:31:09.941: INFO: Pod pod-secrets-ed2a0566-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:31:09.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9076" for this suite.
Dec  3 15:31:15.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:16.096: INFO: namespace secrets-9076 deletion completed in 6.148013072s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:31:16.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:31:16.242: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  3 15:31:16.251: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  3 15:31:21.255: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 15:31:21.255: INFO: Creating deployment "test-rolling-update-deployment"
Dec  3 15:31:21.260: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  3 15:31:21.268: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  3 15:31:23.276: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  3 15:31:23.280: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  3 15:31:23.292: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9355,SelfLink:/apis/apps/v1/namespaces/deployment-9355/deployments/test-rolling-update-deployment,UID:f5200c46-15e1-11ea-a7d5-563937197a2b,ResourceVersion:16036,Generation:1,CreationTimestamp:2019-12-03 15:31:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 15:31:21 +0000 UTC 2019-12-03 15:31:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 15:31:22 +0000 UTC 2019-12-03 15:31:21 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 15:31:23.296: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-9355,SelfLink:/apis/apps/v1/namespaces/deployment-9355/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:f52155f6-15e1-11ea-a7d5-563937197a2b,ResourceVersion:16029,Generation:1,CreationTimestamp:2019-12-03 15:31:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f5200c46-15e1-11ea-a7d5-563937197a2b 0xc0015c8f57 0xc0015c8f58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 15:31:23.296: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  3 15:31:23.296: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9355,SelfLink:/apis/apps/v1/namespaces/deployment-9355/replicasets/test-rolling-update-controller,UID:f2231b44-15e1-11ea-a7d5-563937197a2b,ResourceVersion:16035,Generation:2,CreationTimestamp:2019-12-03 15:31:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f5200c46-15e1-11ea-a7d5-563937197a2b 0xc0015c8e87 0xc0015c8e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:31:23.300: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-gbbqj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-gbbqj,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-9355,SelfLink:/api/v1/namespaces/deployment-9355/pods/test-rolling-update-deployment-57b6b5bb54-gbbqj,UID:f521bb8d-15e1-11ea-a7d5-563937197a2b,ResourceVersion:16028,Generation:0,CreationTimestamp:2019-12-03 15:31:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.124/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 f52155f6-15e1-11ea-a7d5-563937197a2b 0xc0015c9827 0xc0015c9828}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c8g4w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c8g4w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-c8g4w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0015c9890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0015c98b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:31:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:31:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:31:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:31:21 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.124,StartTime:2019-12-03 15:31:21 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 15:31:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6fe26ca24348f12fc22ec0cac96e98529976c48ca0516722baaf973ec8fde84a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:31:23.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9355" for this suite.
Dec  3 15:31:29.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:29.449: INFO: namespace deployment-9355 deletion completed in 6.142960236s
•SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:31:29.449: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Dec  3 15:31:29.602: INFO: Waiting up to 5m0s for pod "client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465" in namespace "containers-1407" to be "success or failure"
Dec  3 15:31:29.605: INFO: Pod "client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.367456ms
Dec  3 15:31:31.610: INFO: Pod "client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008077288s
STEP: Saw pod success
Dec  3 15:31:31.610: INFO: Pod "client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:31:31.614: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:31:31.634: INFO: Waiting for pod client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465 to disappear
Dec  3 15:31:31.638: INFO: Pod client-containers-fa188d4e-15e1-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:31:31.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1407" for this suite.
Dec  3 15:31:37.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:37.794: INFO: namespace containers-1407 deletion completed in 6.15055451s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:31:37.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Dec  3 15:31:39.959: INFO: Pod pod-hostip-ff11769c-15e1-11ea-9142-f2314dba9465 has hostIP: 10.250.16.152
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:31:39.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8393" for this suite.
Dec  3 15:32:01.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:02.107: INFO: namespace pods-8393 deletion completed in 22.141865114s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:32:02.108: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:32:02.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465" in namespace "projected-6102" to be "success or failure"
Dec  3 15:32:02.265: INFO: Pod "downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699755ms
Dec  3 15:32:04.270: INFO: Pod "downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008556676s
STEP: Saw pod success
Dec  3 15:32:04.270: INFO: Pod "downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:32:04.274: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:32:04.293: INFO: Waiting for pod downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465 to disappear
Dec  3 15:32:04.296: INFO: Pod downwardapi-volume-0d8fd2d8-15e2-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:32:04.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6102" for this suite.
Dec  3 15:32:10.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:10.442: INFO: namespace projected-6102 deletion completed in 6.139639384s
•SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:32:10.442: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9825
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:32:10.586: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:32:28.656: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.35 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9825 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:32:28.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:32:30.123: INFO: Found all expected endpoints: [netserver-0]
Dec  3 15:32:30.127: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.128 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9825 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:32:30.127: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:32:31.587: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:32:31.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9825" for this suite.
Dec  3 15:32:53.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:53.738: INFO: namespace pod-network-test-9825 deletion completed in 22.145005525s
•S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:32:53.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  3 15:32:53.884: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:32:57.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9368" for this suite.
Dec  3 15:33:20.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:20.142: INFO: namespace init-container-9368 deletion completed in 22.15114028s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:33:20.143: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5040
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-3c133bf7-15e2-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:33:20.302: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465" in namespace "projected-5040" to be "success or failure"
Dec  3 15:33:20.306: INFO: Pod "pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764035ms
Dec  3 15:33:22.311: INFO: Pod "pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008404882s
STEP: Saw pod success
Dec  3 15:33:22.311: INFO: Pod "pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:33:22.314: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:33:22.336: INFO: Waiting for pod pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465 to disappear
Dec  3 15:33:22.339: INFO: Pod pod-projected-configmaps-3c13e41d-15e2-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:33:22.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5040" for this suite.
Dec  3 15:33:28.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:28.490: INFO: namespace projected-5040 deletion completed in 6.144779809s
•SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:33:28.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9994
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  3 15:33:28.641: INFO: Waiting up to 5m0s for pod "pod-410c69a4-15e2-11ea-9142-f2314dba9465" in namespace "emptydir-9994" to be "success or failure"
Dec  3 15:33:28.646: INFO: Pod "pod-410c69a4-15e2-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.432424ms
Dec  3 15:33:30.650: INFO: Pod "pod-410c69a4-15e2-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008973413s
STEP: Saw pod success
Dec  3 15:33:30.650: INFO: Pod "pod-410c69a4-15e2-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:33:30.654: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-410c69a4-15e2-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:33:30.675: INFO: Waiting for pod pod-410c69a4-15e2-11ea-9142-f2314dba9465 to disappear
Dec  3 15:33:30.678: INFO: Pod pod-410c69a4-15e2-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:33:30.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9994" for this suite.
Dec  3 15:33:36.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:36.875: INFO: namespace emptydir-9994 deletion completed in 6.191132149s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:33:36.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 15:33:41.062: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:41.066: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:43.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:43.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:45.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:45.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:47.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:47.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:49.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:49.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:51.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:51.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:53.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:53.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:55.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:55.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:57.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:57.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:33:59.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:33:59.071: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:34:01.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:34:01.071: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:34:01.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3859" for this suite.
Dec  3 15:34:23.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:23.274: INFO: namespace container-lifecycle-hook-3859 deletion completed in 22.184130485s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:34:23.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Dec  3 15:34:23.420: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Dec  3 15:34:23.591: INFO: stderr: ""
Dec  3 15:34:23.591: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:34:23.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3183" for this suite.
Dec  3 15:34:29.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:29.742: INFO: namespace kubectl-3183 deletion completed in 6.145658584s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:34:29.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  3 15:34:29.885: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:34:29.894: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:34:29.897: INFO: 
Logging pods the kubelet thinks is on node izgw8afzp8040eosj1udh4z before test
Dec  3 15:34:29.923: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-768dbf555c-kc8zf from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:34:29.923: INFO: coredns-647b89dbf-l75g2 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:34:29.923: INFO: calico-typha-horizontal-autoscaler-7567d6945d-h2gzq from kube-system started at 2019-12-03 14:27:29 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:34:29.923: INFO: blackbox-exporter-597fdccb7d-kh2w7 from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:34:29.923: INFO: node-problem-detector-cw56z from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:34:29.923: INFO: calico-node-ldhgc from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:34:29.923: INFO: coredns-647b89dbf-fb7p4 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:34:29.923: INFO: addons-kubernetes-dashboard-9497cfc54-v5l9m from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:34:29.923: INFO: addons-nginx-ingress-controller-58b8956897-thc29 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:34:29.923: INFO: calico-typha-vertical-autoscaler-f947784fd-qx7gr from kube-system started at 2019-12-03 14:27:29 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container autoscaler ready: true, restart count 3
Dec  3 15:34:29.923: INFO: kube-proxy-n9znm from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:34:29.923: INFO: node-exporter-lx8zq from kube-system started at 2019-12-03 14:26:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:34:29.923: INFO: calico-kube-controllers-54d8589c95-vcpcz from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 15:34:29.923: INFO: vpn-shoot-6b8f7bf8ff-29lv8 from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:34:29.923: INFO: metrics-server-84dd8df646-cnl7w from kube-system started at 2019-12-03 14:27:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:34:29.923: INFO: csi-disk-plugin-alicloud-r68mx from kube-system started at 2019-12-03 14:27:28 +0000 UTC (2 container statuses recorded)
Dec  3 15:34:29.923: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  3 15:34:29.923: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  3 15:34:29.923: INFO: 
Logging pods the kubelet thinks is on node izgw8jactz8ahkwwzt6d3gz before test
Dec  3 15:34:29.948: INFO: kube-proxy-llwcl from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.948: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:34:29.948: INFO: node-problem-detector-j4bdz from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.948: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:34:29.948: INFO: csi-disk-plugin-alicloud-8p2lk from kube-system started at 2019-12-03 14:27:31 +0000 UTC (2 container statuses recorded)
Dec  3 15:34:29.948: INFO: 	Container csi-diskplugin ready: true, restart count 0
Dec  3 15:34:29.948: INFO: 	Container driver-registrar ready: true, restart count 0
Dec  3 15:34:29.948: INFO: calico-node-bjs4g from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.948: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:34:29.948: INFO: calico-typha-deploy-f7f8bff5f-86h5q from kube-system started at 2019-12-03 14:30:22 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.948: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:34:29.948: INFO: node-exporter-9ctx5 from kube-system started at 2019-12-03 14:26:51 +0000 UTC (1 container statuses recorded)
Dec  3 15:34:29.948: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dce6601227b4ae], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:34:30.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4005" for this suite.
Dec  3 15:34:36.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:37.125: INFO: namespace sched-pred-4005 deletion completed in 6.144881012s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:34:37.125: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-69f52937-15e2-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:34:37.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465" in namespace "configmap-1800" to be "success or failure"
Dec  3 15:34:37.283: INFO: Pod "pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68281ms
Dec  3 15:34:39.288: INFO: Pod "pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008405149s
STEP: Saw pod success
Dec  3 15:34:39.288: INFO: Pod "pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:34:39.292: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:34:39.312: INFO: Waiting for pod pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465 to disappear
Dec  3 15:34:39.316: INFO: Pod pod-configmaps-69f5e1c3-15e2-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:34:39.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1800" for this suite.
Dec  3 15:34:45.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:45.468: INFO: namespace configmap-1800 deletion completed in 6.145987197s
•SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:34:45.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-37
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:34:45.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:34:47.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-37" for this suite.
Dec  3 15:35:31.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:32.047: INFO: namespace pods-37 deletion completed in 44.151420423s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:35:32.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:35:34.728: INFO: Successfully updated pod "pod-update-8ab21512-15e2-11ea-9142-f2314dba9465"
STEP: verifying the updated pod is in kubernetes
Dec  3 15:35:34.736: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:35:34.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3736" for this suite.
Dec  3 15:35:56.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:56.881: INFO: namespace pods-3736 deletion completed in 22.140048882s
•SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:35:56.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4232
Dec  3 15:35:59.041: INFO: Started pod liveness-http in namespace container-probe-4232
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:35:59.045: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:39:59.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4232" for this suite.
Dec  3 15:40:05.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:05.829: INFO: namespace container-probe-4232 deletion completed in 6.199642603s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:40:05.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465
Dec  3 15:40:05.982: INFO: Pod name my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465: Found 0 pods out of 1
Dec  3 15:40:10.987: INFO: Pod name my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465: Found 1 pods out of 1
Dec  3 15:40:10.987: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465" are running
Dec  3 15:40:10.991: INFO: Pod "my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465-6p29p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:40:05 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:40:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:40:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:40:05 +0000 UTC Reason: Message:}])
Dec  3 15:40:10.991: INFO: Trying to dial the pod
Dec  3 15:40:16.093: INFO: Controller my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465: Got expected result from replica 1 [my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465-6p29p]: "my-hostname-basic-2de1a562-15e3-11ea-9142-f2314dba9465-6p29p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:40:16.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9176" for this suite.
Dec  3 15:40:22.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:22.253: INFO: namespace replication-controller-9176 deletion completed in 6.153329905s
•SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:40:22.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  3 15:40:22.407: INFO: Waiting up to 5m0s for pod "downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465" in namespace "downward-api-9608" to be "success or failure"
Dec  3 15:40:22.411: INFO: Pod "downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.561444ms
Dec  3 15:40:24.415: INFO: Pod "downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008242187s
STEP: Saw pod success
Dec  3 15:40:24.415: INFO: Pod "downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:40:24.420: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:40:24.443: INFO: Waiting for pod downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465 to disappear
Dec  3 15:40:24.447: INFO: Pod downward-api-37ac18e1-15e3-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:40:24.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9608" for this suite.
Dec  3 15:40:30.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:30.597: INFO: namespace downward-api-9608 deletion completed in 6.143887635s
•SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:40:30.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-3ca4f0bc-15e3-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:40:30.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465" in namespace "configmap-8331" to be "success or failure"
Dec  3 15:40:30.756: INFO: Pod "pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.577201ms
Dec  3 15:40:32.761: INFO: Pod "pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008694103s
STEP: Saw pod success
Dec  3 15:40:32.761: INFO: Pod "pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:40:32.765: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:40:32.787: INFO: Waiting for pod pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465 to disappear
Dec  3 15:40:32.790: INFO: Pod pod-configmaps-3ca59acc-15e3-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:40:32.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8331" for this suite.
Dec  3 15:40:38.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:38.937: INFO: namespace configmap-8331 deletion completed in 6.140831164s
•SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:40:38.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1355
I1203 15:40:39.087153    5087 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1355, replica count: 1
I1203 15:40:40.137660    5087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:40:41.137995    5087 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:40:41.246: INFO: Created: latency-svc-cd4x8
Dec  3 15:40:41.249: INFO: Got endpoints: latency-svc-cd4x8 [11.131259ms]
Dec  3 15:40:41.257: INFO: Created: latency-svc-vvx94
Dec  3 15:40:41.262: INFO: Got endpoints: latency-svc-vvx94 [12.351029ms]
Dec  3 15:40:41.263: INFO: Created: latency-svc-4vz69
Dec  3 15:40:41.265: INFO: Got endpoints: latency-svc-4vz69 [15.734701ms]
Dec  3 15:40:41.266: INFO: Created: latency-svc-c5hkk
Dec  3 15:40:41.268: INFO: Got endpoints: latency-svc-c5hkk [19.249343ms]
Dec  3 15:40:41.269: INFO: Created: latency-svc-fpxsc
Dec  3 15:40:41.270: INFO: Got endpoints: latency-svc-fpxsc [21.12114ms]
Dec  3 15:40:41.273: INFO: Created: latency-svc-9gd94
Dec  3 15:40:41.274: INFO: Got endpoints: latency-svc-9gd94 [24.946016ms]
Dec  3 15:40:41.276: INFO: Created: latency-svc-ftjmj
Dec  3 15:40:41.278: INFO: Got endpoints: latency-svc-ftjmj [28.974154ms]
Dec  3 15:40:41.281: INFO: Created: latency-svc-ttpck
Dec  3 15:40:41.283: INFO: Got endpoints: latency-svc-ttpck [33.198563ms]
Dec  3 15:40:41.285: INFO: Created: latency-svc-fb9xc
Dec  3 15:40:41.288: INFO: Got endpoints: latency-svc-fb9xc [38.283139ms]
Dec  3 15:40:41.288: INFO: Created: latency-svc-rxn99
Dec  3 15:40:41.289: INFO: Got endpoints: latency-svc-rxn99 [39.733819ms]
Dec  3 15:40:41.295: INFO: Created: latency-svc-zjmqb
Dec  3 15:40:41.297: INFO: Got endpoints: latency-svc-zjmqb [47.647006ms]
Dec  3 15:40:41.299: INFO: Created: latency-svc-fgmhd
Dec  3 15:40:41.301: INFO: Got endpoints: latency-svc-fgmhd [51.382616ms]
Dec  3 15:40:41.304: INFO: Created: latency-svc-nxtb7
Dec  3 15:40:41.305: INFO: Got endpoints: latency-svc-nxtb7 [55.758872ms]
Dec  3 15:40:41.308: INFO: Created: latency-svc-9cjzs
Dec  3 15:40:41.309: INFO: Got endpoints: latency-svc-9cjzs [60.121971ms]
Dec  3 15:40:41.312: INFO: Created: latency-svc-n76sr
Dec  3 15:40:41.315: INFO: Got endpoints: latency-svc-n76sr [65.370988ms]
Dec  3 15:40:41.316: INFO: Created: latency-svc-bdj67
Dec  3 15:40:41.317: INFO: Got endpoints: latency-svc-bdj67 [67.94298ms]
Dec  3 15:40:41.320: INFO: Created: latency-svc-kskh6
Dec  3 15:40:41.322: INFO: Got endpoints: latency-svc-kskh6 [60.684904ms]
Dec  3 15:40:41.326: INFO: Created: latency-svc-flshk
Dec  3 15:40:41.329: INFO: Got endpoints: latency-svc-flshk [63.929557ms]
Dec  3 15:40:41.330: INFO: Created: latency-svc-pwlbn
Dec  3 15:40:41.331: INFO: Got endpoints: latency-svc-pwlbn [62.537509ms]
Dec  3 15:40:41.334: INFO: Created: latency-svc-6b2td
Dec  3 15:40:41.335: INFO: Got endpoints: latency-svc-6b2td [64.166372ms]
Dec  3 15:40:41.337: INFO: Created: latency-svc-kv7cz
Dec  3 15:40:41.338: INFO: Got endpoints: latency-svc-kv7cz [9.254846ms]
Dec  3 15:40:41.341: INFO: Created: latency-svc-p9t5t
Dec  3 15:40:41.343: INFO: Got endpoints: latency-svc-p9t5t [68.736968ms]
Dec  3 15:40:41.346: INFO: Created: latency-svc-6wp6r
Dec  3 15:40:41.347: INFO: Got endpoints: latency-svc-6wp6r [68.577242ms]
Dec  3 15:40:41.351: INFO: Created: latency-svc-k8fqt
Dec  3 15:40:41.352: INFO: Got endpoints: latency-svc-k8fqt [69.787225ms]
Dec  3 15:40:41.355: INFO: Created: latency-svc-xd6v9
Dec  3 15:40:41.356: INFO: Got endpoints: latency-svc-xd6v9 [68.714765ms]
Dec  3 15:40:41.359: INFO: Created: latency-svc-xt4v8
Dec  3 15:40:41.360: INFO: Got endpoints: latency-svc-xt4v8 [70.777611ms]
Dec  3 15:40:41.362: INFO: Created: latency-svc-rnhbd
Dec  3 15:40:41.364: INFO: Got endpoints: latency-svc-rnhbd [66.976702ms]
Dec  3 15:40:41.366: INFO: Created: latency-svc-rdr9j
Dec  3 15:40:41.369: INFO: Got endpoints: latency-svc-rdr9j [68.128583ms]
Dec  3 15:40:41.369: INFO: Created: latency-svc-q6x9n
Dec  3 15:40:41.371: INFO: Got endpoints: latency-svc-q6x9n [65.437178ms]
Dec  3 15:40:41.373: INFO: Created: latency-svc-p8kct
Dec  3 15:40:41.375: INFO: Got endpoints: latency-svc-p8kct [65.537759ms]
Dec  3 15:40:41.378: INFO: Created: latency-svc-s9524
Dec  3 15:40:41.379: INFO: Got endpoints: latency-svc-s9524 [64.172182ms]
Dec  3 15:40:41.381: INFO: Created: latency-svc-ss4jt
Dec  3 15:40:41.383: INFO: Got endpoints: latency-svc-ss4jt [65.584541ms]
Dec  3 15:40:41.386: INFO: Created: latency-svc-5fh9n
Dec  3 15:40:41.389: INFO: Created: latency-svc-wvxt7
Dec  3 15:40:41.392: INFO: Created: latency-svc-lw8z8
Dec  3 15:40:41.403: INFO: Got endpoints: latency-svc-5fh9n [80.662125ms]
Dec  3 15:40:41.403: INFO: Created: latency-svc-lnhdh
Dec  3 15:40:41.407: INFO: Created: latency-svc-zxzmm
Dec  3 15:40:41.410: INFO: Created: latency-svc-65qhc
Dec  3 15:40:41.413: INFO: Created: latency-svc-rnnmt
Dec  3 15:40:41.418: INFO: Created: latency-svc-dnt6l
Dec  3 15:40:41.421: INFO: Created: latency-svc-fngx8
Dec  3 15:40:41.424: INFO: Created: latency-svc-cxktm
Dec  3 15:40:41.428: INFO: Created: latency-svc-r9bqf
Dec  3 15:40:41.437: INFO: Created: latency-svc-tg57f
Dec  3 15:40:41.440: INFO: Created: latency-svc-c46f8
Dec  3 15:40:41.444: INFO: Created: latency-svc-vzbcv
Dec  3 15:40:41.447: INFO: Created: latency-svc-6zbq9
Dec  3 15:40:41.455: INFO: Got endpoints: latency-svc-wvxt7 [123.652803ms]
Dec  3 15:40:41.459: INFO: Created: latency-svc-crr84
Dec  3 15:40:41.466: INFO: Created: latency-svc-h2526
Dec  3 15:40:41.500: INFO: Got endpoints: latency-svc-lw8z8 [165.042374ms]
Dec  3 15:40:41.514: INFO: Created: latency-svc-vpfx5
Dec  3 15:40:41.549: INFO: Got endpoints: latency-svc-lnhdh [211.094955ms]
Dec  3 15:40:41.557: INFO: Created: latency-svc-zl86d
Dec  3 15:40:41.599: INFO: Got endpoints: latency-svc-zxzmm [256.265226ms]
Dec  3 15:40:41.607: INFO: Created: latency-svc-rfjxm
Dec  3 15:40:41.650: INFO: Got endpoints: latency-svc-65qhc [302.63979ms]
Dec  3 15:40:41.658: INFO: Created: latency-svc-kn6xt
Dec  3 15:40:41.699: INFO: Got endpoints: latency-svc-rnnmt [347.050574ms]
Dec  3 15:40:41.707: INFO: Created: latency-svc-clgjr
Dec  3 15:40:41.749: INFO: Got endpoints: latency-svc-dnt6l [392.930467ms]
Dec  3 15:40:41.757: INFO: Created: latency-svc-4w62w
Dec  3 15:40:41.799: INFO: Got endpoints: latency-svc-fngx8 [439.258024ms]
Dec  3 15:40:41.807: INFO: Created: latency-svc-whvns
Dec  3 15:40:41.849: INFO: Got endpoints: latency-svc-cxktm [485.102349ms]
Dec  3 15:40:41.856: INFO: Created: latency-svc-sw6rj
Dec  3 15:40:41.899: INFO: Got endpoints: latency-svc-r9bqf [530.012651ms]
Dec  3 15:40:41.906: INFO: Created: latency-svc-9md2f
Dec  3 15:40:41.949: INFO: Got endpoints: latency-svc-tg57f [578.477466ms]
Dec  3 15:40:41.957: INFO: Created: latency-svc-h5qm6
Dec  3 15:40:42.000: INFO: Got endpoints: latency-svc-c46f8 [624.422627ms]
Dec  3 15:40:42.007: INFO: Created: latency-svc-kptlg
Dec  3 15:40:42.049: INFO: Got endpoints: latency-svc-vzbcv [669.931049ms]
Dec  3 15:40:42.056: INFO: Created: latency-svc-vhcjb
Dec  3 15:40:42.099: INFO: Got endpoints: latency-svc-6zbq9 [715.92149ms]
Dec  3 15:40:42.106: INFO: Created: latency-svc-8h5g6
Dec  3 15:40:42.149: INFO: Got endpoints: latency-svc-crr84 [745.872829ms]
Dec  3 15:40:42.157: INFO: Created: latency-svc-b9cp8
Dec  3 15:40:42.199: INFO: Got endpoints: latency-svc-h2526 [744.497907ms]
Dec  3 15:40:42.207: INFO: Created: latency-svc-5vvqh
Dec  3 15:40:42.249: INFO: Got endpoints: latency-svc-vpfx5 [749.426184ms]
Dec  3 15:40:42.256: INFO: Created: latency-svc-d499l
Dec  3 15:40:42.299: INFO: Got endpoints: latency-svc-zl86d [749.668348ms]
Dec  3 15:40:42.307: INFO: Created: latency-svc-gt5xk
Dec  3 15:40:42.349: INFO: Got endpoints: latency-svc-rfjxm [749.816272ms]
Dec  3 15:40:42.357: INFO: Created: latency-svc-7nkx4
Dec  3 15:40:42.399: INFO: Got endpoints: latency-svc-kn6xt [749.601277ms]
Dec  3 15:40:42.407: INFO: Created: latency-svc-7n48n
Dec  3 15:40:42.449: INFO: Got endpoints: latency-svc-clgjr [749.578692ms]
Dec  3 15:40:42.457: INFO: Created: latency-svc-2h8l7
Dec  3 15:40:42.499: INFO: Got endpoints: latency-svc-4w62w [749.716843ms]
Dec  3 15:40:42.507: INFO: Created: latency-svc-6b54x
Dec  3 15:40:42.550: INFO: Got endpoints: latency-svc-whvns [750.35221ms]
Dec  3 15:40:42.557: INFO: Created: latency-svc-887zt
Dec  3 15:40:42.599: INFO: Got endpoints: latency-svc-sw6rj [749.971512ms]
Dec  3 15:40:42.606: INFO: Created: latency-svc-6zx5v
Dec  3 15:40:42.649: INFO: Got endpoints: latency-svc-9md2f [750.025532ms]
Dec  3 15:40:42.656: INFO: Created: latency-svc-nmtqm
Dec  3 15:40:42.699: INFO: Got endpoints: latency-svc-h5qm6 [749.649542ms]
Dec  3 15:40:42.706: INFO: Created: latency-svc-65wcv
Dec  3 15:40:42.749: INFO: Got endpoints: latency-svc-kptlg [749.570731ms]
Dec  3 15:40:42.756: INFO: Created: latency-svc-f5p2g
Dec  3 15:40:42.799: INFO: Got endpoints: latency-svc-vhcjb [749.917449ms]
Dec  3 15:40:42.806: INFO: Created: latency-svc-t7r9j
Dec  3 15:40:42.849: INFO: Got endpoints: latency-svc-8h5g6 [750.175845ms]
Dec  3 15:40:42.856: INFO: Created: latency-svc-c2cmc
Dec  3 15:40:42.899: INFO: Got endpoints: latency-svc-b9cp8 [750.201125ms]
Dec  3 15:40:42.907: INFO: Created: latency-svc-5m557
Dec  3 15:40:42.950: INFO: Got endpoints: latency-svc-5vvqh [750.98996ms]
Dec  3 15:40:42.958: INFO: Created: latency-svc-xglgl
Dec  3 15:40:42.999: INFO: Got endpoints: latency-svc-d499l [749.816784ms]
Dec  3 15:40:43.007: INFO: Created: latency-svc-nmbm7
Dec  3 15:40:43.049: INFO: Got endpoints: latency-svc-gt5xk [750.027978ms]
Dec  3 15:40:43.057: INFO: Created: latency-svc-tw69f
Dec  3 15:40:43.099: INFO: Got endpoints: latency-svc-7nkx4 [749.809768ms]
Dec  3 15:40:43.108: INFO: Created: latency-svc-7j6ml
Dec  3 15:40:43.149: INFO: Got endpoints: latency-svc-7n48n [749.741867ms]
Dec  3 15:40:43.159: INFO: Created: latency-svc-285bc
Dec  3 15:40:43.199: INFO: Got endpoints: latency-svc-2h8l7 [749.925946ms]
Dec  3 15:40:43.206: INFO: Created: latency-svc-wlwkd
Dec  3 15:40:43.249: INFO: Got endpoints: latency-svc-6b54x [750.05133ms]
Dec  3 15:40:43.259: INFO: Created: latency-svc-pr8pk
Dec  3 15:40:43.299: INFO: Got endpoints: latency-svc-887zt [749.35194ms]
Dec  3 15:40:43.307: INFO: Created: latency-svc-czjxb
Dec  3 15:40:43.349: INFO: Got endpoints: latency-svc-6zx5v [749.964432ms]
Dec  3 15:40:43.357: INFO: Created: latency-svc-86hbq
Dec  3 15:40:43.399: INFO: Got endpoints: latency-svc-nmtqm [750.176389ms]
Dec  3 15:40:43.407: INFO: Created: latency-svc-ttjk8
Dec  3 15:40:43.449: INFO: Got endpoints: latency-svc-65wcv [749.900343ms]
Dec  3 15:40:43.456: INFO: Created: latency-svc-2c45l
Dec  3 15:40:43.499: INFO: Got endpoints: latency-svc-f5p2g [750.006596ms]
Dec  3 15:40:43.512: INFO: Created: latency-svc-xhhzg
Dec  3 15:40:43.549: INFO: Got endpoints: latency-svc-t7r9j [750.314807ms]
Dec  3 15:40:43.561: INFO: Created: latency-svc-4zh92
Dec  3 15:40:43.599: INFO: Got endpoints: latency-svc-c2cmc [750.331325ms]
Dec  3 15:40:43.607: INFO: Created: latency-svc-nt5dd
Dec  3 15:40:43.649: INFO: Got endpoints: latency-svc-5m557 [749.925592ms]
Dec  3 15:40:43.657: INFO: Created: latency-svc-ffmbg
Dec  3 15:40:43.699: INFO: Got endpoints: latency-svc-xglgl [748.581968ms]
Dec  3 15:40:43.706: INFO: Created: latency-svc-vs4md
Dec  3 15:40:43.749: INFO: Got endpoints: latency-svc-nmbm7 [750.25033ms]
Dec  3 15:40:43.757: INFO: Created: latency-svc-kxl6t
Dec  3 15:40:43.799: INFO: Got endpoints: latency-svc-tw69f [749.884759ms]
Dec  3 15:40:43.807: INFO: Created: latency-svc-wc6dt
Dec  3 15:40:43.849: INFO: Got endpoints: latency-svc-7j6ml [750.140133ms]
Dec  3 15:40:43.857: INFO: Created: latency-svc-blf5z
Dec  3 15:40:43.900: INFO: Got endpoints: latency-svc-285bc [750.609889ms]
Dec  3 15:40:43.907: INFO: Created: latency-svc-4hqsm
Dec  3 15:40:43.949: INFO: Got endpoints: latency-svc-wlwkd [749.785866ms]
Dec  3 15:40:43.956: INFO: Created: latency-svc-4446p
Dec  3 15:40:43.999: INFO: Got endpoints: latency-svc-pr8pk [749.86593ms]
Dec  3 15:40:44.006: INFO: Created: latency-svc-bt6nq
Dec  3 15:40:44.048: INFO: Got endpoints: latency-svc-czjxb [749.152261ms]
Dec  3 15:40:44.055: INFO: Created: latency-svc-z9d8h
Dec  3 15:40:44.099: INFO: Got endpoints: latency-svc-86hbq [749.764402ms]
Dec  3 15:40:44.112: INFO: Created: latency-svc-nwcs6
Dec  3 15:40:44.149: INFO: Got endpoints: latency-svc-ttjk8 [749.630559ms]
Dec  3 15:40:44.158: INFO: Created: latency-svc-rgxwk
Dec  3 15:40:44.199: INFO: Got endpoints: latency-svc-2c45l [750.29441ms]
Dec  3 15:40:44.207: INFO: Created: latency-svc-xvv4k
Dec  3 15:40:44.249: INFO: Got endpoints: latency-svc-xhhzg [749.862978ms]
Dec  3 15:40:44.257: INFO: Created: latency-svc-mtknv
Dec  3 15:40:44.299: INFO: Got endpoints: latency-svc-4zh92 [749.811972ms]
Dec  3 15:40:44.307: INFO: Created: latency-svc-rvxrw
Dec  3 15:40:44.349: INFO: Got endpoints: latency-svc-nt5dd [749.807564ms]
Dec  3 15:40:44.357: INFO: Created: latency-svc-795r6
Dec  3 15:40:44.399: INFO: Got endpoints: latency-svc-ffmbg [749.999913ms]
Dec  3 15:40:44.407: INFO: Created: latency-svc-9nltv
Dec  3 15:40:44.449: INFO: Got endpoints: latency-svc-vs4md [749.858986ms]
Dec  3 15:40:44.456: INFO: Created: latency-svc-s5kbn
Dec  3 15:40:44.500: INFO: Got endpoints: latency-svc-kxl6t [750.582912ms]
Dec  3 15:40:44.508: INFO: Created: latency-svc-r8mzx
Dec  3 15:40:44.549: INFO: Got endpoints: latency-svc-wc6dt [749.371452ms]
Dec  3 15:40:44.556: INFO: Created: latency-svc-q8wqj
Dec  3 15:40:44.600: INFO: Got endpoints: latency-svc-blf5z [750.30651ms]
Dec  3 15:40:44.607: INFO: Created: latency-svc-kwx4r
Dec  3 15:40:44.649: INFO: Got endpoints: latency-svc-4hqsm [749.18004ms]
Dec  3 15:40:44.657: INFO: Created: latency-svc-h9qlp
Dec  3 15:40:44.699: INFO: Got endpoints: latency-svc-4446p [750.083415ms]
Dec  3 15:40:44.709: INFO: Created: latency-svc-6p5s7
Dec  3 15:40:44.749: INFO: Got endpoints: latency-svc-bt6nq [749.948275ms]
Dec  3 15:40:44.757: INFO: Created: latency-svc-nrsdv
Dec  3 15:40:44.799: INFO: Got endpoints: latency-svc-z9d8h [750.889738ms]
Dec  3 15:40:44.810: INFO: Created: latency-svc-hxl9p
Dec  3 15:40:44.849: INFO: Got endpoints: latency-svc-nwcs6 [749.956269ms]
Dec  3 15:40:44.856: INFO: Created: latency-svc-pkqjw
Dec  3 15:40:44.899: INFO: Got endpoints: latency-svc-rgxwk [750.062802ms]
Dec  3 15:40:44.906: INFO: Created: latency-svc-2mqgn
Dec  3 15:40:44.949: INFO: Got endpoints: latency-svc-xvv4k [749.812984ms]
Dec  3 15:40:44.957: INFO: Created: latency-svc-4krz9
Dec  3 15:40:44.999: INFO: Got endpoints: latency-svc-mtknv [749.889514ms]
Dec  3 15:40:45.006: INFO: Created: latency-svc-4mf5h
Dec  3 15:40:45.049: INFO: Got endpoints: latency-svc-rvxrw [749.858218ms]
Dec  3 15:40:45.057: INFO: Created: latency-svc-jr7r7
Dec  3 15:40:45.099: INFO: Got endpoints: latency-svc-795r6 [749.554458ms]
Dec  3 15:40:45.106: INFO: Created: latency-svc-9wx8s
Dec  3 15:40:45.149: INFO: Got endpoints: latency-svc-9nltv [749.614615ms]
Dec  3 15:40:45.158: INFO: Created: latency-svc-nqwm8
Dec  3 15:40:45.199: INFO: Got endpoints: latency-svc-s5kbn [750.473822ms]
Dec  3 15:40:45.207: INFO: Created: latency-svc-w5s92
Dec  3 15:40:45.249: INFO: Got endpoints: latency-svc-r8mzx [749.001281ms]
Dec  3 15:40:45.257: INFO: Created: latency-svc-2sz6c
Dec  3 15:40:45.299: INFO: Got endpoints: latency-svc-q8wqj [750.429403ms]
Dec  3 15:40:45.307: INFO: Created: latency-svc-k8qbs
Dec  3 15:40:45.350: INFO: Got endpoints: latency-svc-kwx4r [749.768177ms]
Dec  3 15:40:45.357: INFO: Created: latency-svc-nt9lg
Dec  3 15:40:45.399: INFO: Got endpoints: latency-svc-h9qlp [750.070422ms]
Dec  3 15:40:45.410: INFO: Created: latency-svc-hlr9x
Dec  3 15:40:45.449: INFO: Got endpoints: latency-svc-6p5s7 [750.01723ms]
Dec  3 15:40:45.456: INFO: Created: latency-svc-k9v2x
Dec  3 15:40:45.499: INFO: Got endpoints: latency-svc-nrsdv [749.899634ms]
Dec  3 15:40:45.507: INFO: Created: latency-svc-qpdf4
Dec  3 15:40:45.549: INFO: Got endpoints: latency-svc-hxl9p [749.973824ms]
Dec  3 15:40:45.557: INFO: Created: latency-svc-tj7vx
Dec  3 15:40:45.599: INFO: Got endpoints: latency-svc-pkqjw [750.136121ms]
Dec  3 15:40:45.608: INFO: Created: latency-svc-gtsfl
Dec  3 15:40:45.649: INFO: Got endpoints: latency-svc-2mqgn [750.102851ms]
Dec  3 15:40:45.657: INFO: Created: latency-svc-vsn86
Dec  3 15:40:45.700: INFO: Got endpoints: latency-svc-4krz9 [750.267008ms]
Dec  3 15:40:45.708: INFO: Created: latency-svc-6fzdp
Dec  3 15:40:45.749: INFO: Got endpoints: latency-svc-4mf5h [749.908013ms]
Dec  3 15:40:45.757: INFO: Created: latency-svc-5gzr8
Dec  3 15:40:45.799: INFO: Got endpoints: latency-svc-jr7r7 [750.237136ms]
Dec  3 15:40:45.807: INFO: Created: latency-svc-b9jxb
Dec  3 15:40:45.849: INFO: Got endpoints: latency-svc-9wx8s [750.253054ms]
Dec  3 15:40:45.858: INFO: Created: latency-svc-k8n48
Dec  3 15:40:45.900: INFO: Got endpoints: latency-svc-nqwm8 [750.314929ms]
Dec  3 15:40:45.914: INFO: Created: latency-svc-dddz4
Dec  3 15:40:45.949: INFO: Got endpoints: latency-svc-w5s92 [749.624811ms]
Dec  3 15:40:45.957: INFO: Created: latency-svc-7kdw4
Dec  3 15:40:45.999: INFO: Got endpoints: latency-svc-2sz6c [750.068644ms]
Dec  3 15:40:46.008: INFO: Created: latency-svc-6pvjp
Dec  3 15:40:46.049: INFO: Got endpoints: latency-svc-k8qbs [749.633372ms]
Dec  3 15:40:46.056: INFO: Created: latency-svc-46d7s
Dec  3 15:40:46.099: INFO: Got endpoints: latency-svc-nt9lg [749.362392ms]
Dec  3 15:40:46.107: INFO: Created: latency-svc-f7cj7
Dec  3 15:40:46.149: INFO: Got endpoints: latency-svc-hlr9x [749.917326ms]
Dec  3 15:40:46.157: INFO: Created: latency-svc-qr8vg
Dec  3 15:40:46.199: INFO: Got endpoints: latency-svc-k9v2x [750.153691ms]
Dec  3 15:40:46.208: INFO: Created: latency-svc-z5vlr
Dec  3 15:40:46.252: INFO: Got endpoints: latency-svc-qpdf4 [752.25745ms]
Dec  3 15:40:46.261: INFO: Created: latency-svc-d54fx
Dec  3 15:40:46.299: INFO: Got endpoints: latency-svc-tj7vx [749.524771ms]
Dec  3 15:40:46.308: INFO: Created: latency-svc-c6fsk
Dec  3 15:40:46.349: INFO: Got endpoints: latency-svc-gtsfl [749.953368ms]
Dec  3 15:40:46.357: INFO: Created: latency-svc-hlkmp
Dec  3 15:40:46.399: INFO: Got endpoints: latency-svc-vsn86 [749.600276ms]
Dec  3 15:40:46.407: INFO: Created: latency-svc-dz8r7
Dec  3 15:40:46.449: INFO: Got endpoints: latency-svc-6fzdp [749.516322ms]
Dec  3 15:40:46.458: INFO: Created: latency-svc-rnkgs
Dec  3 15:40:46.499: INFO: Got endpoints: latency-svc-5gzr8 [750.257966ms]
Dec  3 15:40:46.508: INFO: Created: latency-svc-nsr9g
Dec  3 15:40:46.549: INFO: Got endpoints: latency-svc-b9jxb [749.609008ms]
Dec  3 15:40:46.570: INFO: Created: latency-svc-dqslc
Dec  3 15:40:46.599: INFO: Got endpoints: latency-svc-k8n48 [749.90161ms]
Dec  3 15:40:46.607: INFO: Created: latency-svc-9kg4j
Dec  3 15:40:46.649: INFO: Got endpoints: latency-svc-dddz4 [749.822024ms]
Dec  3 15:40:46.659: INFO: Created: latency-svc-62wgc
Dec  3 15:40:46.700: INFO: Got endpoints: latency-svc-7kdw4 [750.200456ms]
Dec  3 15:40:46.707: INFO: Created: latency-svc-hd75h
Dec  3 15:40:46.749: INFO: Got endpoints: latency-svc-6pvjp [749.914661ms]
Dec  3 15:40:46.757: INFO: Created: latency-svc-n5jdt
Dec  3 15:40:46.799: INFO: Got endpoints: latency-svc-46d7s [750.343578ms]
Dec  3 15:40:46.807: INFO: Created: latency-svc-wfs62
Dec  3 15:40:46.849: INFO: Got endpoints: latency-svc-f7cj7 [749.822321ms]
Dec  3 15:40:46.857: INFO: Created: latency-svc-2msf5
Dec  3 15:40:46.899: INFO: Got endpoints: latency-svc-qr8vg [749.971307ms]
Dec  3 15:40:46.907: INFO: Created: latency-svc-x8czl
Dec  3 15:40:46.949: INFO: Got endpoints: latency-svc-z5vlr [749.858186ms]
Dec  3 15:40:46.957: INFO: Created: latency-svc-qq79x
Dec  3 15:40:46.999: INFO: Got endpoints: latency-svc-d54fx [747.081136ms]
Dec  3 15:40:47.007: INFO: Created: latency-svc-ksdkr
Dec  3 15:40:47.049: INFO: Got endpoints: latency-svc-c6fsk [750.3046ms]
Dec  3 15:40:47.057: INFO: Created: latency-svc-tcn42
Dec  3 15:40:47.100: INFO: Got endpoints: latency-svc-hlkmp [750.186156ms]
Dec  3 15:40:47.108: INFO: Created: latency-svc-lh9bf
Dec  3 15:40:47.149: INFO: Got endpoints: latency-svc-dz8r7 [750.089494ms]
Dec  3 15:40:47.157: INFO: Created: latency-svc-g4jd2
Dec  3 15:40:47.199: INFO: Got endpoints: latency-svc-rnkgs [750.054848ms]
Dec  3 15:40:47.207: INFO: Created: latency-svc-6jxs7
Dec  3 15:40:47.249: INFO: Got endpoints: latency-svc-nsr9g [749.620301ms]
Dec  3 15:40:47.256: INFO: Created: latency-svc-ntm9c
Dec  3 15:40:47.299: INFO: Got endpoints: latency-svc-dqslc [750.064502ms]
Dec  3 15:40:47.307: INFO: Created: latency-svc-fsz7h
Dec  3 15:40:47.349: INFO: Got endpoints: latency-svc-9kg4j [750.028434ms]
Dec  3 15:40:47.358: INFO: Created: latency-svc-bmxdw
Dec  3 15:40:47.399: INFO: Got endpoints: latency-svc-62wgc [749.573471ms]
Dec  3 15:40:47.412: INFO: Created: latency-svc-zsd4c
Dec  3 15:40:47.449: INFO: Got endpoints: latency-svc-hd75h [749.604548ms]
Dec  3 15:40:47.457: INFO: Created: latency-svc-qf9dz
Dec  3 15:40:47.499: INFO: Got endpoints: latency-svc-n5jdt [749.794164ms]
Dec  3 15:40:47.507: INFO: Created: latency-svc-t5nph
Dec  3 15:40:47.549: INFO: Got endpoints: latency-svc-wfs62 [750.077991ms]
Dec  3 15:40:47.557: INFO: Created: latency-svc-m4qs9
Dec  3 15:40:47.599: INFO: Got endpoints: latency-svc-2msf5 [750.2673ms]
Dec  3 15:40:47.607: INFO: Created: latency-svc-s8jmc
Dec  3 15:40:47.649: INFO: Got endpoints: latency-svc-x8czl [749.941383ms]
Dec  3 15:40:47.657: INFO: Created: latency-svc-nzlx8
Dec  3 15:40:47.699: INFO: Got endpoints: latency-svc-qq79x [749.945815ms]
Dec  3 15:40:47.707: INFO: Created: latency-svc-4kdh5
Dec  3 15:40:47.749: INFO: Got endpoints: latency-svc-ksdkr [750.271492ms]
Dec  3 15:40:47.757: INFO: Created: latency-svc-lqkv8
Dec  3 15:40:47.799: INFO: Got endpoints: latency-svc-tcn42 [749.974254ms]
Dec  3 15:40:47.807: INFO: Created: latency-svc-ts5zw
Dec  3 15:40:47.849: INFO: Got endpoints: latency-svc-lh9bf [749.441069ms]
Dec  3 15:40:47.857: INFO: Created: latency-svc-kwspz
Dec  3 15:40:47.899: INFO: Got endpoints: latency-svc-g4jd2 [750.05645ms]
Dec  3 15:40:47.907: INFO: Created: latency-svc-kqm2p
Dec  3 15:40:47.949: INFO: Got endpoints: latency-svc-6jxs7 [749.835166ms]
Dec  3 15:40:47.957: INFO: Created: latency-svc-rxwfm
Dec  3 15:40:48.000: INFO: Got endpoints: latency-svc-ntm9c [751.284733ms]
Dec  3 15:40:48.009: INFO: Created: latency-svc-8gdbs
Dec  3 15:40:48.049: INFO: Got endpoints: latency-svc-fsz7h [749.67874ms]
Dec  3 15:40:48.057: INFO: Created: latency-svc-h259c
Dec  3 15:40:48.100: INFO: Got endpoints: latency-svc-bmxdw [750.155798ms]
Dec  3 15:40:48.109: INFO: Created: latency-svc-jq44w
Dec  3 15:40:48.149: INFO: Got endpoints: latency-svc-zsd4c [749.975918ms]
Dec  3 15:40:48.157: INFO: Created: latency-svc-hm8tf
Dec  3 15:40:48.200: INFO: Got endpoints: latency-svc-qf9dz [750.256647ms]
Dec  3 15:40:48.210: INFO: Created: latency-svc-xlwxc
Dec  3 15:40:48.249: INFO: Got endpoints: latency-svc-t5nph [749.916608ms]
Dec  3 15:40:48.257: INFO: Created: latency-svc-dkxbv
Dec  3 15:40:48.299: INFO: Got endpoints: latency-svc-m4qs9 [749.935815ms]
Dec  3 15:40:48.310: INFO: Created: latency-svc-wnltx
Dec  3 15:40:48.349: INFO: Got endpoints: latency-svc-s8jmc [749.982133ms]
Dec  3 15:40:48.357: INFO: Created: latency-svc-b22rw
Dec  3 15:40:48.399: INFO: Got endpoints: latency-svc-nzlx8 [749.997418ms]
Dec  3 15:40:48.408: INFO: Created: latency-svc-8nz77
Dec  3 15:40:48.449: INFO: Got endpoints: latency-svc-4kdh5 [749.73296ms]
Dec  3 15:40:48.457: INFO: Created: latency-svc-s2ltr
Dec  3 15:40:48.499: INFO: Got endpoints: latency-svc-lqkv8 [750.014431ms]
Dec  3 15:40:48.511: INFO: Created: latency-svc-5745l
Dec  3 15:40:48.550: INFO: Got endpoints: latency-svc-ts5zw [750.242627ms]
Dec  3 15:40:48.558: INFO: Created: latency-svc-gdvfl
Dec  3 15:40:48.599: INFO: Got endpoints: latency-svc-kwspz [750.238056ms]
Dec  3 15:40:48.608: INFO: Created: latency-svc-dn5xb
Dec  3 15:40:48.649: INFO: Got endpoints: latency-svc-kqm2p [749.806372ms]
Dec  3 15:40:48.657: INFO: Created: latency-svc-lvtbr
Dec  3 15:40:48.699: INFO: Got endpoints: latency-svc-rxwfm [750.252204ms]
Dec  3 15:40:48.708: INFO: Created: latency-svc-lmll2
Dec  3 15:40:48.749: INFO: Got endpoints: latency-svc-8gdbs [748.878781ms]
Dec  3 15:40:48.757: INFO: Created: latency-svc-mx2bq
Dec  3 15:40:48.800: INFO: Got endpoints: latency-svc-h259c [750.319162ms]
Dec  3 15:40:48.807: INFO: Created: latency-svc-xcbgh
Dec  3 15:40:48.849: INFO: Got endpoints: latency-svc-jq44w [749.473285ms]
Dec  3 15:40:48.857: INFO: Created: latency-svc-pn64f
Dec  3 15:40:48.899: INFO: Got endpoints: latency-svc-hm8tf [749.908511ms]
Dec  3 15:40:48.907: INFO: Created: latency-svc-scsz7
Dec  3 15:40:48.949: INFO: Got endpoints: latency-svc-xlwxc [749.716661ms]
Dec  3 15:40:48.957: INFO: Created: latency-svc-5sfg4
Dec  3 15:40:48.999: INFO: Got endpoints: latency-svc-dkxbv [750.070868ms]
Dec  3 15:40:49.007: INFO: Created: latency-svc-jh4ng
Dec  3 15:40:49.049: INFO: Got endpoints: latency-svc-wnltx [749.672216ms]
Dec  3 15:40:49.057: INFO: Created: latency-svc-m5296
Dec  3 15:40:49.099: INFO: Got endpoints: latency-svc-b22rw [749.887116ms]
Dec  3 15:40:49.149: INFO: Got endpoints: latency-svc-8nz77 [749.707835ms]
Dec  3 15:40:49.199: INFO: Got endpoints: latency-svc-s2ltr [750.236294ms]
Dec  3 15:40:49.249: INFO: Got endpoints: latency-svc-5745l [749.815127ms]
Dec  3 15:40:49.299: INFO: Got endpoints: latency-svc-gdvfl [749.495045ms]
Dec  3 15:40:49.349: INFO: Got endpoints: latency-svc-dn5xb [749.809227ms]
Dec  3 15:40:49.399: INFO: Got endpoints: latency-svc-lvtbr [750.040838ms]
Dec  3 15:40:49.449: INFO: Got endpoints: latency-svc-lmll2 [749.729034ms]
Dec  3 15:40:49.499: INFO: Got endpoints: latency-svc-mx2bq [749.862872ms]
Dec  3 15:40:49.549: INFO: Got endpoints: latency-svc-xcbgh [749.791798ms]
Dec  3 15:40:49.600: INFO: Got endpoints: latency-svc-pn64f [750.136229ms]
Dec  3 15:40:49.650: INFO: Got endpoints: latency-svc-scsz7 [750.34284ms]
Dec  3 15:40:49.699: INFO: Got endpoints: latency-svc-5sfg4 [749.890997ms]
Dec  3 15:40:49.753: INFO: Got endpoints: latency-svc-jh4ng [753.455119ms]
Dec  3 15:40:49.800: INFO: Got endpoints: latency-svc-m5296 [750.244481ms]
Dec  3 15:40:49.800: INFO: Latencies: [9.254846ms 12.351029ms 15.734701ms 19.249343ms 21.12114ms 24.946016ms 28.974154ms 33.198563ms 38.283139ms 39.733819ms 47.647006ms 51.382616ms 55.758872ms 60.121971ms 60.684904ms 62.537509ms 63.929557ms 64.166372ms 64.172182ms 65.370988ms 65.437178ms 65.537759ms 65.584541ms 66.976702ms 67.94298ms 68.128583ms 68.577242ms 68.714765ms 68.736968ms 69.787225ms 70.777611ms 80.662125ms 123.652803ms 165.042374ms 211.094955ms 256.265226ms 302.63979ms 347.050574ms 392.930467ms 439.258024ms 485.102349ms 530.012651ms 578.477466ms 624.422627ms 669.931049ms 715.92149ms 744.497907ms 745.872829ms 747.081136ms 748.581968ms 748.878781ms 749.001281ms 749.152261ms 749.18004ms 749.35194ms 749.362392ms 749.371452ms 749.426184ms 749.441069ms 749.473285ms 749.495045ms 749.516322ms 749.524771ms 749.554458ms 749.570731ms 749.573471ms 749.578692ms 749.600276ms 749.601277ms 749.604548ms 749.609008ms 749.614615ms 749.620301ms 749.624811ms 749.630559ms 749.633372ms 749.649542ms 749.668348ms 749.672216ms 749.67874ms 749.707835ms 749.716661ms 749.716843ms 749.729034ms 749.73296ms 749.741867ms 749.764402ms 749.768177ms 749.785866ms 749.791798ms 749.794164ms 749.806372ms 749.807564ms 749.809227ms 749.809768ms 749.811972ms 749.812984ms 749.815127ms 749.816272ms 749.816784ms 749.822024ms 749.822321ms 749.835166ms 749.858186ms 749.858218ms 749.858986ms 749.862872ms 749.862978ms 749.86593ms 749.884759ms 749.887116ms 749.889514ms 749.890997ms 749.899634ms 749.900343ms 749.90161ms 749.908013ms 749.908511ms 749.914661ms 749.916608ms 749.917326ms 749.917449ms 749.925592ms 749.925946ms 749.935815ms 749.941383ms 749.945815ms 749.948275ms 749.953368ms 749.956269ms 749.964432ms 749.971307ms 749.971512ms 749.973824ms 749.974254ms 749.975918ms 749.982133ms 749.997418ms 749.999913ms 750.006596ms 750.014431ms 750.01723ms 750.025532ms 750.027978ms 750.028434ms 750.040838ms 750.05133ms 750.054848ms 750.05645ms 750.062802ms 750.064502ms 750.068644ms 750.070422ms 750.070868ms 750.077991ms 750.083415ms 750.089494ms 750.102851ms 750.136121ms 750.136229ms 750.140133ms 750.153691ms 750.155798ms 750.175845ms 750.176389ms 750.186156ms 750.200456ms 750.201125ms 750.236294ms 750.237136ms 750.238056ms 750.242627ms 750.244481ms 750.25033ms 750.252204ms 750.253054ms 750.256647ms 750.257966ms 750.267008ms 750.2673ms 750.271492ms 750.29441ms 750.3046ms 750.30651ms 750.314807ms 750.314929ms 750.319162ms 750.331325ms 750.34284ms 750.343578ms 750.35221ms 750.429403ms 750.473822ms 750.582912ms 750.609889ms 750.889738ms 750.98996ms 751.284733ms 752.25745ms 753.455119ms]
Dec  3 15:40:49.800: INFO: 50 %ile: 749.822024ms
Dec  3 15:40:49.800: INFO: 90 %ile: 750.271492ms
Dec  3 15:40:49.800: INFO: 99 %ile: 752.25745ms
Dec  3 15:40:49.800: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:40:49.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1355" for this suite.
Dec  3 15:41:03.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:03.946: INFO: namespace svc-latency-1355 deletion completed in 14.141922405s
•SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:41:03.947: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-7vlt
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:41:04.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7vlt" in namespace "subpath-9731" to be "success or failure"
Dec  3 15:41:04.109: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.650751ms
Dec  3 15:41:06.113: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008124085s
Dec  3 15:41:08.118: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 4.012759756s
Dec  3 15:41:10.122: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 6.017185753s
Dec  3 15:41:12.127: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 8.022063021s
Dec  3 15:41:14.132: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 10.026342139s
Dec  3 15:41:16.136: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 12.030970634s
Dec  3 15:41:18.141: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 14.035628785s
Dec  3 15:41:20.145: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 16.039970498s
Dec  3 15:41:22.150: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 18.04468592s
Dec  3 15:41:24.155: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Running", Reason="", readiness=true. Elapsed: 20.049282756s
Dec  3 15:41:26.159: INFO: Pod "pod-subpath-test-projected-7vlt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053881076s
STEP: Saw pod success
Dec  3 15:41:26.159: INFO: Pod "pod-subpath-test-projected-7vlt" satisfied condition "success or failure"
Dec  3 15:41:26.163: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-subpath-test-projected-7vlt container test-container-subpath-projected-7vlt: <nil>
STEP: delete the pod
Dec  3 15:41:26.184: INFO: Waiting for pod pod-subpath-test-projected-7vlt to disappear
Dec  3 15:41:26.188: INFO: Pod pod-subpath-test-projected-7vlt no longer exists
STEP: Deleting pod pod-subpath-test-projected-7vlt
Dec  3 15:41:26.188: INFO: Deleting pod "pod-subpath-test-projected-7vlt" in namespace "subpath-9731"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:41:26.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9731" for this suite.
Dec  3 15:41:32.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:32.387: INFO: namespace subpath-9731 deletion completed in 6.189040099s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:41:32.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 15:41:36.587: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:36.591: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:38.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:38.595: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:40.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:40.595: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:42.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:42.596: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:44.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:44.595: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:46.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:46.595: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:48.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:48.595: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:50.593: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:50.597: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:41:52.591: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:41:52.596: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:41:52.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4500" for this suite.
Dec  3 15:42:14.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:42:14.797: INFO: namespace container-lifecycle-hook-4500 deletion completed in 22.194183382s
•SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:42:14.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:42:14.948: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465" in namespace "downward-api-9467" to be "success or failure"
Dec  3 15:42:14.952: INFO: Pod "downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767931ms
Dec  3 15:42:16.957: INFO: Pod "downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008784563s
STEP: Saw pod success
Dec  3 15:42:16.957: INFO: Pod "downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:42:16.961: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:42:16.981: INFO: Waiting for pod downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465 to disappear
Dec  3 15:42:16.984: INFO: Pod downwardapi-volume-7ac06f4d-15e3-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:42:16.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9467" for this suite.
Dec  3 15:42:23.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:42:23.139: INFO: namespace downward-api-9467 deletion completed in 6.148892003s
•SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:42:23.139: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Dec  3 15:42:23.294: INFO: Waiting up to 5m0s for pod "client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465" in namespace "containers-1623" to be "success or failure"
Dec  3 15:42:23.297: INFO: Pod "client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.400402ms
Dec  3 15:42:25.302: INFO: Pod "client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008111745s
STEP: Saw pod success
Dec  3 15:42:25.302: INFO: Pod "client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:42:25.306: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:42:25.326: INFO: Waiting for pod client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465 to disappear
Dec  3 15:42:25.329: INFO: Pod client-containers-7fb9feb7-15e3-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:42:25.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1623" for this suite.
Dec  3 15:42:31.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:42:31.479: INFO: namespace containers-1623 deletion completed in 6.143603908s
•SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:42:31.479: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:42:49.643: INFO: Container started at 2019-12-03 15:42:32 +0000 UTC, pod became ready at 2019-12-03 15:42:49 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:42:49.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5344" for this suite.
Dec  3 15:43:11.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:11.836: INFO: namespace container-probe-5344 deletion completed in 22.186425767s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:43:11.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Dec  3 15:43:11.988: INFO: Waiting up to 5m0s for pod "client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465" in namespace "containers-2546" to be "success or failure"
Dec  3 15:43:11.991: INFO: Pod "client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200333ms
Dec  3 15:43:13.997: INFO: Pod "client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008293741s
STEP: Saw pod success
Dec  3 15:43:13.997: INFO: Pod "client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:43:14.000: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:43:14.020: INFO: Waiting for pod client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465 to disappear
Dec  3 15:43:14.023: INFO: Pod client-containers-9cc021f7-15e3-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:43:14.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2546" for this suite.
Dec  3 15:43:20.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:20.173: INFO: namespace containers-2546 deletion completed in 6.139970994s
•SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:43:20.173: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:43:22.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2905" for this suite.
Dec  3 15:44:02.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:02.500: INFO: namespace kubelet-test-2905 deletion completed in 40.146333068s
•
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:44:02.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3621
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-baf3a19e-15e3-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-baf3a19e-15e3-11ea-9142-f2314dba9465
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:44:06.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3621" for this suite.
Dec  3 15:44:28.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:28.940: INFO: namespace projected-3621 deletion completed in 22.145038814s
•
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:44:28.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:44:31.620: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cab58abb-15e3-11ea-9142-f2314dba9465"
Dec  3 15:44:31.620: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cab58abb-15e3-11ea-9142-f2314dba9465" in namespace "pods-5547" to be "terminated due to deadline exceeded"
Dec  3 15:44:31.624: INFO: Pod "pod-update-activedeadlineseconds-cab58abb-15e3-11ea-9142-f2314dba9465": Phase="Running", Reason="", readiness=true. Elapsed: 3.747942ms
Dec  3 15:44:33.628: INFO: Pod "pod-update-activedeadlineseconds-cab58abb-15e3-11ea-9142-f2314dba9465": Phase="Running", Reason="", readiness=true. Elapsed: 2.008136707s
Dec  3 15:44:35.633: INFO: Pod "pod-update-activedeadlineseconds-cab58abb-15e3-11ea-9142-f2314dba9465": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012794542s
Dec  3 15:44:35.633: INFO: Pod "pod-update-activedeadlineseconds-cab58abb-15e3-11ea-9142-f2314dba9465" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:44:35.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5547" for this suite.
Dec  3 15:44:41.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:41.783: INFO: namespace pods-5547 deletion completed in 6.144006465s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:44:41.783: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d25cc793-15e3-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:44:41.938: INFO: Waiting up to 5m0s for pod "pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465" in namespace "secrets-1381" to be "success or failure"
Dec  3 15:44:41.942: INFO: Pod "pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.899993ms
Dec  3 15:44:43.947: INFO: Pod "pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008474581s
STEP: Saw pod success
Dec  3 15:44:43.947: INFO: Pod "pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:44:43.951: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:44:43.971: INFO: Waiting for pod pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465 to disappear
Dec  3 15:44:43.974: INFO: Pod pod-secrets-d25d68eb-15e3-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:44:43.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1381" for this suite.
Dec  3 15:44:49.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:50.124: INFO: namespace secrets-1381 deletion completed in 6.144179396s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:44:50.125: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:44:50.268: INFO: Creating ReplicaSet my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465
Dec  3 15:44:50.276: INFO: Pod name my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465: Found 0 pods out of 1
Dec  3 15:44:55.281: INFO: Pod name my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465: Found 1 pods out of 1
Dec  3 15:44:55.281: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465" is running
Dec  3 15:44:55.285: INFO: Pod "my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465-pbvmt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:44:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:44:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:44:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:44:50 +0000 UTC Reason: Message:}])
Dec  3 15:44:55.285: INFO: Trying to dial the pod
Dec  3 15:45:00.391: INFO: Controller my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465: Got expected result from replica 1 [my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465-pbvmt]: "my-hostname-basic-d7558823-15e3-11ea-9142-f2314dba9465-pbvmt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:45:00.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3172" for this suite.
Dec  3 15:45:06.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:06.572: INFO: namespace replicaset-3172 deletion completed in 6.17468271s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:45:06.573: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:45:06.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8657'
Dec  3 15:45:07.122: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:45:07.122: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Dec  3 15:45:07.140: INFO: scanned /root for discovery docs: <nil>
Dec  3 15:45:07.140: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8657'
Dec  3 15:45:22.926: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 15:45:22.926: INFO: stdout: "Created e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7\nScaling up e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  3 15:45:22.926: INFO: stdout: "Created e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7\nScaling up e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  3 15:45:22.926: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8657'
Dec  3 15:45:23.051: INFO: stderr: ""
Dec  3 15:45:23.051: INFO: stdout: "e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7-lqvbb "
Dec  3 15:45:23.051: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7-lqvbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8657'
Dec  3 15:45:23.132: INFO: stderr: ""
Dec  3 15:45:23.132: INFO: stdout: "true"
Dec  3 15:45:23.132: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7-lqvbb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8657'
Dec  3 15:45:23.257: INFO: stderr: ""
Dec  3 15:45:23.257: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  3 15:45:23.257: INFO: e2e-test-nginx-rc-de8dcb5bb036ea4e869cfc1f419829c7-lqvbb is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Dec  3 15:45:23.257: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-8657'
Dec  3 15:45:23.392: INFO: stderr: ""
Dec  3 15:45:23.392: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:45:23.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8657" for this suite.
Dec  3 15:45:45.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:45.545: INFO: namespace kubectl-8657 deletion completed in 22.146500218s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:45:45.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:45:45.689: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7539'
Dec  3 15:45:45.787: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:45:45.787: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  3 15:45:45.794: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-l7mn4]
Dec  3 15:45:45.794: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-l7mn4" in namespace "kubectl-7539" to be "running and ready"
Dec  3 15:45:45.797: INFO: Pod "e2e-test-nginx-rc-l7mn4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.362347ms
Dec  3 15:45:47.802: INFO: Pod "e2e-test-nginx-rc-l7mn4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008289474s
Dec  3 15:45:47.802: INFO: Pod "e2e-test-nginx-rc-l7mn4" satisfied condition "running and ready"
Dec  3 15:45:47.802: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-l7mn4]
Dec  3 15:45:47.802: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-7539'
Dec  3 15:45:48.015: INFO: stderr: ""
Dec  3 15:45:48.015: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Dec  3 15:45:48.015: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-7539'
Dec  3 15:45:48.148: INFO: stderr: ""
Dec  3 15:45:48.148: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:45:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7539" for this suite.
Dec  3 15:45:54.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:54.341: INFO: namespace kubectl-7539 deletion completed in 6.185295067s
•SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:45:54.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  3 15:45:54.485: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:45:58.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9803" for this suite.
Dec  3 15:46:04.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:04.750: INFO: namespace init-container-9803 deletion completed in 6.191197333s
•
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:46:04.750: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  3 15:46:04.894: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:46:07.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9414" for this suite.
Dec  3 15:46:13.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:13.836: INFO: namespace init-container-9414 deletion completed in 6.144056428s
•SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:46:13.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:46:13.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465" in namespace "downward-api-4005" to be "success or failure"
Dec  3 15:46:13.991: INFO: Pod "downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.799498ms
Dec  3 15:46:15.995: INFO: Pod "downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008590559s
STEP: Saw pod success
Dec  3 15:46:15.996: INFO: Pod "downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:46:15.999: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:46:16.023: INFO: Waiting for pod downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:46:16.026: INFO: Pod downwardapi-volume-093ae9a9-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:46:16.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4005" for this suite.
Dec  3 15:46:22.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:22.179: INFO: namespace downward-api-4005 deletion completed in 6.146656088s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:46:22.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:46:22.324: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7405'
Dec  3 15:46:22.437: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:46:22.437: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Dec  3 15:46:24.445: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-7405'
Dec  3 15:46:24.554: INFO: stderr: ""
Dec  3 15:46:24.554: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:46:24.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7405" for this suite.
Dec  3 15:46:30.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:30.748: INFO: namespace kubectl-7405 deletion completed in 6.185636457s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:46:30.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:46:30.894: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-9'
Dec  3 15:46:31.042: INFO: stderr: ""
Dec  3 15:46:31.042: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  3 15:46:36.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-9 -o json'
Dec  3 15:46:36.212: INFO: stderr: ""
Dec  3 15:46:36.212: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.64.1.162/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-12-03T15:46:31Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-9\",\n        \"resourceVersion\": \"20849\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9/pods/e2e-test-nginx-pod\",\n        \"uid\": \"1364bb50-15e4-11ea-a7d5-563937197a2b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-k86jn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"izgw8jactz8ahkwwzt6d3gz\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-k86jn\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-k86jn\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:46:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:46:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:46:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:46:31Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6e412173de0fc083390edc5957bffaf3c629b39bdb1779ce1ef5e4ad1b66ed3e\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-03T15:46:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.16.152\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.162\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-03T15:46:31Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  3 15:46:36.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-9'
Dec  3 15:46:36.559: INFO: stderr: ""
Dec  3 15:46:36.559: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec  3 15:46:36.563: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-9'
Dec  3 15:46:38.270: INFO: stderr: ""
Dec  3 15:46:38.270: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:46:38.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9" for this suite.
Dec  3 15:46:44.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:44.437: INFO: namespace kubectl-9 deletion completed in 6.160023108s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:46:44.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:46:44.587: INFO: Waiting up to 5m0s for pod "pod-1b783214-15e4-11ea-9142-f2314dba9465" in namespace "emptydir-5450" to be "success or failure"
Dec  3 15:46:44.591: INFO: Pod "pod-1b783214-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.559131ms
Dec  3 15:46:46.595: INFO: Pod "pod-1b783214-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008255527s
STEP: Saw pod success
Dec  3 15:46:46.596: INFO: Pod "pod-1b783214-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:46:46.599: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-1b783214-15e4-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:46:46.618: INFO: Waiting for pod pod-1b783214-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:46:46.621: INFO: Pod pod-1b783214-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:46:46.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5450" for this suite.
Dec  3 15:46:52.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:52.816: INFO: namespace emptydir-5450 deletion completed in 6.188444419s
•SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:46:52.816: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 15:46:57.014: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:46:57.019: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:46:59.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:46:59.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:01.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:01.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:03.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:03.029: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:05.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:05.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:07.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:07.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:09.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:09.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:11.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:11.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:13.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:13.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:15.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:15.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:17.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:17.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:19.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:19.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:21.019: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:21.024: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:47:23.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:47:23.024: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:47:23.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4668" for this suite.
Dec  3 15:47:37.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:47:37.172: INFO: namespace container-lifecycle-hook-4668 deletion completed in 14.142018766s
•
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:47:37.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-9055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9055 to expose endpoints map[]
Dec  3 15:47:37.327: INFO: successfully validated that service endpoint-test2 in namespace services-9055 exposes endpoints map[] (3.415191ms elapsed)
STEP: Creating pod pod1 in namespace services-9055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9055 to expose endpoints map[pod1:[80]]
Dec  3 15:47:39.356: INFO: successfully validated that service endpoint-test2 in namespace services-9055 exposes endpoints map[pod1:[80]] (2.022869859s elapsed)
STEP: Creating pod pod2 in namespace services-9055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9055 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  3 15:47:40.383: INFO: successfully validated that service endpoint-test2 in namespace services-9055 exposes endpoints map[pod1:[80] pod2:[80]] (1.021914468s elapsed)
STEP: Deleting pod pod1 in namespace services-9055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9055 to expose endpoints map[pod2:[80]]
Dec  3 15:47:40.395: INFO: successfully validated that service endpoint-test2 in namespace services-9055 exposes endpoints map[pod2:[80]] (6.726392ms elapsed)
STEP: Deleting pod pod2 in namespace services-9055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9055 to expose endpoints map[]
Dec  3 15:47:40.403: INFO: successfully validated that service endpoint-test2 in namespace services-9055 exposes endpoints map[] (3.26894ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:47:40.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9055" for this suite.
Dec  3 15:48:02.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:02.567: INFO: namespace services-9055 deletion completed in 22.14814573s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:48:02.567: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7209
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7209 to expose endpoints map[]
Dec  3 15:48:02.724: INFO: successfully validated that service multi-endpoint-test in namespace services-7209 exposes endpoints map[] (3.324952ms elapsed)
STEP: Creating pod pod1 in namespace services-7209
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7209 to expose endpoints map[pod1:[100]]
Dec  3 15:48:04.754: INFO: successfully validated that service multi-endpoint-test in namespace services-7209 exposes endpoints map[pod1:[100]] (2.024162412s elapsed)
STEP: Creating pod pod2 in namespace services-7209
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7209 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  3 15:48:06.795: INFO: successfully validated that service multi-endpoint-test in namespace services-7209 exposes endpoints map[pod1:[100] pod2:[101]] (2.035154598s elapsed)
STEP: Deleting pod pod1 in namespace services-7209
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7209 to expose endpoints map[pod2:[101]]
Dec  3 15:48:06.806: INFO: successfully validated that service multi-endpoint-test in namespace services-7209 exposes endpoints map[pod2:[101]] (6.75005ms elapsed)
STEP: Deleting pod pod2 in namespace services-7209
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7209 to expose endpoints map[]
Dec  3 15:48:06.814: INFO: successfully validated that service multi-endpoint-test in namespace services-7209 exposes endpoints map[] (3.07112ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:48:06.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7209" for this suite.
Dec  3 15:48:28.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:28.976: INFO: namespace services-7209 deletion completed in 22.145971695s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:48:28.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1203 15:49:09.158613    5087 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:49:09.158: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:49:09.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6461" for this suite.
Dec  3 15:49:15.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:15.313: INFO: namespace gc-6461 deletion completed in 6.150131669s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:49:15.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Dec  3 15:49:15.458: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3581'
Dec  3 15:49:15.717: INFO: stderr: ""
Dec  3 15:49:15.717: INFO: stdout: "pod/pause created\n"
Dec  3 15:49:15.717: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  3 15:49:15.718: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3581" to be "running and ready"
Dec  3 15:49:15.726: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042529ms
Dec  3 15:49:17.730: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.012754636s
Dec  3 15:49:17.730: INFO: Pod "pause" satisfied condition "running and ready"
Dec  3 15:49:17.730: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  3 15:49:17.730: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-3581'
Dec  3 15:49:17.864: INFO: stderr: ""
Dec  3 15:49:17.864: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  3 15:49:17.864: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-3581'
Dec  3 15:49:18.033: INFO: stderr: ""
Dec  3 15:49:18.033: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  3 15:49:18.033: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-3581'
Dec  3 15:49:18.190: INFO: stderr: ""
Dec  3 15:49:18.190: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  3 15:49:18.190: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-3581'
Dec  3 15:49:18.317: INFO: stderr: ""
Dec  3 15:49:18.317: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Dec  3 15:49:18.317: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3581'
Dec  3 15:49:18.504: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:49:18.504: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  3 15:49:18.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-3581'
Dec  3 15:49:18.632: INFO: stderr: "No resources found.\n"
Dec  3 15:49:18.632: INFO: stdout: ""
Dec  3 15:49:18.632: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-3581 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:49:18.768: INFO: stderr: ""
Dec  3 15:49:18.768: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:49:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3581" for this suite.
Dec  3 15:49:24.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:24.917: INFO: namespace kubectl-3581 deletion completed in 6.142724505s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:49:24.918: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1071
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-7b2264c8-15e4-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:49:25.090: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465" in namespace "configmap-1071" to be "success or failure"
Dec  3 15:49:25.094: INFO: Pod "pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.532326ms
Dec  3 15:49:27.098: INFO: Pod "pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007994986s
STEP: Saw pod success
Dec  3 15:49:27.099: INFO: Pod "pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:49:27.102: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:49:27.123: INFO: Waiting for pod pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:49:27.126: INFO: Pod pod-configmaps-7b231435-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:49:27.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1071" for this suite.
Dec  3 15:49:33.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:33.279: INFO: namespace configmap-1071 deletion completed in 6.146605227s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:49:33.279: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7195
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:49:33.423: INFO: Creating deployment "test-recreate-deployment"
Dec  3 15:49:33.428: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  3 15:49:33.435: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  3 15:49:35.444: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  3 15:49:35.447: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  3 15:49:35.455: INFO: Updating deployment test-recreate-deployment
Dec  3 15:49:35.456: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  3 15:49:35.483: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7195,SelfLink:/apis/apps/v1/namespaces/deployment-7195/deployments/test-recreate-deployment,UID:801ba7c6-15e4-11ea-a7d5-563937197a2b,ResourceVersion:21735,Generation:2,CreationTimestamp:2019-12-03 15:49:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-03 15:49:35 +0000 UTC 2019-12-03 15:49:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-03 15:49:35 +0000 UTC 2019-12-03 15:49:33 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  3 15:49:35.487: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-7195,SelfLink:/apis/apps/v1/namespaces/deployment-7195/replicasets/test-recreate-deployment-745fb9c84c,UID:81531686-15e4-11ea-a7d5-563937197a2b,ResourceVersion:21734,Generation:1,CreationTimestamp:2019-12-03 15:49:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 801ba7c6-15e4-11ea-a7d5-563937197a2b 0xc001cb52c7 0xc001cb52c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:49:35.487: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  3 15:49:35.487: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-7195,SelfLink:/apis/apps/v1/namespaces/deployment-7195/replicasets/test-recreate-deployment-6566d46b4b,UID:801c27b1-15e4-11ea-a7d5-563937197a2b,ResourceVersion:21727,Generation:2,CreationTimestamp:2019-12-03 15:49:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 801ba7c6-15e4-11ea-a7d5-563937197a2b 0xc001cb5107 0xc001cb5108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:49:35.491: INFO: Pod "test-recreate-deployment-745fb9c84c-5qjb5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-5qjb5,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-7195,SelfLink:/api/v1/namespaces/deployment-7195/pods/test-recreate-deployment-745fb9c84c-5qjb5,UID:8153793b-15e4-11ea-a7d5-563937197a2b,ResourceVersion:21732,Generation:0,CreationTimestamp:2019-12-03 15:49:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c 81531686-15e4-11ea-a7d5-563937197a2b 0xc001cb5b87 0xc001cb5b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-88zkc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-88zkc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-88zkc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001cb5bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001cb5c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:49:35 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:49:35.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7195" for this suite.
Dec  3 15:49:41.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:41.662: INFO: namespace deployment-7195 deletion completed in 6.163742993s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:49:41.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-575
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Dec  3 15:49:41.807: INFO: namespace kubectl-575
Dec  3 15:49:41.807: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-575'
Dec  3 15:49:42.152: INFO: stderr: ""
Dec  3 15:49:42.152: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 15:49:43.157: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:49:43.157: INFO: Found 0 / 1
Dec  3 15:49:44.157: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:49:44.157: INFO: Found 1 / 1
Dec  3 15:49:44.157: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 15:49:44.161: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:49:44.162: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 15:49:44.162: INFO: wait on redis-master startup in kubectl-575 
Dec  3 15:49:44.162: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-9qkct redis-master --namespace=kubectl-575'
Dec  3 15:49:44.341: INFO: stderr: ""
Dec  3 15:49:44.341: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:49:43.001 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:49:43.001 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:49:43.001 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:49:43.001 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  3 15:49:44.342: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-575'
Dec  3 15:49:44.518: INFO: stderr: ""
Dec  3 15:49:44.518: INFO: stdout: "service/rm2 exposed\n"
Dec  3 15:49:44.521: INFO: Service rm2 in namespace kubectl-575 found.
STEP: exposing service
Dec  3 15:49:46.530: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-575'
Dec  3 15:49:46.736: INFO: stderr: ""
Dec  3 15:49:46.736: INFO: stdout: "service/rm3 exposed\n"
Dec  3 15:49:46.740: INFO: Service rm3 in namespace kubectl-575 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:49:48.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-575" for this suite.
Dec  3 15:50:10.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:10.898: INFO: namespace kubectl-575 deletion completed in 22.142567724s
•SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:50:10.898: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-963
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-963
STEP: Creating statefulset with conflicting port in namespace statefulset-963
STEP: Waiting until pod test-pod will start running in namespace statefulset-963
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-963
Dec  3 15:50:13.071: INFO: Observed stateful pod in namespace: statefulset-963, name: ss-0, uid: 97bba9ea-15e4-11ea-a7d5-563937197a2b, status phase: Pending. Waiting for statefulset controller to delete.
Dec  3 15:50:13.078: INFO: Observed stateful pod in namespace: statefulset-963, name: ss-0, uid: 97bba9ea-15e4-11ea-a7d5-563937197a2b, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:50:13.114: INFO: Observed stateful pod in namespace: statefulset-963, name: ss-0, uid: 97bba9ea-15e4-11ea-a7d5-563937197a2b, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:50:13.115: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-963
STEP: Removing pod with conflicting port in namespace statefulset-963
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-963 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  3 15:50:15.129: INFO: Deleting all statefulset in ns statefulset-963
Dec  3 15:50:15.133: INFO: Scaling statefulset ss to 0
Dec  3 15:50:25.151: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:50:25.155: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:50:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-963" for this suite.
Dec  3 15:50:31.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:31.537: INFO: namespace statefulset-963 deletion completed in 6.363726407s
•SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:50:31.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-a2e0560a-15e4-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:50:31.767: INFO: Waiting up to 5m0s for pod "pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465" in namespace "configmap-2911" to be "success or failure"
Dec  3 15:50:31.770: INFO: Pod "pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453241ms
Dec  3 15:50:33.777: INFO: Pod "pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009929257s
STEP: Saw pod success
Dec  3 15:50:33.777: INFO: Pod "pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:50:33.781: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:50:33.802: INFO: Waiting for pod pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:50:33.806: INFO: Pod pod-configmaps-a2e0fba8-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:50:33.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2911" for this suite.
Dec  3 15:50:39.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:39.952: INFO: namespace configmap-2911 deletion completed in 6.140486707s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:50:39.953: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:50:40.096: INFO: Creating deployment "nginx-deployment"
Dec  3 15:50:40.100: INFO: Waiting for observed generation 1
Dec  3 15:50:42.109: INFO: Waiting for all required pods to come up
Dec  3 15:50:42.115: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  3 15:50:44.127: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  3 15:50:44.135: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  3 15:50:44.142: INFO: Updating deployment nginx-deployment
Dec  3 15:50:44.143: INFO: Waiting for observed generation 2
Dec  3 15:50:46.150: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  3 15:50:46.154: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  3 15:50:46.158: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  3 15:50:46.169: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  3 15:50:46.169: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  3 15:50:46.173: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  3 15:50:46.179: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  3 15:50:46.179: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  3 15:50:46.187: INFO: Updating deployment nginx-deployment
Dec  3 15:50:46.187: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  3 15:50:46.195: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  3 15:50:48.202: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  3 15:50:48.211: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-6193,SelfLink:/apis/apps/v1/namespaces/deployment-6193/deployments/nginx-deployment,UID:a7d90edd-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22344,Generation:3,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-03 15:50:46 +0000 UTC 2019-12-03 15:50:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-03 15:50:46 +0000 UTC 2019-12-03 15:50:40 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  3 15:50:48.215: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-6193,SelfLink:/apis/apps/v1/namespaces/deployment-6193/replicasets/nginx-deployment-b79c9d74d,UID:aa425424-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22329,Generation:3,CreationTimestamp:2019-12-03 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a7d90edd-15e4-11ea-a7d5-563937197a2b 0xc001c87377 0xc001c87378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:50:48.215: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  3 15:50:48.215: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-6193,SelfLink:/apis/apps/v1/namespaces/deployment-6193/replicasets/nginx-deployment-85db8c99c5,UID:a7d98f4e-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22328,Generation:3,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a7d90edd-15e4-11ea-a7d5-563937197a2b 0xc001c87287 0xc001c87288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  3 15:50:48.225: INFO: Pod "nginx-deployment-85db8c99c5-24jvb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-24jvb,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-24jvb,UID:ab7f6157-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22343,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002d778f7 0xc002d778f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d77960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d77980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.225: INFO: Pod "nginx-deployment-85db8c99c5-5wlt4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-5wlt4,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-5wlt4,UID:a7db7333-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22193,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002d77a57 0xc002d77a58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d77ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d77ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:100.64.0.43,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://530abadea68c9b97ee66d9fcfbda5bbb6dbbaa5a5ce35e1026735f80f60670dc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.226: INFO: Pod "nginx-deployment-85db8c99c5-989sz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-989sz,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-989sz,UID:a7da43ff-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22211,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.183/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002d77bc0 0xc002d77bc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d77c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d77c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.183,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5723ed8ec92118741711266b3a607b0fe314eb65b24e0a08fb4104b2913d668d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.226: INFO: Pod "nginx-deployment-85db8c99c5-b89ns" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-b89ns,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-b89ns,UID:a7dae4f6-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22199,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.42/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002d77d27 0xc002d77d28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d77d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d77db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:100.64.0.42,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2c15e6df821475c6cd3d12df3b1704b416799c4eba368ab0429e07f0ed28eee8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.226: INFO: Pod "nginx-deployment-85db8c99c5-bfl5x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-bfl5x,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-bfl5x,UID:ab7f6337-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22345,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002d77e80 0xc002d77e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d77ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d77f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.226: INFO: Pod "nginx-deployment-85db8c99c5-c8bck" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-c8bck,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-c8bck,UID:ab7f7cc6-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22347,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002d77fc7 0xc002d77fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027500a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.226: INFO: Pod "nginx-deployment-85db8c99c5-fwmlm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-fwmlm,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-fwmlm,UID:ab7bf6ba-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22358,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002750257 0xc002750258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002750380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.227: INFO: Pod "nginx-deployment-85db8c99c5-h5598" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-h5598,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-h5598,UID:ab7b5799-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22331,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc0027504e7 0xc0027504e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002750650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.227: INFO: Pod "nginx-deployment-85db8c99c5-hzd64" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-hzd64,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-hzd64,UID:a7dc0297-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22196,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.41/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc0027507c7 0xc0027507c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027508d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:100.64.0.41,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://63252c62e4c718b08ed71716b3e1621bc093994851c63151d62ab81d274a7515}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.227: INFO: Pod "nginx-deployment-85db8c99c5-jrss7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-jrss7,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-jrss7,UID:a7db6f3c-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22217,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.187/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002750ab0 0xc002750ab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002750b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.187,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://41ccbaaef60aad95dd081f8f467c0631b4a8675e9944265c2aaa42525c45f60e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.227: INFO: Pod "nginx-deployment-85db8c99c5-k678q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-k678q,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-k678q,UID:ab80bdbe-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22348,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002750cc7 0xc002750cc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002750d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.227: INFO: Pod "nginx-deployment-85db8c99c5-k9m5r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-k9m5r,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-k9m5r,UID:ab80539a-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22349,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002750e17 0xc002750e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002750ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.228: INFO: Pod "nginx-deployment-85db8c99c5-lm6kd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-lm6kd,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-lm6kd,UID:ab7b5843-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22327,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002750f67 0xc002750f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002750fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002750ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.228: INFO: Pod "nginx-deployment-85db8c99c5-lxtw7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-lxtw7,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-lxtw7,UID:ab7bf567-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22337,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc0027510b7 0xc0027510b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.228: INFO: Pod "nginx-deployment-85db8c99c5-mhnfp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-mhnfp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-mhnfp,UID:ab7c0692-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22334,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002751207 0xc002751208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.228: INFO: Pod "nginx-deployment-85db8c99c5-mlq24" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-mlq24,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-mlq24,UID:ab7bfc22-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22335,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002751357 0xc002751358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.228: INFO: Pod "nginx-deployment-85db8c99c5-r6n6v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-r6n6v,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-r6n6v,UID:a7dc04cb-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22214,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.188/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc0027515d7 0xc0027515d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.188,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8ae99434acf2e142b014d7d1a3b5c8882fa7997a4390b4f69894150d17eca41b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.228: INFO: Pod "nginx-deployment-85db8c99c5-rg8jc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-rg8jc,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-rg8jc,UID:ab7acf1e-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22357,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.194/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002751817 0xc002751818}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027518a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.229: INFO: Pod "nginx-deployment-85db8c99c5-rx8n9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-rx8n9,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-rx8n9,UID:a7db5f7f-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22220,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.185/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002751a17 0xc002751a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.185,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e3f2de254a5b49deb00cf6ca164d7abb0e989bd8e33e520d91240a417f420c9e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.229: INFO: Pod "nginx-deployment-85db8c99c5-wxnqj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-wxnqj,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-85db8c99c5-wxnqj,UID:a7dbf431-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22208,Generation:0,CreationTimestamp:2019-12-03 15:50:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.186/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 a7d98f4e-15e4-11ea-a7d5-563937197a2b 0xc002751c87 0xc002751c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:40 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.186,StartTime:2019-12-03 15:50:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://04dfed65f5c6420c827f09f4e763279896d66a9089942e15e724d96ce2415fbc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.229: INFO: Pod "nginx-deployment-b79c9d74d-2ttn6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-2ttn6,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-2ttn6,UID:ab80d55b-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22338,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002751f07 0xc002751f08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002751fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002751ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.229: INFO: Pod "nginx-deployment-b79c9d74d-7f4pz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-7f4pz,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-7f4pz,UID:ab7bcbfd-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22352,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.193/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e560d0 0xc002e560d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.229: INFO: Pod "nginx-deployment-b79c9d74d-7qh6q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-7qh6q,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-7qh6q,UID:aa457fba-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22271,Generation:0,CreationTimestamp:2019-12-03 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.45/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56240 0xc002e56241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e562b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e562d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.230: INFO: Pod "nginx-deployment-b79c9d74d-7sx4n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-7sx4n,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-7sx4n,UID:ab7f2366-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22342,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e563a0 0xc002e563a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.230: INFO: Pod "nginx-deployment-b79c9d74d-8qfg4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-8qfg4,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-8qfg4,UID:aa4510de-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22275,Generation:0,CreationTimestamp:2019-12-03 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.192/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56510 0xc002e56511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e565a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.230: INFO: Pod "nginx-deployment-b79c9d74d-c4dw7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-c4dw7,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-c4dw7,UID:aa434484-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22346,Generation:0,CreationTimestamp:2019-12-03 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56680 0xc002e56681}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e566f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:100.64.0.44,StartTime:2019-12-03 15:50:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.230: INFO: Pod "nginx-deployment-b79c9d74d-dv65j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-dv65j,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-dv65j,UID:ab7b322a-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22353,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.46/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56810 0xc002e56811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e568a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.230: INFO: Pod "nginx-deployment-b79c9d74d-krs86" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-krs86,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-krs86,UID:ab7bbd3f-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22332,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56970 0xc002e56971}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e569e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.230: INFO: Pod "nginx-deployment-b79c9d74d-pvlnq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-pvlnq,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-pvlnq,UID:ab7e991a-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22333,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56ad0 0xc002e56ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.231: INFO: Pod "nginx-deployment-b79c9d74d-rlcbx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-rlcbx,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-rlcbx,UID:ab7c88a9-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22339,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56c30 0xc002e56c31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8afzp8040eosj1udh4z,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.153,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.231: INFO: Pod "nginx-deployment-b79c9d74d-rmrvq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-rmrvq,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-rmrvq,UID:aa434d6b-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22272,Generation:0,CreationTimestamp:2019-12-03 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.190/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56db0 0xc002e56db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.231: INFO: Pod "nginx-deployment-b79c9d74d-wl8gt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-wl8gt,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-wl8gt,UID:ab7fdabd-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22336,Generation:0,CreationTimestamp:2019-12-03 15:50:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e56f10 0xc002e56f11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e56f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e56fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:50:48.231: INFO: Pod "nginx-deployment-b79c9d74d-xl9q6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-xl9q6,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-6193,SelfLink:/api/v1/namespaces/deployment-6193/pods/nginx-deployment-b79c9d74d-xl9q6,UID:aa42bae7-15e4-11ea-a7d5-563937197a2b,ResourceVersion:22274,Generation:0,CreationTimestamp:2019-12-03 15:50:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.191/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d aa425424-15e4-11ea-a7d5-563937197a2b 0xc002e57090 0xc002e57091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qf227 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qf227,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qf227 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e57100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e57120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:,StartTime:2019-12-03 15:50:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:50:48.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6193" for this suite.
Dec  3 15:50:54.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:54.420: INFO: namespace deployment-6193 deletion completed in 6.184596626s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:50:54.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:50:54.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465" in namespace "projected-4665" to be "success or failure"
Dec  3 15:50:54.575: INFO: Pod "downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.665591ms
Dec  3 15:50:56.580: INFO: Pod "downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008511861s
Dec  3 15:50:58.585: INFO: Pod "downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01351324s
STEP: Saw pod success
Dec  3 15:50:58.585: INFO: Pod "downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:50:58.588: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:50:58.611: INFO: Waiting for pod downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:50:58.614: INFO: Pod downwardapi-volume-b078ad90-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:50:58.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4665" for this suite.
Dec  3 15:51:04.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:04.771: INFO: namespace projected-4665 deletion completed in 6.150474627s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:51:04.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-b6a41a01-15e4-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:51:04.925: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465" in namespace "projected-3893" to be "success or failure"
Dec  3 15:51:04.929: INFO: Pod "pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513322ms
Dec  3 15:51:06.934: INFO: Pod "pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00813741s
STEP: Saw pod success
Dec  3 15:51:06.934: INFO: Pod "pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:51:06.937: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:51:06.959: INFO: Waiting for pod pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:51:06.962: INFO: Pod pod-projected-secrets-b6a4ba83-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:51:06.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3893" for this suite.
Dec  3 15:51:12.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:13.112: INFO: namespace projected-3893 deletion completed in 6.144549312s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:51:13.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 15:51:13.263: INFO: Waiting up to 5m0s for pod "pod-bb9cc39d-15e4-11ea-9142-f2314dba9465" in namespace "emptydir-6165" to be "success or failure"
Dec  3 15:51:13.266: INFO: Pod "pod-bb9cc39d-15e4-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580227ms
Dec  3 15:51:15.271: INFO: Pod "pod-bb9cc39d-15e4-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008331612s
STEP: Saw pod success
Dec  3 15:51:15.271: INFO: Pod "pod-bb9cc39d-15e4-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:51:15.275: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-bb9cc39d-15e4-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 15:51:15.294: INFO: Waiting for pod pod-bb9cc39d-15e4-11ea-9142-f2314dba9465 to disappear
Dec  3 15:51:15.298: INFO: Pod pod-bb9cc39d-15e4-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:51:15.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6165" for this suite.
Dec  3 15:51:21.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:21.448: INFO: namespace emptydir-6165 deletion completed in 6.143823998s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:51:21.448: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  3 15:51:24.148: INFO: Successfully updated pod "annotationupdatec095cc6e-15e4-11ea-9142-f2314dba9465"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:51:26.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1537" for this suite.
Dec  3 15:51:48.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:48.366: INFO: namespace downward-api-1537 deletion completed in 22.184869213s
•SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:51:48.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:51:50.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2749" for this suite.
Dec  3 15:52:28.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:28.700: INFO: namespace kubelet-test-2749 deletion completed in 38.145798174s
•SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:52:28.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-2621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-2621
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2621
STEP: Deleting pre-stop pod
Dec  3 15:52:39.977: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:52:39.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2621" for this suite.
Dec  3 15:53:18.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:18.135: INFO: namespace prestop-2621 deletion completed in 38.145786291s
•SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:53:18.135: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-0621ddca-15e5-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:53:18.290: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465" in namespace "projected-6783" to be "success or failure"
Dec  3 15:53:18.294: INFO: Pod "pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575647ms
Dec  3 15:53:20.299: INFO: Pod "pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008617624s
STEP: Saw pod success
Dec  3 15:53:20.299: INFO: Pod "pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:53:20.303: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:53:20.322: INFO: Waiting for pod pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:53:20.325: INFO: Pod pod-projected-configmaps-0622823d-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:53:20.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6783" for this suite.
Dec  3 15:53:26.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:26.475: INFO: namespace projected-6783 deletion completed in 6.144135573s
•SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:53:26.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:53:26.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465" in namespace "downward-api-844" to be "success or failure"
Dec  3 15:53:26.630: INFO: Pod "downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073098ms
Dec  3 15:53:28.634: INFO: Pod "downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008779448s
STEP: Saw pod success
Dec  3 15:53:28.634: INFO: Pod "downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:53:28.638: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:53:28.659: INFO: Waiting for pod downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:53:28.662: INFO: Pod downwardapi-volume-0b1a58ef-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:53:28.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-844" for this suite.
Dec  3 15:53:34.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:34.854: INFO: namespace downward-api-844 deletion completed in 6.185508852s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:53:34.854: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  3 15:53:35.004: INFO: Waiting up to 5m0s for pod "downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465" in namespace "downward-api-9488" to be "success or failure"
Dec  3 15:53:35.007: INFO: Pod "downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475727ms
Dec  3 15:53:37.012: INFO: Pod "downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008351326s
STEP: Saw pod success
Dec  3 15:53:37.012: INFO: Pod "downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:53:37.016: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:53:37.043: INFO: Waiting for pod downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:53:37.046: INFO: Pod downward-api-1018ceb6-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:53:37.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9488" for this suite.
Dec  3 15:53:43.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:43.194: INFO: namespace downward-api-9488 deletion completed in 6.14209054s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:53:43.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:53:43.338: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1109'
Dec  3 15:53:43.517: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:53:43.517: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Dec  3 15:53:43.521: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-1109'
Dec  3 15:53:43.693: INFO: stderr: ""
Dec  3 15:53:43.693: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:53:43.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1109" for this suite.
Dec  3 15:54:05.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:54:05.852: INFO: namespace kubectl-1109 deletion completed in 22.154737903s
•SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:54:05.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:54:08.030: INFO: Waiting up to 5m0s for pod "client-envvars-23c86331-15e5-11ea-9142-f2314dba9465" in namespace "pods-8979" to be "success or failure"
Dec  3 15:54:08.033: INFO: Pod "client-envvars-23c86331-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774827ms
Dec  3 15:54:10.038: INFO: Pod "client-envvars-23c86331-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008376205s
STEP: Saw pod success
Dec  3 15:54:10.038: INFO: Pod "client-envvars-23c86331-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:54:10.042: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod client-envvars-23c86331-15e5-11ea-9142-f2314dba9465 container env3cont: <nil>
STEP: delete the pod
Dec  3 15:54:10.062: INFO: Waiting for pod client-envvars-23c86331-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:54:10.065: INFO: Pod client-envvars-23c86331-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:54:10.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8979" for this suite.
Dec  3 15:54:54.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:54:54.264: INFO: namespace pods-8979 deletion completed in 44.189579632s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:54:54.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Dec  3 15:54:54.409: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5395'
Dec  3 15:54:54.661: INFO: stderr: ""
Dec  3 15:54:54.661: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 15:54:54.661: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5395'
Dec  3 15:54:54.799: INFO: stderr: ""
Dec  3 15:54:54.799: INFO: stdout: "update-demo-nautilus-24mrt update-demo-nautilus-zpd4p "
Dec  3 15:54:54.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-24mrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5395'
Dec  3 15:54:54.891: INFO: stderr: ""
Dec  3 15:54:54.891: INFO: stdout: ""
Dec  3 15:54:54.891: INFO: update-demo-nautilus-24mrt is created but not running
Dec  3 15:54:59.891: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5395'
Dec  3 15:54:59.984: INFO: stderr: ""
Dec  3 15:54:59.984: INFO: stdout: "update-demo-nautilus-24mrt update-demo-nautilus-zpd4p "
Dec  3 15:54:59.984: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-24mrt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5395'
Dec  3 15:55:00.065: INFO: stderr: ""
Dec  3 15:55:00.065: INFO: stdout: "true"
Dec  3 15:55:00.065: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-24mrt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5395'
Dec  3 15:55:00.193: INFO: stderr: ""
Dec  3 15:55:00.193: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 15:55:00.193: INFO: validating pod update-demo-nautilus-24mrt
Dec  3 15:55:00.289: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 15:55:00.289: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 15:55:00.289: INFO: update-demo-nautilus-24mrt is verified up and running
Dec  3 15:55:00.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-zpd4p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5395'
Dec  3 15:55:00.377: INFO: stderr: ""
Dec  3 15:55:00.377: INFO: stdout: "true"
Dec  3 15:55:00.377: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-zpd4p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5395'
Dec  3 15:55:00.498: INFO: stderr: ""
Dec  3 15:55:00.498: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 15:55:00.498: INFO: validating pod update-demo-nautilus-zpd4p
Dec  3 15:55:00.595: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 15:55:00.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 15:55:00.595: INFO: update-demo-nautilus-zpd4p is verified up and running
STEP: using delete to clean up resources
Dec  3 15:55:00.595: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Dec  3 15:55:00.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:55:00.749: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 15:55:00.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5395'
Dec  3 15:55:00.856: INFO: stderr: "No resources found.\n"
Dec  3 15:55:00.856: INFO: stdout: ""
Dec  3 15:55:00.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-5395 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:55:00.943: INFO: stderr: ""
Dec  3 15:55:00.943: INFO: stdout: "update-demo-nautilus-24mrt\nupdate-demo-nautilus-zpd4p\n"
Dec  3 15:55:01.443: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5395'
Dec  3 15:55:01.548: INFO: stderr: "No resources found.\n"
Dec  3 15:55:01.548: INFO: stdout: ""
Dec  3 15:55:01.548: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-5395 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:55:01.701: INFO: stderr: ""
Dec  3 15:55:01.701: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:55:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5395" for this suite.
Dec  3 15:55:23.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:23.851: INFO: namespace kubectl-5395 deletion completed in 22.143579602s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:55:23.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8521
STEP: Creating secret with name secret-test-5111df20-15e5-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:55:24.158: INFO: Waiting up to 5m0s for pod "pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465" in namespace "secrets-6576" to be "success or failure"
Dec  3 15:55:24.161: INFO: Pod "pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541435ms
Dec  3 15:55:26.166: INFO: Pod "pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008413978s
STEP: Saw pod success
Dec  3 15:55:26.166: INFO: Pod "pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:55:26.170: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:55:26.191: INFO: Waiting for pod pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:55:26.194: INFO: Pod pod-secrets-51286bcd-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:55:26.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6576" for this suite.
Dec  3 15:55:32.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:32.344: INFO: namespace secrets-6576 deletion completed in 6.143743114s
STEP: Destroying namespace "secret-namespace-8521" for this suite.
Dec  3 15:55:38.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:38.487: INFO: namespace secret-namespace-8521 deletion completed in 6.142967759s
•SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:55:38.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4736
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:55:38.631: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:55:56.702: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.220:8080/dial?request=hostName&protocol=udp&host=100.64.1.219&port=8081&tries=1'] Namespace:pod-network-test-4736 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:55:56.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:55:57.254: INFO: Waiting for endpoints: map[]
Dec  3 15:55:57.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.220:8080/dial?request=hostName&protocol=udp&host=100.64.0.56&port=8081&tries=1'] Namespace:pod-network-test-4736 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:55:57.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:55:57.652: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:55:57.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4736" for this suite.
Dec  3 15:56:19.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:19.876: INFO: namespace pod-network-test-4736 deletion completed in 22.217001586s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:56:19.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3151/configmap-test-72757bb8-15e5-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:56:20.032: INFO: Waiting up to 5m0s for pod "pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465" in namespace "configmap-3151" to be "success or failure"
Dec  3 15:56:20.035: INFO: Pod "pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.451324ms
Dec  3 15:56:22.040: INFO: Pod "pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008220721s
STEP: Saw pod success
Dec  3 15:56:22.040: INFO: Pod "pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:56:22.044: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465 container env-test: <nil>
STEP: delete the pod
Dec  3 15:56:22.067: INFO: Waiting for pod pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:56:22.070: INFO: Pod pod-configmaps-7276210b-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:56:22.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3151" for this suite.
Dec  3 15:56:28.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:28.220: INFO: namespace configmap-3151 deletion completed in 6.143398437s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:56:28.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4819
Dec  3 15:56:30.380: INFO: Started pod liveness-http in namespace container-probe-4819
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:56:30.384: INFO: Initial restart count of pod liveness-http is 0
Dec  3 15:56:54.447: INFO: Restart count of pod container-probe-4819/liveness-http is now 1 (24.063494311s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:56:54.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4819" for this suite.
Dec  3 15:57:00.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:00.600: INFO: namespace container-probe-4819 deletion completed in 6.138923264s
•SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:57:00.600: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:57:00.768: INFO: Create a RollingUpdate DaemonSet
Dec  3 15:57:00.772: INFO: Check that daemon pods launch on every node of the cluster
Dec  3 15:57:00.780: INFO: Number of nodes with available pods: 0
Dec  3 15:57:00.780: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 15:57:01.790: INFO: Number of nodes with available pods: 1
Dec  3 15:57:01.790: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 15:57:02.791: INFO: Number of nodes with available pods: 2
Dec  3 15:57:02.793: INFO: Number of running nodes: 2, number of available pods: 2
Dec  3 15:57:02.793: INFO: Update the DaemonSet to trigger a rollout
Dec  3 15:57:02.801: INFO: Updating DaemonSet daemon-set
Dec  3 15:57:07.820: INFO: Roll back the DaemonSet before rollout is complete
Dec  3 15:57:07.828: INFO: Updating DaemonSet daemon-set
Dec  3 15:57:07.828: INFO: Make sure DaemonSet rollback is complete
Dec  3 15:57:07.832: INFO: Wrong image for pod: daemon-set-vkz8d. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:57:07.832: INFO: Pod daemon-set-vkz8d is not available
Dec  3 15:57:08.841: INFO: Wrong image for pod: daemon-set-vkz8d. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:57:08.841: INFO: Pod daemon-set-vkz8d is not available
Dec  3 15:57:09.841: INFO: Pod daemon-set-7gskx is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6420, will wait for the garbage collector to delete the pods
Dec  3 15:57:09.916: INFO: Deleting DaemonSet.extensions daemon-set took: 5.802987ms
Dec  3 15:57:10.416: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.34241ms
Dec  3 15:57:21.420: INFO: Number of nodes with available pods: 0
Dec  3 15:57:21.421: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:57:21.424: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6420/daemonsets","resourceVersion":"24167"},"items":null}

Dec  3 15:57:21.428: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6420/pods","resourceVersion":"24167"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:57:21.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6420" for this suite.
Dec  3 15:57:27.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:27.589: INFO: namespace daemonsets-6420 deletion completed in 6.142982631s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:57:27.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:57:27.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6917'
Dec  3 15:57:28.098: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:57:28.098: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Dec  3 15:57:28.102: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-6917'
Dec  3 15:57:28.246: INFO: stderr: ""
Dec  3 15:57:28.246: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:57:28.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6917" for this suite.
Dec  3 15:57:34.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:34.397: INFO: namespace kubectl-6917 deletion completed in 6.144301899s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:57:34.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:57:34.547: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  3 15:57:39.552: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 15:57:39.552: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  3 15:57:41.557: INFO: Creating deployment "test-rollover-deployment"
Dec  3 15:57:41.565: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  3 15:57:43.573: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  3 15:57:43.582: INFO: Ensure that both replica sets have 1 created replica
Dec  3 15:57:43.590: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  3 15:57:43.601: INFO: Updating deployment test-rollover-deployment
Dec  3 15:57:43.601: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  3 15:57:45.609: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  3 15:57:45.617: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  3 15:57:45.625: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:57:45.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985464, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:57:47.635: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:57:47.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985464, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:57:49.634: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:57:49.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985464, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:57:51.637: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:57:51.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985464, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:57:53.634: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:57:53.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985464, loc:(*time.Location)(0x8830100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985461, loc:(*time.Location)(0x8830100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:57:55.634: INFO: 
Dec  3 15:57:55.634: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  3 15:57:55.651: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5304,SelfLink:/apis/apps/v1/namespaces/deployment-5304/deployments/test-rollover-deployment,UID:a30ee546-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24359,Generation:2,CreationTimestamp:2019-12-03 15:57:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 15:57:41 +0000 UTC 2019-12-03 15:57:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 15:57:54 +0000 UTC 2019-12-03 15:57:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 15:57:55.656: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-5304,SelfLink:/apis/apps/v1/namespaces/deployment-5304/replicasets/test-rollover-deployment-659c699649,UID:a4468066-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24352,Generation:2,CreationTimestamp:2019-12-03 15:57:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a30ee546-15e5-11ea-a7d5-563937197a2b 0xc0031aaec7 0xc0031aaec8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 15:57:55.656: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  3 15:57:55.656: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5304,SelfLink:/apis/apps/v1/namespaces/deployment-5304/replicasets/test-rollover-controller,UID:9ee01149-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24358,Generation:2,CreationTimestamp:2019-12-03 15:57:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a30ee546-15e5-11ea-a7d5-563937197a2b 0xc0031aadf7 0xc0031aadf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:57:55.657: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-5304,SelfLink:/apis/apps/v1/namespaces/deployment-5304/replicasets/test-rollover-deployment-7b45b6464,UID:a3101ceb-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24310,Generation:2,CreationTimestamp:2019-12-03 15:57:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a30ee546-15e5-11ea-a7d5-563937197a2b 0xc0031aaf90 0xc0031aaf91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:57:55.661: INFO: Pod "test-rollover-deployment-659c699649-lvxdk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-lvxdk,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-5304,SelfLink:/api/v1/namespaces/deployment-5304/pods/test-rollover-deployment-659c699649-lvxdk,UID:a4485df8-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24319,Generation:0,CreationTimestamp:2019-12-03 15:57:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.227/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 a4468066-15e5-11ea-a7d5-563937197a2b 0xc002d767a7 0xc002d767a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gcj9x {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gcj9x,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-gcj9x true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d76820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d76840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:57:43 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:57:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:57:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:57:43 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.227,StartTime:2019-12-03 15:57:43 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 15:57:44 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6d24efe6fed355648ec36a2c819c0d5943d69effe8145870bcfec8e32c122c9a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:57:55.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5304" for this suite.
Dec  3 15:58:01.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:01.817: INFO: namespace deployment-5304 deletion completed in 6.150114747s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:58:01.818: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:58:01.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6009" for this suite.
Dec  3 15:58:23.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:24.167: INFO: namespace pods-6009 deletion completed in 22.188807653s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:58:24.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 15:58:24.333: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"bc8c8d5f-15e5-11ea-a7d5-563937197a2b", Controller:(*bool)(0xc00259c28a), BlockOwnerDeletion:(*bool)(0xc00259c28b)}}
Dec  3 15:58:24.337: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"bc8b1aeb-15e5-11ea-a7d5-563937197a2b", Controller:(*bool)(0xc002e57d96), BlockOwnerDeletion:(*bool)(0xc002e57d97)}}
Dec  3 15:58:24.342: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"bc8bdfad-15e5-11ea-a7d5-563937197a2b", Controller:(*bool)(0xc0010e9a16), BlockOwnerDeletion:(*bool)(0xc0010e9a17)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:58:29.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7762" for this suite.
Dec  3 15:58:35.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:35.541: INFO: namespace gc-7762 deletion completed in 6.183255797s
•SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:58:35.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-1290/secret-test-c3522802-15e5-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 15:58:35.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465" in namespace "secrets-1290" to be "success or failure"
Dec  3 15:58:35.699: INFO: Pod "pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.417917ms
Dec  3 15:58:37.704: INFO: Pod "pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00814685s
STEP: Saw pod success
Dec  3 15:58:37.704: INFO: Pod "pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:58:37.708: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465 container env-test: <nil>
STEP: delete the pod
Dec  3 15:58:37.730: INFO: Waiting for pod pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:58:37.734: INFO: Pod pod-configmaps-c352ce8a-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:58:37.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1290" for this suite.
Dec  3 15:58:43.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:43.885: INFO: namespace secrets-1290 deletion completed in 6.144696098s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:58:43.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  3 15:58:44.056: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-780,SelfLink:/api/v1/namespaces/watch-780/configmaps/e2e-watch-test-resource-version,UID:c84b586c-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24608,Generation:0,CreationTimestamp:2019-12-03 15:58:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:58:44.056: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-780,SelfLink:/api/v1/namespaces/watch-780/configmaps/e2e-watch-test-resource-version,UID:c84b586c-15e5-11ea-a7d5-563937197a2b,ResourceVersion:24609,Generation:0,CreationTimestamp:2019-12-03 15:58:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:58:44.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-780" for this suite.
Dec  3 15:58:50.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:50.206: INFO: namespace watch-780 deletion completed in 6.146199501s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:58:50.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3397
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-cc109912-15e5-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 15:58:50.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465" in namespace "projected-3397" to be "success or failure"
Dec  3 15:58:50.371: INFO: Pod "pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.746644ms
Dec  3 15:58:52.376: INFO: Pod "pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00867224s
STEP: Saw pod success
Dec  3 15:58:52.376: INFO: Pod "pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:58:52.379: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:58:52.400: INFO: Waiting for pod pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:58:52.404: INFO: Pod pod-projected-configmaps-cc114c4d-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:58:52.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3397" for this suite.
Dec  3 15:58:58.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:58.563: INFO: namespace projected-3397 deletion completed in 6.153037885s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:58:58.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:58:58.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465" in namespace "downward-api-3234" to be "success or failure"
Dec  3 15:58:58.721: INFO: Pod "downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545098ms
Dec  3 15:59:00.726: INFO: Pod "downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008524231s
STEP: Saw pod success
Dec  3 15:59:00.726: INFO: Pod "downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 15:59:00.730: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 15:59:00.749: INFO: Waiting for pod downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 15:59:00.753: INFO: Pod downwardapi-volume-d10b677d-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:59:00.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3234" for this suite.
Dec  3 15:59:06.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:06.903: INFO: namespace downward-api-3234 deletion completed in 6.144833703s
•
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 15:59:06.903: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  3 15:59:13.089: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:13.089: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:13.580: INFO: Exec stderr: ""
Dec  3 15:59:13.580: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:13.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:14.038: INFO: Exec stderr: ""
Dec  3 15:59:14.038: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:14.038: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:14.546: INFO: Exec stderr: ""
Dec  3 15:59:14.546: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:14.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:15.025: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  3 15:59:15.025: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:15.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:15.406: INFO: Exec stderr: ""
Dec  3 15:59:15.406: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:15.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:15.787: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  3 15:59:15.787: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:15.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:16.169: INFO: Exec stderr: ""
Dec  3 15:59:16.169: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:16.169: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:16.568: INFO: Exec stderr: ""
Dec  3 15:59:16.569: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:16.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:17.016: INFO: Exec stderr: ""
Dec  3 15:59:17.016: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4426 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:59:17.016: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:59:17.458: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 15:59:17.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4426" for this suite.
Dec  3 16:00:01.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:01.616: INFO: namespace e2e-kubelet-etc-hosts-4426 deletion completed in 44.150677218s
•SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:00:01.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 16:00:01.794: INFO: Number of nodes with available pods: 0
Dec  3 16:00:01.794: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:00:02.805: INFO: Number of nodes with available pods: 0
Dec  3 16:00:02.805: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:00:03.805: INFO: Number of nodes with available pods: 2
Dec  3 16:00:03.805: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  3 16:00:03.829: INFO: Number of nodes with available pods: 1
Dec  3 16:00:03.829: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 16:00:04.840: INFO: Number of nodes with available pods: 1
Dec  3 16:00:04.840: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 16:00:05.840: INFO: Number of nodes with available pods: 1
Dec  3 16:00:05.840: INFO: Node izgw8jactz8ahkwwzt6d3gz is running more than one daemon pod
Dec  3 16:00:06.840: INFO: Number of nodes with available pods: 2
Dec  3 16:00:06.840: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4661, will wait for the garbage collector to delete the pods
Dec  3 16:00:06.907: INFO: Deleting DaemonSet.extensions daemon-set took: 6.157227ms
Dec  3 16:00:07.007: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.291694ms
Dec  3 16:00:10.012: INFO: Number of nodes with available pods: 0
Dec  3 16:00:10.012: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:00:10.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4661/daemonsets","resourceVersion":"24987"},"items":null}

Dec  3 16:00:10.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4661/pods","resourceVersion":"24987"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:00:10.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4661" for this suite.
Dec  3 16:00:16.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:16.180: INFO: namespace daemonsets-4661 deletion completed in 6.14333248s
•SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:00:16.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:00:16.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465" in namespace "downward-api-4519" to be "success or failure"
Dec  3 16:00:16.335: INFO: Pod "downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689787ms
Dec  3 16:00:18.340: INFO: Pod "downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008633098s
STEP: Saw pod success
Dec  3 16:00:18.340: INFO: Pod "downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:00:18.344: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 16:00:18.364: INFO: Waiting for pod downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465 to disappear
Dec  3 16:00:18.368: INFO: Pod downwardapi-volume-ff4e6d21-15e5-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:00:18.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4519" for this suite.
Dec  3 16:00:24.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:24.516: INFO: namespace downward-api-4519 deletion completed in 6.142561814s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:00:24.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4731
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:00:26.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4731" for this suite.
Dec  3 16:01:04.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:01:04.847: INFO: namespace kubelet-test-4731 deletion completed in 38.145386779s
•SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:01:04.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-1c50c064-15e6-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume configMaps
Dec  3 16:01:05.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465" in namespace "configmap-1483" to be "success or failure"
Dec  3 16:01:05.008: INFO: Pod "pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.789387ms
Dec  3 16:01:07.013: INFO: Pod "pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008708842s
STEP: Saw pod success
Dec  3 16:01:07.013: INFO: Pod "pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:01:07.017: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:01:07.037: INFO: Waiting for pod pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:01:07.041: INFO: Pod pod-configmaps-1c516bda-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:01:07.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1483" for this suite.
Dec  3 16:01:13.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:01:13.190: INFO: namespace configmap-1483 deletion completed in 6.142644413s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:01:13.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6713
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6713
Dec  3 16:01:15.349: INFO: Started pod liveness-exec in namespace container-probe-6713
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 16:01:15.353: INFO: Initial restart count of pod liveness-exec is 0
Dec  3 16:02:01.468: INFO: Restart count of pod container-probe-6713/liveness-exec is now 1 (46.11506452s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:02:01.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6713" for this suite.
Dec  3 16:02:07.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:07.620: INFO: namespace container-probe-6713 deletion completed in 6.139390692s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:02:07.621: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Dec  3 16:02:07.771: INFO: Waiting up to 5m0s for pod "var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465" in namespace "var-expansion-7980" to be "success or failure"
Dec  3 16:02:07.775: INFO: Pod "var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449208ms
Dec  3 16:02:09.779: INFO: Pod "var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008261745s
STEP: Saw pod success
Dec  3 16:02:09.780: INFO: Pod "var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:02:09.783: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:02:09.806: INFO: Waiting for pod var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:02:09.809: INFO: Pod var-expansion-41bae7d9-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:02:09.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7980" for this suite.
Dec  3 16:02:15.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:15.961: INFO: namespace var-expansion-7980 deletion completed in 6.146261481s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:02:15.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 16:02:20.165: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:02:20.169: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:02:22.169: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:02:22.174: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:02:24.169: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:02:24.174: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:02:24.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2276" for this suite.
Dec  3 16:02:54.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:54.344: INFO: namespace container-lifecycle-hook-2276 deletion completed in 30.150316573s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:02:54.345: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  3 16:02:56.516: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-5d94b490-15e6-11ea-9142-f2314dba9465,GenerateName:,Namespace:events-6321,SelfLink:/api/v1/namespaces/events-6321/pods/send-events-5d94b490-15e6-11ea-9142-f2314dba9465,UID:5d951a56-15e6-11ea-a7d5-563937197a2b,ResourceVersion:25637,Generation:0,CreationTimestamp:2019-12-03 16:02:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 490949924,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.242/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rhm99 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rhm99,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-rhm99 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003355610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003355640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:02:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:02:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:02:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:02:54 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.242,StartTime:2019-12-03 16:02:54 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-03 16:02:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://20f4be77b2447c02002f049ca506bfd73e2af47d164a80132c4a9b258e033475}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  3 16:02:58.521: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  3 16:03:00.526: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:03:00.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6321" for this suite.
Dec  3 16:03:42.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:42.683: INFO: namespace events-6321 deletion completed in 42.144490895s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:03:42.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  3 16:03:42.835: INFO: Waiting up to 5m0s for pod "downward-api-7a64611d-15e6-11ea-9142-f2314dba9465" in namespace "downward-api-3777" to be "success or failure"
Dec  3 16:03:42.839: INFO: Pod "downward-api-7a64611d-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784795ms
Dec  3 16:03:44.844: INFO: Pod "downward-api-7a64611d-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008699843s
STEP: Saw pod success
Dec  3 16:03:44.844: INFO: Pod "downward-api-7a64611d-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:03:44.848: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downward-api-7a64611d-15e6-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:03:44.868: INFO: Waiting for pod downward-api-7a64611d-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:03:44.872: INFO: Pod downward-api-7a64611d-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:03:44.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3777" for this suite.
Dec  3 16:03:50.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:51.020: INFO: namespace downward-api-3777 deletion completed in 6.141534556s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:03:51.020: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 16:03:51.170: INFO: Waiting up to 5m0s for pod "pod-7f5c432e-15e6-11ea-9142-f2314dba9465" in namespace "emptydir-6022" to be "success or failure"
Dec  3 16:03:51.174: INFO: Pod "pod-7f5c432e-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.742503ms
Dec  3 16:03:53.178: INFO: Pod "pod-7f5c432e-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008164251s
STEP: Saw pod success
Dec  3 16:03:53.178: INFO: Pod "pod-7f5c432e-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:03:53.182: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-7f5c432e-15e6-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 16:03:53.201: INFO: Waiting for pod pod-7f5c432e-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:03:53.204: INFO: Pod pod-7f5c432e-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:03:53.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6022" for this suite.
Dec  3 16:03:59.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:59.356: INFO: namespace emptydir-6022 deletion completed in 6.146075973s
•SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:03:59.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 16:03:59.519: INFO: (0) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 10.296425ms)
Dec  3 16:03:59.565: INFO: (1) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 45.975262ms)
Dec  3 16:03:59.572: INFO: (2) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 7.314583ms)
Dec  3 16:03:59.579: INFO: (3) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.946268ms)
Dec  3 16:03:59.586: INFO: (4) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.782274ms)
Dec  3 16:03:59.592: INFO: (5) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.321507ms)
Dec  3 16:03:59.599: INFO: (6) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.477974ms)
Dec  3 16:03:59.605: INFO: (7) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.451717ms)
Dec  3 16:03:59.612: INFO: (8) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.27713ms)
Dec  3 16:03:59.618: INFO: (9) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.434109ms)
Dec  3 16:03:59.625: INFO: (10) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.357574ms)
Dec  3 16:03:59.631: INFO: (11) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.411988ms)
Dec  3 16:03:59.638: INFO: (12) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.781417ms)
Dec  3 16:03:59.644: INFO: (13) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.154272ms)
Dec  3 16:03:59.651: INFO: (14) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.360248ms)
Dec  3 16:03:59.657: INFO: (15) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.349507ms)
Dec  3 16:03:59.671: INFO: (16) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 13.917429ms)
Dec  3 16:03:59.678: INFO: (17) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.750799ms)
Dec  3 16:03:59.684: INFO: (18) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.602347ms)
Dec  3 16:03:59.691: INFO: (19) /api/v1/nodes/izgw8afzp8040eosj1udh4z:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">c... (200; 6.567736ms)
[AfterEach] version v1
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:03:59.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4961" for this suite.
Dec  3 16:04:05.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:04:05.847: INFO: namespace proxy-4961 deletion completed in 6.151848119s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:04:05.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-rztm
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 16:04:06.008: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rztm" in namespace "subpath-3544" to be "success or failure"
Dec  3 16:04:06.012: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402722ms
Dec  3 16:04:08.017: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008465509s
Dec  3 16:04:10.022: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 4.01391315s
Dec  3 16:04:12.027: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 6.018989562s
Dec  3 16:04:14.032: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 8.023767298s
Dec  3 16:04:16.037: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 10.028681527s
Dec  3 16:04:18.042: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 12.033907302s
Dec  3 16:04:20.047: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 14.038753322s
Dec  3 16:04:22.052: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 16.043622275s
Dec  3 16:04:24.056: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 18.04822914s
Dec  3 16:04:26.061: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Running", Reason="", readiness=true. Elapsed: 20.053011615s
Dec  3 16:04:28.067: INFO: Pod "pod-subpath-test-downwardapi-rztm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059001259s
STEP: Saw pod success
Dec  3 16:04:28.067: INFO: Pod "pod-subpath-test-downwardapi-rztm" satisfied condition "success or failure"
Dec  3 16:04:28.071: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-subpath-test-downwardapi-rztm container test-container-subpath-downwardapi-rztm: <nil>
STEP: delete the pod
Dec  3 16:04:28.092: INFO: Waiting for pod pod-subpath-test-downwardapi-rztm to disappear
Dec  3 16:04:28.095: INFO: Pod pod-subpath-test-downwardapi-rztm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rztm
Dec  3 16:04:28.096: INFO: Deleting pod "pod-subpath-test-downwardapi-rztm" in namespace "subpath-3544"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:04:28.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3544" for this suite.
Dec  3 16:04:34.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:04:34.254: INFO: namespace subpath-3544 deletion completed in 6.149199505s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:04:34.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:04:34.420: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465" in namespace "projected-7433" to be "success or failure"
Dec  3 16:04:34.424: INFO: Pod "downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692103ms
Dec  3 16:04:36.429: INFO: Pod "downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008788913s
STEP: Saw pod success
Dec  3 16:04:36.429: INFO: Pod "downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:04:36.433: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 16:04:36.456: INFO: Waiting for pod downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:04:36.459: INFO: Pod downwardapi-volume-99239814-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:04:36.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7433" for this suite.
Dec  3 16:04:42.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:04:42.610: INFO: namespace projected-7433 deletion completed in 6.145138202s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:04:42.611: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2915
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  3 16:04:42.761: INFO: Waiting up to 5m0s for pod "pod-9e1c7217-15e6-11ea-9142-f2314dba9465" in namespace "emptydir-2915" to be "success or failure"
Dec  3 16:04:42.765: INFO: Pod "pod-9e1c7217-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.634903ms
Dec  3 16:04:44.769: INFO: Pod "pod-9e1c7217-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008553378s
STEP: Saw pod success
Dec  3 16:04:44.770: INFO: Pod "pod-9e1c7217-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:04:44.773: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-9e1c7217-15e6-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 16:04:44.794: INFO: Waiting for pod pod-9e1c7217-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:04:44.797: INFO: Pod pod-9e1c7217-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:04:44.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2915" for this suite.
Dec  3 16:04:50.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:04:50.949: INFO: namespace emptydir-2915 deletion completed in 6.143865029s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:04:50.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  3 16:04:53.654: INFO: Successfully updated pod "annotationupdatea315b4cd-15e6-11ea-9142-f2314dba9465"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:04:57.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-455" for this suite.
Dec  3 16:05:19.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:19.883: INFO: namespace projected-455 deletion completed in 22.183292448s
•SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:05:19.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5885
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 16:05:20.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:05:20.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5885" for this suite.
Dec  3 16:05:26.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:26.774: INFO: namespace custom-resource-definition-5885 deletion completed in 6.195894097s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:05:26.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Dec  3 16:05:26.918: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7281'
Dec  3 16:05:27.196: INFO: stderr: ""
Dec  3 16:05:27.196: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:05:28.201: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:05:28.201: INFO: Found 0 / 1
Dec  3 16:05:29.201: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:05:29.201: INFO: Found 1 / 1
Dec  3 16:05:29.201: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  3 16:05:29.205: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:05:29.205: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:05:29.205: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-nfxm6 --namespace=kubectl-7281 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  3 16:05:29.343: INFO: stderr: ""
Dec  3 16:05:29.343: INFO: stdout: "pod/redis-master-nfxm6 patched\n"
STEP: checking annotations
Dec  3 16:05:29.348: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:05:29.348: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:05:29.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7281" for this suite.
Dec  3 16:05:51.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:51.497: INFO: namespace kubectl-7281 deletion completed in 22.142589767s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:05:51.498: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-c72bdcd1-15e6-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 16:05:51.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465" in namespace "projected-5901" to be "success or failure"
Dec  3 16:05:51.656: INFO: Pod "pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643777ms
Dec  3 16:05:53.661: INFO: Pod "pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008527488s
STEP: Saw pod success
Dec  3 16:05:53.661: INFO: Pod "pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:05:53.665: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:05:53.685: INFO: Waiting for pod pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465 to disappear
Dec  3 16:05:53.689: INFO: Pod pod-projected-secrets-c72c7fe7-15e6-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:05:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5901" for this suite.
Dec  3 16:05:59.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:59.883: INFO: namespace projected-5901 deletion completed in 6.188221319s
•SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:05:59.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 16:06:00.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:06:02.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8789" for this suite.
Dec  3 16:06:42.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:42.252: INFO: namespace pods-8789 deletion completed in 40.151061863s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:06:42.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  3 16:06:42.609: INFO: Pod name wrapped-volume-race-e5895324-15e6-11ea-9142-f2314dba9465: Found 1 pods out of 5
Dec  3 16:06:47.618: INFO: Pod name wrapped-volume-race-e5895324-15e6-11ea-9142-f2314dba9465: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e5895324-15e6-11ea-9142-f2314dba9465 in namespace emptydir-wrapper-2062, will wait for the garbage collector to delete the pods
Dec  3 16:06:55.710: INFO: Deleting ReplicationController wrapped-volume-race-e5895324-15e6-11ea-9142-f2314dba9465 took: 6.794317ms
Dec  3 16:06:55.811: INFO: Terminating ReplicationController wrapped-volume-race-e5895324-15e6-11ea-9142-f2314dba9465 pods took: 100.344381ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 16:07:38.526: INFO: Pod name wrapped-volume-race-06dec9b3-15e7-11ea-9142-f2314dba9465: Found 0 pods out of 5
Dec  3 16:07:43.535: INFO: Pod name wrapped-volume-race-06dec9b3-15e7-11ea-9142-f2314dba9465: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-06dec9b3-15e7-11ea-9142-f2314dba9465 in namespace emptydir-wrapper-2062, will wait for the garbage collector to delete the pods
Dec  3 16:07:43.620: INFO: Deleting ReplicationController wrapped-volume-race-06dec9b3-15e7-11ea-9142-f2314dba9465 took: 7.043893ms
Dec  3 16:07:43.720: INFO: Terminating ReplicationController wrapped-volume-race-06dec9b3-15e7-11ea-9142-f2314dba9465 pods took: 100.305661ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 16:08:28.436: INFO: Pod name wrapped-volume-race-249e624c-15e7-11ea-9142-f2314dba9465: Found 0 pods out of 5
Dec  3 16:08:33.444: INFO: Pod name wrapped-volume-race-249e624c-15e7-11ea-9142-f2314dba9465: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-249e624c-15e7-11ea-9142-f2314dba9465 in namespace emptydir-wrapper-2062, will wait for the garbage collector to delete the pods
Dec  3 16:08:33.528: INFO: Deleting ReplicationController wrapped-volume-race-249e624c-15e7-11ea-9142-f2314dba9465 took: 6.813927ms
Dec  3 16:08:33.629: INFO: Terminating ReplicationController wrapped-volume-race-249e624c-15e7-11ea-9142-f2314dba9465 pods took: 100.452025ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:09:08.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2062" for this suite.
Dec  3 16:09:14.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:09:14.717: INFO: namespace emptydir-wrapper-2062 deletion completed in 6.144379061s
•SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:09:14.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 16:09:14.884: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  3 16:09:14.892: INFO: Number of nodes with available pods: 0
Dec  3 16:09:14.892: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  3 16:09:14.908: INFO: Number of nodes with available pods: 0
Dec  3 16:09:14.908: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:15.913: INFO: Number of nodes with available pods: 0
Dec  3 16:09:15.913: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:16.913: INFO: Number of nodes with available pods: 1
Dec  3 16:09:16.913: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  3 16:09:16.932: INFO: Number of nodes with available pods: 1
Dec  3 16:09:16.932: INFO: Number of running nodes: 0, number of available pods: 1
Dec  3 16:09:17.937: INFO: Number of nodes with available pods: 0
Dec  3 16:09:17.937: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  3 16:09:17.946: INFO: Number of nodes with available pods: 0
Dec  3 16:09:17.946: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:18.950: INFO: Number of nodes with available pods: 0
Dec  3 16:09:18.950: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:19.955: INFO: Number of nodes with available pods: 0
Dec  3 16:09:19.957: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:20.950: INFO: Number of nodes with available pods: 0
Dec  3 16:09:20.950: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:21.951: INFO: Number of nodes with available pods: 0
Dec  3 16:09:21.951: INFO: Node izgw8afzp8040eosj1udh4z is running more than one daemon pod
Dec  3 16:09:22.950: INFO: Number of nodes with available pods: 1
Dec  3 16:09:22.950: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9019, will wait for the garbage collector to delete the pods
Dec  3 16:09:23.018: INFO: Deleting DaemonSet.extensions daemon-set took: 6.341162ms
Dec  3 16:09:23.519: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.357204ms
Dec  3 16:09:26.923: INFO: Number of nodes with available pods: 0
Dec  3 16:09:26.923: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:09:26.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9019/daemonsets","resourceVersion":"27382"},"items":null}

Dec  3 16:09:26.929: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9019/pods","resourceVersion":"27382"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:09:26.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9019" for this suite.
Dec  3 16:09:32.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:09:33.140: INFO: namespace daemonsets-9019 deletion completed in 6.186934989s
•SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:09:33.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:09:36.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4888" for this suite.
Dec  3 16:09:58.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:09:58.490: INFO: namespace replication-controller-4888 deletion completed in 22.144747915s
•SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:09:58.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  3 16:09:58.641: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  3 16:10:03.646: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 16:10:03.647: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  3 16:10:05.681: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-957,SelfLink:/apis/apps/v1/namespaces/deployment-957/deployments/test-cleanup-deployment,UID:5d61c4b4-15e7-11ea-a7d5-563937197a2b,ResourceVersion:27566,Generation:1,CreationTimestamp:2019-12-03 16:10:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 16:10:03 +0000 UTC 2019-12-03 16:10:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 16:10:05 +0000 UTC 2019-12-03 16:10:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6865c98b76" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 16:10:05.686: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-957,SelfLink:/apis/apps/v1/namespaces/deployment-957/replicasets/test-cleanup-deployment-6865c98b76,UID:5d63125d-15e7-11ea-a7d5-563937197a2b,ResourceVersion:27559,Generation:1,CreationTimestamp:2019-12-03 16:10:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5d61c4b4-15e7-11ea-a7d5-563937197a2b 0xc001096ed7 0xc001096ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 16:10:05.690: INFO: Pod "test-cleanup-deployment-6865c98b76-vx8wl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-vx8wl,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-957,SelfLink:/api/v1/namespaces/deployment-957/pods/test-cleanup-deployment-6865c98b76-vx8wl,UID:5d6379eb-15e7-11ea-a7d5-563937197a2b,ResourceVersion:27558,Generation:0,CreationTimestamp:2019-12-03 16:10:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.254/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 5d63125d-15e7-11ea-a7d5-563937197a2b 0xc0010974e7 0xc0010974e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hz62r {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hz62r,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hz62r true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:izgw8jactz8ahkwwzt6d3gz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001097550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001097570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:10:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:10:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:10:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:10:03 +0000 UTC  }],Message:,Reason:,HostIP:10.250.16.152,PodIP:100.64.1.254,StartTime:2019-12-03 16:10:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 16:10:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://24ce20cc999aeaba2077059f871f2f9417c0427df315ca0e5cd80b8e79ffc5b1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:10:05.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-957" for this suite.
Dec  3 16:10:11.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:11.838: INFO: namespace deployment-957 deletion completed in 6.141591094s
•
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:10:11.838: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:10:11.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2264" for this suite.
Dec  3 16:10:18.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:18.139: INFO: namespace services-2264 deletion completed in 6.14555798s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:10:18.139: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  3 16:10:18.289: INFO: Waiting up to 5m0s for pod "downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465" in namespace "downward-api-2264" to be "success or failure"
Dec  3 16:10:18.293: INFO: Pod "downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537235ms
Dec  3 16:10:20.298: INFO: Pod "downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008140989s
STEP: Saw pod success
Dec  3 16:10:20.298: INFO: Pod "downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:10:20.301: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:10:20.327: INFO: Waiting for pod downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465 to disappear
Dec  3 16:10:20.330: INFO: Pod downward-api-6619fb6d-15e7-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:10:20.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2264" for this suite.
Dec  3 16:10:26.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:26.493: INFO: namespace downward-api-2264 deletion completed in 6.155540304s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:10:26.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:10:26.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465" in namespace "projected-5319" to be "success or failure"
Dec  3 16:10:26.650: INFO: Pod "downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777221ms
Dec  3 16:10:28.655: INFO: Pod "downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008685452s
STEP: Saw pod success
Dec  3 16:10:28.655: INFO: Pod "downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:10:28.659: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 16:10:28.682: INFO: Waiting for pod downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465 to disappear
Dec  3 16:10:28.686: INFO: Pod downwardapi-volume-6b151a43-15e7-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:10:28.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5319" for this suite.
Dec  3 16:10:34.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:34.839: INFO: namespace projected-5319 deletion completed in 6.146729428s
•SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:10:34.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Dec  3 16:10:34.990: INFO: Waiting up to 5m0s for pod "var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465" in namespace "var-expansion-3525" to be "success or failure"
Dec  3 16:10:34.994: INFO: Pod "var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.737486ms
Dec  3 16:10:36.998: INFO: Pod "var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008211672s
STEP: Saw pod success
Dec  3 16:10:36.998: INFO: Pod "var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:10:37.002: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:10:37.021: INFO: Waiting for pod var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465 to disappear
Dec  3 16:10:37.024: INFO: Pod var-expansion-700e4eef-15e7-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:10:37.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3525" for this suite.
Dec  3 16:10:43.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:43.178: INFO: namespace var-expansion-3525 deletion completed in 6.147681151s
•SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:10:43.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2035
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-750756f4-15e7-11ea-9142-f2314dba9465
STEP: Creating secret with name s-test-opt-upd-75075745-15e7-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-750756f4-15e7-11ea-9142-f2314dba9465
STEP: Updating secret s-test-opt-upd-75075745-15e7-11ea-9142-f2314dba9465
STEP: Creating secret with name s-test-opt-create-7507575a-15e7-11ea-9142-f2314dba9465
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:10:47.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2035" for this suite.
Dec  3 16:11:09.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:09.853: INFO: namespace secrets-2035 deletion completed in 22.143351618s
•SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:11:09.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  3 16:11:13.034: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:11:13.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7084" for this suite.
Dec  3 16:11:35.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:35.239: INFO: namespace replicaset-7084 deletion completed in 22.185032678s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:11:35.240: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-940f06d4-15e7-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 16:11:35.398: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465" in namespace "projected-3264" to be "success or failure"
Dec  3 16:11:35.401: INFO: Pod "pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457739ms
Dec  3 16:11:37.406: INFO: Pod "pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008302318s
STEP: Saw pod success
Dec  3 16:11:37.406: INFO: Pod "pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:11:37.410: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:11:37.470: INFO: Waiting for pod pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465 to disappear
Dec  3 16:11:37.474: INFO: Pod pod-projected-secrets-940fbc4e-15e7-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:11:37.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3264" for this suite.
Dec  3 16:11:43.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:43.624: INFO: namespace projected-3264 deletion completed in 6.145039863s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:11:43.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1203 16:11:44.312007    5087 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 16:11:44.312: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:11:44.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8502" for this suite.
Dec  3 16:11:50.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:50.460: INFO: namespace gc-8502 deletion completed in 6.144516538s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:11:50.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Dec  3 16:11:50.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5867'
Dec  3 16:11:51.179: INFO: stderr: ""
Dec  3 16:11:51.179: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:11:51.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5867'
Dec  3 16:11:51.318: INFO: stderr: ""
Dec  3 16:11:51.318: INFO: stdout: "update-demo-nautilus-d5rzz update-demo-nautilus-wdgnl "
Dec  3 16:11:51.318: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-d5rzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:11:51.483: INFO: stderr: ""
Dec  3 16:11:51.483: INFO: stdout: ""
Dec  3 16:11:51.483: INFO: update-demo-nautilus-d5rzz is created but not running
Dec  3 16:11:56.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5867'
Dec  3 16:11:56.592: INFO: stderr: ""
Dec  3 16:11:56.592: INFO: stdout: "update-demo-nautilus-d5rzz update-demo-nautilus-wdgnl "
Dec  3 16:11:56.592: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-d5rzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:11:56.693: INFO: stderr: ""
Dec  3 16:11:56.693: INFO: stdout: "true"
Dec  3 16:11:56.693: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-d5rzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:11:56.838: INFO: stderr: ""
Dec  3 16:11:56.838: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:11:56.838: INFO: validating pod update-demo-nautilus-d5rzz
Dec  3 16:11:56.933: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:11:56.933: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:11:56.933: INFO: update-demo-nautilus-d5rzz is verified up and running
Dec  3 16:11:56.933: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wdgnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:11:57.031: INFO: stderr: ""
Dec  3 16:11:57.031: INFO: stdout: "true"
Dec  3 16:11:57.031: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wdgnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:11:57.133: INFO: stderr: ""
Dec  3 16:11:57.133: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:11:57.133: INFO: validating pod update-demo-nautilus-wdgnl
Dec  3 16:11:57.227: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:11:57.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:11:57.228: INFO: update-demo-nautilus-wdgnl is verified up and running
STEP: scaling down the replication controller
Dec  3 16:11:57.229: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:11:57.229: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5867'
Dec  3 16:11:58.372: INFO: stderr: ""
Dec  3 16:11:58.372: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:11:58.372: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5867'
Dec  3 16:11:58.514: INFO: stderr: ""
Dec  3 16:11:58.514: INFO: stdout: "update-demo-nautilus-d5rzz update-demo-nautilus-wdgnl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 16:12:03.514: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5867'
Dec  3 16:12:03.620: INFO: stderr: ""
Dec  3 16:12:03.620: INFO: stdout: "update-demo-nautilus-wdgnl "
Dec  3 16:12:03.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wdgnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:03.785: INFO: stderr: ""
Dec  3 16:12:03.785: INFO: stdout: "true"
Dec  3 16:12:03.785: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wdgnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:03.926: INFO: stderr: ""
Dec  3 16:12:03.926: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:12:03.926: INFO: validating pod update-demo-nautilus-wdgnl
Dec  3 16:12:03.936: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:12:03.936: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:12:03.936: INFO: update-demo-nautilus-wdgnl is verified up and running
STEP: scaling up the replication controller
Dec  3 16:12:03.938: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:12:03.938: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5867'
Dec  3 16:12:05.092: INFO: stderr: ""
Dec  3 16:12:05.092: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:12:05.092: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5867'
Dec  3 16:12:05.230: INFO: stderr: ""
Dec  3 16:12:05.230: INFO: stdout: "update-demo-nautilus-pnmh9 update-demo-nautilus-wdgnl "
Dec  3 16:12:05.230: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-pnmh9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:05.385: INFO: stderr: ""
Dec  3 16:12:05.385: INFO: stdout: ""
Dec  3 16:12:05.386: INFO: update-demo-nautilus-pnmh9 is created but not running
Dec  3 16:12:10.386: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5867'
Dec  3 16:12:10.526: INFO: stderr: ""
Dec  3 16:12:10.526: INFO: stdout: "update-demo-nautilus-pnmh9 update-demo-nautilus-wdgnl "
Dec  3 16:12:10.526: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-pnmh9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:10.656: INFO: stderr: ""
Dec  3 16:12:10.656: INFO: stdout: "true"
Dec  3 16:12:10.656: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-pnmh9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:10.808: INFO: stderr: ""
Dec  3 16:12:10.808: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:12:10.808: INFO: validating pod update-demo-nautilus-pnmh9
Dec  3 16:12:10.903: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:12:10.903: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:12:10.903: INFO: update-demo-nautilus-pnmh9 is verified up and running
Dec  3 16:12:10.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wdgnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:11.055: INFO: stderr: ""
Dec  3 16:12:11.055: INFO: stdout: "true"
Dec  3 16:12:11.055: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wdgnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5867'
Dec  3 16:12:11.152: INFO: stderr: ""
Dec  3 16:12:11.152: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:12:11.152: INFO: validating pod update-demo-nautilus-wdgnl
Dec  3 16:12:11.162: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:12:11.162: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:12:11.162: INFO: update-demo-nautilus-wdgnl is verified up and running
STEP: using delete to clean up resources
Dec  3 16:12:11.162: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5867'
Dec  3 16:12:11.263: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:12:11.263: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 16:12:11.263: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5867'
Dec  3 16:12:11.416: INFO: stderr: "No resources found.\n"
Dec  3 16:12:11.416: INFO: stdout: ""
Dec  3 16:12:11.416: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-5867 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:12:11.522: INFO: stderr: ""
Dec  3 16:12:11.522: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:12:11.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5867" for this suite.
Dec  3 16:12:33.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:33.681: INFO: namespace kubectl-5867 deletion completed in 22.149112661s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:12:33.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1025
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-b6e44e2d-15e7-11ea-9142-f2314dba9465
STEP: Creating a pod to test consume secrets
Dec  3 16:12:33.836: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465" in namespace "projected-1025" to be "success or failure"
Dec  3 16:12:33.840: INFO: Pod "pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53963ms
Dec  3 16:12:35.845: INFO: Pod "pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008252485s
STEP: Saw pod success
Dec  3 16:12:35.845: INFO: Pod "pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:12:35.849: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:12:35.869: INFO: Waiting for pod pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465 to disappear
Dec  3 16:12:35.872: INFO: Pod pod-projected-secrets-b6e4f1a9-15e7-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:12:35.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1025" for this suite.
Dec  3 16:12:41.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:42.026: INFO: namespace projected-1025 deletion completed in 6.14783525s
•SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:12:42.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:12:42.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465" in namespace "downward-api-9992" to be "success or failure"
Dec  3 16:12:42.184: INFO: Pod "downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089902ms
Dec  3 16:12:44.189: INFO: Pod "downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00868601s
STEP: Saw pod success
Dec  3 16:12:44.189: INFO: Pod "downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:12:44.193: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465 container client-container: <nil>
STEP: delete the pod
Dec  3 16:12:44.215: INFO: Waiting for pod downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465 to disappear
Dec  3 16:12:44.218: INFO: Pod downwardapi-volume-bbdde5e9-15e7-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:12:44.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9992" for this suite.
Dec  3 16:12:50.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:50.374: INFO: namespace downward-api-9992 deletion completed in 6.150050562s
•SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:12:50.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-c0d786b4-15e7-11ea-9142-f2314dba9465
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:12:50.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9195" for this suite.
Dec  3 16:12:56.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:56.723: INFO: namespace configmap-9195 deletion completed in 6.195686181s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:12:56.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Dec  3 16:12:56.869: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8381 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  3 16:12:58.911: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  3 16:12:58.912: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:13:00.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8381" for this suite.
Dec  3 16:13:12.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:13:13.071: INFO: namespace kubectl-8381 deletion completed in 12.145275735s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:13:13.071: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Dec  3 16:13:13.214: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  3 16:13:13.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5600'
Dec  3 16:13:13.577: INFO: stderr: ""
Dec  3 16:13:13.577: INFO: stdout: "service/redis-slave created\n"
Dec  3 16:13:13.578: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  3 16:13:13.578: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5600'
Dec  3 16:13:13.857: INFO: stderr: ""
Dec  3 16:13:13.857: INFO: stdout: "service/redis-master created\n"
Dec  3 16:13:13.857: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  3 16:13:13.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5600'
Dec  3 16:13:14.242: INFO: stderr: ""
Dec  3 16:13:14.242: INFO: stdout: "service/frontend created\n"
Dec  3 16:13:14.242: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  3 16:13:14.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5600'
Dec  3 16:13:14.505: INFO: stderr: ""
Dec  3 16:13:14.506: INFO: stdout: "deployment.apps/frontend created\n"
Dec  3 16:13:14.506: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  3 16:13:14.506: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5600'
Dec  3 16:13:15.332: INFO: stderr: ""
Dec  3 16:13:15.333: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  3 16:13:15.333: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  3 16:13:15.333: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5600'
Dec  3 16:13:15.653: INFO: stderr: ""
Dec  3 16:13:15.653: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  3 16:13:15.653: INFO: Waiting for all frontend pods to be Running.
Dec  3 16:13:40.704: INFO: Waiting for frontend to serve content.
Dec  3 16:13:40.806: INFO: Trying to add a new entry to the guestbook.
Dec  3 16:13:40.939: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  3 16:13:41.073: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5600'
Dec  3 16:13:41.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:13:41.281: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:13:41.281: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5600'
Dec  3 16:13:41.525: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:13:41.525: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:13:41.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5600'
Dec  3 16:13:41.757: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:13:41.757: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:13:41.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5600'
Dec  3 16:13:42.075: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:13:42.075: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:13:42.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5600'
Dec  3 16:13:42.335: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:13:42.335: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:13:42.335: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5600'
Dec  3 16:13:42.587: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:13:42.587: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:13:42.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5600" for this suite.
Dec  3 16:14:20.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:20.740: INFO: namespace kubectl-5600 deletion completed in 38.145162533s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:14:20.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Dec  3 16:14:20.889: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6611'
Dec  3 16:14:21.952: INFO: stderr: ""
Dec  3 16:14:21.952: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:14:21.952: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6611'
Dec  3 16:14:22.179: INFO: stderr: ""
Dec  3 16:14:22.179: INFO: stdout: "update-demo-nautilus-8s5md update-demo-nautilus-ncd5h "
Dec  3 16:14:22.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8s5md -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:22.336: INFO: stderr: ""
Dec  3 16:14:22.336: INFO: stdout: ""
Dec  3 16:14:22.336: INFO: update-demo-nautilus-8s5md is created but not running
Dec  3 16:14:27.337: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6611'
Dec  3 16:14:27.589: INFO: stderr: ""
Dec  3 16:14:27.589: INFO: stdout: "update-demo-nautilus-8s5md update-demo-nautilus-ncd5h "
Dec  3 16:14:27.589: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8s5md -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:27.817: INFO: stderr: ""
Dec  3 16:14:27.817: INFO: stdout: "true"
Dec  3 16:14:27.817: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8s5md -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:28.043: INFO: stderr: ""
Dec  3 16:14:28.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:14:28.043: INFO: validating pod update-demo-nautilus-8s5md
Dec  3 16:14:28.137: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:14:28.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:14:28.137: INFO: update-demo-nautilus-8s5md is verified up and running
Dec  3 16:14:28.138: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ncd5h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:28.394: INFO: stderr: ""
Dec  3 16:14:28.394: INFO: stdout: "true"
Dec  3 16:14:28.394: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ncd5h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:28.693: INFO: stderr: ""
Dec  3 16:14:28.693: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:14:28.693: INFO: validating pod update-demo-nautilus-ncd5h
Dec  3 16:14:28.787: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:14:28.787: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:14:28.787: INFO: update-demo-nautilus-ncd5h is verified up and running
STEP: rolling-update to new replication controller
Dec  3 16:14:28.790: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:14:28.790: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-6611'
Dec  3 16:14:51.463: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 16:14:51.463: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:14:51.463: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6611'
Dec  3 16:14:51.717: INFO: stderr: ""
Dec  3 16:14:51.718: INFO: stdout: "update-demo-kitten-dwwks update-demo-kitten-w8ptp "
Dec  3 16:14:51.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-dwwks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:51.886: INFO: stderr: ""
Dec  3 16:14:51.886: INFO: stdout: "true"
Dec  3 16:14:51.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-dwwks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:52.080: INFO: stderr: ""
Dec  3 16:14:52.080: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 16:14:52.080: INFO: validating pod update-demo-kitten-dwwks
Dec  3 16:14:52.175: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 16:14:52.175: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 16:14:52.176: INFO: update-demo-kitten-dwwks is verified up and running
Dec  3 16:14:52.176: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-w8ptp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:52.420: INFO: stderr: ""
Dec  3 16:14:52.420: INFO: stdout: "true"
Dec  3 16:14:52.420: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmtik-xgy.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-w8ptp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6611'
Dec  3 16:14:52.568: INFO: stderr: ""
Dec  3 16:14:52.568: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 16:14:52.568: INFO: validating pod update-demo-kitten-w8ptp
Dec  3 16:14:52.662: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 16:14:52.662: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 16:14:52.662: INFO: update-demo-kitten-w8ptp is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:14:52.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6611" for this suite.
Dec  3 16:15:14.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:14.825: INFO: namespace kubectl-6611 deletion completed in 22.15599074s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:15:14.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 16:15:14.976: INFO: Waiting up to 5m0s for pod "pod-16f0cc96-15e8-11ea-9142-f2314dba9465" in namespace "emptydir-8218" to be "success or failure"
Dec  3 16:15:14.980: INFO: Pod "pod-16f0cc96-15e8-11ea-9142-f2314dba9465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.693461ms
Dec  3 16:15:16.985: INFO: Pod "pod-16f0cc96-15e8-11ea-9142-f2314dba9465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00885413s
STEP: Saw pod success
Dec  3 16:15:16.985: INFO: Pod "pod-16f0cc96-15e8-11ea-9142-f2314dba9465" satisfied condition "success or failure"
Dec  3 16:15:16.989: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-16f0cc96-15e8-11ea-9142-f2314dba9465 container test-container: <nil>
STEP: delete the pod
Dec  3 16:15:17.008: INFO: Waiting for pod pod-16f0cc96-15e8-11ea-9142-f2314dba9465 to disappear
Dec  3 16:15:17.012: INFO: Pod pod-16f0cc96-15e8-11ea-9142-f2314dba9465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:15:17.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8218" for this suite.
Dec  3 16:15:23.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:23.172: INFO: namespace emptydir-8218 deletion completed in 6.153926862s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:15:23.173: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3021
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-1beb3a9a-15e8-11ea-9142-f2314dba9465
STEP: Creating secret with name s-test-opt-upd-1beb3b0e-15e8-11ea-9142-f2314dba9465
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1beb3a9a-15e8-11ea-9142-f2314dba9465
STEP: Updating secret s-test-opt-upd-1beb3b0e-15e8-11ea-9142-f2314dba9465
STEP: Creating secret with name s-test-opt-create-1beb3b2c-15e8-11ea-9142-f2314dba9465
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:15:27.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3021" for this suite.
Dec  3 16:15:49.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:49.857: INFO: namespace projected-3021 deletion completed in 22.148232794s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  3 16:15:49.858: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-r5m8
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 16:15:50.223: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r5m8" in namespace "subpath-8289" to be "success or failure"
Dec  3 16:15:50.227: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051908ms
Dec  3 16:15:52.232: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008870567s
Dec  3 16:15:54.236: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 4.01338187s
Dec  3 16:15:56.242: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 6.018950388s
Dec  3 16:15:58.247: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 8.023857288s
Dec  3 16:16:00.252: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 10.02884596s
Dec  3 16:16:02.257: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 12.034119042s
Dec  3 16:16:04.262: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 14.038878567s
Dec  3 16:16:06.266: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 16.043473104s
Dec  3 16:16:08.271: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 18.048401872s
Dec  3 16:16:10.276: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Running", Reason="", readiness=true. Elapsed: 20.05323822s
Dec  3 16:16:12.283: INFO: Pod "pod-subpath-test-configmap-r5m8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059669036s
STEP: Saw pod success
Dec  3 16:16:12.283: INFO: Pod "pod-subpath-test-configmap-r5m8" satisfied condition "success or failure"
Dec  3 16:16:12.287: INFO: Trying to get logs from node izgw8jactz8ahkwwzt6d3gz pod pod-subpath-test-configmap-r5m8 container test-container-subpath-configmap-r5m8: <nil>
STEP: delete the pod
Dec  3 16:16:12.308: INFO: Waiting for pod pod-subpath-test-configmap-r5m8 to disappear
Dec  3 16:16:12.311: INFO: Pod pod-subpath-test-configmap-r5m8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r5m8
Dec  3 16:16:12.311: INFO: Deleting pod "pod-subpath-test-configmap-r5m8" in namespace "subpath-8289"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.9-beta.0.44+500f5aba80d712/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  3 16:16:12.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8289" for this suite.
Dec  3 16:16:18.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:18.467: INFO: namespace subpath-8289 deletion completed in 6.146231345s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec  3 16:16:18.468: INFO: Running AfterSuite actions on all nodes
Dec  3 16:16:18.468: INFO: Running AfterSuite actions on node 1
Dec  3 16:16:18.468: INFO: Skipping dumping logs from cluster

Ran 204 of 3586 Specs in 5242.867 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Flaked | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h27m50.258866939s
Test Suite Passed
