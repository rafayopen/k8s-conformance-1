Conformance test: not doing test setup.
I0410 14:40:57.895268    3187 e2e.go:240] Starting e2e run "a6a52bde-5b9e-11e9-ae72-2e2932f810fd" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1554907256 - Will randomize all specs
Will run 204 of 3584 specs

Apr 10 14:40:58.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 14:40:58.091: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 10 14:40:58.111: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 10 14:40:58.151: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 10 14:40:58.151: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Apr 10 14:40:58.151: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 10 14:40:58.160: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 10 14:40:58.160: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 10 14:40:58.160: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Apr 10 14:40:58.160: INFO: e2e test version: v1.14.0
Apr 10 14:40:58.162: INFO: kube-apiserver version: v1.14.0
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:40:58.162: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
Apr 10 14:40:58.303: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr 10 14:40:58.316: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 10 14:41:04.963: INFO: Successfully updated pod "pod-update-a77a5772-5b9e-11e9-ae72-2e2932f810fd"
STEP: verifying the updated pod is in kubernetes
Apr 10 14:41:04.970: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:41:04.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6563" for this suite.
Apr 10 14:41:26.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:41:27.141: INFO: namespace pods-6563 deletion completed in 22.167749977s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:41:27.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0410 14:41:28.368117    3187 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 14:41:28.368: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:41:28.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2228" for this suite.
Apr 10 14:41:34.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:41:34.489: INFO: namespace gc-2228 deletion completed in 6.117045073s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:41:34.489: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:41:34.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2762" for this suite.
Apr 10 14:41:56.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:41:56.901: INFO: namespace pods-2762 deletion completed in 22.168015981s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:41:56.901: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1851
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-ca8441da-5b9e-11e9-ae72-2e2932f810fd
STEP: Creating configMap with name cm-test-opt-upd-ca844221-5b9e-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ca8441da-5b9e-11e9-ae72-2e2932f810fd
STEP: Updating configmap cm-test-opt-upd-ca844221-5b9e-11e9-ae72-2e2932f810fd
STEP: Creating configMap with name cm-test-opt-create-ca844263-5b9e-11e9-ae72-2e2932f810fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:43:16.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1851" for this suite.
Apr 10 14:43:38.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:43:38.265: INFO: namespace projected-1851 deletion completed in 22.113760886s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:43:38.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8523
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 10 14:43:38.431: INFO: Waiting up to 5m0s for pod "pod-06d79901-5b9f-11e9-ae72-2e2932f810fd" in namespace "emptydir-8523" to be "success or failure"
Apr 10 14:43:38.434: INFO: Pod "pod-06d79901-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912946ms
Apr 10 14:43:40.438: INFO: Pod "pod-06d79901-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007387008s
Apr 10 14:43:42.443: INFO: Pod "pod-06d79901-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012299525s
STEP: Saw pod success
Apr 10 14:43:42.443: INFO: Pod "pod-06d79901-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:43:42.447: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-06d79901-5b9f-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 14:43:42.468: INFO: Waiting for pod pod-06d79901-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:43:42.471: INFO: Pod pod-06d79901-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:43:42.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8523" for this suite.
Apr 10 14:43:48.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:43:48.592: INFO: namespace emptydir-8523 deletion completed in 6.118081521s
•SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:43:48.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 14:43:48.828: INFO: Waiting up to 5m0s for pod "downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd" in namespace "downward-api-5254" to be "success or failure"
Apr 10 14:43:48.831: INFO: Pod "downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.995189ms
Apr 10 14:43:50.837: INFO: Pod "downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008249793s
Apr 10 14:43:52.841: INFO: Pod "downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012569754s
STEP: Saw pod success
Apr 10 14:43:52.841: INFO: Pod "downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:43:52.845: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 14:43:52.867: INFO: Waiting for pod downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:43:52.870: INFO: Pod downward-api-0d0a301f-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:43:52.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5254" for this suite.
Apr 10 14:43:58.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:43:58.997: INFO: namespace downward-api-5254 deletion completed in 6.124126175s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:43:58.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:44:59.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7909" for this suite.
Apr 10 14:45:23.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:45:23.380: INFO: namespace container-probe-7909 deletion completed in 24.141925936s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:45:23.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-458ae449-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 14:45:23.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd" in namespace "configmap-3770" to be "success or failure"
Apr 10 14:45:23.632: INFO: Pod "pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35867ms
Apr 10 14:45:25.637: INFO: Pod "pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00813578s
Apr 10 14:45:27.642: INFO: Pod "pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012945832s
STEP: Saw pod success
Apr 10 14:45:27.642: INFO: Pod "pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:45:27.646: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 14:45:27.667: INFO: Waiting for pod pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:45:27.670: INFO: Pod pod-configmaps-458b8adc-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:45:27.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3770" for this suite.
Apr 10 14:45:33.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:45:33.792: INFO: namespace configmap-3770 deletion completed in 6.119017842s
•S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:45:33.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-4bbed2d5-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 14:45:34.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd" in namespace "configmap-750" to be "success or failure"
Apr 10 14:45:34.039: INFO: Pod "pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.611069ms
Apr 10 14:45:36.044: INFO: Pod "pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008655994s
Apr 10 14:45:38.049: INFO: Pod "pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013193317s
STEP: Saw pod success
Apr 10 14:45:38.049: INFO: Pod "pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:45:38.053: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 14:45:38.079: INFO: Waiting for pod pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:45:38.082: INFO: Pod pod-configmaps-4bbf67ea-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:45:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-750" for this suite.
Apr 10 14:45:44.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:45:44.254: INFO: namespace configmap-750 deletion completed in 6.168541061s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:45:44.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Apr 10 14:45:44.514: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7343'
Apr 10 14:45:44.827: INFO: stderr: ""
Apr 10 14:45:44.827: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Apr 10 14:45:45.832: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 14:45:45.832: INFO: Found 0 / 1
Apr 10 14:45:46.832: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 14:45:46.832: INFO: Found 0 / 1
Apr 10 14:45:47.832: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 14:45:47.832: INFO: Found 0 / 1
Apr 10 14:45:48.832: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 14:45:48.832: INFO: Found 1 / 1
Apr 10 14:45:48.832: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 14:45:48.835: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 14:45:48.835: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr 10 14:45:48.835: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-hpwtv redis-master --namespace=kubectl-7343'
Apr 10 14:45:48.942: INFO: stderr: ""
Apr 10 14:45:48.942: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 14:45:47.039 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 14:45:47.039 # Server started, Redis version 3.2.12\n1:M 10 Apr 14:45:47.039 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 14:45:47.039 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr 10 14:45:48.942: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-hpwtv redis-master --namespace=kubectl-7343 --tail=1'
Apr 10 14:45:49.044: INFO: stderr: ""
Apr 10 14:45:49.044: INFO: stdout: "1:M 10 Apr 14:45:47.039 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr 10 14:45:49.044: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-hpwtv redis-master --namespace=kubectl-7343 --limit-bytes=1'
Apr 10 14:45:49.159: INFO: stderr: ""
Apr 10 14:45:49.159: INFO: stdout: " "
STEP: exposing timestamps
Apr 10 14:45:49.159: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-hpwtv redis-master --namespace=kubectl-7343 --tail=1 --timestamps'
Apr 10 14:45:49.253: INFO: stderr: ""
Apr 10 14:45:49.253: INFO: stdout: "2019-04-10T14:45:47.040089896Z 1:M 10 Apr 14:45:47.039 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr 10 14:45:51.754: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-hpwtv redis-master --namespace=kubectl-7343 --since=1s'
Apr 10 14:45:51.852: INFO: stderr: ""
Apr 10 14:45:51.852: INFO: stdout: ""
Apr 10 14:45:51.853: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-hpwtv redis-master --namespace=kubectl-7343 --since=24h'
Apr 10 14:45:51.953: INFO: stderr: ""
Apr 10 14:45:51.953: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 14:45:47.039 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 14:45:47.039 # Server started, Redis version 3.2.12\n1:M 10 Apr 14:45:47.039 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 14:45:47.039 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Apr 10 14:45:51.953: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7343'
Apr 10 14:45:52.044: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 14:45:52.044: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr 10 14:45:52.044: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-7343'
Apr 10 14:45:52.143: INFO: stderr: "No resources found.\n"
Apr 10 14:45:52.143: INFO: stdout: ""
Apr 10 14:45:52.143: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-7343 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 14:45:52.225: INFO: stderr: ""
Apr 10 14:45:52.225: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:45:52.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7343" for this suite.
Apr 10 14:45:58.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:45:58.346: INFO: namespace kubectl-7343 deletion completed in 6.115864632s
•SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:45:58.346: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 14:45:58.520: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:46:03.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5709" for this suite.
Apr 10 14:46:09.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:46:10.015: INFO: namespace init-container-5709 deletion completed in 6.118444656s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:46:10.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-71
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:46:10.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd" in namespace "downward-api-71" to be "success or failure"
Apr 10 14:46:10.235: INFO: Pod "downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.42612ms
Apr 10 14:46:12.239: INFO: Pod "downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008403618s
Apr 10 14:46:14.244: INFO: Pod "downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012931344s
STEP: Saw pod success
Apr 10 14:46:14.244: INFO: Pod "downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:46:14.247: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 14:46:14.267: INFO: Waiting for pod downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:46:14.270: INFO: Pod downwardapi-volume-61527f76-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:46:14.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-71" for this suite.
Apr 10 14:46:20.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:46:20.403: INFO: namespace downward-api-71 deletion completed in 6.129958096s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:46:20.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 10 14:46:20.629: INFO: Waiting up to 5m0s for pod "pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd" in namespace "emptydir-8010" to be "success or failure"
Apr 10 14:46:20.633: INFO: Pod "pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226211ms
Apr 10 14:46:22.637: INFO: Pod "pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00784075s
Apr 10 14:46:24.642: INFO: Pod "pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012159414s
STEP: Saw pod success
Apr 10 14:46:24.642: INFO: Pod "pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:46:24.646: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 14:46:24.666: INFO: Waiting for pod pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:46:24.669: INFO: Pod pod-67851d5d-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:46:24.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8010" for this suite.
Apr 10 14:46:30.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:46:30.791: INFO: namespace emptydir-8010 deletion completed in 6.118541074s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:46:30.791: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8916
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-6db845bd-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:46:33.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8916" for this suite.
Apr 10 14:46:57.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:46:57.310: INFO: namespace configmap-8916 deletion completed in 24.163923344s
•SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:46:57.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Apr 10 14:46:57.527: INFO: Waiting up to 5m0s for pod "client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd" in namespace "containers-4186" to be "success or failure"
Apr 10 14:46:57.531: INFO: Pod "client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.173829ms
Apr 10 14:46:59.536: INFO: Pod "client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008260911s
Apr 10 14:47:01.540: INFO: Pod "client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012782192s
STEP: Saw pod success
Apr 10 14:47:01.540: INFO: Pod "client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:47:01.544: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 14:47:01.566: INFO: Waiting for pod client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:47:01.569: INFO: Pod client-containers-7d836238-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:47:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4186" for this suite.
Apr 10 14:47:07.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:47:07.687: INFO: namespace containers-4186 deletion completed in 6.114025806s
•
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:47:07.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-437
Apr 10 14:47:12.030: INFO: Started pod liveness-http in namespace container-probe-437
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 14:47:12.034: INFO: Initial restart count of pod liveness-http is 0
Apr 10 14:47:36.094: INFO: Restart count of pod container-probe-437/liveness-http is now 1 (24.059335318s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:47:36.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-437" for this suite.
Apr 10 14:47:42.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:47:42.428: INFO: namespace container-probe-437 deletion completed in 6.319984927s
•S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:47:42.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3500
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-98672290-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating secret with name s-test-opt-upd-986722e7-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-98672290-5b9f-11e9-ae72-2e2932f810fd
STEP: Updating secret s-test-opt-upd-986722e7-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating secret with name s-test-opt-create-98672303-5b9f-11e9-ae72-2e2932f810fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:47:46.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3500" for this suite.
Apr 10 14:48:11.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:48:11.109: INFO: namespace projected-3500 deletion completed in 24.116836393s
•SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:48:11.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 10 14:48:11.336: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4226,SelfLink:/api/v1/namespaces/watch-4226/configmaps/e2e-watch-test-label-changed,UID:a9808ef0-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3023,Generation:0,CreationTimestamp:2019-04-10 14:48:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 14:48:11.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4226,SelfLink:/api/v1/namespaces/watch-4226/configmaps/e2e-watch-test-label-changed,UID:a9808ef0-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3024,Generation:0,CreationTimestamp:2019-04-10 14:48:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 10 14:48:11.336: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4226,SelfLink:/api/v1/namespaces/watch-4226/configmaps/e2e-watch-test-label-changed,UID:a9808ef0-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3025,Generation:0,CreationTimestamp:2019-04-10 14:48:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 10 14:48:21.370: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4226,SelfLink:/api/v1/namespaces/watch-4226/configmaps/e2e-watch-test-label-changed,UID:a9808ef0-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3048,Generation:0,CreationTimestamp:2019-04-10 14:48:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 14:48:21.370: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4226,SelfLink:/api/v1/namespaces/watch-4226/configmaps/e2e-watch-test-label-changed,UID:a9808ef0-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3049,Generation:0,CreationTimestamp:2019-04-10 14:48:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 10 14:48:21.370: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4226,SelfLink:/api/v1/namespaces/watch-4226/configmaps/e2e-watch-test-label-changed,UID:a9808ef0-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3050,Generation:0,CreationTimestamp:2019-04-10 14:48:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:48:21.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4226" for this suite.
Apr 10 14:48:27.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:48:27.536: INFO: namespace watch-4226 deletion completed in 6.161776122s
•SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:48:27.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8727
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b346e355-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 14:48:27.733: INFO: Waiting up to 5m0s for pod "pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd" in namespace "secrets-8727" to be "success or failure"
Apr 10 14:48:27.736: INFO: Pod "pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426294ms
Apr 10 14:48:29.741: INFO: Pod "pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008170931s
STEP: Saw pod success
Apr 10 14:48:29.741: INFO: Pod "pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:48:29.745: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:48:29.769: INFO: Waiting for pod pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:48:29.772: INFO: Pod pod-secrets-b3478960-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:48:29.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8727" for this suite.
Apr 10 14:48:35.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:48:35.966: INFO: namespace secrets-8727 deletion completed in 6.190364569s
•SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:48:35.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-b848c2ce-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 14:48:36.132: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd" in namespace "projected-8632" to be "success or failure"
Apr 10 14:48:36.136: INFO: Pod "pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719998ms
Apr 10 14:48:38.143: INFO: Pod "pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01014313s
STEP: Saw pod success
Apr 10 14:48:38.143: INFO: Pod "pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:48:38.147: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:48:38.168: INFO: Waiting for pod pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:48:38.172: INFO: Pod pod-projected-secrets-b8495ca3-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:48:38.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8632" for this suite.
Apr 10 14:48:44.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:48:44.284: INFO: namespace projected-8632 deletion completed in 6.108604434s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:48:44.284: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 10 14:48:44.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3151,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 14:48:44.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3151,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 10 14:48:54.538: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3174,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 10 14:48:54.539: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3174,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 10 14:49:04.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3196,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 14:49:04.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3196,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 10 14:49:14.558: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3218,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 14:49:14.558: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-a,UID:bd4b11e4-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3218,Generation:0,CreationTimestamp:2019-04-10 14:48:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 10 14:49:24.567: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-b,UID:d5281f82-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3241,Generation:0,CreationTimestamp:2019-04-10 14:49:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 14:49:24.567: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-b,UID:d5281f82-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3241,Generation:0,CreationTimestamp:2019-04-10 14:49:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 10 14:49:34.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-b,UID:d5281f82-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3262,Generation:0,CreationTimestamp:2019-04-10 14:49:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 14:49:34.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3158,SelfLink:/api/v1/namespaces/watch-3158/configmaps/e2e-watch-test-configmap-b,UID:d5281f82-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3262,Generation:0,CreationTimestamp:2019-04-10 14:49:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:49:44.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3158" for this suite.
Apr 10 14:49:50.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:49:50.698: INFO: namespace watch-3158 deletion completed in 6.115284439s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:49:50.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 10 14:49:50.947: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5134,SelfLink:/api/v1/namespaces/watch-5134/configmaps/e2e-watch-test-resource-version,UID:e4de7dbb-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3313,Generation:0,CreationTimestamp:2019-04-10 14:49:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 14:49:50.947: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5134,SelfLink:/api/v1/namespaces/watch-5134/configmaps/e2e-watch-test-resource-version,UID:e4de7dbb-5b9f-11e9-82f7-160dbbe32fdc,ResourceVersion:3314,Generation:0,CreationTimestamp:2019-04-10 14:49:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:49:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5134" for this suite.
Apr 10 14:49:56.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:49:57.113: INFO: namespace watch-5134 deletion completed in 6.162395868s
•SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:49:57.113: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-e8aeb275-5b9f-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 14:49:57.344: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd" in namespace "projected-957" to be "success or failure"
Apr 10 14:49:57.349: INFO: Pod "pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.993899ms
Apr 10 14:49:59.354: INFO: Pod "pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010025589s
Apr 10 14:50:01.359: INFO: Pod "pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015078313s
STEP: Saw pod success
Apr 10 14:50:01.359: INFO: Pod "pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:50:01.363: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 14:50:01.384: INFO: Waiting for pod pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:50:01.388: INFO: Pod pod-projected-configmaps-e8af5b5c-5b9f-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:50:01.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-957" for this suite.
Apr 10 14:50:09.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:50:09.502: INFO: namespace projected-957 deletion completed in 8.11020898s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:50:09.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-5548
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5548
STEP: Deleting pre-stop pod
Apr 10 14:50:20.945: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:50:20.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5548" for this suite.
Apr 10 14:51:00.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:51:01.075: INFO: namespace prestop-5548 deletion completed in 40.120386944s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:51:01.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-217
Apr 10 14:51:03.489: INFO: Started pod liveness-exec in namespace container-probe-217
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 14:51:03.494: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:55:04.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-217" for this suite.
Apr 10 14:55:10.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:55:10.208: INFO: namespace container-probe-217 deletion completed in 6.128612248s
•SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:55:10.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:55:10.427: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd" in namespace "downward-api-4111" to be "success or failure"
Apr 10 14:55:10.431: INFO: Pod "downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29649ms
Apr 10 14:55:12.436: INFO: Pod "downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008903973s
Apr 10 14:55:14.441: INFO: Pod "downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013618339s
STEP: Saw pod success
Apr 10 14:55:14.441: INFO: Pod "downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:55:14.445: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 14:55:14.469: INFO: Waiting for pod downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:55:14.472: INFO: Pod downwardapi-volume-a34ddbeb-5ba0-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:55:14.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4111" for this suite.
Apr 10 14:55:20.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:55:20.858: INFO: namespace downward-api-4111 deletion completed in 6.381830793s
•SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:55:20.858: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-a99fb35a-5ba0-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 14:55:21.035: INFO: Waiting up to 5m0s for pod "pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd" in namespace "secrets-3868" to be "success or failure"
Apr 10 14:55:21.038: INFO: Pod "pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.553836ms
Apr 10 14:55:23.043: INFO: Pod "pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007994795s
STEP: Saw pod success
Apr 10 14:55:23.043: INFO: Pod "pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:55:23.046: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:55:23.067: INFO: Waiting for pod pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:55:23.070: INFO: Pod pod-secrets-a9a07448-5ba0-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:55:23.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3868" for this suite.
Apr 10 14:55:29.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:55:29.200: INFO: namespace secrets-3868 deletion completed in 6.126377351s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:55:29.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-ps6d
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 14:55:29.436: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ps6d" in namespace "subpath-6394" to be "success or failure"
Apr 10 14:55:29.439: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170267ms
Apr 10 14:55:31.444: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007998837s
Apr 10 14:55:33.449: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 4.012995749s
Apr 10 14:55:35.454: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 6.018003289s
Apr 10 14:55:37.458: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 8.02230551s
Apr 10 14:55:39.463: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 10.027152791s
Apr 10 14:55:41.467: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 12.0315827s
Apr 10 14:55:43.472: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 14.036877883s
Apr 10 14:55:45.477: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 16.041220886s
Apr 10 14:55:47.481: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 18.045734278s
Apr 10 14:55:49.486: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Running", Reason="", readiness=true. Elapsed: 20.050087639s
Apr 10 14:55:51.490: INFO: Pod "pod-subpath-test-projected-ps6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.054888438s
STEP: Saw pod success
Apr 10 14:55:51.491: INFO: Pod "pod-subpath-test-projected-ps6d" satisfied condition "success or failure"
Apr 10 14:55:51.494: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-subpath-test-projected-ps6d container test-container-subpath-projected-ps6d: <nil>
STEP: delete the pod
Apr 10 14:55:51.515: INFO: Waiting for pod pod-subpath-test-projected-ps6d to disappear
Apr 10 14:55:51.518: INFO: Pod pod-subpath-test-projected-ps6d no longer exists
STEP: Deleting pod pod-subpath-test-projected-ps6d
Apr 10 14:55:51.518: INFO: Deleting pod "pod-subpath-test-projected-ps6d" in namespace "subpath-6394"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:55:51.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6394" for this suite.
Apr 10 14:55:59.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:55:59.641: INFO: namespace subpath-6394 deletion completed in 8.11754761s
•SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:55:59.642: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3459
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:55:59.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:56:00.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3459" for this suite.
Apr 10 14:56:06.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:56:07.361: INFO: namespace custom-resource-definition-3459 deletion completed in 6.385407641s
•S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:56:07.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 14:56:07.521: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 14:56:07.529: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 14:56:07.533: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l before test
Apr 10 14:56:07.544: INFO: metrics-server-84f8f5f44f-5nx55 from kube-system started at 2019-04-10 14:34:30 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 14:56:07.544: INFO: coredns-7f7f7978c8-sb9zb from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container coredns ready: true, restart count 0
Apr 10 14:56:07.544: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-gkzbt from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 14:56:07.544: INFO: kube-proxy-w7thh from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 14:56:07.544: INFO: node-exporter-dsgq7 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 14:56:07.544: INFO: blackbox-exporter-6dc58dcffc-6rds9 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 14:56:07.544: INFO: calico-node-4cqp8 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.544: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 14:56:07.544: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt before test
Apr 10 14:56:07.590: INFO: kube-proxy-86z4l from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 14:56:07.590: INFO: calico-node-vjz47 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 14:56:07.590: INFO: node-exporter-qd9pz from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 14:56:07.590: INFO: addons-nginx-ingress-controller-d4f8c9cc5-swsd5 from kube-system started at 2019-04-10 14:34:30 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 14:56:07.590: INFO: vpn-shoot-798b6484c-m4qbr from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 14:56:07.590: INFO: coredns-7f7f7978c8-pbh8n from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container coredns ready: true, restart count 0
Apr 10 14:56:07.590: INFO: addons-kubernetes-dashboard-665df4b66d-ltjvt from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 14:56:07.590: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
STEP: verifying the node has the label node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod addons-kubernetes-dashboard-665df4b66d-ltjvt requesting resource cpu=50m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod addons-nginx-ingress-controller-d4f8c9cc5-swsd5 requesting resource cpu=100m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-gkzbt requesting resource cpu=0m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod blackbox-exporter-6dc58dcffc-6rds9 requesting resource cpu=5m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod calico-node-4cqp8 requesting resource cpu=100m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod calico-node-vjz47 requesting resource cpu=100m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod coredns-7f7f7978c8-pbh8n requesting resource cpu=50m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod coredns-7f7f7978c8-sb9zb requesting resource cpu=50m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod kube-proxy-86z4l requesting resource cpu=20m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod kube-proxy-w7thh requesting resource cpu=20m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod metrics-server-84f8f5f44f-5nx55 requesting resource cpu=20m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod node-exporter-dsgq7 requesting resource cpu=5m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
Apr 10 14:56:07.623: INFO: Pod node-exporter-qd9pz requesting resource cpu=5m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
Apr 10 14:56:07.623: INFO: Pod vpn-shoot-798b6484c-m4qbr requesting resource cpu=50m on Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd.159424bd7e51f0cd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6191/filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd to shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd.159424bda6bea9e1], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd.159424bdbf14d540], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd.159424bdc25e41b7], Reason = [Created], Message = [Created container filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd.159424bdcb9a9eca], Reason = [Started], Message = [Started container filler-pod-c5667c83-5ba0-11e9-ae72-2e2932f810fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd.159424bd7edb2874], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6191/filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd to shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd.159424bda742cff6], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd.159424bdc088058a], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd.159424bdc3c19df4], Reason = [Created], Message = [Created container filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd.159424bdcc9b2a2c], Reason = [Started], Message = [Started container filler-pod-c567b06e-5ba0-11e9-ae72-2e2932f810fd]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159424bdf722ce9b], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:56:10.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6191" for this suite.
Apr 10 14:56:16.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:56:16.807: INFO: namespace sched-pred-6191 deletion completed in 6.112378084s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:56:16.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Apr 10 14:56:19.041: INFO: Pod pod-hostip-cb00441f-5ba0-11e9-ae72-2e2932f810fd has hostIP: 10.250.0.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:56:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-838" for this suite.
Apr 10 14:56:43.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:56:43.164: INFO: namespace pods-838 deletion completed in 24.119710753s
•SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:56:43.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Apr 10 14:56:43.876: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 10 14:56:45.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 14:56:47.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 14:56:49.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 14:56:51.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505003, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 14:56:55.203: INFO: Waited 1.282459896s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:56:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7729" for this suite.
Apr 10 14:57:02.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:57:02.201: INFO: namespace aggregator-7729 deletion completed in 6.173182219s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:57:02.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Apr 10 14:57:02.421: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3008'
Apr 10 14:57:02.851: INFO: stderr: ""
Apr 10 14:57:02.851: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 14:57:02.851: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3008'
Apr 10 14:57:02.946: INFO: stderr: ""
Apr 10 14:57:02.946: INFO: stdout: "update-demo-nautilus-b28wp update-demo-nautilus-nq6bd "
Apr 10 14:57:02.946: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-b28wp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:03.029: INFO: stderr: ""
Apr 10 14:57:03.029: INFO: stdout: ""
Apr 10 14:57:03.029: INFO: update-demo-nautilus-b28wp is created but not running
Apr 10 14:57:08.029: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3008'
Apr 10 14:57:08.111: INFO: stderr: ""
Apr 10 14:57:08.111: INFO: stdout: "update-demo-nautilus-b28wp update-demo-nautilus-nq6bd "
Apr 10 14:57:08.112: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-b28wp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:08.198: INFO: stderr: ""
Apr 10 14:57:08.198: INFO: stdout: "true"
Apr 10 14:57:08.198: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-b28wp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:08.275: INFO: stderr: ""
Apr 10 14:57:08.275: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:57:08.275: INFO: validating pod update-demo-nautilus-b28wp
Apr 10 14:57:08.366: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:57:08.366: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:57:08.366: INFO: update-demo-nautilus-b28wp is verified up and running
Apr 10 14:57:08.366: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nq6bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:08.449: INFO: stderr: ""
Apr 10 14:57:08.449: INFO: stdout: "true"
Apr 10 14:57:08.449: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nq6bd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:08.531: INFO: stderr: ""
Apr 10 14:57:08.531: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 14:57:08.531: INFO: validating pod update-demo-nautilus-nq6bd
Apr 10 14:57:08.619: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 14:57:08.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 14:57:08.619: INFO: update-demo-nautilus-nq6bd is verified up and running
STEP: rolling-update to new replication controller
Apr 10 14:57:08.622: INFO: scanned /root for discovery docs: <nil>
Apr 10 14:57:08.622: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3008'
Apr 10 14:57:31.008: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 10 14:57:31.008: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 14:57:31.008: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3008'
Apr 10 14:57:31.107: INFO: stderr: ""
Apr 10 14:57:31.107: INFO: stdout: "update-demo-kitten-9hhb8 update-demo-kitten-pb2fg "
Apr 10 14:57:31.107: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-9hhb8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:31.198: INFO: stderr: ""
Apr 10 14:57:31.198: INFO: stdout: "true"
Apr 10 14:57:31.199: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-9hhb8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:31.272: INFO: stderr: ""
Apr 10 14:57:31.272: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 10 14:57:31.272: INFO: validating pod update-demo-kitten-9hhb8
Apr 10 14:57:31.362: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 10 14:57:31.362: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 10 14:57:31.362: INFO: update-demo-kitten-9hhb8 is verified up and running
Apr 10 14:57:31.363: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-pb2fg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:31.438: INFO: stderr: ""
Apr 10 14:57:31.438: INFO: stdout: "true"
Apr 10 14:57:31.439: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-pb2fg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3008'
Apr 10 14:57:31.514: INFO: stderr: ""
Apr 10 14:57:31.514: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 10 14:57:31.514: INFO: validating pod update-demo-kitten-pb2fg
Apr 10 14:57:31.602: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 10 14:57:31.602: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 10 14:57:31.602: INFO: update-demo-kitten-pb2fg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:57:31.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3008" for this suite.
Apr 10 14:58:03.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:03.727: INFO: namespace kubectl-3008 deletion completed in 32.120451895s
•S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:03.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 14:58:03.931: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd" in namespace "downward-api-5326" to be "success or failure"
Apr 10 14:58:03.934: INFO: Pod "downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08052ms
Apr 10 14:58:05.939: INFO: Pod "downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007603969s
STEP: Saw pod success
Apr 10 14:58:05.939: INFO: Pod "downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:58:05.943: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 14:58:05.963: INFO: Waiting for pod downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:58:05.966: INFO: Pod downwardapi-volume-0ab8709a-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:05.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5326" for this suite.
Apr 10 14:58:11.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:12.083: INFO: namespace downward-api-5326 deletion completed in 6.112672171s
•S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:12.083: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 14:58:12.532: INFO: (0) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.414954ms)
Apr 10 14:58:12.575: INFO: (1) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 42.988329ms)
Apr 10 14:58:12.581: INFO: (2) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.214239ms)
Apr 10 14:58:12.587: INFO: (3) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.398623ms)
Apr 10 14:58:12.593: INFO: (4) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.758978ms)
Apr 10 14:58:12.599: INFO: (5) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.949493ms)
Apr 10 14:58:12.604: INFO: (6) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.30821ms)
Apr 10 14:58:12.609: INFO: (7) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.514211ms)
Apr 10 14:58:12.615: INFO: (8) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.795814ms)
Apr 10 14:58:12.621: INFO: (9) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.410555ms)
Apr 10 14:58:12.626: INFO: (10) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.666426ms)
Apr 10 14:58:12.632: INFO: (11) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.782357ms)
Apr 10 14:58:12.638: INFO: (12) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.699688ms)
Apr 10 14:58:12.644: INFO: (13) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.712838ms)
Apr 10 14:58:12.650: INFO: (14) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.77245ms)
Apr 10 14:58:12.655: INFO: (15) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.13103ms)
Apr 10 14:58:12.660: INFO: (16) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.021233ms)
Apr 10 14:58:12.665: INFO: (17) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.908808ms)
Apr 10 14:58:12.671: INFO: (18) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.793054ms)
Apr 10 14:58:12.675: INFO: (19) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.938057ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:12.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8123" for this suite.
Apr 10 14:58:18.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:18.799: INFO: namespace proxy-8123 deletion completed in 6.120185864s
•S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:18.799: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 14:58:19.108: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:23.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-326" for this suite.
Apr 10 14:58:31.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:31.326: INFO: namespace init-container-326 deletion completed in 8.105642195s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:31.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-2274/configmap-test-1b2b5e92-5ba1-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 14:58:31.531: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd" in namespace "configmap-2274" to be "success or failure"
Apr 10 14:58:31.534: INFO: Pod "pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.988942ms
Apr 10 14:58:33.538: INFO: Pod "pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007288022s
STEP: Saw pod success
Apr 10 14:58:33.538: INFO: Pod "pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:58:33.542: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd container env-test: <nil>
STEP: delete the pod
Apr 10 14:58:33.562: INFO: Waiting for pod pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:58:33.564: INFO: Pod pod-configmaps-1b2bf171-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:33.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2274" for this suite.
Apr 10 14:58:39.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:39.704: INFO: namespace configmap-2274 deletion completed in 6.136643897s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:39.705: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Apr 10 14:58:39.927: INFO: Asynchronously running '/bin/kubectl kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:40.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4747" for this suite.
Apr 10 14:58:46.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:46.159: INFO: namespace kubectl-4747 deletion completed in 6.120135402s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:46.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Apr 10 14:58:46.324: INFO: Asynchronously running '/bin/kubectl kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix310182084/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:46.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2959" for this suite.
Apr 10 14:58:52.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:58:52.507: INFO: namespace kubectl-2959 deletion completed in 6.115439713s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:58:52.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-27cdb709-5ba1-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 14:58:52.727: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd" in namespace "projected-7416" to be "success or failure"
Apr 10 14:58:52.730: INFO: Pod "pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.009762ms
Apr 10 14:58:54.734: INFO: Pod "pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007283945s
STEP: Saw pod success
Apr 10 14:58:54.734: INFO: Pod "pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:58:54.737: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:58:54.755: INFO: Waiting for pod pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:58:54.759: INFO: Pod pod-projected-secrets-27ce4771-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:58:54.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7416" for this suite.
Apr 10 14:59:00.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:59:00.879: INFO: namespace projected-7416 deletion completed in 6.115659424s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:59:00.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 14:59:01.213: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2973'
Apr 10 14:59:01.316: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 14:59:01.316: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Apr 10 14:59:01.323: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Apr 10 14:59:01.328: INFO: scanned /root for discovery docs: <nil>
Apr 10 14:59:01.328: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2973'
Apr 10 14:59:17.120: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 10 14:59:17.120: INFO: stdout: "Created e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287\nScaling up e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr 10 14:59:17.120: INFO: stdout: "Created e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287\nScaling up e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr 10 14:59:17.120: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-2973'
Apr 10 14:59:17.240: INFO: stderr: ""
Apr 10 14:59:17.240: INFO: stdout: "e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287-zww49 "
Apr 10 14:59:17.240: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287-zww49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2973'
Apr 10 14:59:17.355: INFO: stderr: ""
Apr 10 14:59:17.355: INFO: stdout: "true"
Apr 10 14:59:17.355: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287-zww49 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2973'
Apr 10 14:59:17.457: INFO: stderr: ""
Apr 10 14:59:17.457: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr 10 14:59:17.457: INFO: e2e-test-nginx-rc-af92cba5813fcc231e6a6f6cf4360287-zww49 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Apr 10 14:59:17.457: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-2973'
Apr 10 14:59:17.587: INFO: stderr: ""
Apr 10 14:59:17.587: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:59:17.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2973" for this suite.
Apr 10 14:59:39.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:59:39.708: INFO: namespace kubectl-2973 deletion completed in 22.115031545s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:59:39.709: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 14:59:39.929: INFO: Waiting up to 5m0s for pod "downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd" in namespace "downward-api-6032" to be "success or failure"
Apr 10 14:59:39.933: INFO: Pod "downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194406ms
Apr 10 14:59:41.938: INFO: Pod "downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008067414s
STEP: Saw pod success
Apr 10 14:59:41.938: INFO: Pod "downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:59:41.941: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 14:59:41.979: INFO: Waiting for pod downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:59:41.982: INFO: Pod downward-api-43f08681-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:59:41.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6032" for this suite.
Apr 10 14:59:47.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:59:48.310: INFO: namespace downward-api-6032 deletion completed in 6.322956961s
•SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 14:59:48.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6806
STEP: Creating secret with name secret-test-4911096b-5ba1-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 14:59:48.821: INFO: Waiting up to 5m0s for pod "pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd" in namespace "secrets-9503" to be "success or failure"
Apr 10 14:59:48.824: INFO: Pod "pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.25621ms
Apr 10 14:59:50.829: INFO: Pod "pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007641231s
STEP: Saw pod success
Apr 10 14:59:50.829: INFO: Pod "pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 14:59:50.832: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 14:59:50.852: INFO: Waiting for pod pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 14:59:50.855: INFO: Pod pod-secrets-493d84f3-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 14:59:50.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9503" for this suite.
Apr 10 14:59:58.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 14:59:58.970: INFO: namespace secrets-9503 deletion completed in 8.110169606s
STEP: Destroying namespace "secret-namespace-6806" for this suite.
Apr 10 15:00:04.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:00:05.081: INFO: namespace secret-namespace-6806 deletion completed in 6.11086257s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:00:05.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 10 15:00:08.350: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:00:09.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8901" for this suite.
Apr 10 15:00:31.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:00:31.498: INFO: namespace replicaset-8901 deletion completed in 22.125926191s
•SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:00:31.498: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8107
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-397
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:00:56.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6772" for this suite.
Apr 10 15:01:02.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:01:02.245: INFO: namespace namespaces-6772 deletion completed in 6.121506153s
STEP: Destroying namespace "nsdeletetest-8107" for this suite.
Apr 10 15:01:02.249: INFO: Namespace nsdeletetest-8107 was already deleted
STEP: Destroying namespace "nsdeletetest-397" for this suite.
Apr 10 15:01:08.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:01:08.368: INFO: namespace nsdeletetest-397 deletion completed in 6.119399603s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:01:08.369: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-78c01729-5ba1-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:01:08.534: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd" in namespace "projected-352" to be "success or failure"
Apr 10 15:01:08.538: INFO: Pod "pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217161ms
Apr 10 15:01:10.544: INFO: Pod "pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00959725s
STEP: Saw pod success
Apr 10 15:01:10.544: INFO: Pod "pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:01:10.548: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 15:01:10.571: INFO: Waiting for pod pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:01:10.574: INFO: Pod pod-projected-configmaps-78c0b30c-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:01:10.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-352" for this suite.
Apr 10 15:01:16.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:01:16.694: INFO: namespace projected-352 deletion completed in 6.115584289s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:01:16.694: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5338
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-7ddca9b9-5ba1-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7ddca9b9-5ba1-11e9-ae72-2e2932f810fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:01:21.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5338" for this suite.
Apr 10 15:01:45.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:01:45.439: INFO: namespace configmap-5338 deletion completed in 24.11937382s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:01:45.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 10 15:01:45.630: INFO: Waiting up to 5m0s for pod "pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd" in namespace "emptydir-9286" to be "success or failure"
Apr 10 15:01:45.633: INFO: Pod "pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663358ms
Apr 10 15:01:47.638: INFO: Pod "pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008071113s
STEP: Saw pod success
Apr 10 15:01:47.638: INFO: Pod "pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:01:47.641: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:01:47.661: INFO: Waiting for pod pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:01:47.664: INFO: Pod pod-8edcfde7-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:01:47.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9286" for this suite.
Apr 10 15:01:53.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:01:53.787: INFO: namespace emptydir-9286 deletion completed in 6.120342481s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:01:53.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 10 15:02:00.060: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 15:02:00.064: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 15:02:02.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 15:02:02.068: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 15:02:04.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 15:02:04.068: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 15:02:06.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 15:02:06.069: INFO: Pod pod-with-prestop-http-hook still exists
Apr 10 15:02:08.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 10 15:02:08.068: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:02:08.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3096" for this suite.
Apr 10 15:02:32.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:02:32.241: INFO: namespace container-lifecycle-hook-3096 deletion completed in 24.157351345s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:02:32.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 15:02:32.513: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:02:36.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9250" for this suite.
Apr 10 15:02:58.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:02:59.084: INFO: namespace init-container-9250 deletion completed in 22.124447273s
•SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:02:59.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 15:02:59.327: INFO: Waiting up to 5m0s for pod "downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd" in namespace "downward-api-1806" to be "success or failure"
Apr 10 15:02:59.330: INFO: Pod "downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.14229ms
Apr 10 15:03:01.334: INFO: Pod "downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007121257s
STEP: Saw pod success
Apr 10 15:03:01.334: INFO: Pod "downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:03:01.338: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 15:03:01.361: INFO: Waiting for pod downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:03:01.364: INFO: Pod downward-api-baca4187-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:03:01.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1806" for this suite.
Apr 10 15:03:07.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:03:07.489: INFO: namespace downward-api-1806 deletion completed in 6.122266156s
•SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:03:07.489: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-bfcc2cf0-5ba1-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:03:07.732: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd" in namespace "projected-8954" to be "success or failure"
Apr 10 15:03:07.744: INFO: Pod "pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.307808ms
Apr 10 15:03:09.749: INFO: Pod "pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017032873s
STEP: Saw pod success
Apr 10 15:03:09.749: INFO: Pod "pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:03:09.752: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 15:03:09.775: INFO: Waiting for pod pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:03:09.777: INFO: Pod pod-projected-configmaps-bfccce45-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:03:09.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8954" for this suite.
Apr 10 15:03:15.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:03:15.903: INFO: namespace projected-8954 deletion completed in 6.122355287s
•SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:03:15.903: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:03:16.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd" in namespace "projected-1555" to be "success or failure"
Apr 10 15:03:16.135: INFO: Pod "downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457409ms
Apr 10 15:03:18.147: INFO: Pod "downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015430997s
STEP: Saw pod success
Apr 10 15:03:18.147: INFO: Pod "downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:03:18.150: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:03:18.170: INFO: Waiting for pod downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:03:18.172: INFO: Pod downwardapi-volume-c4ce57d5-5ba1-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:03:18.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1555" for this suite.
Apr 10 15:03:26.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:03:26.300: INFO: namespace projected-1555 deletion completed in 8.12424613s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:03:26.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 10 15:03:29.055: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cb00fe09-5ba1-11e9-ae72-2e2932f810fd"
Apr 10 15:03:29.055: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cb00fe09-5ba1-11e9-ae72-2e2932f810fd" in namespace "pods-2628" to be "terminated due to deadline exceeded"
Apr 10 15:03:29.059: INFO: Pod "pod-update-activedeadlineseconds-cb00fe09-5ba1-11e9-ae72-2e2932f810fd": Phase="Running", Reason="", readiness=true. Elapsed: 3.200809ms
Apr 10 15:03:31.063: INFO: Pod "pod-update-activedeadlineseconds-cb00fe09-5ba1-11e9-ae72-2e2932f810fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007385237s
Apr 10 15:03:33.074: INFO: Pod "pod-update-activedeadlineseconds-cb00fe09-5ba1-11e9-ae72-2e2932f810fd": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.018511353s
Apr 10 15:03:33.074: INFO: Pod "pod-update-activedeadlineseconds-cb00fe09-5ba1-11e9-ae72-2e2932f810fd" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:03:33.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2628" for this suite.
Apr 10 15:03:39.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:03:39.207: INFO: namespace pods-2628 deletion completed in 6.128947454s
•SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:03:39.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9512
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9512
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-9512
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9512
Apr 10 15:03:39.520: INFO: Found 0 stateful pods, waiting for 1
Apr 10 15:03:49.527: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 10 15:03:49.531: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 15:03:50.098: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 15:03:50.098: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 15:03:50.098: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 15:03:50.102: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 10 15:04:00.108: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 15:04:00.108: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 15:04:00.124: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:00.124: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:00.124: INFO: 
Apr 10 15:04:00.124: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 10 15:04:01.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996079208s
Apr 10 15:04:02.136: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989091623s
Apr 10 15:04:03.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984560693s
Apr 10 15:04:04.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979555677s
Apr 10 15:04:05.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974537024s
Apr 10 15:04:06.156: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969628126s
Apr 10 15:04:07.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964810488s
Apr 10 15:04:08.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960204242s
Apr 10 15:04:09.169: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.473292ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9512
Apr 10 15:04:10.174: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:04:10.715: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 15:04:10.715: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 15:04:10.715: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 15:04:10.715: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:04:11.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 10 15:04:11.182: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 15:04:11.182: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 15:04:11.182: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:04:11.700: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 10 15:04:11.700: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 15:04:11.700: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 15:04:11.705: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 15:04:11.705: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 15:04:11.705: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 10 15:04:11.709: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 15:04:12.210: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 15:04:12.210: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 15:04:12.210: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 15:04:12.210: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 15:04:12.678: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 15:04:12.678: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 15:04:12.678: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 15:04:12.678: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 15:04:13.208: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 15:04:13.208: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 15:04:13.208: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 15:04:13.208: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 15:04:13.212: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Apr 10 15:04:23.222: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 15:04:23.222: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 15:04:23.222: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 15:04:23.234: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:23.234: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:23.234: INFO: ss-1  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:23.234: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:23.234: INFO: 
Apr 10 15:04:23.234: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 15:04:24.239: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:24.239: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:24.239: INFO: ss-1  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:24.239: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:24.239: INFO: 
Apr 10 15:04:24.239: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 15:04:25.246: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:25.246: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:25.246: INFO: ss-1  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:25.246: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:25.246: INFO: 
Apr 10 15:04:25.246: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 10 15:04:26.252: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:26.252: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:26.252: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:26.252: INFO: 
Apr 10 15:04:26.252: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 15:04:27.256: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:27.256: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:27.256: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:27.256: INFO: 
Apr 10 15:04:27.256: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 15:04:28.262: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:28.262: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:28.262: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:28.262: INFO: 
Apr 10 15:04:28.262: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 15:04:29.266: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:29.266: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:29.266: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:29.266: INFO: 
Apr 10 15:04:29.266: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 15:04:30.271: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:30.271: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:30.271: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:30.271: INFO: 
Apr 10 15:04:30.271: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 15:04:31.275: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:31.275: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:31.275: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:31.275: INFO: 
Apr 10 15:04:31.275: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 10 15:04:32.280: INFO: POD   NODE                                               PHASE    GRACE  CONDITIONS
Apr 10 15:04:32.280: INFO: ss-0  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:03:39 +0000 UTC  }]
Apr 10 15:04:32.280: INFO: ss-2  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:04:00 +0000 UTC  }]
Apr 10 15:04:32.280: INFO: 
Apr 10 15:04:32.280: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9512
Apr 10 15:04:33.285: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:04:33.486: INFO: rc: 1
Apr 10 15:04:33.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0024001b0 exit status 1 <nil> <nil> true [0xc002c04a78 0xc002c04a90 0xc002c04aa8] [0xc002c04a78 0xc002c04a90 0xc002c04aa8] [0xc002c04a88 0xc002c04aa0] [0x9bf9f0 0x9bf9f0] 0xc002cc9620 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Apr 10 15:04:43.487: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:04:43.593: INFO: rc: 1
Apr 10 15:04:43.593: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024ca3c0 exit status 1 <nil> <nil> true [0xc00215d578 0xc00215d5a0 0xc00215d5c0] [0xc00215d578 0xc00215d5a0 0xc00215d5c0] [0xc00215d590 0xc00215d5b8] [0x9bf9f0 0x9bf9f0] 0xc002e36120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:04:53.594: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:04:53.673: INFO: rc: 1
Apr 10 15:04:53.673: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002400870 exit status 1 <nil> <nil> true [0xc002c04ab0 0xc002c04ac8 0xc002c04ae0] [0xc002c04ab0 0xc002c04ac8 0xc002c04ae0] [0xc002c04ac0 0xc002c04ad8] [0x9bf9f0 0x9bf9f0] 0xc002cc9c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:05:03.674: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:05:03.753: INFO: rc: 1
Apr 10 15:05:03.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c245d0 exit status 1 <nil> <nil> true [0xc0023e04a8 0xc0023e04c0 0xc0023e04d8] [0xc0023e04a8 0xc0023e04c0 0xc0023e04d8] [0xc0023e04b8 0xc0023e04d0] [0x9bf9f0 0x9bf9f0] 0xc0024689c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:05:13.754: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:05:13.829: INFO: rc: 1
Apr 10 15:05:13.829: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e9e6c0 exit status 1 <nil> <nil> true [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2078 0xc0009b21a8] [0x9bf9f0 0x9bf9f0] 0xc001f804e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:05:23.830: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:05:23.906: INFO: rc: 1
Apr 10 15:05:23.906: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c47e0 exit status 1 <nil> <nil> true [0xc000010690 0xc0000108c8 0xc0000109c0] [0xc000010690 0xc0000108c8 0xc0000109c0] [0xc000010888 0xc000010938] [0x9bf9f0 0x9bf9f0] 0xc002d90c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:05:33.907: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:05:33.976: INFO: rc: 1
Apr 10 15:05:33.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e9ede0 exit status 1 <nil> <nil> true [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2270 0xc0009b2368] [0x9bf9f0 0x9bf9f0] 0xc001f81020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:05:43.977: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:05:44.051: INFO: rc: 1
Apr 10 15:05:44.051: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00321c6f0 exit status 1 <nil> <nil> true [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a2e0 0xc000a1a3c8] [0x9bf9f0 0x9bf9f0] 0xc002448060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:05:54.052: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:05:54.196: INFO: rc: 1
Apr 10 15:05:54.196: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c4ed0 exit status 1 <nil> <nil> true [0xc000010a20 0xc000010b60 0xc000010c00] [0xc000010a20 0xc000010b60 0xc000010c00] [0xc000010b28 0xc000010b78] [0x9bf9f0 0x9bf9f0] 0xc002d90f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:06:04.196: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:06:04.267: INFO: rc: 1
Apr 10 15:06:04.267: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e0e6f0 exit status 1 <nil> <nil> true [0xc00019c1b8 0xc00019c2d0 0xc00019c3d0] [0xc00019c1b8 0xc00019c2d0 0xc00019c3d0] [0xc00019c270 0xc00019c3c0] [0x9bf9f0 0x9bf9f0] 0xc0023ba960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:06:14.268: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:06:14.349: INFO: rc: 1
Apr 10 15:06:14.349: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c55c0 exit status 1 <nil> <nil> true [0xc000010c80 0xc000010d10 0xc000010fa8] [0xc000010c80 0xc000010d10 0xc000010fa8] [0xc000010cd8 0xc000010ed8] [0x9bf9f0 0x9bf9f0] 0xc002d91320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:06:24.350: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:06:24.423: INFO: rc: 1
Apr 10 15:06:24.424: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c5c80 exit status 1 <nil> <nil> true [0xc000010fe8 0xc000011060 0xc000011118] [0xc000010fe8 0xc000011060 0xc000011118] [0xc000011048 0xc0000110a8] [0x9bf9f0 0x9bf9f0] 0xc002d91620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:06:34.424: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:06:34.500: INFO: rc: 1
Apr 10 15:06:34.500: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00321cde0 exit status 1 <nil> <nil> true [0xc000a1a448 0xc000a1a4f8 0xc000a1a740] [0xc000a1a448 0xc000a1a4f8 0xc000a1a740] [0xc000a1a490 0xc000a1a678] [0x9bf9f0 0x9bf9f0] 0xc002448a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:06:44.500: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:06:44.583: INFO: rc: 1
Apr 10 15:06:44.583: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0015b44e0 exit status 1 <nil> <nil> true [0xc000011140 0xc000011308 0xc000011540] [0xc000011140 0xc000011308 0xc000011540] [0xc000011250 0xc0000114b8] [0x9bf9f0 0x9bf9f0] 0xc002d91980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:06:54.584: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:06:54.660: INFO: rc: 1
Apr 10 15:06:54.660: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e0f110 exit status 1 <nil> <nil> true [0xc00019c458 0xc00019c4c8 0xc00019c528] [0xc00019c458 0xc00019c4c8 0xc00019c528] [0xc00019c4a8 0xc00019c4f8] [0x9bf9f0 0x9bf9f0] 0xc0023bb440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:07:04.660: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:07:04.742: INFO: rc: 1
Apr 10 15:07:04.742: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0015b4030 exit status 1 <nil> <nil> true [0xc0009b2420 0xc0009b24e0 0xc0009b29f8] [0xc0009b2420 0xc0009b24e0 0xc0009b29f8] [0xc0009b24c8 0xc0009b2958] [0x9bf9f0 0x9bf9f0] 0xc002448060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:07:14.743: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:07:14.835: INFO: rc: 1
Apr 10 15:07:14.835: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0015b4720 exit status 1 <nil> <nil> true [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a2e0 0xc000a1a3c8] [0x9bf9f0 0x9bf9f0] 0xc002448a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:07:24.835: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:07:24.911: INFO: rc: 1
Apr 10 15:07:24.911: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0015b4de0 exit status 1 <nil> <nil> true [0xc000a1a448 0xc000a1a4f8 0xc000a1a740] [0xc000a1a448 0xc000a1a4f8 0xc000a1a740] [0xc000a1a490 0xc000a1a678] [0x9bf9f0 0x9bf9f0] 0xc002449500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:07:34.911: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:07:34.997: INFO: rc: 1
Apr 10 15:07:34.997: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c48a0 exit status 1 <nil> <nil> true [0xc000010690 0xc0000108c8 0xc0000109c0] [0xc000010690 0xc0000108c8 0xc0000109c0] [0xc000010888 0xc000010938] [0x9bf9f0 0x9bf9f0] 0xc00164dbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:07:44.998: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:07:45.072: INFO: rc: 1
Apr 10 15:07:45.073: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00321c6c0 exit status 1 <nil> <nil> true [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2078 0xc0009b21a8] [0x9bf9f0 0x9bf9f0] 0xc002d90c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:07:55.073: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:07:55.190: INFO: rc: 1
Apr 10 15:07:55.190: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e9e6f0 exit status 1 <nil> <nil> true [0xc00019c1b8 0xc00019c2d0 0xc00019c3d0] [0xc00019c1b8 0xc00019c2d0 0xc00019c3d0] [0xc00019c270 0xc00019c3c0] [0x9bf9f0 0x9bf9f0] 0xc001f80780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:08:05.191: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:08:05.282: INFO: rc: 1
Apr 10 15:08:05.282: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00321ce10 exit status 1 <nil> <nil> true [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2270 0xc0009b2368] [0x9bf9f0 0x9bf9f0] 0xc002d90f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:08:15.282: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:08:15.363: INFO: rc: 1
Apr 10 15:08:15.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c4f90 exit status 1 <nil> <nil> true [0xc000010a20 0xc000010b60 0xc000010c00] [0xc000010a20 0xc000010b60 0xc000010c00] [0xc000010b28 0xc000010b78] [0x9bf9f0 0x9bf9f0] 0xc0023ba960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:08:25.364: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:08:25.448: INFO: rc: 1
Apr 10 15:08:25.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e9f470 exit status 1 <nil> <nil> true [0xc00019c458 0xc00019c4c8 0xc00019c528] [0xc00019c458 0xc00019c4c8 0xc00019c528] [0xc00019c4a8 0xc00019c4f8] [0x9bf9f0 0x9bf9f0] 0xc001f81aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:08:35.449: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:08:35.548: INFO: rc: 1
Apr 10 15:08:35.548: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c5680 exit status 1 <nil> <nil> true [0xc000010c80 0xc000010d10 0xc000010fa8] [0xc000010c80 0xc000010d10 0xc000010fa8] [0xc000010cd8 0xc000010ed8] [0x9bf9f0 0x9bf9f0] 0xc0023bb440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:08:45.549: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:08:45.681: INFO: rc: 1
Apr 10 15:08:45.681: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00321d500 exit status 1 <nil> <nil> true [0xc0009b2a68 0xc0009b2b78 0xc0009b2c78] [0xc0009b2a68 0xc0009b2b78 0xc0009b2c78] [0xc0009b2b10 0xc0009b2c00] [0x9bf9f0 0x9bf9f0] 0xc002d91380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:08:55.681: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:08:55.755: INFO: rc: 1
Apr 10 15:08:55.755: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00321dbf0 exit status 1 <nil> <nil> true [0xc0009b2c80 0xc0009b2dd8 0xc0009b2e70] [0xc0009b2c80 0xc0009b2dd8 0xc0009b2e70] [0xc0009b2d88 0xc0009b2e58] [0x9bf9f0 0x9bf9f0] 0xc002d91680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:09:05.756: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:09:05.859: INFO: rc: 1
Apr 10 15:09:05.859: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e0e000 exit status 1 <nil> <nil> true [0xc000011000 0xc000011098 0xc000011140] [0xc000011000 0xc000011098 0xc000011140] [0xc000011060 0xc000011118] [0x9bf9f0 0x9bf9f0] 0xc0022aa000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:09:15.860: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:09:15.944: INFO: rc: 1
Apr 10 15:09:15.944: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e0e6f0 exit status 1 <nil> <nil> true [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2078 0xc0009b21a8] [0x9bf9f0 0x9bf9f0] 0xc002ab6180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:09:25.944: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:09:26.033: INFO: rc: 1
Apr 10 15:09:26.033: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e0f0e0 exit status 1 <nil> <nil> true [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2270 0xc0009b2368] [0x9bf9f0 0x9bf9f0] 0xc0022aaae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 10 15:09:36.033: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9512 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 15:09:36.130: INFO: rc: 1
Apr 10 15:09:36.130: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Apr 10 15:09:36.130: INFO: Scaling statefulset ss to 0
Apr 10 15:09:36.142: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 15:09:36.146: INFO: Deleting all statefulset in ns statefulset-9512
Apr 10 15:09:36.149: INFO: Scaling statefulset ss to 0
Apr 10 15:09:36.160: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 15:09:36.164: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:09:36.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9512" for this suite.
Apr 10 15:09:42.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:09:42.300: INFO: namespace statefulset-9512 deletion completed in 6.120861938s

• [SLOW TEST:363.093 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:09:42.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:09:42.520: INFO: Creating deployment "test-recreate-deployment"
Apr 10 15:09:42.525: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 10 15:09:42.532: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 10 15:09:44.541: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 10 15:09:44.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505782, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505782, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505782, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690505782, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 15:09:46.548: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 10 15:09:46.556: INFO: Updating deployment test-recreate-deployment
Apr 10 15:09:46.556: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 15:09:46.618: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7041,SelfLink:/apis/apps/v1/namespaces/deployment-7041/deployments/test-recreate-deployment,UID:ab1e27f9-5ba2-11e9-82f7-160dbbe32fdc,ResourceVersion:6862,Generation:2,CreationTimestamp:2019-04-10 15:09:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-10 15:09:46 +0000 UTC 2019-04-10 15:09:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-10 15:09:46 +0000 UTC 2019-04-10 15:09:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr 10 15:09:46.622: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-7041,SelfLink:/apis/apps/v1/namespaces/deployment-7041/replicasets/test-recreate-deployment-c9cbd8684,UID:ad88fa5f-5ba2-11e9-82f7-160dbbe32fdc,ResourceVersion:6861,Generation:1,CreationTimestamp:2019-04-10 15:09:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ab1e27f9-5ba2-11e9-82f7-160dbbe32fdc 0xc001389730 0xc001389731}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:09:46.622: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 10 15:09:46.622: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-7041,SelfLink:/apis/apps/v1/namespaces/deployment-7041/replicasets/test-recreate-deployment-7d57d5ff7c,UID:ab1ef7a0-5ba2-11e9-82f7-160dbbe32fdc,ResourceVersion:6853,Generation:2,CreationTimestamp:2019-04-10 15:09:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ab1e27f9-5ba2-11e9-82f7-160dbbe32fdc 0xc001389677 0xc001389678}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:09:46.626: INFO: Pod "test-recreate-deployment-c9cbd8684-src8m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-src8m,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-7041,SelfLink:/api/v1/namespaces/deployment-7041/pods/test-recreate-deployment-c9cbd8684-src8m,UID:ad897f06-5ba2-11e9-82f7-160dbbe32fdc,ResourceVersion:6859,Generation:0,CreationTimestamp:2019-04-10 15:09:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 ad88fa5f-5ba2-11e9-82f7-160dbbe32fdc 0xc001389f40 0xc001389f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-22v4q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-22v4q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-22v4q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001389fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001389fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:09:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:09:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:09:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:09:46 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-04-10 15:09:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:09:46.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7041" for this suite.
Apr 10 15:09:52.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:09:52.748: INFO: namespace deployment-7041 deletion completed in 6.118927905s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:09:52.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:09:52.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd" in namespace "projected-2798" to be "success or failure"
Apr 10 15:09:52.934: INFO: Pod "downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089281ms
Apr 10 15:09:54.939: INFO: Pod "downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008799959s
STEP: Saw pod success
Apr 10 15:09:54.939: INFO: Pod "downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:09:54.943: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:09:54.971: INFO: Waiting for pod downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:09:54.974: INFO: Pod downwardapi-volume-b1511772-5ba2-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:09:54.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2798" for this suite.
Apr 10 15:10:00.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:10:01.096: INFO: namespace projected-2798 deletion completed in 6.119011112s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:10:01.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:10:01.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd" in namespace "downward-api-4968" to be "success or failure"
Apr 10 15:10:01.331: INFO: Pod "downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568278ms
Apr 10 15:10:03.335: INFO: Pod "downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007944776s
Apr 10 15:10:05.340: INFO: Pod "downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013359364s
STEP: Saw pod success
Apr 10 15:10:05.340: INFO: Pod "downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:10:05.344: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:10:05.368: INFO: Waiting for pod downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:10:05.372: INFO: Pod downwardapi-volume-b6525eea-5ba2-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:10:05.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4968" for this suite.
Apr 10 15:10:11.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:10:11.501: INFO: namespace downward-api-4968 deletion completed in 6.12384326s
•SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:10:11.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:10:11.729: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd" in namespace "downward-api-3046" to be "success or failure"
Apr 10 15:10:11.735: INFO: Pod "downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.471606ms
Apr 10 15:10:13.740: INFO: Pod "downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01029029s
STEP: Saw pod success
Apr 10 15:10:13.740: INFO: Pod "downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:10:13.744: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:10:13.768: INFO: Waiting for pod downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:10:13.771: INFO: Pod downwardapi-volume-bc8579a8-5ba2-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:10:13.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3046" for this suite.
Apr 10 15:10:19.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:10:19.895: INFO: namespace downward-api-3046 deletion completed in 6.120043357s
•SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:10:19.895: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 15:10:20.132: INFO: Waiting up to 5m0s for pod "downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd" in namespace "downward-api-3823" to be "success or failure"
Apr 10 15:10:20.138: INFO: Pod "downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.5751ms
Apr 10 15:10:22.143: INFO: Pod "downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010393149s
STEP: Saw pod success
Apr 10 15:10:22.143: INFO: Pod "downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:10:22.147: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 15:10:22.169: INFO: Waiting for pod downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:10:22.172: INFO: Pod downward-api-c187e69a-5ba2-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:10:22.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3823" for this suite.
Apr 10 15:10:28.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:10:28.292: INFO: namespace downward-api-3823 deletion completed in 6.1167683s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:10:28.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6112.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6112.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 156.208.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.208.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.208.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.208.156_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6112.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6112.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 156.208.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.208.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.208.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.208.156_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 15:10:43.989: INFO: DNS probes using dns-6112/dns-test-c69a7a35-5ba2-11e9-ae72-2e2932f810fd succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:10:44.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6112" for this suite.
Apr 10 15:10:50.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:10:50.167: INFO: namespace dns-6112 deletion completed in 6.13192237s
•SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:10:50.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:10:50.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-871" for this suite.
Apr 10 15:10:56.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:10:56.571: INFO: namespace services-871 deletion completed in 6.155942411s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:10:56.571: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 10 15:11:02.779: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:02.782: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:04.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:04.787: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:06.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:06.787: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:08.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:08.787: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:10.792: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:10.797: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:12.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:12.787: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:14.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:14.787: INFO: Pod pod-with-poststart-http-hook still exists
Apr 10 15:11:16.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 10 15:11:16.788: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:11:16.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3514" for this suite.
Apr 10 15:11:40.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:11:40.915: INFO: namespace container-lifecycle-hook-3514 deletion completed in 24.122289699s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:11:40.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0410 15:12:21.159238    3187 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 15:12:21.159: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:12:21.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1597" for this suite.
Apr 10 15:12:27.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:12:27.323: INFO: namespace gc-1597 deletion completed in 6.160525014s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:12:27.324: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:12:27.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd" in namespace "downward-api-1040" to be "success or failure"
Apr 10 15:12:27.533: INFO: Pod "downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.333961ms
Apr 10 15:12:29.539: INFO: Pod "downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009599668s
STEP: Saw pod success
Apr 10 15:12:29.539: INFO: Pod "downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:12:29.543: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:12:29.563: INFO: Waiting for pod downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:12:29.565: INFO: Pod downwardapi-volume-0d771ae1-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:12:29.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1040" for this suite.
Apr 10 15:12:35.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:12:35.709: INFO: namespace downward-api-1040 deletion completed in 6.139554397s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:12:35.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Apr 10 15:12:36.021: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 10 15:12:36.021: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3825'
Apr 10 15:12:36.508: INFO: stderr: ""
Apr 10 15:12:36.508: INFO: stdout: "service/redis-slave created\n"
Apr 10 15:12:36.531: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 10 15:12:36.532: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3825'
Apr 10 15:12:36.799: INFO: stderr: ""
Apr 10 15:12:36.799: INFO: stdout: "service/redis-master created\n"
Apr 10 15:12:36.800: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 10 15:12:36.800: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3825'
Apr 10 15:12:37.047: INFO: stderr: ""
Apr 10 15:12:37.047: INFO: stdout: "service/frontend created\n"
Apr 10 15:12:37.048: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 10 15:12:37.048: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3825'
Apr 10 15:12:37.311: INFO: stderr: ""
Apr 10 15:12:37.311: INFO: stdout: "deployment.apps/frontend created\n"
Apr 10 15:12:37.312: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 10 15:12:37.312: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3825'
Apr 10 15:12:37.550: INFO: stderr: ""
Apr 10 15:12:37.550: INFO: stdout: "deployment.apps/redis-master created\n"
Apr 10 15:12:37.551: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 10 15:12:37.551: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3825'
Apr 10 15:12:37.804: INFO: stderr: ""
Apr 10 15:12:37.804: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr 10 15:12:37.804: INFO: Waiting for all frontend pods to be Running.
Apr 10 15:12:57.855: INFO: Waiting for frontend to serve content.
Apr 10 15:12:57.946: INFO: Trying to add a new entry to the guestbook.
Apr 10 15:12:58.070: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 10 15:12:58.125: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3825'
Apr 10 15:12:58.261: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:12:58.261: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 15:12:58.261: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3825'
Apr 10 15:12:58.387: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:12:58.387: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 15:12:58.387: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3825'
Apr 10 15:12:58.500: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:12:58.500: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 15:12:58.500: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3825'
Apr 10 15:12:58.604: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:12:58.604: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 15:12:58.604: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3825'
Apr 10 15:12:58.714: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:12:58.714: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 10 15:12:58.714: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3825'
Apr 10 15:12:58.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:12:58.827: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:12:58.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3825" for this suite.
Apr 10 15:13:38.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:13:38.950: INFO: namespace kubectl-3825 deletion completed in 40.118605712s
•SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:13:38.950: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0410 15:13:49.234757    3187 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 15:13:49.234: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:13:49.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2812" for this suite.
Apr 10 15:13:55.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:13:55.399: INFO: namespace gc-2812 deletion completed in 6.161155492s
•SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:13:55.399: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Apr 10 15:13:55.628: INFO: Waiting up to 5m0s for pod "client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd" in namespace "containers-6660" to be "success or failure"
Apr 10 15:13:55.632: INFO: Pod "client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.361997ms
Apr 10 15:13:57.636: INFO: Pod "client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007909285s
STEP: Saw pod success
Apr 10 15:13:57.636: INFO: Pod "client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:13:57.640: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:13:57.661: INFO: Waiting for pod client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:13:57.664: INFO: Pod client-containers-41f9e96b-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:13:57.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6660" for this suite.
Apr 10 15:14:03.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:14:03.784: INFO: namespace containers-6660 deletion completed in 6.115241222s
•SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:14:03.784: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1910
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:14:04.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd" in namespace "projected-1910" to be "success or failure"
Apr 10 15:14:04.029: INFO: Pod "downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.18655ms
Apr 10 15:14:06.034: INFO: Pod "downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00857965s
STEP: Saw pod success
Apr 10 15:14:06.034: INFO: Pod "downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:14:06.038: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:14:06.061: INFO: Waiting for pod downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:14:06.064: INFO: Pod downwardapi-volume-46fb577f-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:14:06.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1910" for this suite.
Apr 10 15:14:12.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:14:12.247: INFO: namespace projected-1910 deletion completed in 6.179139903s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:14:12.248: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd
Apr 10 15:14:12.517: INFO: Pod name my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd: Found 0 pods out of 1
Apr 10 15:14:17.522: INFO: Pod name my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd: Found 1 pods out of 1
Apr 10 15:14:17.522: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd" are running
Apr 10 15:14:17.526: INFO: Pod "my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd-ffpds" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 15:14:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 15:14:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 15:14:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 15:14:12 +0000 UTC Reason: Message:}])
Apr 10 15:14:17.526: INFO: Trying to dial the pod
Apr 10 15:14:22.599: INFO: Controller my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd: Got expected result from replica 1 [my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd-ffpds]: "my-hostname-basic-4c0aea36-5ba3-11e9-ae72-2e2932f810fd-ffpds", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:14:22.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9324" for this suite.
Apr 10 15:14:28.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:14:28.722: INFO: namespace replication-controller-9324 deletion completed in 6.118582996s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:14:28.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7216
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 10 15:14:28.931: INFO: Waiting up to 5m0s for pod "pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd" in namespace "emptydir-7216" to be "success or failure"
Apr 10 15:14:28.935: INFO: Pod "pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.16229ms
Apr 10 15:14:30.943: INFO: Pod "pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011419676s
Apr 10 15:14:32.948: INFO: Pod "pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01685223s
STEP: Saw pod success
Apr 10 15:14:32.948: INFO: Pod "pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:14:32.952: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:14:32.974: INFO: Waiting for pod pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:14:32.977: INFO: Pod pod-55d3424a-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:14:32.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7216" for this suite.
Apr 10 15:14:38.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:14:39.092: INFO: namespace emptydir-7216 deletion completed in 6.111486078s
•SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:14:39.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 15:14:39.410: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6011'
Apr 10 15:14:39.531: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 15:14:39.531: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Apr 10 15:14:41.539: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-6011'
Apr 10 15:14:41.655: INFO: stderr: ""
Apr 10 15:14:41.655: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:14:41.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6011" for this suite.
Apr 10 15:14:47.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:14:47.777: INFO: namespace kubectl-6011 deletion completed in 6.118168001s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:14:47.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 15:14:48.020: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3602'
Apr 10 15:14:48.154: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 15:14:48.155: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Apr 10 15:14:48.159: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-3602'
Apr 10 15:14:48.284: INFO: stderr: ""
Apr 10 15:14:48.284: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:14:48.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3602" for this suite.
Apr 10 15:15:12.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:15:12.435: INFO: namespace kubectl-3602 deletion completed in 24.147565974s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:15:12.435: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 10 15:15:12.724: INFO: Waiting up to 5m0s for pod "pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd" in namespace "emptydir-7956" to be "success or failure"
Apr 10 15:15:12.727: INFO: Pod "pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.454243ms
Apr 10 15:15:14.733: INFO: Pod "pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008641771s
STEP: Saw pod success
Apr 10 15:15:14.733: INFO: Pod "pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:15:14.737: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:15:14.756: INFO: Waiting for pod pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:15:14.759: INFO: Pod pod-6fedba9e-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:15:14.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7956" for this suite.
Apr 10 15:15:20.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:15:20.882: INFO: namespace emptydir-7956 deletion completed in 6.119830482s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:15:20.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:15:21.216: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd" in namespace "downward-api-4643" to be "success or failure"
Apr 10 15:15:21.222: INFO: Pod "downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.65317ms
Apr 10 15:15:23.227: INFO: Pod "downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011064863s
Apr 10 15:15:25.232: INFO: Pod "downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016157762s
STEP: Saw pod success
Apr 10 15:15:25.232: INFO: Pod "downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:15:25.237: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:15:25.262: INFO: Waiting for pod downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:15:25.265: INFO: Pod downwardapi-volume-74fda654-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:15:25.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4643" for this suite.
Apr 10 15:15:33.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:15:33.388: INFO: namespace downward-api-4643 deletion completed in 8.116933956s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:15:33.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-25
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 10 15:15:33.721: INFO: Waiting up to 5m0s for pod "pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd" in namespace "emptydir-25" to be "success or failure"
Apr 10 15:15:33.725: INFO: Pod "pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935888ms
Apr 10 15:15:35.730: INFO: Pod "pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008249804s
STEP: Saw pod success
Apr 10 15:15:35.730: INFO: Pod "pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:15:35.733: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:15:35.754: INFO: Waiting for pod pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:15:35.756: INFO: Pod pod-7c719cef-5ba3-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:15:35.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-25" for this suite.
Apr 10 15:15:41.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:15:41.879: INFO: namespace emptydir-25 deletion completed in 6.119736608s
•SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:15:41.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8155
Apr 10 15:15:44.145: INFO: Started pod liveness-http in namespace container-probe-8155
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 15:15:44.149: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:19:44.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8155" for this suite.
Apr 10 15:19:50.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:19:50.966: INFO: namespace container-probe-8155 deletion completed in 6.158134178s
•SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:19:50.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 15:19:53.753: INFO: Successfully updated pod "labelsupdate15eceabe-5ba4-11e9-ae72-2e2932f810fd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:19:57.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-844" for this suite.
Apr 10 15:20:21.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:20:21.967: INFO: namespace projected-844 deletion completed in 24.176977157s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:20:21.967: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-28592e58-5ba4-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:20:22.134: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd" in namespace "projected-534" to be "success or failure"
Apr 10 15:20:22.142: INFO: Pod "pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.919758ms
Apr 10 15:20:24.148: INFO: Pod "pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014036688s
STEP: Saw pod success
Apr 10 15:20:24.148: INFO: Pod "pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:20:24.156: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 15:20:24.384: INFO: Waiting for pod pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:20:24.391: INFO: Pod pod-projected-secrets-2859cd69-5ba4-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:20:24.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-534" for this suite.
Apr 10 15:20:30.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:20:30.520: INFO: namespace projected-534 deletion completed in 6.125050316s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:20:30.521: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-2d79d59a-5ba4-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:20:30.735: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd" in namespace "projected-7203" to be "success or failure"
Apr 10 15:20:30.739: INFO: Pod "pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415745ms
Apr 10 15:20:32.744: INFO: Pod "pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008587193s
STEP: Saw pod success
Apr 10 15:20:32.744: INFO: Pod "pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:20:32.748: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 15:20:32.771: INFO: Waiting for pod pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:20:32.774: INFO: Pod pod-projected-secrets-2d7a7783-5ba4-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:20:32.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7203" for this suite.
Apr 10 15:20:38.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:20:38.914: INFO: namespace projected-7203 deletion completed in 6.136372327s
•S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:20:38.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0410 15:21:09.751933    3187 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 15:21:09.751: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:21:09.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4425" for this suite.
Apr 10 15:21:17.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:21:18.201: INFO: namespace gc-4425 deletion completed in 8.446148281s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:21:18.202: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 15:21:18.454: INFO: Number of nodes with available pods: 0
Apr 10 15:21:18.455: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:21:19.463: INFO: Number of nodes with available pods: 0
Apr 10 15:21:19.463: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:21:20.464: INFO: Number of nodes with available pods: 2
Apr 10 15:21:20.464: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 10 15:21:20.486: INFO: Number of nodes with available pods: 1
Apr 10 15:21:20.486: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:21.495: INFO: Number of nodes with available pods: 1
Apr 10 15:21:21.495: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:22.496: INFO: Number of nodes with available pods: 1
Apr 10 15:21:22.496: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:23.495: INFO: Number of nodes with available pods: 1
Apr 10 15:21:23.495: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:24.495: INFO: Number of nodes with available pods: 1
Apr 10 15:21:24.495: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:25.494: INFO: Number of nodes with available pods: 1
Apr 10 15:21:25.494: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:26.494: INFO: Number of nodes with available pods: 1
Apr 10 15:21:26.494: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:27.495: INFO: Number of nodes with available pods: 1
Apr 10 15:21:27.495: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:21:28.495: INFO: Number of nodes with available pods: 2
Apr 10 15:21:28.495: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1594, will wait for the garbage collector to delete the pods
Apr 10 15:21:28.560: INFO: Deleting DaemonSet.extensions daemon-set took: 7.50762ms
Apr 10 15:21:28.660: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.403744ms
Apr 10 15:21:36.564: INFO: Number of nodes with available pods: 0
Apr 10 15:21:36.565: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 15:21:36.570: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1594/daemonsets","resourceVersion":"9195"},"items":null}

Apr 10 15:21:36.574: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1594/pods","resourceVersion":"9195"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:21:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1594" for this suite.
Apr 10 15:21:42.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:21:42.716: INFO: namespace daemonsets-1594 deletion completed in 6.124840307s
•SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:21:42.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:21:45.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3340" for this suite.
Apr 10 15:22:27.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:22:27.186: INFO: namespace kubelet-test-3340 deletion completed in 42.14188938s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:22:27.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Apr 10 15:22:27.748: INFO: Waiting up to 5m0s for pod "var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd" in namespace "var-expansion-5755" to be "success or failure"
Apr 10 15:22:27.752: INFO: Pod "var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.943026ms
Apr 10 15:22:29.757: INFO: Pod "var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009036939s
STEP: Saw pod success
Apr 10 15:22:29.757: INFO: Pod "var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:22:29.761: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 15:22:29.781: INFO: Waiting for pod var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:22:29.785: INFO: Pod var-expansion-7339191f-5ba4-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:22:29.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5755" for this suite.
Apr 10 15:22:37.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:22:37.913: INFO: namespace var-expansion-5755 deletion completed in 8.123489645s
•SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:22:37.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:22:38.228: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 15:22:38.239: INFO: Number of nodes with available pods: 0
Apr 10 15:22:38.239: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:22:39.248: INFO: Number of nodes with available pods: 1
Apr 10 15:22:39.248: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:22:40.249: INFO: Number of nodes with available pods: 2
Apr 10 15:22:40.249: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 10 15:22:40.277: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:40.277: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:41.287: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:41.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:42.288: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:42.288: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:43.289: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:43.289: INFO: Pod daemon-set-dgjkn is not available
Apr 10 15:22:43.289: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:44.287: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:44.287: INFO: Pod daemon-set-dgjkn is not available
Apr 10 15:22:44.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:45.287: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:45.287: INFO: Pod daemon-set-dgjkn is not available
Apr 10 15:22:45.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:46.287: INFO: Wrong image for pod: daemon-set-dgjkn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:46.287: INFO: Pod daemon-set-dgjkn is not available
Apr 10 15:22:46.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:47.287: INFO: Pod daemon-set-94bqc is not available
Apr 10 15:22:47.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:48.289: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:49.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:50.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:50.287: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:51.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:51.287: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:52.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:52.287: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:53.286: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:53.286: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:54.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:54.287: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:55.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:55.287: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:56.287: INFO: Wrong image for pod: daemon-set-tdgn2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 10 15:22:56.287: INFO: Pod daemon-set-tdgn2 is not available
Apr 10 15:22:57.287: INFO: Pod daemon-set-wvzs6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 10 15:22:57.298: INFO: Number of nodes with available pods: 1
Apr 10 15:22:57.298: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:22:58.307: INFO: Number of nodes with available pods: 1
Apr 10 15:22:58.307: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt is running more than one daemon pod
Apr 10 15:22:59.306: INFO: Number of nodes with available pods: 2
Apr 10 15:22:59.306: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7455, will wait for the garbage collector to delete the pods
Apr 10 15:22:59.385: INFO: Deleting DaemonSet.extensions daemon-set took: 7.668805ms
Apr 10 15:22:59.486: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.256013ms
Apr 10 15:23:02.690: INFO: Number of nodes with available pods: 0
Apr 10 15:23:02.690: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 15:23:02.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7455/daemonsets","resourceVersion":"9486"},"items":null}

Apr 10 15:23:02.696: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7455/pods","resourceVersion":"9486"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:23:02.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7455" for this suite.
Apr 10 15:23:08.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:23:08.871: INFO: namespace daemonsets-7455 deletion completed in 6.16044936s
•
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:23:08.871: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:23:09.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd" in namespace "downward-api-1798" to be "success or failure"
Apr 10 15:23:09.129: INFO: Pod "downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.526727ms
Apr 10 15:23:11.133: INFO: Pod "downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008104525s
STEP: Saw pod success
Apr 10 15:23:11.133: INFO: Pod "downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:23:11.137: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:23:11.162: INFO: Waiting for pod downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:23:11.165: INFO: Pod downwardapi-volume-8be2de7e-5ba4-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:23:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1798" for this suite.
Apr 10 15:23:17.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:23:17.299: INFO: namespace downward-api-1798 deletion completed in 6.129236121s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:23:17.300: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-7256
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7256 to expose endpoints map[]
Apr 10 15:23:17.538: INFO: Get endpoints failed (3.432714ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Apr 10 15:23:18.543: INFO: successfully validated that service endpoint-test2 in namespace services-7256 exposes endpoints map[] (1.008001205s elapsed)
STEP: Creating pod pod1 in namespace services-7256
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7256 to expose endpoints map[pod1:[80]]
Apr 10 15:23:20.573: INFO: successfully validated that service endpoint-test2 in namespace services-7256 exposes endpoints map[pod1:[80]] (2.02247931s elapsed)
STEP: Creating pod pod2 in namespace services-7256
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7256 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 10 15:23:22.614: INFO: successfully validated that service endpoint-test2 in namespace services-7256 exposes endpoints map[pod1:[80] pod2:[80]] (2.034694099s elapsed)
STEP: Deleting pod pod1 in namespace services-7256
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7256 to expose endpoints map[pod2:[80]]
Apr 10 15:23:22.627: INFO: successfully validated that service endpoint-test2 in namespace services-7256 exposes endpoints map[pod2:[80]] (6.888856ms elapsed)
STEP: Deleting pod pod2 in namespace services-7256
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7256 to expose endpoints map[]
Apr 10 15:23:22.637: INFO: successfully validated that service endpoint-test2 in namespace services-7256 exposes endpoints map[] (4.801599ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:23:22.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7256" for this suite.
Apr 10 15:23:46.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:23:46.823: INFO: namespace services-7256 deletion completed in 24.16693698s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:23:46.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Apr 10 15:23:47.016: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2447 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 10 15:23:49.825: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 10 15:23:49.825: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:23:51.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2447" for this suite.
Apr 10 15:23:57.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:23:57.954: INFO: namespace kubectl-2447 deletion completed in 6.115059436s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:23:57.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:23:58.130: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 10 15:24:03.135: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 15:24:03.135: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 15:24:03.157: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8179,SelfLink:/apis/apps/v1/namespaces/deployment-8179/deployments/test-cleanup-deployment,UID:ac165182-5ba4-11e9-82f7-160dbbe32fdc,ResourceVersion:9723,Generation:1,CreationTimestamp:2019-04-10 15:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Apr 10 15:24:03.161: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-8179,SelfLink:/apis/apps/v1/namespaces/deployment-8179/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:ac183f56-5ba4-11e9-82f7-160dbbe32fdc,ResourceVersion:9725,Generation:1,CreationTimestamp:2019-04-10 15:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment ac165182-5ba4-11e9-82f7-160dbbe32fdc 0xc001981a07 0xc001981a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:24:03.161: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 10 15:24:03.161: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8179,SelfLink:/apis/apps/v1/namespaces/deployment-8179/replicasets/test-cleanup-controller,UID:a9188b3a-5ba4-11e9-82f7-160dbbe32fdc,ResourceVersion:9724,Generation:1,CreationTimestamp:2019-04-10 15:23:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment ac165182-5ba4-11e9-82f7-160dbbe32fdc 0xc001981947 0xc001981948}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 15:24:03.165: INFO: Pod "test-cleanup-controller-46k64" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-46k64,GenerateName:test-cleanup-controller-,Namespace:deployment-8179,SelfLink:/api/v1/namespaces/deployment-8179/pods/test-cleanup-controller-46k64,UID:a919deb9-5ba4-11e9-82f7-160dbbe32fdc,ResourceVersion:9711,Generation:0,CreationTimestamp:2019-04-10 15:23:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.69/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller a9188b3a-5ba4-11e9-82f7-160dbbe32fdc 0xc00177059f 0xc0017705c0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c8tfd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c8tfd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c8tfd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001770680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017706a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:23:58 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:24:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:24:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:23:58 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.69,StartTime:2019-04-10 15:23:58 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:23:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d0b885d1332a91beb10d7aa5b918be44aa08135f0d54ec52d0429b15719773b2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:24:03.165: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-sjknl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-sjknl,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-8179,SelfLink:/api/v1/namespaces/deployment-8179/pods/test-cleanup-deployment-55cbfbc8f5-sjknl,UID:ac18cfc5-5ba4-11e9-82f7-160dbbe32fdc,ResourceVersion:9727,Generation:0,CreationTimestamp:2019-04-10 15:24:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 ac183f56-5ba4-11e9-82f7-160dbbe32fdc 0xc0017708a7 0xc0017708a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c8tfd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c8tfd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-c8tfd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001770980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017709a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:24:03.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8179" for this suite.
Apr 10 15:24:09.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:24:09.329: INFO: namespace deployment-8179 deletion completed in 6.160116324s
•S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:24:09.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-afe3b926-5ba4-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:24:09.534: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd" in namespace "projected-819" to be "success or failure"
Apr 10 15:24:09.537: INFO: Pod "pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407254ms
Apr 10 15:24:11.543: INFO: Pod "pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008956216s
STEP: Saw pod success
Apr 10 15:24:11.543: INFO: Pod "pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:24:11.547: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 15:24:11.569: INFO: Waiting for pod pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:24:11.572: INFO: Pod pod-projected-configmaps-afe46621-5ba4-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:24:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-819" for this suite.
Apr 10 15:24:17.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:24:17.701: INFO: namespace projected-819 deletion completed in 6.125447472s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:24:17.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:24:18.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd" in namespace "projected-5895" to be "success or failure"
Apr 10 15:24:18.020: INFO: Pod "downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562735ms
Apr 10 15:24:20.025: INFO: Pod "downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008721975s
STEP: Saw pod success
Apr 10 15:24:20.025: INFO: Pod "downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:24:20.029: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:24:20.052: INFO: Waiting for pod downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:24:20.054: INFO: Pod downwardapi-volume-b4f2bc11-5ba4-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:24:20.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5895" for this suite.
Apr 10 15:24:26.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:24:26.186: INFO: namespace projected-5895 deletion completed in 6.127426575s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:24:26.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2990
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2990
STEP: Creating statefulset with conflicting port in namespace statefulset-2990
STEP: Waiting until pod test-pod will start running in namespace statefulset-2990
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2990
Apr 10 15:24:30.454: INFO: Observed stateful pod in namespace: statefulset-2990, name: ss-0, uid: bc550373-5ba4-11e9-82f7-160dbbe32fdc, status phase: Failed. Waiting for statefulset controller to delete.
Apr 10 15:24:30.454: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2990
STEP: Removing pod with conflicting port in namespace statefulset-2990
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2990 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 15:24:32.471: INFO: Deleting all statefulset in ns statefulset-2990
Apr 10 15:24:32.475: INFO: Scaling statefulset ss to 0
Apr 10 15:24:52.497: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 15:24:52.500: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:24:52.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2990" for this suite.
Apr 10 15:24:58.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:24:58.673: INFO: namespace statefulset-2990 deletion completed in 6.158552204s
•SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:24:58.674: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7293
I0410 15:24:58.915223    3187 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7293, replica count: 1
I0410 15:24:59.965954    3187 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 10 15:25:00.076: INFO: Created: latency-svc-z5lrp
Apr 10 15:25:00.080: INFO: Got endpoints: latency-svc-z5lrp [14.548042ms]
Apr 10 15:25:00.097: INFO: Created: latency-svc-ctsgn
Apr 10 15:25:00.103: INFO: Got endpoints: latency-svc-ctsgn [22.396542ms]
Apr 10 15:25:00.103: INFO: Created: latency-svc-mp74v
Apr 10 15:25:00.109: INFO: Got endpoints: latency-svc-mp74v [28.034124ms]
Apr 10 15:25:00.109: INFO: Created: latency-svc-7mmxr
Apr 10 15:25:00.113: INFO: Got endpoints: latency-svc-7mmxr [32.883465ms]
Apr 10 15:25:00.114: INFO: Created: latency-svc-rdvxv
Apr 10 15:25:00.124: INFO: Created: latency-svc-lcxcb
Apr 10 15:25:00.124: INFO: Got endpoints: latency-svc-rdvxv [43.26956ms]
Apr 10 15:25:00.124: INFO: Created: latency-svc-fhgg5
Apr 10 15:25:00.124: INFO: Got endpoints: latency-svc-fhgg5 [43.487303ms]
Apr 10 15:25:00.129: INFO: Got endpoints: latency-svc-lcxcb [47.81515ms]
Apr 10 15:25:00.129: INFO: Created: latency-svc-mljt9
Apr 10 15:25:00.133: INFO: Got endpoints: latency-svc-mljt9 [52.575937ms]
Apr 10 15:25:00.134: INFO: Created: latency-svc-4csn2
Apr 10 15:25:00.139: INFO: Got endpoints: latency-svc-4csn2 [57.842887ms]
Apr 10 15:25:00.139: INFO: Created: latency-svc-tb6l4
Apr 10 15:25:00.144: INFO: Created: latency-svc-nx5kz
Apr 10 15:25:00.207: INFO: Got endpoints: latency-svc-nx5kz [126.30946ms]
Apr 10 15:25:00.207: INFO: Got endpoints: latency-svc-tb6l4 [126.351446ms]
Apr 10 15:25:00.207: INFO: Created: latency-svc-ph52k
Apr 10 15:25:00.210: INFO: Got endpoints: latency-svc-ph52k [128.803097ms]
Apr 10 15:25:00.215: INFO: Created: latency-svc-9x8v4
Apr 10 15:25:00.222: INFO: Created: latency-svc-zlvvk
Apr 10 15:25:00.222: INFO: Got endpoints: latency-svc-9x8v4 [141.04508ms]
Apr 10 15:25:00.225: INFO: Got endpoints: latency-svc-zlvvk [143.845266ms]
Apr 10 15:25:00.231: INFO: Created: latency-svc-sglkm
Apr 10 15:25:00.238: INFO: Created: latency-svc-6jz95
Apr 10 15:25:00.238: INFO: Got endpoints: latency-svc-sglkm [157.011499ms]
Apr 10 15:25:00.244: INFO: Got endpoints: latency-svc-6jz95 [163.026883ms]
Apr 10 15:25:00.244: INFO: Created: latency-svc-vnz7p
Apr 10 15:25:00.247: INFO: Got endpoints: latency-svc-vnz7p [144.309398ms]
Apr 10 15:25:00.252: INFO: Created: latency-svc-jk4gh
Apr 10 15:25:00.258: INFO: Got endpoints: latency-svc-jk4gh [148.9774ms]
Apr 10 15:25:00.258: INFO: Created: latency-svc-zzw2d
Apr 10 15:25:00.285: INFO: Got endpoints: latency-svc-zzw2d [171.833497ms]
Apr 10 15:25:00.298: INFO: Created: latency-svc-b6tl2
Apr 10 15:25:00.300: INFO: Got endpoints: latency-svc-b6tl2 [176.068935ms]
Apr 10 15:25:00.305: INFO: Created: latency-svc-gsg9l
Apr 10 15:25:00.342: INFO: Got endpoints: latency-svc-gsg9l [218.046506ms]
Apr 10 15:25:00.345: INFO: Created: latency-svc-mxmcc
Apr 10 15:25:00.351: INFO: Got endpoints: latency-svc-mxmcc [222.011281ms]
Apr 10 15:25:00.351: INFO: Created: latency-svc-h2qjw
Apr 10 15:25:00.357: INFO: Created: latency-svc-hs7n6
Apr 10 15:25:00.357: INFO: Got endpoints: latency-svc-h2qjw [224.1335ms]
Apr 10 15:25:00.364: INFO: Got endpoints: latency-svc-hs7n6 [225.431273ms]
Apr 10 15:25:00.364: INFO: Created: latency-svc-4v2r6
Apr 10 15:25:00.371: INFO: Got endpoints: latency-svc-4v2r6 [163.372583ms]
Apr 10 15:25:00.371: INFO: Created: latency-svc-582nz
Apr 10 15:25:00.378: INFO: Got endpoints: latency-svc-582nz [170.705779ms]
Apr 10 15:25:00.378: INFO: Created: latency-svc-hrgk9
Apr 10 15:25:00.381: INFO: Got endpoints: latency-svc-hrgk9 [171.525323ms]
Apr 10 15:25:00.487: INFO: Created: latency-svc-s2w5w
Apr 10 15:25:00.495: INFO: Got endpoints: latency-svc-s2w5w [272.836414ms]
Apr 10 15:25:00.586: INFO: Created: latency-svc-6cvkw
Apr 10 15:25:00.590: INFO: Got endpoints: latency-svc-6cvkw [365.332452ms]
Apr 10 15:25:00.595: INFO: Created: latency-svc-rkz5z
Apr 10 15:25:00.691: INFO: Got endpoints: latency-svc-rkz5z [452.98501ms]
Apr 10 15:25:00.692: INFO: Created: latency-svc-bb7cz
Apr 10 15:25:00.695: INFO: Got endpoints: latency-svc-bb7cz [451.339242ms]
Apr 10 15:25:00.700: INFO: Created: latency-svc-5k9vf
Apr 10 15:25:00.706: INFO: Got endpoints: latency-svc-5k9vf [458.288821ms]
Apr 10 15:25:00.784: INFO: Created: latency-svc-pcb5f
Apr 10 15:25:00.786: INFO: Got endpoints: latency-svc-pcb5f [527.882653ms]
Apr 10 15:25:00.792: INFO: Created: latency-svc-bslhr
Apr 10 15:25:00.795: INFO: Got endpoints: latency-svc-bslhr [509.521049ms]
Apr 10 15:25:00.804: INFO: Created: latency-svc-8cthm
Apr 10 15:25:00.807: INFO: Got endpoints: latency-svc-8cthm [506.811439ms]
Apr 10 15:25:00.813: INFO: Created: latency-svc-s2r2r
Apr 10 15:25:00.815: INFO: Got endpoints: latency-svc-s2r2r [472.965638ms]
Apr 10 15:25:00.820: INFO: Created: latency-svc-c2dk7
Apr 10 15:25:00.826: INFO: Got endpoints: latency-svc-c2dk7 [475.528201ms]
Apr 10 15:25:00.827: INFO: Created: latency-svc-ztbc6
Apr 10 15:25:00.829: INFO: Got endpoints: latency-svc-ztbc6 [471.932359ms]
Apr 10 15:25:00.835: INFO: Created: latency-svc-hlxsm
Apr 10 15:25:00.842: INFO: Created: latency-svc-qpq4g
Apr 10 15:25:00.842: INFO: Got endpoints: latency-svc-hlxsm [477.714978ms]
Apr 10 15:25:00.848: INFO: Got endpoints: latency-svc-qpq4g [477.16217ms]
Apr 10 15:25:00.848: INFO: Created: latency-svc-z4r4k
Apr 10 15:25:00.885: INFO: Got endpoints: latency-svc-z4r4k [507.555972ms]
Apr 10 15:25:00.890: INFO: Created: latency-svc-t4dnj
Apr 10 15:25:00.896: INFO: Got endpoints: latency-svc-t4dnj [515.357525ms]
Apr 10 15:25:00.897: INFO: Created: latency-svc-j5jwb
Apr 10 15:25:00.903: INFO: Created: latency-svc-njsjm
Apr 10 15:25:00.905: INFO: Got endpoints: latency-svc-j5jwb [410.078921ms]
Apr 10 15:25:00.906: INFO: Got endpoints: latency-svc-njsjm [315.818098ms]
Apr 10 15:25:00.939: INFO: Created: latency-svc-6rsfc
Apr 10 15:25:00.942: INFO: Got endpoints: latency-svc-6rsfc [250.787251ms]
Apr 10 15:25:00.947: INFO: Created: latency-svc-2sxw4
Apr 10 15:25:00.956: INFO: Created: latency-svc-7vx4q
Apr 10 15:25:00.963: INFO: Created: latency-svc-vg9m7
Apr 10 15:25:00.969: INFO: Created: latency-svc-cd4qn
Apr 10 15:25:00.985: INFO: Created: latency-svc-8vrf2
Apr 10 15:25:00.990: INFO: Created: latency-svc-jvjf7
Apr 10 15:25:00.996: INFO: Created: latency-svc-cgz5x
Apr 10 15:25:01.002: INFO: Created: latency-svc-9k5xp
Apr 10 15:25:01.003: INFO: Got endpoints: latency-svc-2sxw4 [307.415797ms]
Apr 10 15:25:01.003: INFO: Got endpoints: latency-svc-7vx4q [297.51009ms]
Apr 10 15:25:01.004: INFO: Got endpoints: latency-svc-vg9m7 [218.677339ms]
Apr 10 15:25:01.008: INFO: Created: latency-svc-jkfzd
Apr 10 15:25:01.037: INFO: Created: latency-svc-mtsmr
Apr 10 15:25:01.037: INFO: Got endpoints: latency-svc-cd4qn [241.671961ms]
Apr 10 15:25:01.043: INFO: Created: latency-svc-wx6sj
Apr 10 15:25:01.049: INFO: Created: latency-svc-kxd5k
Apr 10 15:25:01.055: INFO: Created: latency-svc-274pz
Apr 10 15:25:01.090: INFO: Created: latency-svc-fv2s8
Apr 10 15:25:01.091: INFO: Got endpoints: latency-svc-8vrf2 [284.41329ms]
Apr 10 15:25:01.098: INFO: Created: latency-svc-fgbqv
Apr 10 15:25:01.106: INFO: Created: latency-svc-jcpmw
Apr 10 15:25:01.116: INFO: Created: latency-svc-4kj7w
Apr 10 15:25:01.119: INFO: Created: latency-svc-hr9z5
Apr 10 15:25:01.126: INFO: Created: latency-svc-66gsz
Apr 10 15:25:01.132: INFO: Got endpoints: latency-svc-jvjf7 [316.948609ms]
Apr 10 15:25:01.133: INFO: Created: latency-svc-nx7ck
Apr 10 15:25:01.144: INFO: Created: latency-svc-2rcbf
Apr 10 15:25:01.181: INFO: Got endpoints: latency-svc-cgz5x [355.022837ms]
Apr 10 15:25:01.200: INFO: Created: latency-svc-8vqdf
Apr 10 15:25:01.232: INFO: Got endpoints: latency-svc-9k5xp [402.012388ms]
Apr 10 15:25:01.242: INFO: Created: latency-svc-299df
Apr 10 15:25:01.282: INFO: Got endpoints: latency-svc-jkfzd [434.411656ms]
Apr 10 15:25:01.294: INFO: Created: latency-svc-f4p45
Apr 10 15:25:01.331: INFO: Got endpoints: latency-svc-mtsmr [489.20794ms]
Apr 10 15:25:01.343: INFO: Created: latency-svc-bwwgl
Apr 10 15:25:01.381: INFO: Got endpoints: latency-svc-wx6sj [495.402845ms]
Apr 10 15:25:01.393: INFO: Created: latency-svc-bvftr
Apr 10 15:25:01.431: INFO: Got endpoints: latency-svc-kxd5k [534.050798ms]
Apr 10 15:25:01.447: INFO: Created: latency-svc-4p799
Apr 10 15:25:01.481: INFO: Got endpoints: latency-svc-274pz [575.992483ms]
Apr 10 15:25:01.492: INFO: Created: latency-svc-8v72r
Apr 10 15:25:01.531: INFO: Got endpoints: latency-svc-fv2s8 [625.218336ms]
Apr 10 15:25:01.543: INFO: Created: latency-svc-fwwc7
Apr 10 15:25:01.581: INFO: Got endpoints: latency-svc-fgbqv [639.086146ms]
Apr 10 15:25:01.595: INFO: Created: latency-svc-lmgjn
Apr 10 15:25:01.631: INFO: Got endpoints: latency-svc-jcpmw [628.260967ms]
Apr 10 15:25:01.642: INFO: Created: latency-svc-vxm97
Apr 10 15:25:01.681: INFO: Got endpoints: latency-svc-4kj7w [677.501333ms]
Apr 10 15:25:01.692: INFO: Created: latency-svc-j5zmh
Apr 10 15:25:01.731: INFO: Got endpoints: latency-svc-hr9z5 [726.686274ms]
Apr 10 15:25:01.742: INFO: Created: latency-svc-cpbzl
Apr 10 15:25:01.782: INFO: Got endpoints: latency-svc-66gsz [744.988498ms]
Apr 10 15:25:01.794: INFO: Created: latency-svc-qz8wx
Apr 10 15:25:01.835: INFO: Got endpoints: latency-svc-nx7ck [743.326748ms]
Apr 10 15:25:01.846: INFO: Created: latency-svc-242dh
Apr 10 15:25:01.881: INFO: Got endpoints: latency-svc-2rcbf [748.456158ms]
Apr 10 15:25:01.892: INFO: Created: latency-svc-426ts
Apr 10 15:25:01.931: INFO: Got endpoints: latency-svc-8vqdf [749.312093ms]
Apr 10 15:25:01.949: INFO: Created: latency-svc-rgjtg
Apr 10 15:25:01.981: INFO: Got endpoints: latency-svc-299df [749.910995ms]
Apr 10 15:25:01.993: INFO: Created: latency-svc-42gj6
Apr 10 15:25:02.037: INFO: Got endpoints: latency-svc-f4p45 [754.33309ms]
Apr 10 15:25:02.050: INFO: Created: latency-svc-wsb7p
Apr 10 15:25:02.082: INFO: Got endpoints: latency-svc-bwwgl [750.823699ms]
Apr 10 15:25:02.094: INFO: Created: latency-svc-hm4mh
Apr 10 15:25:02.131: INFO: Got endpoints: latency-svc-bvftr [750.003298ms]
Apr 10 15:25:02.155: INFO: Created: latency-svc-lfw8f
Apr 10 15:25:02.181: INFO: Got endpoints: latency-svc-4p799 [750.16334ms]
Apr 10 15:25:02.192: INFO: Created: latency-svc-xjjl5
Apr 10 15:25:02.231: INFO: Got endpoints: latency-svc-8v72r [750.311777ms]
Apr 10 15:25:02.243: INFO: Created: latency-svc-cd5qj
Apr 10 15:25:02.281: INFO: Got endpoints: latency-svc-fwwc7 [750.060124ms]
Apr 10 15:25:02.295: INFO: Created: latency-svc-dqzw8
Apr 10 15:25:02.336: INFO: Got endpoints: latency-svc-lmgjn [754.826943ms]
Apr 10 15:25:02.351: INFO: Created: latency-svc-g8sxx
Apr 10 15:25:02.381: INFO: Got endpoints: latency-svc-vxm97 [749.51628ms]
Apr 10 15:25:02.392: INFO: Created: latency-svc-6b5hf
Apr 10 15:25:02.431: INFO: Got endpoints: latency-svc-j5zmh [750.081665ms]
Apr 10 15:25:02.442: INFO: Created: latency-svc-gp9ft
Apr 10 15:25:02.481: INFO: Got endpoints: latency-svc-cpbzl [749.725527ms]
Apr 10 15:25:02.497: INFO: Created: latency-svc-hvqcg
Apr 10 15:25:02.531: INFO: Got endpoints: latency-svc-qz8wx [749.119901ms]
Apr 10 15:25:02.547: INFO: Created: latency-svc-wwfhd
Apr 10 15:25:02.584: INFO: Got endpoints: latency-svc-242dh [749.019111ms]
Apr 10 15:25:02.600: INFO: Created: latency-svc-ssccm
Apr 10 15:25:02.631: INFO: Got endpoints: latency-svc-426ts [750.801746ms]
Apr 10 15:25:02.645: INFO: Created: latency-svc-l2vbs
Apr 10 15:25:02.681: INFO: Got endpoints: latency-svc-rgjtg [750.135736ms]
Apr 10 15:25:02.699: INFO: Created: latency-svc-5wlrs
Apr 10 15:25:02.731: INFO: Got endpoints: latency-svc-42gj6 [749.860142ms]
Apr 10 15:25:02.743: INFO: Created: latency-svc-6g874
Apr 10 15:25:02.781: INFO: Got endpoints: latency-svc-wsb7p [744.063553ms]
Apr 10 15:25:02.797: INFO: Created: latency-svc-vcdjj
Apr 10 15:25:02.831: INFO: Got endpoints: latency-svc-hm4mh [748.996781ms]
Apr 10 15:25:02.842: INFO: Created: latency-svc-wwj9f
Apr 10 15:25:02.881: INFO: Got endpoints: latency-svc-lfw8f [750.36291ms]
Apr 10 15:25:02.894: INFO: Created: latency-svc-pdj4z
Apr 10 15:25:02.931: INFO: Got endpoints: latency-svc-xjjl5 [750.455652ms]
Apr 10 15:25:02.942: INFO: Created: latency-svc-pxgjp
Apr 10 15:25:02.980: INFO: Got endpoints: latency-svc-cd5qj [748.948605ms]
Apr 10 15:25:02.992: INFO: Created: latency-svc-pw648
Apr 10 15:25:03.031: INFO: Got endpoints: latency-svc-dqzw8 [749.407456ms]
Apr 10 15:25:03.048: INFO: Created: latency-svc-f4hsz
Apr 10 15:25:03.081: INFO: Got endpoints: latency-svc-g8sxx [744.986235ms]
Apr 10 15:25:03.095: INFO: Created: latency-svc-bbldt
Apr 10 15:25:03.131: INFO: Got endpoints: latency-svc-6b5hf [750.370002ms]
Apr 10 15:25:03.147: INFO: Created: latency-svc-74kzt
Apr 10 15:25:03.182: INFO: Got endpoints: latency-svc-gp9ft [750.536903ms]
Apr 10 15:25:03.197: INFO: Created: latency-svc-b7vsv
Apr 10 15:25:03.231: INFO: Got endpoints: latency-svc-hvqcg [750.002533ms]
Apr 10 15:25:03.244: INFO: Created: latency-svc-d7bxv
Apr 10 15:25:03.281: INFO: Got endpoints: latency-svc-wwfhd [749.872673ms]
Apr 10 15:25:03.292: INFO: Created: latency-svc-57vxm
Apr 10 15:25:03.331: INFO: Got endpoints: latency-svc-ssccm [746.960205ms]
Apr 10 15:25:03.343: INFO: Created: latency-svc-z8vss
Apr 10 15:25:03.382: INFO: Got endpoints: latency-svc-l2vbs [750.248312ms]
Apr 10 15:25:03.396: INFO: Created: latency-svc-zr7bc
Apr 10 15:25:03.431: INFO: Got endpoints: latency-svc-5wlrs [749.925099ms]
Apr 10 15:25:03.443: INFO: Created: latency-svc-plhrp
Apr 10 15:25:03.481: INFO: Got endpoints: latency-svc-6g874 [749.473421ms]
Apr 10 15:25:03.497: INFO: Created: latency-svc-67tvg
Apr 10 15:25:03.542: INFO: Got endpoints: latency-svc-vcdjj [761.663785ms]
Apr 10 15:25:03.554: INFO: Created: latency-svc-zmqqc
Apr 10 15:25:03.581: INFO: Got endpoints: latency-svc-wwj9f [749.860747ms]
Apr 10 15:25:03.593: INFO: Created: latency-svc-j65jp
Apr 10 15:25:03.631: INFO: Got endpoints: latency-svc-pdj4z [749.560733ms]
Apr 10 15:25:03.658: INFO: Created: latency-svc-695sw
Apr 10 15:25:03.681: INFO: Got endpoints: latency-svc-pxgjp [749.513815ms]
Apr 10 15:25:03.692: INFO: Created: latency-svc-f5wrw
Apr 10 15:25:03.731: INFO: Got endpoints: latency-svc-pw648 [750.325006ms]
Apr 10 15:25:03.742: INFO: Created: latency-svc-bxhq2
Apr 10 15:25:03.782: INFO: Got endpoints: latency-svc-f4hsz [750.688366ms]
Apr 10 15:25:03.794: INFO: Created: latency-svc-8r8nl
Apr 10 15:25:03.831: INFO: Got endpoints: latency-svc-bbldt [749.994232ms]
Apr 10 15:25:03.841: INFO: Created: latency-svc-85rxr
Apr 10 15:25:03.881: INFO: Got endpoints: latency-svc-74kzt [750.220196ms]
Apr 10 15:25:03.894: INFO: Created: latency-svc-698fm
Apr 10 15:25:03.931: INFO: Got endpoints: latency-svc-b7vsv [749.061744ms]
Apr 10 15:25:03.942: INFO: Created: latency-svc-prvbr
Apr 10 15:25:03.982: INFO: Got endpoints: latency-svc-d7bxv [751.161097ms]
Apr 10 15:25:03.995: INFO: Created: latency-svc-flpdk
Apr 10 15:25:04.031: INFO: Got endpoints: latency-svc-57vxm [749.962089ms]
Apr 10 15:25:04.041: INFO: Created: latency-svc-hptmw
Apr 10 15:25:04.081: INFO: Got endpoints: latency-svc-z8vss [750.19152ms]
Apr 10 15:25:04.093: INFO: Created: latency-svc-kgxrc
Apr 10 15:25:04.132: INFO: Got endpoints: latency-svc-zr7bc [750.478341ms]
Apr 10 15:25:04.154: INFO: Created: latency-svc-ks8fv
Apr 10 15:25:04.181: INFO: Got endpoints: latency-svc-plhrp [750.046625ms]
Apr 10 15:25:04.193: INFO: Created: latency-svc-8q9l8
Apr 10 15:25:04.232: INFO: Got endpoints: latency-svc-67tvg [750.573202ms]
Apr 10 15:25:04.243: INFO: Created: latency-svc-mjqpd
Apr 10 15:25:04.285: INFO: Got endpoints: latency-svc-zmqqc [742.631946ms]
Apr 10 15:25:04.299: INFO: Created: latency-svc-9jndx
Apr 10 15:25:04.331: INFO: Got endpoints: latency-svc-j65jp [750.074288ms]
Apr 10 15:25:04.344: INFO: Created: latency-svc-lmdlf
Apr 10 15:25:04.382: INFO: Got endpoints: latency-svc-695sw [750.479133ms]
Apr 10 15:25:04.394: INFO: Created: latency-svc-hv7qp
Apr 10 15:25:04.431: INFO: Got endpoints: latency-svc-f5wrw [750.006988ms]
Apr 10 15:25:04.444: INFO: Created: latency-svc-4wqvs
Apr 10 15:25:04.481: INFO: Got endpoints: latency-svc-bxhq2 [750.369359ms]
Apr 10 15:25:04.497: INFO: Created: latency-svc-8bnch
Apr 10 15:25:04.531: INFO: Got endpoints: latency-svc-8r8nl [749.845804ms]
Apr 10 15:25:04.543: INFO: Created: latency-svc-6f7qb
Apr 10 15:25:04.586: INFO: Got endpoints: latency-svc-85rxr [754.629326ms]
Apr 10 15:25:04.596: INFO: Created: latency-svc-tbj4x
Apr 10 15:25:04.631: INFO: Got endpoints: latency-svc-698fm [749.910765ms]
Apr 10 15:25:04.686: INFO: Created: latency-svc-kn892
Apr 10 15:25:04.687: INFO: Got endpoints: latency-svc-prvbr [755.776818ms]
Apr 10 15:25:04.699: INFO: Created: latency-svc-sxm7j
Apr 10 15:25:04.731: INFO: Got endpoints: latency-svc-flpdk [748.863111ms]
Apr 10 15:25:04.743: INFO: Created: latency-svc-8q4qb
Apr 10 15:25:04.786: INFO: Got endpoints: latency-svc-hptmw [755.336408ms]
Apr 10 15:25:04.799: INFO: Created: latency-svc-56dxt
Apr 10 15:25:04.831: INFO: Got endpoints: latency-svc-kgxrc [749.496535ms]
Apr 10 15:25:04.986: INFO: Got endpoints: latency-svc-ks8fv [853.630876ms]
Apr 10 15:25:04.986: INFO: Got endpoints: latency-svc-8q9l8 [805.366056ms]
Apr 10 15:25:04.990: INFO: Created: latency-svc-wg6cz
Apr 10 15:25:05.085: INFO: Got endpoints: latency-svc-mjqpd [853.827293ms]
Apr 10 15:25:05.087: INFO: Got endpoints: latency-svc-lmdlf [755.561033ms]
Apr 10 15:25:05.087: INFO: Created: latency-svc-55dqp
Apr 10 15:25:05.089: INFO: Got endpoints: latency-svc-9jndx [803.824688ms]
Apr 10 15:25:05.101: INFO: Created: latency-svc-9g56q
Apr 10 15:25:05.186: INFO: Got endpoints: latency-svc-hv7qp [804.602671ms]
Apr 10 15:25:05.188: INFO: Got endpoints: latency-svc-4wqvs [757.042937ms]
Apr 10 15:25:05.192: INFO: Created: latency-svc-c4pk4
Apr 10 15:25:05.284: INFO: Created: latency-svc-wb6st
Apr 10 15:25:05.286: INFO: Got endpoints: latency-svc-8bnch [804.433413ms]
Apr 10 15:25:05.287: INFO: Got endpoints: latency-svc-6f7qb [755.711943ms]
Apr 10 15:25:05.292: INFO: Created: latency-svc-6fb48
Apr 10 15:25:05.298: INFO: Created: latency-svc-nnz7w
Apr 10 15:25:05.304: INFO: Created: latency-svc-djzr4
Apr 10 15:25:05.310: INFO: Created: latency-svc-fgm45
Apr 10 15:25:05.316: INFO: Created: latency-svc-2wtkn
Apr 10 15:25:05.331: INFO: Got endpoints: latency-svc-tbj4x [745.366607ms]
Apr 10 15:25:05.344: INFO: Created: latency-svc-b5p4n
Apr 10 15:25:05.381: INFO: Got endpoints: latency-svc-kn892 [749.290212ms]
Apr 10 15:25:05.392: INFO: Created: latency-svc-8h4dc
Apr 10 15:25:05.431: INFO: Got endpoints: latency-svc-sxm7j [744.658907ms]
Apr 10 15:25:05.449: INFO: Created: latency-svc-h2kqp
Apr 10 15:25:05.481: INFO: Got endpoints: latency-svc-8q4qb [750.124744ms]
Apr 10 15:25:05.492: INFO: Created: latency-svc-9725m
Apr 10 15:25:05.531: INFO: Got endpoints: latency-svc-56dxt [744.482027ms]
Apr 10 15:25:05.543: INFO: Created: latency-svc-xk7jx
Apr 10 15:25:05.581: INFO: Got endpoints: latency-svc-wg6cz [750.373107ms]
Apr 10 15:25:05.592: INFO: Created: latency-svc-g2lbc
Apr 10 15:25:05.631: INFO: Got endpoints: latency-svc-55dqp [645.146571ms]
Apr 10 15:25:05.643: INFO: Created: latency-svc-7ccmt
Apr 10 15:25:05.680: INFO: Got endpoints: latency-svc-9g56q [694.033372ms]
Apr 10 15:25:05.691: INFO: Created: latency-svc-6fmgj
Apr 10 15:25:05.731: INFO: Got endpoints: latency-svc-c4pk4 [645.478681ms]
Apr 10 15:25:05.745: INFO: Created: latency-svc-87j4l
Apr 10 15:25:05.781: INFO: Got endpoints: latency-svc-wb6st [693.791439ms]
Apr 10 15:25:05.793: INFO: Created: latency-svc-5hjzt
Apr 10 15:25:05.831: INFO: Got endpoints: latency-svc-6fb48 [741.986696ms]
Apr 10 15:25:05.841: INFO: Created: latency-svc-whf9b
Apr 10 15:25:05.881: INFO: Got endpoints: latency-svc-nnz7w [694.425811ms]
Apr 10 15:25:05.893: INFO: Created: latency-svc-q2bft
Apr 10 15:25:05.931: INFO: Got endpoints: latency-svc-djzr4 [742.723293ms]
Apr 10 15:25:05.942: INFO: Created: latency-svc-xghfr
Apr 10 15:25:05.981: INFO: Got endpoints: latency-svc-fgm45 [695.507716ms]
Apr 10 15:25:05.992: INFO: Created: latency-svc-h59sn
Apr 10 15:25:06.032: INFO: Got endpoints: latency-svc-2wtkn [744.257369ms]
Apr 10 15:25:06.043: INFO: Created: latency-svc-gqzvl
Apr 10 15:25:06.085: INFO: Got endpoints: latency-svc-b5p4n [754.011938ms]
Apr 10 15:25:06.096: INFO: Created: latency-svc-9hgtq
Apr 10 15:25:06.131: INFO: Got endpoints: latency-svc-8h4dc [750.284613ms]
Apr 10 15:25:06.142: INFO: Created: latency-svc-ccxvv
Apr 10 15:25:06.180: INFO: Got endpoints: latency-svc-h2kqp [748.757008ms]
Apr 10 15:25:06.193: INFO: Created: latency-svc-h4kj9
Apr 10 15:25:06.231: INFO: Got endpoints: latency-svc-9725m [749.805818ms]
Apr 10 15:25:06.242: INFO: Created: latency-svc-zxwvd
Apr 10 15:25:06.282: INFO: Got endpoints: latency-svc-xk7jx [750.765869ms]
Apr 10 15:25:06.297: INFO: Created: latency-svc-7rc2m
Apr 10 15:25:06.331: INFO: Got endpoints: latency-svc-g2lbc [749.989114ms]
Apr 10 15:25:06.342: INFO: Created: latency-svc-mbt8h
Apr 10 15:25:06.381: INFO: Got endpoints: latency-svc-7ccmt [749.905372ms]
Apr 10 15:25:06.394: INFO: Created: latency-svc-bfspk
Apr 10 15:25:06.432: INFO: Got endpoints: latency-svc-6fmgj [751.43466ms]
Apr 10 15:25:06.444: INFO: Created: latency-svc-x86p5
Apr 10 15:25:06.481: INFO: Got endpoints: latency-svc-87j4l [749.795985ms]
Apr 10 15:25:06.493: INFO: Created: latency-svc-pcqmv
Apr 10 15:25:06.531: INFO: Got endpoints: latency-svc-5hjzt [750.613032ms]
Apr 10 15:25:06.543: INFO: Created: latency-svc-v9zt9
Apr 10 15:25:06.581: INFO: Got endpoints: latency-svc-whf9b [749.836278ms]
Apr 10 15:25:06.592: INFO: Created: latency-svc-6qsbw
Apr 10 15:25:06.631: INFO: Got endpoints: latency-svc-q2bft [750.401723ms]
Apr 10 15:25:06.643: INFO: Created: latency-svc-8jjpd
Apr 10 15:25:06.681: INFO: Got endpoints: latency-svc-xghfr [750.305305ms]
Apr 10 15:25:06.692: INFO: Created: latency-svc-wn75g
Apr 10 15:25:06.731: INFO: Got endpoints: latency-svc-h59sn [749.715915ms]
Apr 10 15:25:06.742: INFO: Created: latency-svc-gnz99
Apr 10 15:25:06.781: INFO: Got endpoints: latency-svc-gqzvl [749.884558ms]
Apr 10 15:25:06.793: INFO: Created: latency-svc-wtljq
Apr 10 15:25:06.831: INFO: Got endpoints: latency-svc-9hgtq [745.824432ms]
Apr 10 15:25:06.846: INFO: Created: latency-svc-9hfkw
Apr 10 15:25:06.881: INFO: Got endpoints: latency-svc-ccxvv [749.766477ms]
Apr 10 15:25:06.893: INFO: Created: latency-svc-xdb2g
Apr 10 15:25:06.932: INFO: Got endpoints: latency-svc-h4kj9 [751.527707ms]
Apr 10 15:25:06.947: INFO: Created: latency-svc-6n4m8
Apr 10 15:25:06.981: INFO: Got endpoints: latency-svc-zxwvd [750.105844ms]
Apr 10 15:25:06.994: INFO: Created: latency-svc-vxhtf
Apr 10 15:25:07.036: INFO: Got endpoints: latency-svc-7rc2m [754.128547ms]
Apr 10 15:25:07.047: INFO: Created: latency-svc-5d77s
Apr 10 15:25:07.081: INFO: Got endpoints: latency-svc-mbt8h [750.025796ms]
Apr 10 15:25:07.095: INFO: Created: latency-svc-bt9zn
Apr 10 15:25:07.132: INFO: Got endpoints: latency-svc-bfspk [750.222714ms]
Apr 10 15:25:07.145: INFO: Created: latency-svc-cblph
Apr 10 15:25:07.183: INFO: Got endpoints: latency-svc-x86p5 [750.75103ms]
Apr 10 15:25:07.199: INFO: Created: latency-svc-t66fq
Apr 10 15:25:07.231: INFO: Got endpoints: latency-svc-pcqmv [750.247118ms]
Apr 10 15:25:07.252: INFO: Created: latency-svc-ff6jp
Apr 10 15:25:07.281: INFO: Got endpoints: latency-svc-v9zt9 [749.589949ms]
Apr 10 15:25:07.293: INFO: Created: latency-svc-drwd6
Apr 10 15:25:07.331: INFO: Got endpoints: latency-svc-6qsbw [749.769571ms]
Apr 10 15:25:07.344: INFO: Created: latency-svc-9nsxb
Apr 10 15:25:07.381: INFO: Got endpoints: latency-svc-8jjpd [749.835678ms]
Apr 10 15:25:07.395: INFO: Created: latency-svc-2vsfv
Apr 10 15:25:07.431: INFO: Got endpoints: latency-svc-wn75g [749.795663ms]
Apr 10 15:25:07.443: INFO: Created: latency-svc-pkqll
Apr 10 15:25:07.481: INFO: Got endpoints: latency-svc-gnz99 [750.345661ms]
Apr 10 15:25:07.493: INFO: Created: latency-svc-v86hn
Apr 10 15:25:07.531: INFO: Got endpoints: latency-svc-wtljq [749.618712ms]
Apr 10 15:25:07.543: INFO: Created: latency-svc-s77pz
Apr 10 15:25:07.581: INFO: Got endpoints: latency-svc-9hfkw [750.007943ms]
Apr 10 15:25:07.592: INFO: Created: latency-svc-bpbzx
Apr 10 15:25:07.631: INFO: Got endpoints: latency-svc-xdb2g [750.367809ms]
Apr 10 15:25:07.644: INFO: Created: latency-svc-2pb4s
Apr 10 15:25:07.681: INFO: Got endpoints: latency-svc-6n4m8 [749.1042ms]
Apr 10 15:25:07.692: INFO: Created: latency-svc-fmtf4
Apr 10 15:25:07.731: INFO: Got endpoints: latency-svc-vxhtf [749.510064ms]
Apr 10 15:25:07.743: INFO: Created: latency-svc-2bzqg
Apr 10 15:25:07.786: INFO: Got endpoints: latency-svc-5d77s [749.884263ms]
Apr 10 15:25:07.797: INFO: Created: latency-svc-tzsmb
Apr 10 15:25:07.831: INFO: Got endpoints: latency-svc-bt9zn [749.642952ms]
Apr 10 15:25:07.842: INFO: Created: latency-svc-lzldn
Apr 10 15:25:07.881: INFO: Got endpoints: latency-svc-cblph [749.416017ms]
Apr 10 15:25:07.894: INFO: Created: latency-svc-wsp95
Apr 10 15:25:07.932: INFO: Got endpoints: latency-svc-t66fq [749.443188ms]
Apr 10 15:25:07.981: INFO: Got endpoints: latency-svc-ff6jp [749.7951ms]
Apr 10 15:25:08.032: INFO: Got endpoints: latency-svc-drwd6 [750.59222ms]
Apr 10 15:25:08.082: INFO: Got endpoints: latency-svc-9nsxb [750.847433ms]
Apr 10 15:25:08.131: INFO: Got endpoints: latency-svc-2vsfv [749.741364ms]
Apr 10 15:25:08.181: INFO: Got endpoints: latency-svc-pkqll [749.846227ms]
Apr 10 15:25:08.232: INFO: Got endpoints: latency-svc-v86hn [750.149914ms]
Apr 10 15:25:08.281: INFO: Got endpoints: latency-svc-s77pz [750.094352ms]
Apr 10 15:25:08.332: INFO: Got endpoints: latency-svc-bpbzx [750.713415ms]
Apr 10 15:25:08.381: INFO: Got endpoints: latency-svc-2pb4s [750.14739ms]
Apr 10 15:25:08.431: INFO: Got endpoints: latency-svc-fmtf4 [750.552624ms]
Apr 10 15:25:08.481: INFO: Got endpoints: latency-svc-2bzqg [750.149474ms]
Apr 10 15:25:08.533: INFO: Got endpoints: latency-svc-tzsmb [747.052247ms]
Apr 10 15:25:08.582: INFO: Got endpoints: latency-svc-lzldn [751.131669ms]
Apr 10 15:25:08.631: INFO: Got endpoints: latency-svc-wsp95 [750.256861ms]
Apr 10 15:25:08.632: INFO: Latencies: [22.396542ms 28.034124ms 32.883465ms 43.26956ms 43.487303ms 47.81515ms 52.575937ms 57.842887ms 126.30946ms 126.351446ms 128.803097ms 141.04508ms 143.845266ms 144.309398ms 148.9774ms 157.011499ms 163.026883ms 163.372583ms 170.705779ms 171.525323ms 171.833497ms 176.068935ms 218.046506ms 218.677339ms 222.011281ms 224.1335ms 225.431273ms 241.671961ms 250.787251ms 272.836414ms 284.41329ms 297.51009ms 307.415797ms 315.818098ms 316.948609ms 355.022837ms 365.332452ms 402.012388ms 410.078921ms 434.411656ms 451.339242ms 452.98501ms 458.288821ms 471.932359ms 472.965638ms 475.528201ms 477.16217ms 477.714978ms 489.20794ms 495.402845ms 506.811439ms 507.555972ms 509.521049ms 515.357525ms 527.882653ms 534.050798ms 575.992483ms 625.218336ms 628.260967ms 639.086146ms 645.146571ms 645.478681ms 677.501333ms 693.791439ms 694.033372ms 694.425811ms 695.507716ms 726.686274ms 741.986696ms 742.631946ms 742.723293ms 743.326748ms 744.063553ms 744.257369ms 744.482027ms 744.658907ms 744.986235ms 744.988498ms 745.366607ms 745.824432ms 746.960205ms 747.052247ms 748.456158ms 748.757008ms 748.863111ms 748.948605ms 748.996781ms 749.019111ms 749.061744ms 749.1042ms 749.119901ms 749.290212ms 749.312093ms 749.407456ms 749.416017ms 749.443188ms 749.473421ms 749.496535ms 749.510064ms 749.513815ms 749.51628ms 749.560733ms 749.589949ms 749.618712ms 749.642952ms 749.715915ms 749.725527ms 749.741364ms 749.766477ms 749.769571ms 749.7951ms 749.795663ms 749.795985ms 749.805818ms 749.835678ms 749.836278ms 749.845804ms 749.846227ms 749.860142ms 749.860747ms 749.872673ms 749.884263ms 749.884558ms 749.905372ms 749.910765ms 749.910995ms 749.925099ms 749.962089ms 749.989114ms 749.994232ms 750.002533ms 750.003298ms 750.006988ms 750.007943ms 750.025796ms 750.046625ms 750.060124ms 750.074288ms 750.081665ms 750.094352ms 750.105844ms 750.124744ms 750.135736ms 750.14739ms 750.149474ms 750.149914ms 750.16334ms 750.19152ms 750.220196ms 750.222714ms 750.247118ms 750.248312ms 750.256861ms 750.284613ms 750.305305ms 750.311777ms 750.325006ms 750.345661ms 750.36291ms 750.367809ms 750.369359ms 750.370002ms 750.373107ms 750.401723ms 750.455652ms 750.478341ms 750.479133ms 750.536903ms 750.552624ms 750.573202ms 750.59222ms 750.613032ms 750.688366ms 750.713415ms 750.75103ms 750.765869ms 750.801746ms 750.823699ms 750.847433ms 751.131669ms 751.161097ms 751.43466ms 751.527707ms 754.011938ms 754.128547ms 754.33309ms 754.629326ms 754.826943ms 755.336408ms 755.561033ms 755.711943ms 755.776818ms 757.042937ms 761.663785ms 803.824688ms 804.433413ms 804.602671ms 805.366056ms 853.630876ms 853.827293ms]
Apr 10 15:25:08.632: INFO: 50 %ile: 749.51628ms
Apr 10 15:25:08.632: INFO: 90 %ile: 751.161097ms
Apr 10 15:25:08.632: INFO: 99 %ile: 853.630876ms
Apr 10 15:25:08.632: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:25:08.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7293" for this suite.
Apr 10 15:25:22.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:25:22.800: INFO: namespace svc-latency-7293 deletion completed in 14.163134282s
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:25:22.800: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 10 15:25:23.022: INFO: PodSpec: initContainers in spec.initContainers
Apr 10 15:26:10.944: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dbb30f27-5ba4-11e9-ae72-2e2932f810fd", GenerateName:"", Namespace:"init-container-4045", SelfLink:"/api/v1/namespaces/init-container-4045/pods/pod-init-dbb30f27-5ba4-11e9-ae72-2e2932f810fd", UID:"dbb3e79d-5ba4-11e9-82f7-160dbbe32fdc", ResourceVersion:"11514", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690506723, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"22014740"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.0.59/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-nnlvb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002ef9700), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nnlvb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nnlvb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nnlvb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001363788), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002661a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001363810)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001363830)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001363838), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00136383c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690506723, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690506723, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690506723, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690506723, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.3", PodIP:"100.96.0.59", StartTime:(*v1.Time)(0xc002f900e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002011420)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002011490)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e5e3c9ad2dc628c8b792cf9e32492ec4650aca103b257a669da73ca7959360c9"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f90120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f90100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:26:10.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4045" for this suite.
Apr 10 15:26:34.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:26:35.074: INFO: namespace init-container-4045 deletion completed in 24.125574796s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:26:35.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Apr 10 15:26:35.453: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5933'
Apr 10 15:26:35.701: INFO: stderr: ""
Apr 10 15:26:35.701: INFO: stdout: "pod/pause created\n"
Apr 10 15:26:35.701: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 10 15:26:35.701: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5933" to be "running and ready"
Apr 10 15:26:35.705: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399426ms
Apr 10 15:26:37.710: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008386736s
Apr 10 15:26:37.710: INFO: Pod "pause" satisfied condition "running and ready"
Apr 10 15:26:37.710: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 10 15:26:37.710: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-5933'
Apr 10 15:26:37.821: INFO: stderr: ""
Apr 10 15:26:37.821: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 10 15:26:37.821: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-5933'
Apr 10 15:26:37.921: INFO: stderr: ""
Apr 10 15:26:37.921: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 10 15:26:37.921: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-5933'
Apr 10 15:26:38.037: INFO: stderr: ""
Apr 10 15:26:38.037: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 10 15:26:38.037: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-5933'
Apr 10 15:26:38.152: INFO: stderr: ""
Apr 10 15:26:38.153: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Apr 10 15:26:38.153: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5933'
Apr 10 15:26:38.268: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 15:26:38.268: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 10 15:26:38.268: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-5933'
Apr 10 15:26:38.383: INFO: stderr: "No resources found.\n"
Apr 10 15:26:38.383: INFO: stdout: ""
Apr 10 15:26:38.383: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-5933 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 15:26:38.484: INFO: stderr: ""
Apr 10 15:26:38.484: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:26:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5933" for this suite.
Apr 10 15:26:44.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:26:44.620: INFO: namespace kubectl-5933 deletion completed in 6.130935359s
•SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:26:44.620: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:26:44.821: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 10 15:26:44.829: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 10 15:26:49.834: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 15:26:49.834: INFO: Creating deployment "test-rolling-update-deployment"
Apr 10 15:26:49.839: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 10 15:26:49.847: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 10 15:26:51.855: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 10 15:26:51.859: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 15:26:51.869: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-8265,SelfLink:/apis/apps/v1/namespaces/deployment-8265/deployments/test-rolling-update-deployment,UID:0f71dc0e-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:11665,Generation:1,CreationTimestamp:2019-04-10 15:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 15:26:49 +0000 UTC 2019-04-10 15:26:49 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 15:26:51 +0000 UTC 2019-04-10 15:26:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 15:26:51.874: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-8265,SelfLink:/apis/apps/v1/namespaces/deployment-8265/replicasets/test-rolling-update-deployment-67599b4d9,UID:0f73b2e6-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:11658,Generation:1,CreationTimestamp:2019-04-10 15:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0f71dc0e-5ba5-11e9-82f7-160dbbe32fdc 0xc0026d0da0 0xc0026d0da1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 15:26:51.874: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 10 15:26:51.874: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-8265,SelfLink:/apis/apps/v1/namespaces/deployment-8265/replicasets/test-rolling-update-controller,UID:0c74e1bb-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:11664,Generation:2,CreationTimestamp:2019-04-10 15:26:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0f71dc0e-5ba5-11e9-82f7-160dbbe32fdc 0xc0026d0cc7 0xc0026d0cc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:26:51.878: INFO: Pod "test-rolling-update-deployment-67599b4d9-hp9ww" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-hp9ww,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-8265,SelfLink:/api/v1/namespaces/deployment-8265/pods/test-rolling-update-deployment-67599b4d9-hp9ww,UID:0f745f70-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:11657,Generation:0,CreationTimestamp:2019-04-10 15:26:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.75/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 0f73b2e6-5ba5-11e9-82f7-160dbbe32fdc 0xc002e17290 0xc002e17291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jnrtx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jnrtx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-jnrtx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e172f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e17310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:26:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:26:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:26:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:26:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.75,StartTime:2019-04-10 15:26:49 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 15:26:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://85e6b097077700c48fdc9962f2e2717e49f03b7c83f2d1ef99ee1075a844d924}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:26:51.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8265" for this suite.
Apr 10 15:26:57.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:26:58.009: INFO: namespace deployment-8265 deletion completed in 6.12681757s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:26:58.010: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-61
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-1470fd72-5ba5-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:26:58.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd" in namespace "projected-61" to be "success or failure"
Apr 10 15:26:58.235: INFO: Pod "pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.275727ms
Apr 10 15:27:00.240: INFO: Pod "pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007860472s
STEP: Saw pod success
Apr 10 15:27:00.240: INFO: Pod "pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:27:00.244: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 15:27:00.265: INFO: Waiting for pod pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:27:00.268: INFO: Pod pod-projected-configmaps-14719c5e-5ba5-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:27:00.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-61" for this suite.
Apr 10 15:27:06.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:27:06.442: INFO: namespace projected-61 deletion completed in 6.169954602s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:27:06.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2289
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9868
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:27:13.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6097" for this suite.
Apr 10 15:27:19.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:27:19.392: INFO: namespace namespaces-6097 deletion completed in 6.164328618s
STEP: Destroying namespace "nsdeletetest-2289" for this suite.
Apr 10 15:27:19.396: INFO: Namespace nsdeletetest-2289 was already deleted
STEP: Destroying namespace "nsdeletetest-9868" for this suite.
Apr 10 15:27:25.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:27:25.515: INFO: namespace nsdeletetest-9868 deletion completed in 6.119130072s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:27:25.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-907
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 10 15:27:25.741: INFO: Found 0 stateful pods, waiting for 3
Apr 10 15:27:35.748: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 15:27:35.748: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 15:27:35.748: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 10 15:27:35.780: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 10 15:27:45.816: INFO: Updating stateful set ss2
Apr 10 15:27:45.825: INFO: Waiting for Pod statefulset-907/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 15:27:55.835: INFO: Waiting for Pod statefulset-907/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr 10 15:28:05.856: INFO: Found 2 stateful pods, waiting for 3
Apr 10 15:28:15.863: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 15:28:15.863: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 15:28:15.863: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 10 15:28:15.892: INFO: Updating stateful set ss2
Apr 10 15:28:15.901: INFO: Waiting for Pod statefulset-907/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 15:28:25.911: INFO: Waiting for Pod statefulset-907/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 15:28:35.931: INFO: Updating stateful set ss2
Apr 10 15:28:35.940: INFO: Waiting for StatefulSet statefulset-907/ss2 to complete update
Apr 10 15:28:35.940: INFO: Waiting for Pod statefulset-907/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 15:28:45.950: INFO: Waiting for StatefulSet statefulset-907/ss2 to complete update
Apr 10 15:28:45.950: INFO: Waiting for Pod statefulset-907/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 15:28:55.950: INFO: Deleting all statefulset in ns statefulset-907
Apr 10 15:28:55.955: INFO: Scaling statefulset ss2 to 0
Apr 10 15:29:35.974: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 15:29:35.977: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:29:35.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-907" for this suite.
Apr 10 15:29:42.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:29:42.115: INFO: namespace statefulset-907 deletion completed in 6.120903846s
•SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:29:42.115: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-764172d7-5ba5-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:29:42.351: INFO: Waiting up to 5m0s for pod "pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd" in namespace "configmap-9083" to be "success or failure"
Apr 10 15:29:42.354: INFO: Pod "pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426406ms
Apr 10 15:29:44.359: INFO: Pod "pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008270926s
Apr 10 15:29:46.365: INFO: Pod "pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013776199s
STEP: Saw pod success
Apr 10 15:29:46.365: INFO: Pod "pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:29:46.369: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 15:29:46.395: INFO: Waiting for pod pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:29:46.399: INFO: Pod pod-configmaps-7642208e-5ba5-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:29:46.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9083" for this suite.
Apr 10 15:29:52.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:29:52.568: INFO: namespace configmap-9083 deletion completed in 6.164559568s
•
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:29:52.568: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:29:55.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4776" for this suite.
Apr 10 15:30:19.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:30:19.892: INFO: namespace replication-controller-4776 deletion completed in 24.126803575s
•SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:30:19.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4495.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4495.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 15:30:34.883: INFO: DNS probes using dns-4495/dns-test-8cd6f760-5ba5-11e9-ae72-2e2932f810fd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:30:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4495" for this suite.
Apr 10 15:30:40.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:30:41.064: INFO: namespace dns-4495 deletion completed in 6.164494962s
•SSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:30:41.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:30:41.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:30:43.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4363" for this suite.
Apr 10 15:31:23.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:31:23.422: INFO: namespace pods-4363 deletion completed in 40.136739343s
•S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:31:23.422: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:31:23.627: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 10 15:31:28.632: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 10 15:31:28.632: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 10 15:31:30.637: INFO: Creating deployment "test-rollover-deployment"
Apr 10 15:31:30.646: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 10 15:31:32.654: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 10 15:31:32.662: INFO: Ensure that both replica sets have 1 created replica
Apr 10 15:31:32.670: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 10 15:31:32.679: INFO: Updating deployment test-rollover-deployment
Apr 10 15:31:32.679: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 10 15:31:34.688: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 10 15:31:34.697: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 10 15:31:34.705: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 15:31:34.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507093, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 15:31:36.714: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 15:31:36.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507093, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 15:31:38.714: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 15:31:38.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507093, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 15:31:40.714: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 15:31:40.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507093, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 15:31:42.714: INFO: all replica sets need to contain the pod-template-hash label
Apr 10 15:31:42.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507093, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690507090, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 10 15:31:44.713: INFO: 
Apr 10 15:31:44.713: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 15:31:44.724: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5838,SelfLink:/apis/apps/v1/namespaces/deployment-5838/deployments/test-rollover-deployment,UID:b6d0f3ad-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:12676,Generation:2,CreationTimestamp:2019-04-10 15:31:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-10 15:31:30 +0000 UTC 2019-04-10 15:31:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-10 15:31:44 +0000 UTC 2019-04-10 15:31:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 10 15:31:44.728: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-5838,SelfLink:/apis/apps/v1/namespaces/deployment-5838/replicasets/test-rollover-deployment-766b4d6c9d,UID:b8089979-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:12669,Generation:2,CreationTimestamp:2019-04-10 15:31:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b6d0f3ad-5ba5-11e9-82f7-160dbbe32fdc 0xc001b16df7 0xc001b16df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 10 15:31:44.728: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 10 15:31:44.728: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5838,SelfLink:/apis/apps/v1/namespaces/deployment-5838/replicasets/test-rollover-controller,UID:b2a1eda4-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:12675,Generation:2,CreationTimestamp:2019-04-10 15:31:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b6d0f3ad-5ba5-11e9-82f7-160dbbe32fdc 0xc001b16b87 0xc001b16b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:31:44.728: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-5838,SelfLink:/apis/apps/v1/namespaces/deployment-5838/replicasets/test-rollover-deployment-6455657675,UID:b6d305a8-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:12634,Generation:2,CreationTimestamp:2019-04-10 15:31:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b6d0f3ad-5ba5-11e9-82f7-160dbbe32fdc 0xc001b16c67 0xc001b16c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:31:44.732: INFO: Pod "test-rollover-deployment-766b4d6c9d-p2prq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-p2prq,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-5838,SelfLink:/api/v1/namespaces/deployment-5838/pods/test-rollover-deployment-766b4d6c9d-p2prq,UID:b80bea91-5ba5-11e9-82f7-160dbbe32fdc,ResourceVersion:12645,Generation:0,CreationTimestamp:2019-04-10 15:31:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.85/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d b8089979-5ba5-11e9-82f7-160dbbe32fdc 0xc0016e60b7 0xc0016e60b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-v8bgs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v8bgs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-v8bgs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0016e6120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0016e6140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:31:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:31:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:31:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:31:32 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.85,StartTime:2019-04-10 15:31:32 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-10 15:31:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1284506a98ffe0a7848a5d6d20aa06037bc9d02af0adb67505dcf7b154832a55}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:31:44.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5838" for this suite.
Apr 10 15:31:50.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:31:50.861: INFO: namespace deployment-5838 deletion completed in 6.126180271s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:31:50.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1768
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 15:31:51.019: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 15:32:11.100: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1768 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:32:11.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:32:12.703: INFO: Found all expected endpoints: [netserver-0]
Apr 10 15:32:12.708: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1768 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:32:12.708: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:32:14.198: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:32:14.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1768" for this suite.
Apr 10 15:32:38.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:32:38.346: INFO: namespace pod-network-test-1768 deletion completed in 24.143168319s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:32:38.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8950
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 10 15:32:38.550: INFO: Number of nodes with available pods: 0
Apr 10 15:32:38.550: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:32:39.559: INFO: Number of nodes with available pods: 0
Apr 10 15:32:39.559: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:32:40.560: INFO: Number of nodes with available pods: 2
Apr 10 15:32:40.560: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 10 15:32:40.579: INFO: Number of nodes with available pods: 1
Apr 10 15:32:40.579: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:32:41.589: INFO: Number of nodes with available pods: 1
Apr 10 15:32:41.590: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:32:42.588: INFO: Number of nodes with available pods: 2
Apr 10 15:32:42.588: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8950, will wait for the garbage collector to delete the pods
Apr 10 15:32:42.656: INFO: Deleting DaemonSet.extensions daemon-set took: 7.085382ms
Apr 10 15:32:42.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.341447ms
Apr 10 15:32:56.661: INFO: Number of nodes with available pods: 0
Apr 10 15:32:56.661: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 15:32:56.665: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8950/daemonsets","resourceVersion":"12931"},"items":null}

Apr 10 15:32:56.668: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8950/pods","resourceVersion":"12931"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:32:56.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8950" for this suite.
Apr 10 15:33:02.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:33:02.840: INFO: namespace daemonsets-8950 deletion completed in 6.155507324s
•SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:33:02.840: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5423
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-ee11c40e-5ba5-11e9-ae72-2e2932f810fd
STEP: Creating secret with name s-test-opt-upd-ee11c46c-5ba5-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ee11c40e-5ba5-11e9-ae72-2e2932f810fd
STEP: Updating secret s-test-opt-upd-ee11c46c-5ba5-11e9-ae72-2e2932f810fd
STEP: Creating secret with name s-test-opt-create-ee11c48b-5ba5-11e9-ae72-2e2932f810fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:34:18.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5423" for this suite.
Apr 10 15:34:40.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:34:40.158: INFO: namespace secrets-5423 deletion completed in 22.138838798s
•SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:34:40.158: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:34:42.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4438" for this suite.
Apr 10 15:35:30.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:35:30.591: INFO: namespace kubelet-test-4438 deletion completed in 48.133109901s
•SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:35:30.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Apr 10 15:35:30.829: INFO: Waiting up to 5m0s for pod "client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd" in namespace "containers-5351" to be "success or failure"
Apr 10 15:35:30.832: INFO: Pod "client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119582ms
Apr 10 15:35:32.836: INFO: Pod "client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00785796s
STEP: Saw pod success
Apr 10 15:35:32.837: INFO: Pod "client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:35:32.840: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:35:32.860: INFO: Waiting for pod client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:35:32.864: INFO: Pod client-containers-45f9ccaa-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:35:32.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5351" for this suite.
Apr 10 15:35:40.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:35:41.036: INFO: namespace containers-5351 deletion completed in 8.167277899s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:35:41.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 10 15:35:41.235: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3942,SelfLink:/api/v1/namespaces/watch-3942/configmaps/e2e-watch-test-watch-closed,UID:4c2d5b3d-5ba6-11e9-82f7-160dbbe32fdc,ResourceVersion:13361,Generation:0,CreationTimestamp:2019-04-10 15:35:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 10 15:35:41.235: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3942,SelfLink:/api/v1/namespaces/watch-3942/configmaps/e2e-watch-test-watch-closed,UID:4c2d5b3d-5ba6-11e9-82f7-160dbbe32fdc,ResourceVersion:13363,Generation:0,CreationTimestamp:2019-04-10 15:35:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 10 15:35:41.252: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3942,SelfLink:/api/v1/namespaces/watch-3942/configmaps/e2e-watch-test-watch-closed,UID:4c2d5b3d-5ba6-11e9-82f7-160dbbe32fdc,ResourceVersion:13364,Generation:0,CreationTimestamp:2019-04-10 15:35:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 10 15:35:41.252: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3942,SelfLink:/api/v1/namespaces/watch-3942/configmaps/e2e-watch-test-watch-closed,UID:4c2d5b3d-5ba6-11e9-82f7-160dbbe32fdc,ResourceVersion:13365,Generation:0,CreationTimestamp:2019-04-10 15:35:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:35:41.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3942" for this suite.
Apr 10 15:35:47.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:35:47.379: INFO: namespace watch-3942 deletion completed in 6.122596909s
•SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:35:47.379: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-4ffd05ff-5ba6-11e9-ae72-2e2932f810fd
STEP: Creating secret with name secret-projected-all-test-volume-4ffd05e7-5ba6-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 10 15:35:47.635: INFO: Waiting up to 5m0s for pod "projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd" in namespace "projected-8075" to be "success or failure"
Apr 10 15:35:47.639: INFO: Pod "projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158912ms
Apr 10 15:35:49.644: INFO: Pod "projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008297493s
STEP: Saw pod success
Apr 10 15:35:49.644: INFO: Pod "projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:35:49.647: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 10 15:35:49.676: INFO: Waiting for pod projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:35:49.678: INFO: Pod projected-volume-4ffd05a9-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:35:49.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8075" for this suite.
Apr 10 15:35:55.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:35:55.858: INFO: namespace projected-8075 deletion completed in 6.174524295s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:35:55.858: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 10 15:35:56.029: INFO: Waiting up to 5m0s for pod "pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd" in namespace "emptydir-339" to be "success or failure"
Apr 10 15:35:56.032: INFO: Pod "pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80958ms
Apr 10 15:35:58.037: INFO: Pod "pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008646303s
STEP: Saw pod success
Apr 10 15:35:58.037: INFO: Pod "pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:35:58.042: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:35:58.065: INFO: Waiting for pod pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:35:58.068: INFO: Pod pod-54ff1eaf-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:35:58.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-339" for this suite.
Apr 10 15:36:04.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:36:04.209: INFO: namespace emptydir-339 deletion completed in 6.135269919s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:36:04.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-5a012e62-5ba6-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:36:04.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd" in namespace "configmap-9918" to be "success or failure"
Apr 10 15:36:04.441: INFO: Pod "pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583002ms
Apr 10 15:36:06.446: INFO: Pod "pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009100998s
STEP: Saw pod success
Apr 10 15:36:06.446: INFO: Pod "pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:36:06.450: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 15:36:06.474: INFO: Waiting for pod pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:36:06.477: INFO: Pod pod-configmaps-5a01e648-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:36:06.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9918" for this suite.
Apr 10 15:36:12.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:36:12.607: INFO: namespace configmap-9918 deletion completed in 6.125007376s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:36:12.607: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-5f028ac7-5ba6-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:36:12.833: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd" in namespace "projected-4433" to be "success or failure"
Apr 10 15:36:12.837: INFO: Pod "pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018824ms
Apr 10 15:36:14.842: INFO: Pod "pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008341128s
STEP: Saw pod success
Apr 10 15:36:14.842: INFO: Pod "pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:36:14.846: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 15:36:14.866: INFO: Waiting for pod pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:36:14.869: INFO: Pod pod-projected-secrets-5f032c81-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:36:14.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4433" for this suite.
Apr 10 15:36:20.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:36:20.995: INFO: namespace projected-4433 deletion completed in 6.122092134s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:36:20.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7960
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 10 15:36:21.234: INFO: Waiting up to 5m0s for pod "pod-640519b3-5ba6-11e9-ae72-2e2932f810fd" in namespace "emptydir-7960" to be "success or failure"
Apr 10 15:36:21.238: INFO: Pod "pod-640519b3-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.590009ms
Apr 10 15:36:23.243: INFO: Pod "pod-640519b3-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008622881s
STEP: Saw pod success
Apr 10 15:36:23.243: INFO: Pod "pod-640519b3-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:36:23.247: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-640519b3-5ba6-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:36:23.271: INFO: Waiting for pod pod-640519b3-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:36:23.274: INFO: Pod pod-640519b3-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:36:23.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7960" for this suite.
Apr 10 15:36:29.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:36:29.404: INFO: namespace emptydir-7960 deletion completed in 6.125941526s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:36:29.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-4998
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4998 to expose endpoints map[]
Apr 10 15:36:29.636: INFO: Get endpoints failed (5.826862ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Apr 10 15:36:30.640: INFO: successfully validated that service multi-endpoint-test in namespace services-4998 exposes endpoints map[] (1.010032059s elapsed)
STEP: Creating pod pod1 in namespace services-4998
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4998 to expose endpoints map[pod1:[100]]
Apr 10 15:36:32.672: INFO: successfully validated that service multi-endpoint-test in namespace services-4998 exposes endpoints map[pod1:[100]] (2.023475062s elapsed)
STEP: Creating pod pod2 in namespace services-4998
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4998 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 10 15:36:34.712: INFO: successfully validated that service multi-endpoint-test in namespace services-4998 exposes endpoints map[pod1:[100] pod2:[101]] (2.033581804s elapsed)
STEP: Deleting pod pod1 in namespace services-4998
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4998 to expose endpoints map[pod2:[101]]
Apr 10 15:36:35.732: INFO: successfully validated that service multi-endpoint-test in namespace services-4998 exposes endpoints map[pod2:[101]] (1.01419238s elapsed)
STEP: Deleting pod pod2 in namespace services-4998
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4998 to expose endpoints map[]
Apr 10 15:36:36.744: INFO: successfully validated that service multi-endpoint-test in namespace services-4998 exposes endpoints map[] (1.006448412s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:36:36.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4998" for this suite.
Apr 10 15:36:58.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:36:58.886: INFO: namespace services-4998 deletion completed in 22.123315389s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:36:58.887: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-8vpv
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 15:36:59.142: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8vpv" in namespace "subpath-4679" to be "success or failure"
Apr 10 15:36:59.149: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.179372ms
Apr 10 15:37:01.155: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012704881s
Apr 10 15:37:03.159: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 4.017113984s
Apr 10 15:37:05.164: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 6.022191862s
Apr 10 15:37:07.169: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 8.02674868s
Apr 10 15:37:09.174: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 10.031854456s
Apr 10 15:37:11.179: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 12.037261639s
Apr 10 15:37:13.184: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 14.042396176s
Apr 10 15:37:15.189: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 16.047277772s
Apr 10 15:37:17.194: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 18.052179543s
Apr 10 15:37:19.199: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Running", Reason="", readiness=true. Elapsed: 20.057212224s
Apr 10 15:37:21.204: INFO: Pod "pod-subpath-test-downwardapi-8vpv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061808414s
STEP: Saw pod success
Apr 10 15:37:21.204: INFO: Pod "pod-subpath-test-downwardapi-8vpv" satisfied condition "success or failure"
Apr 10 15:37:21.207: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-subpath-test-downwardapi-8vpv container test-container-subpath-downwardapi-8vpv: <nil>
STEP: delete the pod
Apr 10 15:37:21.230: INFO: Waiting for pod pod-subpath-test-downwardapi-8vpv to disappear
Apr 10 15:37:21.233: INFO: Pod pod-subpath-test-downwardapi-8vpv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8vpv
Apr 10 15:37:21.233: INFO: Deleting pod "pod-subpath-test-downwardapi-8vpv" in namespace "subpath-4679"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:37:21.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4679" for this suite.
Apr 10 15:37:27.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:37:27.410: INFO: namespace subpath-4679 deletion completed in 6.169438691s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:37:27.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7963
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-8b986cd9-5ba6-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:37:27.636: INFO: Waiting up to 5m0s for pod "pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd" in namespace "secrets-7963" to be "success or failure"
Apr 10 15:37:27.640: INFO: Pod "pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639984ms
Apr 10 15:37:29.644: INFO: Pod "pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008141753s
STEP: Saw pod success
Apr 10 15:37:29.644: INFO: Pod "pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:37:29.648: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 15:37:29.668: INFO: Waiting for pod pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:37:29.671: INFO: Pod pod-secrets-8b992516-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:37:29.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7963" for this suite.
Apr 10 15:37:35.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:37:35.835: INFO: namespace secrets-7963 deletion completed in 6.159968606s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:37:35.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:37:36.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd" in namespace "projected-2478" to be "success or failure"
Apr 10 15:37:36.028: INFO: Pod "downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.458638ms
Apr 10 15:37:38.032: INFO: Pod "downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007621951s
STEP: Saw pod success
Apr 10 15:37:38.032: INFO: Pod "downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:37:38.037: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:37:38.064: INFO: Waiting for pod downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:37:38.067: INFO: Pod downwardapi-volume-90990bed-5ba6-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:37:38.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2478" for this suite.
Apr 10 15:37:44.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:37:44.234: INFO: namespace projected-2478 deletion completed in 6.162816428s
•SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:37:44.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-gcpj
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 15:37:44.437: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gcpj" in namespace "subpath-5163" to be "success or failure"
Apr 10 15:37:44.440: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.163597ms
Apr 10 15:37:46.446: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 2.008922492s
Apr 10 15:37:48.450: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 4.013622647s
Apr 10 15:37:50.455: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 6.018347142s
Apr 10 15:37:52.460: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 8.023594914s
Apr 10 15:37:54.466: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 10.028814071s
Apr 10 15:37:56.470: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 12.033160022s
Apr 10 15:37:58.474: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 14.037393337s
Apr 10 15:38:00.479: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 16.04183793s
Apr 10 15:38:02.483: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 18.046246502s
Apr 10 15:38:04.487: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Running", Reason="", readiness=true. Elapsed: 20.050572908s
Apr 10 15:38:06.492: INFO: Pod "pod-subpath-test-configmap-gcpj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.055418845s
STEP: Saw pod success
Apr 10 15:38:06.492: INFO: Pod "pod-subpath-test-configmap-gcpj" satisfied condition "success or failure"
Apr 10 15:38:06.496: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-subpath-test-configmap-gcpj container test-container-subpath-configmap-gcpj: <nil>
STEP: delete the pod
Apr 10 15:38:06.518: INFO: Waiting for pod pod-subpath-test-configmap-gcpj to disappear
Apr 10 15:38:06.521: INFO: Pod pod-subpath-test-configmap-gcpj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gcpj
Apr 10 15:38:06.521: INFO: Deleting pod "pod-subpath-test-configmap-gcpj" in namespace "subpath-5163"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:38:06.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5163" for this suite.
Apr 10 15:38:12.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:38:12.676: INFO: namespace subpath-5163 deletion completed in 6.14811834s
•SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:38:12.676: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5488.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5488.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5488.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5488.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5488.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 10 15:38:15.892: INFO: DNS probes using dns-5488/dns-test-a6d526fa-5ba6-11e9-ae72-2e2932f810fd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:38:15.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5488" for this suite.
Apr 10 15:38:21.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:38:22.026: INFO: namespace dns-5488 deletion completed in 6.118843609s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:38:22.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:38:42.239: INFO: Container started at 2019-04-10 15:38:23 +0000 UTC, pod became ready at 2019-04-10 15:38:40 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:38:42.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9066" for this suite.
Apr 10 15:39:04.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:39:04.360: INFO: namespace container-probe-9066 deletion completed in 22.117003985s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:39:04.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 10 15:39:10.662: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:10.666: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:12.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:12.670: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:14.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:14.670: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:16.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:16.673: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:18.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:18.671: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:20.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:20.671: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:22.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:22.671: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:24.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:24.670: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:26.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:26.670: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:28.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:28.670: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:30.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:30.671: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:32.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:32.671: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:34.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:34.684: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 10 15:39:36.666: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 10 15:39:36.670: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:39:36.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8721" for this suite.
Apr 10 15:39:58.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:39:58.806: INFO: namespace container-lifecycle-hook-8721 deletion completed in 22.131392514s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:39:58.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:39:59.040: INFO: Create a RollingUpdate DaemonSet
Apr 10 15:39:59.045: INFO: Check that daemon pods launch on every node of the cluster
Apr 10 15:39:59.052: INFO: Number of nodes with available pods: 0
Apr 10 15:39:59.052: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:40:00.062: INFO: Number of nodes with available pods: 1
Apr 10 15:40:00.062: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:40:01.062: INFO: Number of nodes with available pods: 2
Apr 10 15:40:01.062: INFO: Number of running nodes: 2, number of available pods: 2
Apr 10 15:40:01.062: INFO: Update the DaemonSet to trigger a rollout
Apr 10 15:40:01.071: INFO: Updating DaemonSet daemon-set
Apr 10 15:40:07.085: INFO: Roll back the DaemonSet before rollout is complete
Apr 10 15:40:07.093: INFO: Updating DaemonSet daemon-set
Apr 10 15:40:07.093: INFO: Make sure DaemonSet rollback is complete
Apr 10 15:40:07.097: INFO: Wrong image for pod: daemon-set-tscbp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 15:40:07.097: INFO: Pod daemon-set-tscbp is not available
Apr 10 15:40:08.107: INFO: Wrong image for pod: daemon-set-tscbp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 15:40:08.107: INFO: Pod daemon-set-tscbp is not available
Apr 10 15:40:09.107: INFO: Wrong image for pod: daemon-set-tscbp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 10 15:40:09.107: INFO: Pod daemon-set-tscbp is not available
Apr 10 15:40:10.106: INFO: Pod daemon-set-27zz2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1712, will wait for the garbage collector to delete the pods
Apr 10 15:40:10.182: INFO: Deleting DaemonSet.extensions daemon-set took: 8.506047ms
Apr 10 15:40:10.582: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.370964ms
Apr 10 15:40:16.587: INFO: Number of nodes with available pods: 0
Apr 10 15:40:16.587: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 15:40:16.590: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1712/daemonsets","resourceVersion":"14317"},"items":null}

Apr 10 15:40:16.593: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1712/pods","resourceVersion":"14317"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:40:16.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1712" for this suite.
Apr 10 15:40:24.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:40:24.731: INFO: namespace daemonsets-1712 deletion completed in 8.119897011s
•
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:40:24.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 10 15:40:25.120: INFO: Pod name wrapped-volume-race-f5615a9c-5ba6-11e9-ae72-2e2932f810fd: Found 0 pods out of 5
Apr 10 15:40:30.130: INFO: Pod name wrapped-volume-race-f5615a9c-5ba6-11e9-ae72-2e2932f810fd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f5615a9c-5ba6-11e9-ae72-2e2932f810fd in namespace emptydir-wrapper-7702, will wait for the garbage collector to delete the pods
Apr 10 15:40:30.220: INFO: Deleting ReplicationController wrapped-volume-race-f5615a9c-5ba6-11e9-ae72-2e2932f810fd took: 9.810747ms
Apr 10 15:40:30.621: INFO: Terminating ReplicationController wrapped-volume-race-f5615a9c-5ba6-11e9-ae72-2e2932f810fd pods took: 400.278098ms
STEP: Creating RC which spawns configmap-volume pods
Apr 10 15:41:07.037: INFO: Pod name wrapped-volume-race-0e5def11-5ba7-11e9-ae72-2e2932f810fd: Found 0 pods out of 5
Apr 10 15:41:12.045: INFO: Pod name wrapped-volume-race-0e5def11-5ba7-11e9-ae72-2e2932f810fd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0e5def11-5ba7-11e9-ae72-2e2932f810fd in namespace emptydir-wrapper-7702, will wait for the garbage collector to delete the pods
Apr 10 15:41:12.142: INFO: Deleting ReplicationController wrapped-volume-race-0e5def11-5ba7-11e9-ae72-2e2932f810fd took: 15.474292ms
Apr 10 15:41:12.242: INFO: Terminating ReplicationController wrapped-volume-race-0e5def11-5ba7-11e9-ae72-2e2932f810fd pods took: 100.41874ms
STEP: Creating RC which spawns configmap-volume pods
Apr 10 15:41:56.660: INFO: Pod name wrapped-volume-race-2bf194c6-5ba7-11e9-ae72-2e2932f810fd: Found 0 pods out of 5
Apr 10 15:42:01.667: INFO: Pod name wrapped-volume-race-2bf194c6-5ba7-11e9-ae72-2e2932f810fd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2bf194c6-5ba7-11e9-ae72-2e2932f810fd in namespace emptydir-wrapper-7702, will wait for the garbage collector to delete the pods
Apr 10 15:42:01.754: INFO: Deleting ReplicationController wrapped-volume-race-2bf194c6-5ba7-11e9-ae72-2e2932f810fd took: 8.590326ms
Apr 10 15:42:02.255: INFO: Terminating ReplicationController wrapped-volume-race-2bf194c6-5ba7-11e9-ae72-2e2932f810fd pods took: 500.45752ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:42:47.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7702" for this suite.
Apr 10 15:42:55.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:42:55.206: INFO: namespace emptydir-wrapper-7702 deletion completed in 8.127755635s
•SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:42:55.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Apr 10 15:42:55.947: INFO: created pod pod-service-account-defaultsa
Apr 10 15:42:55.947: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 10 15:42:55.953: INFO: created pod pod-service-account-mountsa
Apr 10 15:42:55.953: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 10 15:42:55.960: INFO: created pod pod-service-account-nomountsa
Apr 10 15:42:55.960: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 10 15:42:55.964: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 10 15:42:55.964: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 10 15:42:55.970: INFO: created pod pod-service-account-mountsa-mountspec
Apr 10 15:42:55.970: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 10 15:42:55.976: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 10 15:42:55.976: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 10 15:42:55.987: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 10 15:42:55.987: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 10 15:42:55.991: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 10 15:42:55.991: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 10 15:42:55.996: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 10 15:42:55.996: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:42:55.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3098" for this suite.
Apr 10 15:43:04.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:43:04.121: INFO: namespace svcaccounts-3098 deletion completed in 8.12112264s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:43:04.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5596
Apr 10 15:43:06.342: INFO: Started pod liveness-exec in namespace container-probe-5596
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 15:43:06.346: INFO: Initial restart count of pod liveness-exec is 0
Apr 10 15:43:56.478: INFO: Restart count of pod container-probe-5596/liveness-exec is now 1 (50.132122249s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:43:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5596" for this suite.
Apr 10 15:44:04.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:44:04.809: INFO: namespace container-probe-5596 deletion completed in 8.315348811s
•SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:44:04.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 10 15:44:05.025: INFO: Waiting up to 5m0s for pod "pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd" in namespace "emptydir-5065" to be "success or failure"
Apr 10 15:44:05.028: INFO: Pod "pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063225ms
Apr 10 15:44:07.032: INFO: Pod "pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007853162s
STEP: Saw pod success
Apr 10 15:44:07.033: INFO: Pod "pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:44:07.036: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:44:07.057: INFO: Waiting for pod pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:44:07.060: INFO: Pod pod-7875e9b8-5ba7-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:44:07.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5065" for this suite.
Apr 10 15:44:13.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:44:13.199: INFO: namespace emptydir-5065 deletion completed in 6.135312058s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:44:13.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-242/configmap-test-7d78d9e8-5ba7-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 15:44:13.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd" in namespace "configmap-242" to be "success or failure"
Apr 10 15:44:13.440: INFO: Pod "pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587801ms
Apr 10 15:44:15.446: INFO: Pod "pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008952405s
STEP: Saw pod success
Apr 10 15:44:15.446: INFO: Pod "pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:44:15.450: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd container env-test: <nil>
STEP: delete the pod
Apr 10 15:44:15.475: INFO: Waiting for pod pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:44:15.478: INFO: Pod pod-configmaps-7d79871b-5ba7-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:44:15.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-242" for this suite.
Apr 10 15:44:21.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:44:21.609: INFO: namespace configmap-242 deletion completed in 6.127077576s
•S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:44:21.609: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 15:44:24.453: INFO: Successfully updated pod "labelsupdate828774c3-5ba7-11e9-ae72-2e2932f810fd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:44:28.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7711" for this suite.
Apr 10 15:44:50.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:44:50.623: INFO: namespace downward-api-7711 deletion completed in 22.129387655s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:44:50.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:44:50.932: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 10 15:44:50.942: INFO: Number of nodes with available pods: 0
Apr 10 15:44:50.942: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 10 15:44:50.959: INFO: Number of nodes with available pods: 0
Apr 10 15:44:50.959: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:51.965: INFO: Number of nodes with available pods: 0
Apr 10 15:44:51.965: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:52.964: INFO: Number of nodes with available pods: 1
Apr 10 15:44:52.964: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 10 15:44:52.982: INFO: Number of nodes with available pods: 1
Apr 10 15:44:52.982: INFO: Number of running nodes: 0, number of available pods: 1
Apr 10 15:44:53.986: INFO: Number of nodes with available pods: 0
Apr 10 15:44:53.986: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 10 15:44:53.994: INFO: Number of nodes with available pods: 0
Apr 10 15:44:53.994: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:54.999: INFO: Number of nodes with available pods: 0
Apr 10 15:44:54.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:55.998: INFO: Number of nodes with available pods: 0
Apr 10 15:44:55.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:56.999: INFO: Number of nodes with available pods: 0
Apr 10 15:44:56.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:57.999: INFO: Number of nodes with available pods: 0
Apr 10 15:44:57.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:58.998: INFO: Number of nodes with available pods: 0
Apr 10 15:44:58.998: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:44:59.999: INFO: Number of nodes with available pods: 0
Apr 10 15:44:59.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:00.999: INFO: Number of nodes with available pods: 0
Apr 10 15:45:00.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:01.998: INFO: Number of nodes with available pods: 0
Apr 10 15:45:01.998: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:02.998: INFO: Number of nodes with available pods: 0
Apr 10 15:45:02.998: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:03.999: INFO: Number of nodes with available pods: 0
Apr 10 15:45:03.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:04.999: INFO: Number of nodes with available pods: 0
Apr 10 15:45:04.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:05.999: INFO: Number of nodes with available pods: 0
Apr 10 15:45:05.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:06.999: INFO: Number of nodes with available pods: 0
Apr 10 15:45:06.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:07.999: INFO: Number of nodes with available pods: 0
Apr 10 15:45:07.999: INFO: Node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l is running more than one daemon pod
Apr 10 15:45:08.999: INFO: Number of nodes with available pods: 1
Apr 10 15:45:08.999: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9362, will wait for the garbage collector to delete the pods
Apr 10 15:45:09.069: INFO: Deleting DaemonSet.extensions daemon-set took: 8.596367ms
Apr 10 15:45:09.469: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.383972ms
Apr 10 15:45:16.573: INFO: Number of nodes with available pods: 0
Apr 10 15:45:16.573: INFO: Number of running nodes: 0, number of available pods: 0
Apr 10 15:45:16.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9362/daemonsets","resourceVersion":"15455"},"items":null}

Apr 10 15:45:16.580: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9362/pods","resourceVersion":"15455"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:45:16.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9362" for this suite.
Apr 10 15:45:22.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:45:22.744: INFO: namespace daemonsets-9362 deletion completed in 6.140718953s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:45:22.745: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-92
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:45:22.940: INFO: (0) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 11.522092ms)
Apr 10 15:45:22.984: INFO: (1) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 43.921392ms)
Apr 10 15:45:22.992: INFO: (2) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.710924ms)
Apr 10 15:45:22.999: INFO: (3) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.851377ms)
Apr 10 15:45:23.007: INFO: (4) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.482706ms)
Apr 10 15:45:23.014: INFO: (5) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.979725ms)
Apr 10 15:45:23.021: INFO: (6) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.484321ms)
Apr 10 15:45:23.028: INFO: (7) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.604455ms)
Apr 10 15:45:23.035: INFO: (8) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.049805ms)
Apr 10 15:45:23.041: INFO: (9) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.024106ms)
Apr 10 15:45:23.048: INFO: (10) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.792734ms)
Apr 10 15:45:23.055: INFO: (11) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.74275ms)
Apr 10 15:45:23.061: INFO: (12) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.695411ms)
Apr 10 15:45:23.068: INFO: (13) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.733837ms)
Apr 10 15:45:23.075: INFO: (14) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.771829ms)
Apr 10 15:45:23.082: INFO: (15) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.214834ms)
Apr 10 15:45:23.089: INFO: (16) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.166792ms)
Apr 10 15:45:23.096: INFO: (17) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.056798ms)
Apr 10 15:45:23.102: INFO: (18) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.096657ms)
Apr 10 15:45:23.109: INFO: (19) /api/v1/nodes/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.065991ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:45:23.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-92" for this suite.
Apr 10 15:45:29.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:45:29.294: INFO: namespace proxy-92 deletion completed in 6.179495705s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:45:29.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:45:29.522: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version --client'
Apr 10 15:45:29.581: INFO: stderr: ""
Apr 10 15:45:29.581: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr 10 15:45:29.603: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6651'
Apr 10 15:45:30.006: INFO: stderr: ""
Apr 10 15:45:30.006: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 10 15:45:30.006: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6651'
Apr 10 15:45:30.184: INFO: stderr: ""
Apr 10 15:45:30.184: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 15:45:31.190: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:45:31.190: INFO: Found 0 / 1
Apr 10 15:45:32.190: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:45:32.190: INFO: Found 1 / 1
Apr 10 15:45:32.190: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 15:45:32.194: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:45:32.194: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 15:45:32.194: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-f279s --namespace=kubectl-6651'
Apr 10 15:45:32.309: INFO: stderr: ""
Apr 10 15:45:32.309: INFO: stdout: "Name:               redis-master-f279s\nNamespace:          kubectl-6651\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l/10.250.0.2\nStart Time:         Wed, 10 Apr 2019 15:45:29 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.123/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.123\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://0306f6466c35b1326c5f3a1eb0aa7429d8a21787780af907e922bf2291fadf56\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 10 Apr 2019 15:45:30 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tbxx2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tbxx2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tbxx2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                        Message\n  ----    ------     ----  ----                                                        -------\n  Normal  Scheduled  2s    default-scheduler                                           Successfully assigned kubectl-6651/redis-master-f279s to shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l\n  Normal  Pulled     2s    kubelet, shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Created container redis-master\n  Normal  Started    2s    kubelet, shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l  Started container redis-master\n"
Apr 10 15:45:32.309: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-6651'
Apr 10 15:45:32.410: INFO: stderr: ""
Apr 10 15:45:32.410: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6651\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-f279s\n"
Apr 10 15:45:32.411: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-6651'
Apr 10 15:45:32.506: INFO: stderr: ""
Apr 10 15:45:32.506: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6651\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.65.213.183\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.123:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 10 15:45:32.512: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l'
Apr 10 15:45:32.618: INFO: stderr: ""
Apr 10 15:45:32.618: INFO: stdout: "Name:               shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west1\n                    failure-domain.beta.kubernetes.io/zone=europe-west1-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.2/32\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 10 Apr 2019 14:34:06 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 10 Apr 2019 14:34:24 +0000   Wed, 10 Apr 2019 14:34:24 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Wed, 10 Apr 2019 15:45:24 +0000   Wed, 10 Apr 2019 14:34:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 10 Apr 2019 15:45:24 +0000   Wed, 10 Apr 2019 14:34:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 10 Apr 2019 15:45:24 +0000   Wed, 10 Apr 2019 14:34:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 10 Apr 2019 15:45:24 +0000   Wed, 10 Apr 2019 14:34:26 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.0.2\n  ExternalIP:   104.155.78.179\n  InternalDNS:  shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l.c.sap-se-gcp-scp-k8s-dev.internal\n  Hostname:     shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l.c.sap-se-gcp-scp-k8s-dev.internal\nCapacity:\n attachable-volumes-gce-pd:  64\n cpu:                        4\n ephemeral-storage:          17897500Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     15395148Ki\n pods:                       110\nAllocatable:\n attachable-volumes-gce-pd:  64\n cpu:                        3920m\n ephemeral-storage:          17410687987\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     13297996Ki\n pods:                       110\nSystem Info:\n Machine ID:                 a3d6c854cd9d2602eb529262c8c8039d\n System UUID:                a3d6c854-cd9d-2602-eb52-9262c8c8039d\n Boot ID:                    9a19fadb-6f70-415b-b9bc-edcbc7ae48f1\n Kernel Version:             4.19.25-coreos\n OS Image:                   Container Linux by CoreOS 2023.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     100.96.1.0/24\nProviderID:                  gce://sap-se-gcp-scp-k8s-dev/europe-west1-b/shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-gkzbt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         72m\n  kube-system                blackbox-exporter-6dc58dcffc-6rds9                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      72m\n  kube-system                calico-node-4cqp8                                                  100m (2%)     500m (12%)  100Mi (0%)       700Mi (5%)     71m\n  kube-system                coredns-7f7f7978c8-sb9zb                                           50m (1%)      100m (2%)   15Mi (0%)        100Mi (0%)     72m\n  kube-system                kube-proxy-w7thh                                                   20m (0%)      0 (0%)      64Mi (0%)        0 (0%)         71m\n  kube-system                metrics-server-84f8f5f44f-5nx55                                    20m (0%)      80m (2%)    100Mi (0%)       400Mi (3%)     72m\n  kube-system                node-exporter-dsgq7                                                5m (0%)       15m (0%)    10Mi (0%)        100Mi (0%)     71m\n  kubectl-6651               redis-master-f279s                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        200m (5%)   705m (17%)\n  memory                     294Mi (2%)  1335Mi (10%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-gce-pd  0           0\nEvents:                      <none>\n"
Apr 10 15:45:32.618: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-6651'
Apr 10 15:45:32.712: INFO: stderr: ""
Apr 10 15:45:32.712: INFO: stdout: "Name:         kubectl-6651\nLabels:       e2e-framework=kubectl\n              e2e-run=a6a52bde-5b9e-11e9-ae72-2e2932f810fd\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:45:32.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6651" for this suite.
Apr 10 15:45:56.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:45:56.883: INFO: namespace kubectl-6651 deletion completed in 24.165603254s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:45:56.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 10 15:45:59.295: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-bb46fe80-5ba7-11e9-ae72-2e2932f810fd,GenerateName:,Namespace:events-527,SelfLink:/api/v1/namespaces/events-527/pods/send-events-bb46fe80-5ba7-11e9-ae72-2e2932f810fd,UID:bb47b2f3-5ba7-11e9-82f7-160dbbe32fdc,ResourceVersion:15607,Generation:0,CreationTimestamp:2019-04-10 15:45:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 116895598,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.88/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvjrv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvjrv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-qvjrv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001771350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001771380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:45:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:45:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:45:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:45:57 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.88,StartTime:2019-04-10 15:45:57 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-10 15:45:58 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://5e676a6ab201d0358af17774d40c91677d57c3a7b0b861835ac617e1b5dae60e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr 10 15:46:01.300: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 10 15:46:03.305: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:46:03.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-527" for this suite.
Apr 10 15:46:43.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:46:43.518: INFO: namespace events-527 deletion completed in 40.203696451s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:46:43.518: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Apr 10 15:46:43.731: INFO: Waiting up to 5m0s for pod "client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd" in namespace "containers-8381" to be "success or failure"
Apr 10 15:46:43.734: INFO: Pod "client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531419ms
Apr 10 15:46:45.739: INFO: Pod "client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008527822s
Apr 10 15:46:47.744: INFO: Pod "client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013342027s
STEP: Saw pod success
Apr 10 15:46:47.744: INFO: Pod "client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:46:47.748: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:46:47.769: INFO: Waiting for pod client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:46:47.772: INFO: Pod client-containers-d70e740f-5ba7-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:46:47.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8381" for this suite.
Apr 10 15:46:53.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:46:53.900: INFO: namespace containers-8381 deletion completed in 6.124095235s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:46:53.901: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6674
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 15:46:54.120: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 15:47:18.203: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.90:8080/dial?request=hostName&protocol=http&host=100.96.1.125&port=8080&tries=1'] Namespace:pod-network-test-6674 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:47:18.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:47:18.654: INFO: Waiting for endpoints: map[]
Apr 10 15:47:18.658: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.90:8080/dial?request=hostName&protocol=http&host=100.96.0.89&port=8080&tries=1'] Namespace:pod-network-test-6674 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:47:18.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:47:19.061: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:47:19.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6674" for this suite.
Apr 10 15:47:41.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:47:41.200: INFO: namespace pod-network-test-6674 deletion completed in 22.134884864s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:47:41.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Apr 10 15:47:41.518: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Apr 10 15:47:41.610: INFO: stderr: ""
Apr 10 15:47:41.610: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm-f85cf.it.internal.dev.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm-f85cf.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm-f85cf.it.internal.dev.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:47:41.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2669" for this suite.
Apr 10 15:47:47.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:47:47.743: INFO: namespace kubectl-2669 deletion completed in 6.12756803s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:47:47.743: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tw92s in namespace proxy-4124
I0410 15:47:47.940340    3187 runners.go:184] Created replication controller with name: proxy-service-tw92s, namespace: proxy-4124, replica count: 1
I0410 15:47:48.991000    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 15:47:49.991201    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0410 15:47:50.991489    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:51.991819    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:52.992208    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:53.992480    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:54.992846    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:55.993246    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:56.993514    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:57.993828    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0410 15:47:58.994170    3187 runners.go:184] proxy-service-tw92s Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 10 15:47:58.998: INFO: setup took 11.074352162s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 48.914572ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 48.858687ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 49.119896ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 48.859745ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 48.896865ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 49.108723ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 49.572142ms)
Apr 10 15:47:59.047: INFO: (0) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 49.640557ms)
Apr 10 15:47:59.051: INFO: (0) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 53.228748ms)
Apr 10 15:47:59.051: INFO: (0) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 53.265906ms)
Apr 10 15:47:59.051: INFO: (0) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 53.416658ms)
Apr 10 15:47:59.054: INFO: (0) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 55.800618ms)
Apr 10 15:47:59.056: INFO: (0) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 57.647289ms)
Apr 10 15:47:59.056: INFO: (0) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 58.357287ms)
Apr 10 15:47:59.070: INFO: (0) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 72.406154ms)
Apr 10 15:47:59.070: INFO: (0) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 72.492523ms)
Apr 10 15:47:59.076: INFO: (1) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.794916ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.911347ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.923579ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.742672ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 6.728426ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.816348ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.732496ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.856195ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.80703ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 7.007421ms)
Apr 10 15:47:59.077: INFO: (1) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.783067ms)
Apr 10 15:47:59.078: INFO: (1) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.197811ms)
Apr 10 15:47:59.078: INFO: (1) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.290065ms)
Apr 10 15:47:59.078: INFO: (1) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.335435ms)
Apr 10 15:47:59.078: INFO: (1) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.610463ms)
Apr 10 15:47:59.078: INFO: (1) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.684161ms)
Apr 10 15:47:59.087: INFO: (2) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 8.595296ms)
Apr 10 15:47:59.087: INFO: (2) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 8.558433ms)
Apr 10 15:47:59.087: INFO: (2) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 8.731119ms)
Apr 10 15:47:59.087: INFO: (2) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 8.769321ms)
Apr 10 15:47:59.087: INFO: (2) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 8.672446ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 8.975541ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 9.060773ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 9.000795ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 9.088023ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 9.580494ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 9.912186ms)
Apr 10 15:47:59.088: INFO: (2) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 9.877143ms)
Apr 10 15:47:59.089: INFO: (2) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 10.249396ms)
Apr 10 15:47:59.089: INFO: (2) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 10.212682ms)
Apr 10 15:47:59.089: INFO: (2) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 10.210352ms)
Apr 10 15:47:59.089: INFO: (2) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 10.376394ms)
Apr 10 15:47:59.095: INFO: (3) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 5.561024ms)
Apr 10 15:47:59.095: INFO: (3) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.599064ms)
Apr 10 15:47:59.095: INFO: (3) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.698812ms)
Apr 10 15:47:59.095: INFO: (3) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 5.825775ms)
Apr 10 15:47:59.095: INFO: (3) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.883569ms)
Apr 10 15:47:59.095: INFO: (3) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.068101ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.661195ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.653824ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.654371ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.834381ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 6.790678ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 6.663074ms)
Apr 10 15:47:59.096: INFO: (3) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.149303ms)
Apr 10 15:47:59.097: INFO: (3) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.790427ms)
Apr 10 15:47:59.097: INFO: (3) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.766568ms)
Apr 10 15:47:59.097: INFO: (3) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.844159ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.679084ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.335756ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.442281ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.404186ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.382188ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 6.418869ms)
Apr 10 15:47:59.103: INFO: (4) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.565978ms)
Apr 10 15:47:59.104: INFO: (4) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.384624ms)
Apr 10 15:47:59.104: INFO: (4) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.544768ms)
Apr 10 15:47:59.104: INFO: (4) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.392681ms)
Apr 10 15:47:59.104: INFO: (4) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.302452ms)
Apr 10 15:47:59.104: INFO: (4) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.321096ms)
Apr 10 15:47:59.104: INFO: (4) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 7.304538ms)
Apr 10 15:47:59.105: INFO: (4) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.856623ms)
Apr 10 15:47:59.105: INFO: (4) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.834676ms)
Apr 10 15:47:59.105: INFO: (4) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.798655ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.144427ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.242939ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.178799ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.179091ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.270795ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.453904ms)
Apr 10 15:47:59.111: INFO: (5) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 5.973199ms)
Apr 10 15:47:59.112: INFO: (5) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 6.690047ms)
Apr 10 15:47:59.113: INFO: (5) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.163354ms)
Apr 10 15:47:59.113: INFO: (5) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 7.524489ms)
Apr 10 15:47:59.113: INFO: (5) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.470227ms)
Apr 10 15:47:59.113: INFO: (5) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.581929ms)
Apr 10 15:47:59.113: INFO: (5) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.892399ms)
Apr 10 15:47:59.196: INFO: (5) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 90.260687ms)
Apr 10 15:47:59.196: INFO: (5) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 91.10923ms)
Apr 10 15:47:59.196: INFO: (5) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 91.076933ms)
Apr 10 15:47:59.207: INFO: (6) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 10.175207ms)
Apr 10 15:47:59.208: INFO: (6) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 11.105123ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 87.956559ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 88.257985ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 88.223379ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 88.265434ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 88.239713ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 88.49482ms)
Apr 10 15:47:59.285: INFO: (6) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 88.504108ms)
Apr 10 15:47:59.286: INFO: (6) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 88.986269ms)
Apr 10 15:47:59.289: INFO: (6) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 92.5771ms)
Apr 10 15:47:59.289: INFO: (6) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 92.619258ms)
Apr 10 15:47:59.289: INFO: (6) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 92.824908ms)
Apr 10 15:47:59.289: INFO: (6) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 92.779784ms)
Apr 10 15:47:59.403: INFO: (6) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 206.548449ms)
Apr 10 15:47:59.403: INFO: (6) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 206.645092ms)
Apr 10 15:47:59.410: INFO: (7) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.187657ms)
Apr 10 15:47:59.410: INFO: (7) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.93161ms)
Apr 10 15:47:59.411: INFO: (7) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 7.139438ms)
Apr 10 15:47:59.411: INFO: (7) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.230478ms)
Apr 10 15:47:59.411: INFO: (7) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 7.535592ms)
Apr 10 15:47:59.412: INFO: (7) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 8.017587ms)
Apr 10 15:47:59.412: INFO: (7) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 7.990907ms)
Apr 10 15:47:59.485: INFO: (7) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 81.075067ms)
Apr 10 15:47:59.485: INFO: (7) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 81.007259ms)
Apr 10 15:47:59.485: INFO: (7) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 80.960078ms)
Apr 10 15:47:59.485: INFO: (7) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 81.384012ms)
Apr 10 15:47:59.489: INFO: (7) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 85.495177ms)
Apr 10 15:47:59.584: INFO: (7) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 180.203579ms)
Apr 10 15:47:59.589: INFO: (7) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 185.853742ms)
Apr 10 15:47:59.589: INFO: (7) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 185.922897ms)
Apr 10 15:47:59.590: INFO: (7) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 185.998414ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 94.766549ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 94.89651ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 94.966792ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 94.850525ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 94.801851ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 94.896716ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 94.89186ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 95.310757ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 95.403883ms)
Apr 10 15:47:59.685: INFO: (8) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 95.417397ms)
Apr 10 15:47:59.686: INFO: (8) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 96.472453ms)
Apr 10 15:47:59.687: INFO: (8) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 96.622677ms)
Apr 10 15:47:59.687: INFO: (8) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 96.767848ms)
Apr 10 15:47:59.687: INFO: (8) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 96.670875ms)
Apr 10 15:47:59.687: INFO: (8) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 97.415696ms)
Apr 10 15:47:59.687: INFO: (8) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 97.682003ms)
Apr 10 15:47:59.694: INFO: (9) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.688112ms)
Apr 10 15:47:59.694: INFO: (9) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.897628ms)
Apr 10 15:47:59.694: INFO: (9) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.716862ms)
Apr 10 15:47:59.694: INFO: (9) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.794998ms)
Apr 10 15:47:59.694: INFO: (9) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.827417ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 7.291455ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 7.547909ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 7.492215ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 7.546422ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 7.496996ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 7.469641ms)
Apr 10 15:47:59.695: INFO: (9) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.563538ms)
Apr 10 15:47:59.696: INFO: (9) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 8.021606ms)
Apr 10 15:47:59.737: INFO: (9) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 49.607188ms)
Apr 10 15:47:59.737: INFO: (9) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 49.569923ms)
Apr 10 15:47:59.737: INFO: (9) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 49.62016ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.430306ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 5.292449ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 5.1953ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 5.262024ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 5.430023ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.243776ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 5.279389ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 5.241415ms)
Apr 10 15:47:59.743: INFO: (10) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.407035ms)
Apr 10 15:47:59.744: INFO: (10) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 6.849769ms)
Apr 10 15:47:59.744: INFO: (10) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 6.804846ms)
Apr 10 15:47:59.744: INFO: (10) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 6.929928ms)
Apr 10 15:47:59.744: INFO: (10) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 6.868376ms)
Apr 10 15:47:59.744: INFO: (10) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 6.841718ms)
Apr 10 15:47:59.744: INFO: (10) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 6.856754ms)
Apr 10 15:47:59.863: INFO: (10) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 125.905265ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 6.151189ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.051577ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.349155ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.361836ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.480502ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 6.512708ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.423781ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.528232ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.476235ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.83564ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 6.655121ms)
Apr 10 15:47:59.870: INFO: (11) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 6.776257ms)
Apr 10 15:47:59.872: INFO: (11) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 8.126505ms)
Apr 10 15:47:59.913: INFO: (11) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 49.205913ms)
Apr 10 15:47:59.913: INFO: (11) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 49.156782ms)
Apr 10 15:47:59.913: INFO: (11) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 49.155886ms)
Apr 10 15:47:59.919: INFO: (12) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.050115ms)
Apr 10 15:47:59.919: INFO: (12) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.830921ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.355249ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.274219ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.371566ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.281633ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.344828ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.232921ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.219449ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 7.035157ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 7.016428ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.03035ms)
Apr 10 15:47:59.920: INFO: (12) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.338547ms)
Apr 10 15:47:59.921: INFO: (12) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.654401ms)
Apr 10 15:47:59.921: INFO: (12) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.828859ms)
Apr 10 15:47:59.921: INFO: (12) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.741602ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.48429ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.662403ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 5.595585ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.579194ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 5.67945ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 5.637804ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 5.708195ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 5.610171ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 5.725796ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.714985ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 6.0012ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 6.385994ms)
Apr 10 15:47:59.927: INFO: (13) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 6.317537ms)
Apr 10 15:47:59.928: INFO: (13) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 6.680377ms)
Apr 10 15:47:59.928: INFO: (13) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 6.883372ms)
Apr 10 15:47:59.928: INFO: (13) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 6.877646ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 10.164079ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 10.429607ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 10.278375ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 10.303534ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 10.319345ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 10.610341ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 10.390928ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 10.412593ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 10.601641ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 10.442555ms)
Apr 10 15:47:59.939: INFO: (14) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 10.340136ms)
Apr 10 15:47:59.984: INFO: (14) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 55.192892ms)
Apr 10 15:47:59.984: INFO: (14) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 55.267527ms)
Apr 10 15:47:59.984: INFO: (14) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 55.261007ms)
Apr 10 15:47:59.984: INFO: (14) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 55.244869ms)
Apr 10 15:48:00.022: INFO: (14) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 93.877255ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 6.165558ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.467112ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.290318ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.471859ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.27771ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.275905ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.371921ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.358343ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.263147ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.701726ms)
Apr 10 15:48:00.029: INFO: (15) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 6.969527ms)
Apr 10 15:48:00.030: INFO: (15) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.125612ms)
Apr 10 15:48:00.030: INFO: (15) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.069162ms)
Apr 10 15:48:00.030: INFO: (15) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.40179ms)
Apr 10 15:48:00.030: INFO: (15) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.523375ms)
Apr 10 15:48:00.031: INFO: (15) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 8.28923ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.276855ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 6.387027ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.229129ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 6.21242ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.263571ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.295492ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.427356ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 6.255076ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 6.347428ms)
Apr 10 15:48:00.037: INFO: (16) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.675684ms)
Apr 10 15:48:00.038: INFO: (16) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 6.887729ms)
Apr 10 15:48:00.038: INFO: (16) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 6.932331ms)
Apr 10 15:48:00.038: INFO: (16) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.179358ms)
Apr 10 15:48:00.038: INFO: (16) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 7.264015ms)
Apr 10 15:48:00.038: INFO: (16) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.444778ms)
Apr 10 15:48:00.039: INFO: (16) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.93309ms)
Apr 10 15:48:00.044: INFO: (17) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.592257ms)
Apr 10 15:48:00.044: INFO: (17) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 5.548007ms)
Apr 10 15:48:00.045: INFO: (17) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 5.90435ms)
Apr 10 15:48:00.045: INFO: (17) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.937421ms)
Apr 10 15:48:00.045: INFO: (17) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 5.971775ms)
Apr 10 15:48:00.045: INFO: (17) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 6.006823ms)
Apr 10 15:48:00.045: INFO: (17) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 6.055229ms)
Apr 10 15:48:00.047: INFO: (17) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 7.566647ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 9.20188ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 9.070896ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 9.121662ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 9.15387ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 9.090374ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 9.378505ms)
Apr 10 15:48:00.048: INFO: (17) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 9.374591ms)
Apr 10 15:48:00.090: INFO: (17) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 50.601055ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 8.042657ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 8.216365ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 8.119198ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 8.106171ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 8.434616ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 8.331737ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 8.164814ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 8.308648ms)
Apr 10 15:48:00.098: INFO: (18) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 8.189466ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 8.665429ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 8.722516ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 8.744745ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 9.039657ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 9.316465ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 9.287652ms)
Apr 10 15:48:00.099: INFO: (18) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 9.434877ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.836041ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.728705ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:1080/proxy/rewriteme">... (200; 5.830459ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:160/proxy/: foo (200; 5.808689ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:462/proxy/: tls qux (200; 5.886526ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:443/proxy/tlsrewritem... (200; 5.919699ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6:1080/proxy/rewriteme">test<... (200; 5.867922ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/https:proxy-service-tw92s-r98c6:460/proxy/: tls baz (200; 6.09334ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/: <a href="/api/v1/namespaces/proxy-4124/pods/proxy-service-tw92s-r98c6/proxy/rewriteme">test</a> (200; 6.084138ms)
Apr 10 15:48:00.105: INFO: (19) /api/v1/namespaces/proxy-4124/pods/http:proxy-service-tw92s-r98c6:162/proxy/: bar (200; 5.985561ms)
Apr 10 15:48:00.107: INFO: (19) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname1/proxy/: tls baz (200; 7.920618ms)
Apr 10 15:48:00.107: INFO: (19) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname1/proxy/: foo (200; 7.973477ms)
Apr 10 15:48:00.107: INFO: (19) /api/v1/namespaces/proxy-4124/services/proxy-service-tw92s:portname2/proxy/: bar (200; 7.937244ms)
Apr 10 15:48:00.107: INFO: (19) /api/v1/namespaces/proxy-4124/services/https:proxy-service-tw92s:tlsportname2/proxy/: tls qux (200; 7.943772ms)
Apr 10 15:48:00.107: INFO: (19) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname1/proxy/: foo (200; 7.96092ms)
Apr 10 15:48:00.107: INFO: (19) /api/v1/namespaces/proxy-4124/services/http:proxy-service-tw92s:portname2/proxy/: bar (200; 8.015279ms)
STEP: deleting ReplicationController proxy-service-tw92s in namespace proxy-4124, will wait for the garbage collector to delete the pods
Apr 10 15:48:00.170: INFO: Deleting ReplicationController proxy-service-tw92s took: 6.86718ms
Apr 10 15:48:00.570: INFO: Terminating ReplicationController proxy-service-tw92s pods took: 400.37673ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:48:06.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4124" for this suite.
Apr 10 15:48:12.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:48:12.703: INFO: namespace proxy-4124 deletion completed in 6.126870011s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:48:12.704: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-0c3963b6-5ba8-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:48:12.935: INFO: Waiting up to 5m0s for pod "pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd" in namespace "secrets-1606" to be "success or failure"
Apr 10 15:48:12.938: INFO: Pod "pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196367ms
Apr 10 15:48:14.943: INFO: Pod "pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008358294s
STEP: Saw pod success
Apr 10 15:48:14.943: INFO: Pod "pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:48:14.947: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 15:48:14.969: INFO: Waiting for pod pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:48:14.972: INFO: Pod pod-secrets-0c39fc55-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:48:14.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1606" for this suite.
Apr 10 15:48:20.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:48:21.107: INFO: namespace secrets-1606 deletion completed in 6.130768829s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:48:21.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 10 15:48:21.424: INFO: Waiting up to 5m0s for pod "pod-11494823-5ba8-11e9-ae72-2e2932f810fd" in namespace "emptydir-5150" to be "success or failure"
Apr 10 15:48:21.428: INFO: Pod "pod-11494823-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765143ms
Apr 10 15:48:23.433: INFO: Pod "pod-11494823-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008710247s
STEP: Saw pod success
Apr 10 15:48:23.433: INFO: Pod "pod-11494823-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:48:23.438: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-11494823-5ba8-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:48:23.462: INFO: Waiting for pod pod-11494823-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:48:23.465: INFO: Pod pod-11494823-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:48:23.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5150" for this suite.
Apr 10 15:48:29.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:48:29.598: INFO: namespace emptydir-5150 deletion completed in 6.128358797s
•SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:48:29.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-164c1bda-5ba8-11e9-ae72-2e2932f810fd
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:48:29.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-444" for this suite.
Apr 10 15:48:35.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:48:35.954: INFO: namespace configmap-444 deletion completed in 6.124250399s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:48:35.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 15:48:36.215: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9025'
Apr 10 15:48:36.303: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 15:48:36.303: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr 10 15:48:36.311: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-4jc5l]
Apr 10 15:48:36.311: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-4jc5l" in namespace "kubectl-9025" to be "running and ready"
Apr 10 15:48:36.314: INFO: Pod "e2e-test-nginx-rc-4jc5l": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084752ms
Apr 10 15:48:38.319: INFO: Pod "e2e-test-nginx-rc-4jc5l": Phase="Running", Reason="", readiness=true. Elapsed: 2.008000106s
Apr 10 15:48:38.319: INFO: Pod "e2e-test-nginx-rc-4jc5l" satisfied condition "running and ready"
Apr 10 15:48:38.319: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-4jc5l]
Apr 10 15:48:38.319: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-9025'
Apr 10 15:48:38.438: INFO: stderr: ""
Apr 10 15:48:38.438: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Apr 10 15:48:38.438: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-9025'
Apr 10 15:48:38.524: INFO: stderr: ""
Apr 10 15:48:38.524: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:48:38.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9025" for this suite.
Apr 10 15:49:00.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:49:00.659: INFO: namespace kubectl-9025 deletion completed in 22.130156035s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:49:00.659: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 10 15:49:00.923: INFO: Waiting up to 5m0s for pod "pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd" in namespace "emptydir-6514" to be "success or failure"
Apr 10 15:49:00.927: INFO: Pod "pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566858ms
Apr 10 15:49:02.931: INFO: Pod "pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008073207s
Apr 10 15:49:04.935: INFO: Pod "pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012121275s
STEP: Saw pod success
Apr 10 15:49:04.935: INFO: Pod "pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:49:04.938: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:49:04.957: INFO: Waiting for pod pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:49:04.960: INFO: Pod pod-28d45dce-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:49:04.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6514" for this suite.
Apr 10 15:49:10.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:49:11.078: INFO: namespace emptydir-6514 deletion completed in 6.114064475s
•SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:49:11.078: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 10 15:49:13.854: INFO: Running '/bin/kubectl exec --namespace=svcaccounts-286 pod-service-account-2f5749aa-5ba8-11e9-ae72-2e2932f810fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 10 15:49:14.367: INFO: Running '/bin/kubectl exec --namespace=svcaccounts-286 pod-service-account-2f5749aa-5ba8-11e9-ae72-2e2932f810fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 10 15:49:14.792: INFO: Running '/bin/kubectl exec --namespace=svcaccounts-286 pod-service-account-2f5749aa-5ba8-11e9-ae72-2e2932f810fd -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:49:15.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-286" for this suite.
Apr 10 15:49:21.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:49:21.472: INFO: namespace svcaccounts-286 deletion completed in 6.164991742s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:49:21.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:49:21.714: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Apr 10 15:49:21.795: INFO: stderr: ""
Apr 10 15:49:21.795: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:49:21.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4837" for this suite.
Apr 10 15:49:29.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:49:29.921: INFO: namespace kubectl-4837 deletion completed in 8.121254592s
•SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:49:29.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3034
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-3a4c2382-5ba8-11e9-ae72-2e2932f810fd
STEP: Creating configMap with name cm-test-opt-upd-3a4c23ba-5ba8-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3a4c2382-5ba8-11e9-ae72-2e2932f810fd
STEP: Updating configmap cm-test-opt-upd-3a4c23ba-5ba8-11e9-ae72-2e2932f810fd
STEP: Creating configMap with name cm-test-opt-create-3a4c23cb-5ba8-11e9-ae72-2e2932f810fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:50:43.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3034" for this suite.
Apr 10 15:51:07.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:51:07.222: INFO: namespace configmap-3034 deletion completed in 24.124735508s
•SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:51:07.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:51:07.514: INFO: Creating deployment "nginx-deployment"
Apr 10 15:51:07.519: INFO: Waiting for observed generation 1
Apr 10 15:51:09.527: INFO: Waiting for all required pods to come up
Apr 10 15:51:09.532: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 10 15:51:11.541: INFO: Waiting for deployment "nginx-deployment" to complete
Apr 10 15:51:11.548: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr 10 15:51:11.556: INFO: Updating deployment nginx-deployment
Apr 10 15:51:11.556: INFO: Waiting for observed generation 2
Apr 10 15:51:13.564: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 10 15:51:13.567: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 10 15:51:13.570: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 10 15:51:13.580: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 10 15:51:13.580: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 10 15:51:13.584: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 10 15:51:13.591: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr 10 15:51:13.591: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr 10 15:51:13.599: INFO: Updating deployment nginx-deployment
Apr 10 15:51:13.599: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr 10 15:51:13.604: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 10 15:51:13.609: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 10 15:51:13.616: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9702,SelfLink:/apis/apps/v1/namespaces/deployment-9702/deployments/nginx-deployment,UID:744a19a9-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16687,Generation:3,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-04-10 15:51:11 +0000 UTC 2019-04-10 15:51:07 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-04-10 15:51:13 +0000 UTC 2019-04-10 15:51:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr 10 15:51:13.620: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-9702,SelfLink:/apis/apps/v1/namespaces/deployment-9702/replicasets/nginx-deployment-5f9595f595,UID:76b2d31c-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16682,Generation:3,CreationTimestamp:2019-04-10 15:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 744a19a9-5ba8-11e9-82f7-160dbbe32fdc 0xc002f339f7 0xc002f339f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 10 15:51:13.620: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr 10 15:51:13.620: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-9702,SelfLink:/apis/apps/v1/namespaces/deployment-9702/replicasets/nginx-deployment-6f478d8d8,UID:744ae919-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16681,Generation:3,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 744a19a9-5ba8-11e9-82f7-160dbbe32fdc 0xc002f33ac7 0xc002f33ac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr 10 15:51:13.685: INFO: Pod "nginx-deployment-5f9595f595-4nsdj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4nsdj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-4nsdj,UID:76c99656-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16674,Generation:0,CreationTimestamp:2019-04-10 15:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.137/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1457 0xc0026d1458}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d14c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d14e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-04-10 15:51:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.685: INFO: Pod "nginx-deployment-5f9595f595-4sknf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4sknf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-4sknf,UID:77ebcf35-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16688,Generation:0,CreationTimestamp:2019-04-10 15:51:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d15b0 0xc0026d15b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d1620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-68j5j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-68j5j,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-68j5j,UID:76b37c82-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16671,Generation:0,CreationTimestamp:2019-04-10 15:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.135/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d16d0 0xc0026d16d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d1740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-04-10 15:51:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-6q7xb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-6q7xb,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-6q7xb,UID:77ec8e82-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16692,Generation:0,CreationTimestamp:2019-04-10 15:51:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1840 0xc0026d1841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d18b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d18d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-8nqf8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-8nqf8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-8nqf8,UID:76b426d0-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16679,Generation:0,CreationTimestamp:2019-04-10 15:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.99/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1960 0xc0026d1961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d19f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.99,StartTime:2019-04-10 15:51:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-8x4mv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-8x4mv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-8x4mv,UID:76c8e8d9-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16673,Generation:0,CreationTimestamp:2019-04-10 15:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.100/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1b10 0xc0026d1b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d1b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-04-10 15:51:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-jftw2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jftw2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-jftw2,UID:76b41cfc-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16672,Generation:0,CreationTimestamp:2019-04-10 15:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.136/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1c80 0xc0026d1c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d1cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-04-10 15:51:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-lbfq7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lbfq7,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-lbfq7,UID:77ec98ad-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16691,Generation:0,CreationTimestamp:2019-04-10 15:51:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1e00 0xc0026d1e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d1e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-m7fwr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-m7fwr,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-m7fwr,UID:77ed6c96-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16693,Generation:0,CreationTimestamp:2019-04-10 15:51:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc0026d1f20 0xc0026d1f21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026d1f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026d1fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.686: INFO: Pod "nginx-deployment-5f9595f595-t8mj5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-t8mj5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-5f9595f595-t8mj5,UID:77ed841b-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16694,Generation:0,CreationTimestamp:2019-04-10 15:51:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 76b2d31c-5ba8-11e9-82f7-160dbbe32fdc 0xc001f74017 0xc001f74018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f740a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-49lhv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-49lhv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-49lhv,UID:7457d5ba-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16627,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.132/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f741d7 0xc001f741d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f742b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.132,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1e00889979c3a6102795f5bd6d6b6f360ca9b0063beb495c4b7db0cc62a8fd66}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-6m9tw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6m9tw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-6m9tw,UID:744c84aa-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16618,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.130/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f743f0 0xc001f743f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f74450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.130,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ab442812a30995e2060c9c56498778f2b5b0e1e81366a378734587c786611c90}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-f4sxv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-f4sxv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-f4sxv,UID:7456a3ea-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16606,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.96/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f745d0 0xc001f745d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f74780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f747a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.96,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f7c67a883264a5e353cc8bab0f01335ea5a96624dc22a28b10439db9b69a9f78}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-jbll9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jbll9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-jbll9,UID:7456b31d-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16625,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.133/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f74880 0xc001f74881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f748e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.133,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2d3fd69d9371e751ca36fc6c708ae2a4619126fc1abbb322cb3730158e178700}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-pf626" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pf626,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-pf626,UID:7456a3bb-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16624,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.131/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f749f0 0xc001f749f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f74a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74a80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.131,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://428f9408298823327585016e75f79be44c073cc10a3581a97491b2ab03f51376}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-q69lc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q69lc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-q69lc,UID:7457c9ec-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16615,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.98/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f74b60 0xc001f74b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f74bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.98,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://24a929bd012fda48d856c0a9a00315716d4b9bc3ec356196cc5aeaf1109be6ba}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-r2k9x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-r2k9x,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-r2k9x,UID:7456a694-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16609,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.97/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f74cc0 0xc001f74cc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f74d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.97,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6fd5092b4fb21ed3cb8fc036d72ff2a0d9a1e58f6582a7acb7fce0cdc63a5e88}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-sqw6q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sqw6q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-sqw6q,UID:7457cfa8-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16621,Generation:0,CreationTimestamp:2019-04-10 15:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.134/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f74e50 0xc001f74e51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f74eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f74ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.134,StartTime:2019-04-10 15:51:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-10 15:51:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9c9b13fd0e63da2fe4c6bd37e50fe8039655149ebe5d9cb509d52e8aaaaa5f4b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 10 15:51:13.687: INFO: Pod "nginx-deployment-6f478d8d8-wqstt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wqstt,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9702,SelfLink:/api/v1/namespaces/deployment-9702/pods/nginx-deployment-6f478d8d8-wqstt,UID:77eb44c1-5ba8-11e9-82f7-160dbbe32fdc,ResourceVersion:16686,Generation:0,CreationTimestamp:2019-04-10 15:51:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 744ae919-5ba8-11e9-82f7-160dbbe32fdc 0xc001f74fe0 0xc001f74fe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7wpd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7wpd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z7wpd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f75040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f75060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-10 15:51:13 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:51:13.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9702" for this suite.
Apr 10 15:51:21.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:51:21.822: INFO: namespace deployment-9702 deletion completed in 8.130365421s
•SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:51:21.822: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 10 15:51:26.064: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:26.068: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:28.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:28.074: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:30.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:30.073: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:32.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:32.073: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:34.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:34.074: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:36.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:36.074: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:38.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:38.074: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:40.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:40.074: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:42.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:42.074: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:44.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:44.073: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 10 15:51:46.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 10 15:51:46.074: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:51:46.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2090" for this suite.
Apr 10 15:52:10.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:52:10.208: INFO: namespace container-lifecycle-hook-2090 deletion completed in 24.118117964s
•SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:52:10.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-99d6ed5a-5ba8-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:52:10.526: INFO: Waiting up to 5m0s for pod "pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd" in namespace "secrets-9651" to be "success or failure"
Apr 10 15:52:10.530: INFO: Pod "pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508872ms
Apr 10 15:52:12.534: INFO: Pod "pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008111973s
STEP: Saw pod success
Apr 10 15:52:12.534: INFO: Pod "pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:52:12.539: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 15:52:12.560: INFO: Waiting for pod pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:52:12.563: INFO: Pod pod-secrets-99d79796-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:52:12.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9651" for this suite.
Apr 10 15:52:18.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:52:18.692: INFO: namespace secrets-9651 deletion completed in 6.123487127s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:52:18.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-1532/secret-test-9ee72c4a-5ba8-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 15:52:19.021: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd" in namespace "secrets-1532" to be "success or failure"
Apr 10 15:52:19.025: INFO: Pod "pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.254955ms
Apr 10 15:52:21.030: INFO: Pod "pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008623523s
STEP: Saw pod success
Apr 10 15:52:21.030: INFO: Pod "pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:52:21.035: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd container env-test: <nil>
STEP: delete the pod
Apr 10 15:52:21.058: INFO: Waiting for pod pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:52:21.061: INFO: Pod pod-configmaps-9ee7ce7c-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:52:21.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1532" for this suite.
Apr 10 15:52:27.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:52:27.197: INFO: namespace secrets-1532 deletion completed in 6.131864246s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:52:27.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 15:52:27.426: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-5594'
Apr 10 15:52:27.529: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 10 15:52:27.529: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Apr 10 15:52:29.536: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-5594'
Apr 10 15:52:29.639: INFO: stderr: ""
Apr 10 15:52:29.639: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:52:29.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5594" for this suite.
Apr 10 15:52:45.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:52:45.772: INFO: namespace kubectl-5594 deletion completed in 16.128913876s
•S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:52:45.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:52:50.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4131" for this suite.
Apr 10 15:52:56.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:52:56.205: INFO: namespace kubelet-test-4131 deletion completed in 6.163981535s
•SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:52:56.205: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:52:56.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:52:58.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1222" for this suite.
Apr 10 15:53:52.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:53:52.781: INFO: namespace pods-1222 deletion completed in 54.12535605s
•SSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:53:52.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 10 15:53:57.156: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:57.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:53:57.559: INFO: Exec stderr: ""
Apr 10 15:53:57.559: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:57.559: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:53:57.913: INFO: Exec stderr: ""
Apr 10 15:53:57.913: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:57.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:53:58.383: INFO: Exec stderr: ""
Apr 10 15:53:58.384: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:58.384: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:53:58.839: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 10 15:53:58.839: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:58.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:53:59.245: INFO: Exec stderr: ""
Apr 10 15:53:59.245: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:59.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:53:59.600: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 10 15:53:59.600: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:53:59.600: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:54:00.081: INFO: Exec stderr: ""
Apr 10 15:54:00.081: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:54:00.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:54:00.533: INFO: Exec stderr: ""
Apr 10 15:54:00.533: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:54:00.533: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:54:00.922: INFO: Exec stderr: ""
Apr 10 15:54:00.922: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1066 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 15:54:00.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 15:54:01.303: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:54:01.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1066" for this suite.
Apr 10 15:54:51.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:54:51.475: INFO: namespace e2e-kubelet-etc-hosts-1066 deletion completed in 50.166680327s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:54:51.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 10 15:54:51.728: INFO: Waiting up to 5m0s for pod "pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd" in namespace "emptydir-4440" to be "success or failure"
Apr 10 15:54:51.733: INFO: Pod "pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.370082ms
Apr 10 15:54:53.739: INFO: Pod "pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010399104s
STEP: Saw pod success
Apr 10 15:54:53.739: INFO: Pod "pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:54:53.742: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 15:54:53.765: INFO: Waiting for pod pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:54:53.768: INFO: Pod pod-f9ece6ec-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:54:53.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4440" for this suite.
Apr 10 15:54:59.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:54:59.895: INFO: namespace emptydir-4440 deletion completed in 6.122652796s
•SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:54:59.895: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7900
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Apr 10 15:55:00.223: INFO: Waiting up to 5m0s for pod "var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd" in namespace "var-expansion-7900" to be "success or failure"
Apr 10 15:55:00.227: INFO: Pod "var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.824681ms
Apr 10 15:55:02.231: INFO: Pod "var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008404898s
STEP: Saw pod success
Apr 10 15:55:02.231: INFO: Pod "var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:55:02.235: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 15:55:02.260: INFO: Waiting for pod var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:55:02.263: INFO: Pod var-expansion-fefd3898-5ba8-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:55:02.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7900" for this suite.
Apr 10 15:55:08.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:55:08.387: INFO: namespace var-expansion-7900 deletion completed in 6.120120942s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:55:08.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:55:31.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6291" for this suite.
Apr 10 15:55:37.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:55:38.424: INFO: namespace container-runtime-6291 deletion completed in 6.556005046s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:55:38.424: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 15:55:41.174: INFO: Successfully updated pod "annotationupdate15e18604-5ba9-11e9-ae72-2e2932f810fd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:55:45.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1006" for this suite.
Apr 10 15:56:07.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:56:07.343: INFO: namespace projected-1006 deletion completed in 22.128414021s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:56:07.343: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 10 15:56:07.522: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1546'
Apr 10 15:56:07.949: INFO: stderr: ""
Apr 10 15:56:07.949: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 15:56:08.954: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:56:08.954: INFO: Found 0 / 1
Apr 10 15:56:09.954: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:56:09.954: INFO: Found 0 / 1
Apr 10 15:56:10.954: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:56:10.954: INFO: Found 1 / 1
Apr 10 15:56:10.954: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 10 15:56:10.957: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:56:10.957: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 15:56:10.958: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-zx8bq --namespace=kubectl-1546 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 10 15:56:11.046: INFO: stderr: ""
Apr 10 15:56:11.046: INFO: stdout: "pod/redis-master-zx8bq patched\n"
STEP: checking annotations
Apr 10 15:56:11.050: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 15:56:11.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:56:11.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1546" for this suite.
Apr 10 15:56:35.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:56:35.239: INFO: namespace kubectl-1546 deletion completed in 24.184531551s
•SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:56:35.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 15:56:35.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd" in namespace "projected-3827" to be "success or failure"
Apr 10 15:56:35.524: INFO: Pod "downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016962ms
Apr 10 15:56:37.529: INFO: Pod "downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009426433s
STEP: Saw pod success
Apr 10 15:56:37.529: INFO: Pod "downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:56:37.533: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 15:56:37.557: INFO: Waiting for pod downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:56:37.561: INFO: Pod downwardapi-volume-37ca3300-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:56:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3827" for this suite.
Apr 10 15:56:43.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:56:43.748: INFO: namespace projected-3827 deletion completed in 6.182804388s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:56:43.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 10 15:56:43.935: INFO: Waiting up to 5m0s for pod "downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd" in namespace "downward-api-7891" to be "success or failure"
Apr 10 15:56:43.945: INFO: Pod "downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024288ms
Apr 10 15:56:45.950: INFO: Pod "downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015565117s
STEP: Saw pod success
Apr 10 15:56:45.951: INFO: Pod "downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:56:45.955: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 15:56:45.979: INFO: Waiting for pod downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:56:45.982: INFO: Pod downward-api-3cce59ff-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:56:45.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7891" for this suite.
Apr 10 15:56:52.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:56:52.126: INFO: namespace downward-api-7891 deletion completed in 6.139005451s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:56:52.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9006
Apr 10 15:56:56.432: INFO: Started pod liveness-http in namespace container-probe-9006
STEP: checking the pod's current state and verifying that restartCount is present
Apr 10 15:56:56.436: INFO: Initial restart count of pod liveness-http is 0
Apr 10 15:57:12.481: INFO: Restart count of pod container-probe-9006/liveness-http is now 1 (16.045138578s elapsed)
Apr 10 15:57:30.524: INFO: Restart count of pod container-probe-9006/liveness-http is now 2 (34.088796397s elapsed)
Apr 10 15:57:50.574: INFO: Restart count of pod container-probe-9006/liveness-http is now 3 (54.138904053s elapsed)
Apr 10 15:58:10.623: INFO: Restart count of pod container-probe-9006/liveness-http is now 4 (1m14.187186617s elapsed)
Apr 10 15:59:10.798: INFO: Restart count of pod container-probe-9006/liveness-http is now 5 (2m14.362512353s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:59:10.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9006" for this suite.
Apr 10 15:59:18.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 15:59:18.937: INFO: namespace container-probe-9006 deletion completed in 8.125337328s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 15:59:18.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 15:59:23.251: INFO: Waiting up to 5m0s for pod "client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd" in namespace "pods-6050" to be "success or failure"
Apr 10 15:59:23.255: INFO: Pod "client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485106ms
Apr 10 15:59:25.260: INFO: Pod "client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008598783s
STEP: Saw pod success
Apr 10 15:59:25.260: INFO: Pod "client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 15:59:25.264: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd container env3cont: <nil>
STEP: delete the pod
Apr 10 15:59:25.287: INFO: Waiting for pod client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 15:59:25.290: INFO: Pod client-envvars-9bc4001a-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 15:59:25.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6050" for this suite.
Apr 10 16:00:05.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:00:05.429: INFO: namespace pods-6050 deletion completed in 40.135671035s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:00:05.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 16:00:05.625: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 16:00:05.635: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 16:00:05.638: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l before test
Apr 10 16:00:05.650: INFO: calico-node-4cqp8 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 16:00:05.650: INFO: kube-proxy-w7thh from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 16:00:05.650: INFO: coredns-7f7f7978c8-sb9zb from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container coredns ready: true, restart count 0
Apr 10 16:00:05.650: INFO: blackbox-exporter-6dc58dcffc-6rds9 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 16:00:05.650: INFO: node-exporter-dsgq7 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 16:00:05.650: INFO: metrics-server-84f8f5f44f-5nx55 from kube-system started at 2019-04-10 14:34:30 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 16:00:05.650: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-gkzbt from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.650: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 16:00:05.650: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt before test
Apr 10 16:00:05.695: INFO: kube-proxy-86z4l from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 16:00:05.695: INFO: calico-node-vjz47 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 16:00:05.695: INFO: addons-nginx-ingress-controller-d4f8c9cc5-swsd5 from kube-system started at 2019-04-10 14:34:30 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 16:00:05.695: INFO: vpn-shoot-798b6484c-m4qbr from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 16:00:05.695: INFO: coredns-7f7f7978c8-pbh8n from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container coredns ready: true, restart count 0
Apr 10 16:00:05.695: INFO: node-exporter-qd9pz from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 16:00:05.695: INFO: addons-kubernetes-dashboard-665df4b66d-ltjvt from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:00:05.695: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b778ea22-5ba9-11e9-ae72-2e2932f810fd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b778ea22-5ba9-11e9-ae72-2e2932f810fd off the node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b778ea22-5ba9-11e9-ae72-2e2932f810fd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:00:11.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2271" for this suite.
Apr 10 16:00:19.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:00:19.905: INFO: namespace sched-pred-2271 deletion completed in 8.13243146s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:00:19.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 16:00:20.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd" in namespace "downward-api-9842" to be "success or failure"
Apr 10 16:00:20.136: INFO: Pod "downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.911192ms
Apr 10 16:00:22.142: INFO: Pod "downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010535122s
STEP: Saw pod success
Apr 10 16:00:22.142: INFO: Pod "downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:00:22.146: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 16:00:22.167: INFO: Waiting for pod downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:00:22.170: INFO: Pod downwardapi-volume-bdab22d1-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:00:22.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9842" for this suite.
Apr 10 16:00:28.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:00:28.315: INFO: namespace downward-api-9842 deletion completed in 6.140194032s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:00:28.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 10 16:00:28.576: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5759'
Apr 10 16:00:28.759: INFO: stderr: ""
Apr 10 16:00:28.759: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 16:00:28.759: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5759'
Apr 10 16:00:28.850: INFO: stderr: ""
Apr 10 16:00:28.850: INFO: stdout: "update-demo-nautilus-8dzn4 update-demo-nautilus-xzc82 "
Apr 10 16:00:28.851: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8dzn4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5759'
Apr 10 16:00:28.934: INFO: stderr: ""
Apr 10 16:00:28.934: INFO: stdout: ""
Apr 10 16:00:28.934: INFO: update-demo-nautilus-8dzn4 is created but not running
Apr 10 16:00:33.934: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5759'
Apr 10 16:00:34.023: INFO: stderr: ""
Apr 10 16:00:34.023: INFO: stdout: "update-demo-nautilus-8dzn4 update-demo-nautilus-xzc82 "
Apr 10 16:00:34.023: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8dzn4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5759'
Apr 10 16:00:34.103: INFO: stderr: ""
Apr 10 16:00:34.103: INFO: stdout: "true"
Apr 10 16:00:34.104: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8dzn4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5759'
Apr 10 16:00:34.185: INFO: stderr: ""
Apr 10 16:00:34.185: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:00:34.185: INFO: validating pod update-demo-nautilus-8dzn4
Apr 10 16:00:34.276: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:00:34.276: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:00:34.276: INFO: update-demo-nautilus-8dzn4 is verified up and running
Apr 10 16:00:34.276: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-xzc82 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5759'
Apr 10 16:00:34.354: INFO: stderr: ""
Apr 10 16:00:34.354: INFO: stdout: "true"
Apr 10 16:00:34.354: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-xzc82 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5759'
Apr 10 16:00:34.432: INFO: stderr: ""
Apr 10 16:00:34.432: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:00:34.432: INFO: validating pod update-demo-nautilus-xzc82
Apr 10 16:00:34.524: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:00:34.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:00:34.524: INFO: update-demo-nautilus-xzc82 is verified up and running
STEP: using delete to clean up resources
Apr 10 16:00:34.524: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5759'
Apr 10 16:00:34.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 16:00:34.605: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 10 16:00:34.605: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5759'
Apr 10 16:00:34.704: INFO: stderr: "No resources found.\n"
Apr 10 16:00:34.704: INFO: stdout: ""
Apr 10 16:00:34.704: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-5759 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 16:00:34.802: INFO: stderr: ""
Apr 10 16:00:34.802: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:00:34.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5759" for this suite.
Apr 10 16:00:40.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:00:40.945: INFO: namespace kubectl-5759 deletion completed in 6.136845104s
•SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:00:40.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9344
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ca3069ca-5ba9-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 16:00:41.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd" in namespace "configmap-9344" to be "success or failure"
Apr 10 16:00:41.145: INFO: Pod "pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.366488ms
Apr 10 16:00:43.150: INFO: Pod "pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007864045s
STEP: Saw pod success
Apr 10 16:00:43.150: INFO: Pod "pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:00:43.154: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 16:00:43.176: INFO: Waiting for pod pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:00:43.179: INFO: Pod pod-configmaps-ca314fda-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:00:43.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9344" for this suite.
Apr 10 16:00:49.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:00:49.305: INFO: namespace configmap-9344 deletion completed in 6.121981939s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:00:49.306: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 10 16:00:49.610: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8986'
Apr 10 16:00:49.788: INFO: stderr: ""
Apr 10 16:00:49.788: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 16:00:49.788: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8986'
Apr 10 16:00:49.874: INFO: stderr: ""
Apr 10 16:00:49.874: INFO: stdout: "update-demo-nautilus-8mtrs update-demo-nautilus-g2w79 "
Apr 10 16:00:49.874: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8mtrs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:00:49.962: INFO: stderr: ""
Apr 10 16:00:49.962: INFO: stdout: ""
Apr 10 16:00:49.962: INFO: update-demo-nautilus-8mtrs is created but not running
Apr 10 16:00:54.963: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8986'
Apr 10 16:00:55.050: INFO: stderr: ""
Apr 10 16:00:55.050: INFO: stdout: "update-demo-nautilus-8mtrs update-demo-nautilus-g2w79 "
Apr 10 16:00:55.050: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8mtrs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:00:55.148: INFO: stderr: ""
Apr 10 16:00:55.148: INFO: stdout: "true"
Apr 10 16:00:55.148: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8mtrs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:00:55.227: INFO: stderr: ""
Apr 10 16:00:55.227: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:00:55.227: INFO: validating pod update-demo-nautilus-8mtrs
Apr 10 16:00:55.327: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:00:55.327: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:00:55.327: INFO: update-demo-nautilus-8mtrs is verified up and running
Apr 10 16:00:55.327: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-g2w79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:00:55.407: INFO: stderr: ""
Apr 10 16:00:55.407: INFO: stdout: "true"
Apr 10 16:00:55.407: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-g2w79 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:00:55.487: INFO: stderr: ""
Apr 10 16:00:55.487: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:00:55.487: INFO: validating pod update-demo-nautilus-g2w79
Apr 10 16:00:55.579: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:00:55.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:00:55.579: INFO: update-demo-nautilus-g2w79 is verified up and running
STEP: scaling down the replication controller
Apr 10 16:00:55.581: INFO: scanned /root for discovery docs: <nil>
Apr 10 16:00:55.581: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8986'
Apr 10 16:00:56.690: INFO: stderr: ""
Apr 10 16:00:56.690: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 16:00:56.690: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8986'
Apr 10 16:00:56.771: INFO: stderr: ""
Apr 10 16:00:56.771: INFO: stdout: "update-demo-nautilus-8mtrs update-demo-nautilus-g2w79 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 10 16:01:01.772: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8986'
Apr 10 16:01:01.870: INFO: stderr: ""
Apr 10 16:01:01.870: INFO: stdout: "update-demo-nautilus-g2w79 "
Apr 10 16:01:01.871: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-g2w79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:01:01.953: INFO: stderr: ""
Apr 10 16:01:01.953: INFO: stdout: "true"
Apr 10 16:01:01.953: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-g2w79 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:01:02.046: INFO: stderr: ""
Apr 10 16:01:02.046: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:01:02.046: INFO: validating pod update-demo-nautilus-g2w79
Apr 10 16:01:02.056: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:01:02.056: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:01:02.056: INFO: update-demo-nautilus-g2w79 is verified up and running
STEP: scaling up the replication controller
Apr 10 16:01:02.058: INFO: scanned /root for discovery docs: <nil>
Apr 10 16:01:02.058: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8986'
Apr 10 16:01:03.190: INFO: stderr: ""
Apr 10 16:01:03.190: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 10 16:01:03.190: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8986'
Apr 10 16:01:03.282: INFO: stderr: ""
Apr 10 16:01:03.282: INFO: stdout: "update-demo-nautilus-g2w79 update-demo-nautilus-s6wvq "
Apr 10 16:01:03.282: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-g2w79 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:01:03.372: INFO: stderr: ""
Apr 10 16:01:03.372: INFO: stdout: "true"
Apr 10 16:01:03.372: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-g2w79 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:01:03.458: INFO: stderr: ""
Apr 10 16:01:03.458: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:01:03.458: INFO: validating pod update-demo-nautilus-g2w79
Apr 10 16:01:03.466: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:01:03.466: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:01:03.466: INFO: update-demo-nautilus-g2w79 is verified up and running
Apr 10 16:01:03.466: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-s6wvq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:01:03.557: INFO: stderr: ""
Apr 10 16:01:03.557: INFO: stdout: "true"
Apr 10 16:01:03.558: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-s6wvq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8986'
Apr 10 16:01:03.645: INFO: stderr: ""
Apr 10 16:01:03.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 10 16:01:03.645: INFO: validating pod update-demo-nautilus-s6wvq
Apr 10 16:01:03.735: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 10 16:01:03.735: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 10 16:01:03.735: INFO: update-demo-nautilus-s6wvq is verified up and running
STEP: using delete to clean up resources
Apr 10 16:01:03.735: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8986'
Apr 10 16:01:03.843: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 10 16:01:03.843: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 10 16:01:03.843: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8986'
Apr 10 16:01:03.936: INFO: stderr: "No resources found.\n"
Apr 10 16:01:03.936: INFO: stdout: ""
Apr 10 16:01:03.936: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-8986 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 10 16:01:04.029: INFO: stderr: ""
Apr 10 16:01:04.029: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:01:04.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8986" for this suite.
Apr 10 16:01:26.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:01:26.160: INFO: namespace kubectl-8986 deletion completed in 22.12552245s
•
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:01:26.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Apr 10 16:01:26.422: INFO: Waiting up to 5m0s for pod "var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd" in namespace "var-expansion-2699" to be "success or failure"
Apr 10 16:01:26.425: INFO: Pod "var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481192ms
Apr 10 16:01:28.430: INFO: Pod "var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007748231s
STEP: Saw pod success
Apr 10 16:01:28.430: INFO: Pod "var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:01:28.434: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd container dapi-container: <nil>
STEP: delete the pod
Apr 10 16:01:28.457: INFO: Waiting for pod var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:01:28.460: INFO: Pod var-expansion-e52e95b7-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:01:28.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2699" for this suite.
Apr 10 16:01:34.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:01:34.798: INFO: namespace var-expansion-2699 deletion completed in 6.333897983s
•S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:01:34.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-ea4fb65c-5ba9-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 16:01:35.032: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd" in namespace "projected-766" to be "success or failure"
Apr 10 16:01:35.036: INFO: Pod "pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.409049ms
Apr 10 16:01:37.040: INFO: Pod "pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007882952s
STEP: Saw pod success
Apr 10 16:01:37.040: INFO: Pod "pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:01:37.045: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 16:01:37.066: INFO: Waiting for pod pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:01:37.069: INFO: Pod pod-projected-configmaps-ea505c27-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:01:37.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-766" for this suite.
Apr 10 16:01:43.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:01:43.208: INFO: namespace projected-766 deletion completed in 6.134864301s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:01:43.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ef528d25-5ba9-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 16:01:43.440: INFO: Waiting up to 5m0s for pod "pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd" in namespace "secrets-706" to be "success or failure"
Apr 10 16:01:43.443: INFO: Pod "pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.45223ms
Apr 10 16:01:45.449: INFO: Pod "pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008916716s
STEP: Saw pod success
Apr 10 16:01:45.449: INFO: Pod "pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:01:45.453: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd container secret-volume-test: <nil>
STEP: delete the pod
Apr 10 16:01:45.474: INFO: Waiting for pod pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:01:45.477: INFO: Pod pod-secrets-ef533e3b-5ba9-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:01:45.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-706" for this suite.
Apr 10 16:01:51.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:01:51.615: INFO: namespace secrets-706 deletion completed in 6.134267817s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:01:51.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9033
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f461dc03-5ba9-11e9-ae72-2e2932f810fd
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f461dc03-5ba9-11e9-ae72-2e2932f810fd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:01:56.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9033" for this suite.
Apr 10 16:02:20.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:02:20.196: INFO: namespace projected-9033 deletion completed in 24.135925423s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:02:20.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 16:02:20.424: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6573'
Apr 10 16:02:20.526: INFO: stderr: ""
Apr 10 16:02:20.526: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr 10 16:02:25.577: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-6573 -o json'
Apr 10 16:02:25.665: INFO: stderr: ""
Apr 10 16:02:25.665: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.157/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-04-10T16:02:20Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6573\",\n        \"resourceVersion\": \"18849\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6573/pods/e2e-test-nginx-pod\",\n        \"uid\": \"056d7cca-5baa-11e9-82f7-160dbbe32fdc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-swvdd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-swvdd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-swvdd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T16:02:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T16:02:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T16:02:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-10T16:02:20Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://db51cdab8ed89f7dc1baf9b164f03d5d7933757ac1feb9beda7541795e5c0390\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-10T16:02:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.157\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-10T16:02:20Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 10 16:02:25.665: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-6573'
Apr 10 16:02:25.857: INFO: stderr: ""
Apr 10 16:02:25.857: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Apr 10 16:02:25.861: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-6573'
Apr 10 16:02:36.547: INFO: stderr: ""
Apr 10 16:02:36.547: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:02:36.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6573" for this suite.
Apr 10 16:02:42.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:02:42.682: INFO: namespace kubectl-6573 deletion completed in 6.12940013s
•SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:02:42.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0410 16:02:53.087917    3187 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 16:02:53.088: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:02:53.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6446" for this suite.
Apr 10 16:02:59.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:02:59.223: INFO: namespace gc-6446 deletion completed in 6.131464907s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:02:59.224: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-1cac3e37-5baa-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 16:02:59.525: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd" in namespace "projected-6077" to be "success or failure"
Apr 10 16:02:59.529: INFO: Pod "pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.727803ms
Apr 10 16:03:01.534: INFO: Pod "pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008580781s
STEP: Saw pod success
Apr 10 16:03:01.534: INFO: Pod "pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:03:01.539: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 16:03:01.559: INFO: Waiting for pod pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:03:01.564: INFO: Pod pod-projected-configmaps-1cace781-5baa-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:03:01.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6077" for this suite.
Apr 10 16:03:07.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:03:07.686: INFO: namespace projected-6077 deletion completed in 6.117276933s
•SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:03:07.686: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 10 16:03:07.925: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 10 16:03:12.930: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:03:13.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1750" for this suite.
Apr 10 16:03:19.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:03:20.121: INFO: namespace replication-controller-1750 deletion completed in 6.168244086s
•SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:03:20.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-cpwh
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 16:03:20.430: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cpwh" in namespace "subpath-674" to be "success or failure"
Apr 10 16:03:20.434: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619357ms
Apr 10 16:03:22.439: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.008518074s
Apr 10 16:03:24.443: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 4.012819506s
Apr 10 16:03:26.449: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 6.018691122s
Apr 10 16:03:28.454: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 8.024245591s
Apr 10 16:03:30.460: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 10.029614624s
Apr 10 16:03:32.465: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 12.034651618s
Apr 10 16:03:34.470: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 14.039535527s
Apr 10 16:03:36.475: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 16.04450701s
Apr 10 16:03:38.480: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 18.049675162s
Apr 10 16:03:40.485: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Running", Reason="", readiness=true. Elapsed: 20.054655301s
Apr 10 16:03:42.491: INFO: Pod "pod-subpath-test-configmap-cpwh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.060269039s
STEP: Saw pod success
Apr 10 16:03:42.491: INFO: Pod "pod-subpath-test-configmap-cpwh" satisfied condition "success or failure"
Apr 10 16:03:42.495: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-subpath-test-configmap-cpwh container test-container-subpath-configmap-cpwh: <nil>
STEP: delete the pod
Apr 10 16:03:42.524: INFO: Waiting for pod pod-subpath-test-configmap-cpwh to disappear
Apr 10 16:03:42.528: INFO: Pod pod-subpath-test-configmap-cpwh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cpwh
Apr 10 16:03:42.528: INFO: Deleting pod "pod-subpath-test-configmap-cpwh" in namespace "subpath-674"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:03:42.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-674" for this suite.
Apr 10 16:03:48.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:03:48.667: INFO: namespace subpath-674 deletion completed in 6.129691831s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:03:48.668: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:03:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9260" for this suite.
Apr 10 16:04:38.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:04:39.074: INFO: namespace kubelet-test-9260 deletion completed in 48.121195579s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:04:39.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 16:04:39.340: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"582b6fb7-5baa-11e9-82f7-160dbbe32fdc", Controller:(*bool)(0xc001771b2a), BlockOwnerDeletion:(*bool)(0xc001771b2b)}}
Apr 10 16:04:39.345: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5829d541-5baa-11e9-82f7-160dbbe32fdc", Controller:(*bool)(0xc002a17e86), BlockOwnerDeletion:(*bool)(0xc002a17e87)}}
Apr 10 16:04:39.350: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"582ab919-5baa-11e9-82f7-160dbbe32fdc", Controller:(*bool)(0xc0027019f6), BlockOwnerDeletion:(*bool)(0xc0027019f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:04:44.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3808" for this suite.
Apr 10 16:04:50.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:04:50.500: INFO: namespace gc-3808 deletion completed in 6.133986687s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:04:50.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7404
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 16:04:50.724: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 16:05:12.809: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.166:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7404 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 16:05:12.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 16:05:13.266: INFO: Found all expected endpoints: [netserver-0]
Apr 10 16:05:13.271: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.130:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7404 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 16:05:13.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 16:05:13.716: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:05:13.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7404" for this suite.
Apr 10 16:05:37.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:05:37.849: INFO: namespace pod-network-test-7404 deletion completed in 24.126795066s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:05:37.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9154
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 10 16:05:38.016: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 10 16:05:58.088: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.133:8080/dial?request=hostName&protocol=udp&host=100.96.1.167&port=8081&tries=1'] Namespace:pod-network-test-9154 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 16:05:58.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 16:05:58.537: INFO: Waiting for endpoints: map[]
Apr 10 16:05:58.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.0.133:8080/dial?request=hostName&protocol=udp&host=100.96.0.132&port=8081&tries=1'] Namespace:pod-network-test-9154 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 10 16:05:58.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Apr 10 16:05:59.006: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:05:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9154" for this suite.
Apr 10 16:06:23.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:06:23.146: INFO: namespace pod-network-test-9154 deletion completed in 24.13335366s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:06:23.146: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3017
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-964166ad-5baa-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 16:06:23.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd" in namespace "configmap-3017" to be "success or failure"
Apr 10 16:06:23.602: INFO: Pod "pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104493ms
Apr 10 16:06:25.607: INFO: Pod "pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008940626s
STEP: Saw pod success
Apr 10 16:06:25.607: INFO: Pod "pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:06:25.611: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 16:06:25.697: INFO: Waiting for pod pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:06:25.700: INFO: Pod pod-configmaps-96420b15-5baa-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:06:25.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3017" for this suite.
Apr 10 16:06:31.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:06:31.836: INFO: namespace configmap-3017 deletion completed in 6.132211655s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:06:31.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-9b565e03-5baa-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 16:06:32.033: INFO: Waiting up to 5m0s for pod "pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd" in namespace "secrets-1662" to be "success or failure"
Apr 10 16:06:32.037: INFO: Pod "pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825976ms
Apr 10 16:06:34.042: INFO: Pod "pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008649018s
STEP: Saw pod success
Apr 10 16:06:34.042: INFO: Pod "pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:06:34.045: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd container secret-env-test: <nil>
STEP: delete the pod
Apr 10 16:06:34.069: INFO: Waiting for pod pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:06:34.072: INFO: Pod pod-secrets-9b56f7e0-5baa-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:06:34.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1662" for this suite.
Apr 10 16:06:40.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:06:40.199: INFO: namespace secrets-1662 deletion completed in 6.122749591s
•SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:06:40.199: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:06:42.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-633" for this suite.
Apr 10 16:06:48.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:06:48.644: INFO: namespace emptydir-wrapper-633 deletion completed in 6.165365466s
•SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:06:48.645: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 10 16:06:48.913: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7852'
Apr 10 16:06:49.195: INFO: stderr: ""
Apr 10 16:06:49.195: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Apr 10 16:06:49.199: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-7852'
Apr 10 16:06:56.545: INFO: stderr: ""
Apr 10 16:06:56.545: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:06:56.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7852" for this suite.
Apr 10 16:07:02.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:07:02.683: INFO: namespace kubectl-7852 deletion completed in 6.133092069s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:07:02.684: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 10 16:07:02.929: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Apr 10 16:07:09.980: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:07:09.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-104" for this suite.
Apr 10 16:07:18.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:07:18.176: INFO: namespace pods-104 deletion completed in 8.187211451s
•SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:07:18.176: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5063
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 10 16:07:18.436: INFO: Found 0 stateful pods, waiting for 3
Apr 10 16:07:28.445: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 16:07:28.445: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 16:07:28.445: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 16:07:28.458: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5063 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 16:07:28.946: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 16:07:28.946: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 16:07:28.946: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 10 16:07:38.984: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 10 16:07:49.005: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5063 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:07:49.480: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 16:07:49.480: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 16:07:49.480: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 16:07:59.507: INFO: Waiting for StatefulSet statefulset-5063/ss2 to complete update
Apr 10 16:07:59.507: INFO: Waiting for Pod statefulset-5063/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 16:07:59.507: INFO: Waiting for Pod statefulset-5063/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 10 16:08:09.517: INFO: Waiting for StatefulSet statefulset-5063/ss2 to complete update
Apr 10 16:08:09.517: INFO: Waiting for Pod statefulset-5063/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Apr 10 16:08:19.516: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5063 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 16:08:20.061: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 16:08:20.062: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 16:08:20.062: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 16:08:30.100: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 10 16:08:40.123: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5063 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:08:40.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 16:08:40.649: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 16:08:40.649: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 16:09:00.675: INFO: Deleting all statefulset in ns statefulset-5063
Apr 10 16:09:00.679: INFO: Scaling statefulset ss2 to 0
Apr 10 16:09:30.699: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 16:09:30.704: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:09:30.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5063" for this suite.
Apr 10 16:09:36.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:09:37.262: INFO: namespace statefulset-5063 deletion completed in 6.541577469s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:09:37.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 10 16:09:37.523: INFO: Waiting up to 5m0s for pod "pod-09e68112-5bab-11e9-ae72-2e2932f810fd" in namespace "emptydir-8326" to be "success or failure"
Apr 10 16:09:37.527: INFO: Pod "pod-09e68112-5bab-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.66635ms
Apr 10 16:09:39.532: INFO: Pod "pod-09e68112-5bab-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00886094s
STEP: Saw pod success
Apr 10 16:09:39.532: INFO: Pod "pod-09e68112-5bab-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:09:39.536: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-09e68112-5bab-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 16:09:39.559: INFO: Waiting for pod pod-09e68112-5bab-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:09:39.562: INFO: Pod pod-09e68112-5bab-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:09:39.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8326" for this suite.
Apr 10 16:09:47.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:09:47.685: INFO: namespace emptydir-8326 deletion completed in 8.119379273s
•SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:09:47.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0410 16:09:53.955717    3187 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 10 16:09:53.955: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:09:53.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7269" for this suite.
Apr 10 16:09:59.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:00.075: INFO: namespace gc-7269 deletion completed in 6.115856765s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:00.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 10 16:10:00.334: INFO: Waiting up to 5m0s for pod "pod-177e1e97-5bab-11e9-ae72-2e2932f810fd" in namespace "emptydir-5712" to be "success or failure"
Apr 10 16:10:00.339: INFO: Pod "pod-177e1e97-5bab-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13183ms
Apr 10 16:10:02.344: INFO: Pod "pod-177e1e97-5bab-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009538159s
STEP: Saw pod success
Apr 10 16:10:02.344: INFO: Pod "pod-177e1e97-5bab-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:10:02.348: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-177e1e97-5bab-11e9-ae72-2e2932f810fd container test-container: <nil>
STEP: delete the pod
Apr 10 16:10:02.371: INFO: Waiting for pod pod-177e1e97-5bab-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:10:02.374: INFO: Pod pod-177e1e97-5bab-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:02.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5712" for this suite.
Apr 10 16:10:08.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:08.515: INFO: namespace emptydir-5712 deletion completed in 6.137058214s
•
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:08.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-9893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Apr 10 16:10:08.736: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9893" to be "success or failure"
Apr 10 16:10:08.740: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003584ms
Apr 10 16:10:10.746: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010137272s
STEP: Saw pod success
Apr 10 16:10:10.746: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 10 16:10:10.750: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 10 16:10:10.774: INFO: Waiting for pod pod-host-path-test to disappear
Apr 10 16:10:10.777: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:10.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9893" for this suite.
Apr 10 16:10:16.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:16.913: INFO: namespace hostpath-9893 deletion completed in 6.131483251s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:16.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Apr 10 16:10:17.123: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Apr 10 16:10:17.227: INFO: stderr: ""
Apr 10 16:10:17.227: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:17.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4761" for this suite.
Apr 10 16:10:23.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:23.365: INFO: namespace kubectl-4761 deletion completed in 6.133332948s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:23.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 10 16:10:23.521: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 10 16:10:23.530: INFO: Waiting for terminating namespaces to be deleted...
Apr 10 16:10:23.534: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l before test
Apr 10 16:10:23.545: INFO: kube-proxy-w7thh from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 10 16:10:23.545: INFO: coredns-7f7f7978c8-sb9zb from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container coredns ready: true, restart count 0
Apr 10 16:10:23.545: INFO: blackbox-exporter-6dc58dcffc-6rds9 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container blackbox-exporter ready: true, restart count 0
Apr 10 16:10:23.545: INFO: node-exporter-dsgq7 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 16:10:23.545: INFO: metrics-server-84f8f5f44f-5nx55 from kube-system started at 2019-04-10 14:34:30 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container metrics-server ready: true, restart count 0
Apr 10 16:10:23.545: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-754f8f5dcb-gkzbt from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Apr 10 16:10:23.545: INFO: calico-node-4cqp8 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.545: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 16:10:23.545: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt before test
Apr 10 16:10:23.589: INFO: vpn-shoot-798b6484c-m4qbr from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container vpn-shoot ready: true, restart count 0
Apr 10 16:10:23.589: INFO: coredns-7f7f7978c8-pbh8n from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container coredns ready: true, restart count 0
Apr 10 16:10:23.589: INFO: addons-nginx-ingress-controller-d4f8c9cc5-swsd5 from kube-system started at 2019-04-10 14:34:30 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Apr 10 16:10:23.589: INFO: node-exporter-qd9pz from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container node-exporter ready: true, restart count 0
Apr 10 16:10:23.589: INFO: addons-kubernetes-dashboard-665df4b66d-ltjvt from kube-system started at 2019-04-10 14:34:33 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 10 16:10:23.589: INFO: calico-node-vjz47 from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container calico-node ready: true, restart count 0
Apr 10 16:10:23.589: INFO: kube-proxy-86z4l from kube-system started at 2019-04-10 14:34:06 +0000 UTC (1 container statuses recorded)
Apr 10 16:10:23.589: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159428cafafed76e], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:24.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4505" for this suite.
Apr 10 16:10:32.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:32.746: INFO: namespace sched-pred-4505 deletion completed in 8.126672226s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:32.747: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:32.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7308" for this suite.
Apr 10 16:10:38.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:39.088: INFO: namespace kubelet-test-7308 deletion completed in 6.139057286s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:39.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 16:10:39.424: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd" in namespace "projected-9743" to be "success or failure"
Apr 10 16:10:39.428: INFO: Pod "downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.809155ms
Apr 10 16:10:41.432: INFO: Pod "downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008517076s
STEP: Saw pod success
Apr 10 16:10:41.432: INFO: Pod "downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:10:41.437: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 16:10:41.462: INFO: Waiting for pod downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:10:41.464: INFO: Pod downwardapi-volume-2ecbad0e-5bab-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:41.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9743" for this suite.
Apr 10 16:10:47.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:47.607: INFO: namespace projected-9743 deletion completed in 6.137909329s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:47.607: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 16:10:47.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd" in namespace "projected-2657" to be "success or failure"
Apr 10 16:10:47.838: INFO: Pod "downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381166ms
Apr 10 16:10:49.844: INFO: Pod "downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010628102s
STEP: Saw pod success
Apr 10 16:10:49.844: INFO: Pod "downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:10:49.848: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 16:10:49.871: INFO: Waiting for pod downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:10:49.874: INFO: Pod downwardapi-volume-33cf0e9b-5bab-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:10:49.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2657" for this suite.
Apr 10 16:10:55.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:10:56.045: INFO: namespace projected-2657 deletion completed in 6.16570709s
•SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:10:56.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 10 16:10:58.768: INFO: Successfully updated pod "annotationupdate38cffccf-5bab-11e9-ae72-2e2932f810fd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:11:00.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7104" for this suite.
Apr 10 16:11:24.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:11:24.934: INFO: namespace downward-api-7104 deletion completed in 24.137950749s
•SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:11:24.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 10 16:11:25.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd" in namespace "projected-9938" to be "success or failure"
Apr 10 16:11:25.138: INFO: Pod "downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31282ms
Apr 10 16:11:27.143: INFO: Pod "downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009264252s
STEP: Saw pod success
Apr 10 16:11:27.143: INFO: Pod "downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:11:27.147: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd container client-container: <nil>
STEP: delete the pod
Apr 10 16:11:27.170: INFO: Waiting for pod downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:11:27.173: INFO: Pod downwardapi-volume-4a0a5144-5bab-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:11:27.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9938" for this suite.
Apr 10 16:11:33.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:11:33.317: INFO: namespace projected-9938 deletion completed in 6.140331817s
•SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:11:33.317: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8368
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-4f0c0858-5bab-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume configMaps
Apr 10 16:11:33.538: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd" in namespace "configmap-8368" to be "success or failure"
Apr 10 16:11:33.541: INFO: Pod "pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.301992ms
Apr 10 16:11:35.547: INFO: Pod "pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008895357s
STEP: Saw pod success
Apr 10 16:11:35.547: INFO: Pod "pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:11:35.551: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 10 16:11:35.575: INFO: Waiting for pod pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:11:35.579: INFO: Pod pod-configmaps-4f0ce258-5bab-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:11:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8368" for this suite.
Apr 10 16:11:41.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:11:41.717: INFO: namespace configmap-8368 deletion completed in 6.133803838s
•SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:11:41.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9066
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9066
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9066
Apr 10 16:11:41.937: INFO: Found 0 stateful pods, waiting for 1
Apr 10 16:11:51.944: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 10 16:11:51.949: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 16:11:52.434: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 16:11:52.434: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 16:11:52.434: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 16:11:52.439: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 10 16:12:02.445: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 16:12:02.445: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 16:12:02.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999909s
Apr 10 16:12:03.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995990619s
Apr 10 16:12:04.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991077087s
Apr 10 16:12:05.476: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986220039s
Apr 10 16:12:06.481: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981250744s
Apr 10 16:12:07.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976434133s
Apr 10 16:12:08.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971358714s
Apr 10 16:12:09.495: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967077657s
Apr 10 16:12:10.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962047576s
Apr 10 16:12:11.506: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.878543ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9066
Apr 10 16:12:12.512: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:12:13.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 16:12:13.027: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 16:12:13.027: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 16:12:13.031: INFO: Found 1 stateful pods, waiting for 3
Apr 10 16:12:23.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 16:12:23.037: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 10 16:12:23.037: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 10 16:12:23.044: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 16:12:23.535: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 16:12:23.535: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 16:12:23.535: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 16:12:23.535: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 16:12:24.101: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 16:12:24.101: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 16:12:24.101: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 16:12:24.101: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 10 16:12:24.608: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 10 16:12:24.608: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 10 16:12:24.608: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 10 16:12:24.608: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 16:12:24.612: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 10 16:12:34.623: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 16:12:34.623: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 16:12:34.623: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 10 16:12:34.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999536s
Apr 10 16:12:35.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.947843617s
Apr 10 16:12:36.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.942370073s
Apr 10 16:12:37.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.937170932s
Apr 10 16:12:38.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931570302s
Apr 10 16:12:39.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.926369223s
Apr 10 16:12:40.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.921177355s
Apr 10 16:12:41.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.914882924s
Apr 10 16:12:42.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.909601244s
Apr 10 16:12:43.733: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.073673ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9066
Apr 10 16:12:44.738: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:12:45.268: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 16:12:45.268: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 16:12:45.269: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 16:12:45.269: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:12:45.699: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 10 16:12:45.699: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 10 16:12:45.699: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 10 16:12:45.699: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:12:46.186: INFO: rc: 1
Apr 10 16:12:46.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (a04ef02f41fcd43e71a66b54715b534ab9bfe0622758fa02a13a7de45abca855)
 [] <nil> 0xc001e9e6f0 exit status 1 <nil> <nil> true [0xc000754c30 0xc000754c48 0xc000754c60] [0xc000754c30 0xc000754c48 0xc000754c60] [0xc000754c40 0xc000754c58] [0x9bf9f0 0x9bf9f0] 0xc00174b380 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (a04ef02f41fcd43e71a66b54715b534ab9bfe0622758fa02a13a7de45abca855)

error:
exit status 1

Apr 10 16:12:56.186: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:12:56.401: INFO: rc: 1
Apr 10 16:12:56.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0028acba0 exit status 1 <nil> <nil> true [0xc00215cc80 0xc00215ccb8 0xc00215cce0] [0xc00215cc80 0xc00215ccb8 0xc00215cce0] [0xc00215cca8 0xc00215ccd0] [0x9bf9f0 0x9bf9f0] 0xc001a77380 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Apr 10 16:13:06.401: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:13:06.477: INFO: rc: 1
Apr 10 16:13:06.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00166b6b0 exit status 1 <nil> <nil> true [0xc002c04370 0xc002c04388 0xc002c043a0] [0xc002c04370 0xc002c04388 0xc002c043a0] [0xc002c04380 0xc002c04398] [0x9bf9f0 0x9bf9f0] 0xc001677b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:13:16.478: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:13:16.563: INFO: rc: 1
Apr 10 16:13:16.563: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028ad260 exit status 1 <nil> <nil> true [0xc00215ccf0 0xc00215cd10 0xc00215cd38] [0xc00215ccf0 0xc00215cd10 0xc00215cd38] [0xc00215cd08 0xc00215cd28] [0x9bf9f0 0x9bf9f0] 0xc001ad7da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:13:26.563: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:13:26.647: INFO: rc: 1
Apr 10 16:13:26.647: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003758690 exit status 1 <nil> <nil> true [0xc000010798 0xc000010910 0xc000010a20] [0xc000010798 0xc000010910 0xc000010a20] [0xc0000108c8 0xc0000109c0] [0x9bf9f0 0x9bf9f0] 0xc001400360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:13:36.648: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:13:36.887: INFO: rc: 1
Apr 10 16:13:36.887: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003758ed0 exit status 1 <nil> <nil> true [0xc000010af8 0xc000010b70 0xc000010c80] [0xc000010af8 0xc000010b70 0xc000010c80] [0xc000010b60 0xc000010c00] [0x9bf9f0 0x9bf9f0] 0xc001c7c120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:13:46.887: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:13:46.967: INFO: rc: 1
Apr 10 16:13:46.967: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ffc6c0 exit status 1 <nil> <nil> true [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2078 0xc0009b21a8] [0x9bf9f0 0x9bf9f0] 0xc001a768a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:13:56.968: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:13:57.067: INFO: rc: 1
Apr 10 16:13:57.067: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ffcf00 exit status 1 <nil> <nil> true [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2238 0xc0009b22a0 0xc0009b23e8] [0xc0009b2270 0xc0009b2368] [0x9bf9f0 0x9bf9f0] 0xc001a775c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:14:07.067: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:14:07.153: INFO: rc: 1
Apr 10 16:14:07.153: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ffd5c0 exit status 1 <nil> <nil> true [0xc0009b23f0 0xc0009b24c8 0xc0009b2958] [0xc0009b23f0 0xc0009b24c8 0xc0009b2958] [0xc0009b2478 0xc0009b2940] [0x9bf9f0 0x9bf9f0] 0xc002c7c1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:14:17.154: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:14:17.237: INFO: rc: 1
Apr 10 16:14:17.238: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ffdc80 exit status 1 <nil> <nil> true [0xc0009b29f8 0xc0009b2b10 0xc0009b2c00] [0xc0009b29f8 0xc0009b2b10 0xc0009b2c00] [0xc0009b2ad8 0xc0009b2be0] [0x9bf9f0 0x9bf9f0] 0xc002c7c6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:14:27.238: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:14:27.324: INFO: rc: 1
Apr 10 16:14:27.324: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00050be30 exit status 1 <nil> <nil> true [0xc000010cc8 0xc000010df8 0xc000010fe8] [0xc000010cc8 0xc000010df8 0xc000010fe8] [0xc000010d10 0xc000010fa8] [0x9bf9f0 0x9bf9f0] 0xc001938180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:14:37.325: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:14:37.401: INFO: rc: 1
Apr 10 16:14:37.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0e510 exit status 1 <nil> <nil> true [0xc000011000 0xc000011098 0xc000011140] [0xc000011000 0xc000011098 0xc000011140] [0xc000011060 0xc000011118] [0x9bf9f0 0x9bf9f0] 0xc001938780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:14:47.401: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:14:47.477: INFO: rc: 1
Apr 10 16:14:47.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00232a990 exit status 1 <nil> <nil> true [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a2e0 0xc000a1a3c8] [0x9bf9f0 0x9bf9f0] 0xc00262e420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:14:57.478: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:14:57.562: INFO: rc: 1
Apr 10 16:14:57.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0ebd0 exit status 1 <nil> <nil> true [0xc000011228 0xc0000113f8 0xc000011598] [0xc000011228 0xc0000113f8 0xc000011598] [0xc000011308 0xc000011540] [0x9bf9f0 0x9bf9f0] 0xc001939200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:15:07.562: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:15:07.635: INFO: rc: 1
Apr 10 16:15:07.635: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ef6720 exit status 1 <nil> <nil> true [0xc002c04000 0xc002c04018 0xc002c04030] [0xc002c04000 0xc002c04018 0xc002c04030] [0xc002c04010 0xc002c04028] [0x9bf9f0 0x9bf9f0] 0xc0022aa7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:15:17.636: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:15:17.715: INFO: rc: 1
Apr 10 16:15:17.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0f5c0 exit status 1 <nil> <nil> true [0xc0000115d8 0xc000011780 0xc000011810] [0xc0000115d8 0xc000011780 0xc000011810] [0xc0000116d8 0xc0000117c8] [0x9bf9f0 0x9bf9f0] 0xc001939b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:15:27.716: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:15:27.811: INFO: rc: 1
Apr 10 16:15:27.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0037580f0 exit status 1 <nil> <nil> true [0xc000010798 0xc000010910 0xc000010a20] [0xc000010798 0xc000010910 0xc000010a20] [0xc0000108c8 0xc0000109c0] [0x9bf9f0 0x9bf9f0] 0xc001a768a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:15:37.811: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:15:37.895: INFO: rc: 1
Apr 10 16:15:37.895: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0e6f0 exit status 1 <nil> <nil> true [0xc002c04000 0xc002c04018 0xc002c04030] [0xc002c04000 0xc002c04018 0xc002c04030] [0xc002c04010 0xc002c04028] [0x9bf9f0 0x9bf9f0] 0xc001c7c120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:15:47.895: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:15:47.971: INFO: rc: 1
Apr 10 16:15:47.971: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0f110 exit status 1 <nil> <nil> true [0xc002c04038 0xc002c04050 0xc002c04068] [0xc002c04038 0xc002c04050 0xc002c04068] [0xc002c04048 0xc002c04060] [0x9bf9f0 0x9bf9f0] 0xc001400360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:15:57.972: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:15:58.055: INFO: rc: 1
Apr 10 16:15:58.055: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003758810 exit status 1 <nil> <nil> true [0xc000010af8 0xc000010b70 0xc000010c80] [0xc000010af8 0xc000010b70 0xc000010c80] [0xc000010b60 0xc000010c00] [0x9bf9f0 0x9bf9f0] 0xc001a775c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:16:08.055: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:16:08.147: INFO: rc: 1
Apr 10 16:16:08.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0037594d0 exit status 1 <nil> <nil> true [0xc000010cc8 0xc000010df8 0xc000010fe8] [0xc000010cc8 0xc000010df8 0xc000010fe8] [0xc000010d10 0xc000010fa8] [0x9bf9f0 0x9bf9f0] 0xc001ad7da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:16:18.147: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:16:18.222: INFO: rc: 1
Apr 10 16:16:18.222: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00232a960 exit status 1 <nil> <nil> true [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a030 0xc000a1a358 0xc000a1a3f8] [0xc000a1a2e0 0xc000a1a3c8] [0x9bf9f0 0x9bf9f0] 0xc0019383c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:16:28.222: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:16:28.316: INFO: rc: 1
Apr 10 16:16:28.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0f860 exit status 1 <nil> <nil> true [0xc002c04070 0xc002c04088 0xc002c040c8] [0xc002c04070 0xc002c04088 0xc002c040c8] [0xc002c04080 0xc002c040b0] [0x9bf9f0 0x9bf9f0] 0xc0022aa7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:16:38.316: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:16:38.403: INFO: rc: 1
Apr 10 16:16:38.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ffc240 exit status 1 <nil> <nil> true [0xc002c040d0 0xc002c040e8 0xc002c04108] [0xc002c040d0 0xc002c040e8 0xc002c04108] [0xc002c040e0 0xc002c04100] [0x9bf9f0 0x9bf9f0] 0xc0022ab380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:16:48.404: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:16:48.485: INFO: rc: 1
Apr 10 16:16:48.485: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ffc960 exit status 1 <nil> <nil> true [0xc002c04110 0xc002c04128 0xc002c04140] [0xc002c04110 0xc002c04128 0xc002c04140] [0xc002c04120 0xc002c04138] [0x9bf9f0 0x9bf9f0] 0xc00262e060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:16:58.485: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:16:58.574: INFO: rc: 1
Apr 10 16:16:58.574: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002594660 exit status 1 <nil> <nil> true [0xc000011000 0xc000011098 0xc000011140] [0xc000011000 0xc000011098 0xc000011140] [0xc000011060 0xc000011118] [0x9bf9f0 0x9bf9f0] 0xc002c7c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:17:08.574: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:17:08.670: INFO: rc: 1
Apr 10 16:17:08.670: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ef66f0 exit status 1 <nil> <nil> true [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2018 0xc0009b2148 0xc0009b2208] [0xc0009b2078 0xc0009b21a8] [0x9bf9f0 0x9bf9f0] 0xc0024490e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:17:18.670: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:17:18.757: INFO: rc: 1
Apr 10 16:17:18.757: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002594d20 exit status 1 <nil> <nil> true [0xc000011228 0xc0000113f8 0xc000011598] [0xc000011228 0xc0000113f8 0xc000011598] [0xc000011308 0xc000011540] [0x9bf9f0 0x9bf9f0] 0xc0023ba060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:17:28.757: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:17:28.847: INFO: rc: 1
Apr 10 16:17:28.847: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0e690 exit status 1 <nil> <nil> true [0xc0009b2048 0xc0009b2178 0xc0009b2238] [0xc0009b2048 0xc0009b2178 0xc0009b2238] [0xc0009b2148 0xc0009b2208] [0x9bf9f0 0x9bf9f0] 0xc002c7c540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:17:38.848: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:17:38.942: INFO: rc: 1
Apr 10 16:17:38.942: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/bin/kubectl [kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e0f050 exit status 1 <nil> <nil> true [0xc0009b2250 0xc0009b2330 0xc0009b23f0] [0xc0009b2250 0xc0009b2330 0xc0009b23f0] [0xc0009b22a0 0xc0009b23e8] [0x9bf9f0 0x9bf9f0] 0xc0022aa000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Apr 10 16:17:48.943: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-9066 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 10 16:17:49.023: INFO: rc: 1
Apr 10 16:17:49.023: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Apr 10 16:17:49.024: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 10 16:17:49.039: INFO: Deleting all statefulset in ns statefulset-9066
Apr 10 16:17:49.043: INFO: Scaling statefulset ss to 0
Apr 10 16:17:49.056: INFO: Waiting for statefulset status.replicas updated to 0
Apr 10 16:17:49.060: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:17:49.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9066" for this suite.
Apr 10 16:17:55.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:17:55.252: INFO: namespace statefulset-9066 deletion completed in 6.175301524s

• [SLOW TEST:373.535 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:17:55.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 10 16:17:55.425: INFO: namespace kubectl-535
Apr 10 16:17:55.425: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-535'
Apr 10 16:17:55.797: INFO: stderr: ""
Apr 10 16:17:55.797: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 10 16:17:56.802: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 16:17:56.802: INFO: Found 0 / 1
Apr 10 16:17:57.802: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 16:17:57.803: INFO: Found 0 / 1
Apr 10 16:17:58.802: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 16:17:58.802: INFO: Found 1 / 1
Apr 10 16:17:58.802: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 10 16:17:58.806: INFO: Selector matched 1 pods for map[app:redis]
Apr 10 16:17:58.806: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 10 16:17:58.806: INFO: wait on redis-master startup in kubectl-535 
Apr 10 16:17:58.806: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-kdv54 redis-master --namespace=kubectl-535'
Apr 10 16:17:58.908: INFO: stderr: ""
Apr 10 16:17:58.908: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 10 Apr 16:17:56.735 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 10 Apr 16:17:56.735 # Server started, Redis version 3.2.12\n1:M 10 Apr 16:17:56.735 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 10 Apr 16:17:56.735 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr 10 16:17:58.909: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-535'
Apr 10 16:17:59.012: INFO: stderr: ""
Apr 10 16:17:59.012: INFO: stdout: "service/rm2 exposed\n"
Apr 10 16:17:59.016: INFO: Service rm2 in namespace kubectl-535 found.
STEP: exposing service
Apr 10 16:18:01.024: INFO: Running '/bin/kubectl --server=https://api.tm-f85cf.it.internal.dev.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-535'
Apr 10 16:18:01.132: INFO: stderr: ""
Apr 10 16:18:01.132: INFO: stdout: "service/rm3 exposed\n"
Apr 10 16:18:01.135: INFO: Service rm3 in namespace kubectl-535 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:18:03.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-535" for this suite.
Apr 10 16:18:25.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:18:25.317: INFO: namespace kubectl-535 deletion completed in 22.169558734s
•SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:18:25.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 10 16:18:25.527: INFO: Creating ReplicaSet my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd
Apr 10 16:18:25.535: INFO: Pod name my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd: Found 0 pods out of 1
Apr 10 16:18:30.540: INFO: Pod name my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd: Found 1 pods out of 1
Apr 10 16:18:30.540: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd" is running
Apr 10 16:18:30.544: INFO: Pod "my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd-bmx7h" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 16:18:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 16:18:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 16:18:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-10 16:18:25 +0000 UTC Reason: Message:}])
Apr 10 16:18:30.544: INFO: Trying to dial the pod
Apr 10 16:18:35.643: INFO: Controller my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd: Got expected result from replica 1 [my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd-bmx7h]: "my-hostname-basic-449ecc5e-5bac-11e9-ae72-2e2932f810fd-bmx7h", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:18:35.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6546" for this suite.
Apr 10 16:18:41.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:18:41.779: INFO: namespace replicaset-6546 deletion completed in 6.13152521s
•SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:18:41.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-4dfh
STEP: Creating a pod to test atomic-volume-subpath
Apr 10 16:18:42.133: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4dfh" in namespace "subpath-3109" to be "success or failure"
Apr 10 16:18:42.137: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773538ms
Apr 10 16:18:44.148: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01501793s
Apr 10 16:18:46.153: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 4.019660001s
Apr 10 16:18:48.158: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 6.024759685s
Apr 10 16:18:50.163: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 8.029439655s
Apr 10 16:18:52.168: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 10.034418877s
Apr 10 16:18:54.173: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 12.039385101s
Apr 10 16:18:56.178: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 14.044533956s
Apr 10 16:18:58.183: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 16.049248453s
Apr 10 16:19:00.187: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 18.053459448s
Apr 10 16:19:02.191: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Running", Reason="", readiness=true. Elapsed: 20.057989275s
Apr 10 16:19:04.197: INFO: Pod "pod-subpath-test-secret-4dfh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063923485s
STEP: Saw pod success
Apr 10 16:19:04.197: INFO: Pod "pod-subpath-test-secret-4dfh" satisfied condition "success or failure"
Apr 10 16:19:04.202: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-xqbxt pod pod-subpath-test-secret-4dfh container test-container-subpath-secret-4dfh: <nil>
STEP: delete the pod
Apr 10 16:19:04.224: INFO: Waiting for pod pod-subpath-test-secret-4dfh to disappear
Apr 10 16:19:04.227: INFO: Pod pod-subpath-test-secret-4dfh no longer exists
STEP: Deleting pod pod-subpath-test-secret-4dfh
Apr 10 16:19:04.227: INFO: Deleting pod "pod-subpath-test-secret-4dfh" in namespace "subpath-3109"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:19:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3109" for this suite.
Apr 10 16:19:10.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:19:10.367: INFO: namespace subpath-3109 deletion completed in 6.131242208s
•SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 10 16:19:10.368: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-5f71c387-5bac-11e9-ae72-2e2932f810fd
STEP: Creating a pod to test consume secrets
Apr 10 16:19:10.542: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd" in namespace "projected-1491" to be "success or failure"
Apr 10 16:19:10.545: INFO: Pod "pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.387012ms
Apr 10 16:19:12.550: INFO: Pod "pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007957612s
STEP: Saw pod success
Apr 10 16:19:12.550: INFO: Pod "pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd" satisfied condition "success or failure"
Apr 10 16:19:12.554: INFO: Trying to get logs from node shoot--it--tm-f85cf-cpu-worker-z1-5d7f88c65-qql5l pod pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 10 16:19:12.578: INFO: Waiting for pod pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd to disappear
Apr 10 16:19:12.580: INFO: Pod pod-projected-secrets-5f725e8d-5bac-11e9-ae72-2e2932f810fd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 10 16:19:12.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1491" for this suite.
Apr 10 16:19:18.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 10 16:19:18.755: INFO: namespace projected-1491 deletion completed in 6.170876177s
•SSSSSSSApr 10 16:19:18.755: INFO: Running AfterSuite actions on all nodes
Apr 10 16:19:18.755: INFO: Running AfterSuite actions on node 1
Apr 10 16:19:18.755: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5900.667 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Flaked | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h38m21.806751348s
Test Suite Passed
