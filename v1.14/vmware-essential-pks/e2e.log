I0429 10:13:54.367872      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-008546913
I0429 10:13:54.367961      15 e2e.go:240] Starting e2e run "7d8e90f8-6a67-11e9-aeaf-0a580af401c3" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1556532833 - Will randomize all specs
Will run 204 of 3584 specs

Apr 29 10:13:54.540: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 10:13:54.543: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 29 10:13:54.563: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 29 10:13:54.593: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 29 10:13:54.593: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Apr 29 10:13:54.593: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 29 10:13:54.602: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Apr 29 10:13:54.603: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Apr 29 10:13:54.603: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Apr 29 10:13:54.603: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Apr 29 10:13:54.603: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Apr 29 10:13:54.603: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 29 10:13:54.603: INFO: e2e test version: v1.14.0
Apr 29 10:13:54.604: INFO: kube-apiserver version: v1.14.1+vmware.1
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:13:54.604: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
Apr 29 10:13:54.628: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7e65fc76-6a67-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:13:54.640: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3" in namespace "projected-9820" to be "success or failure"
Apr 29 10:13:54.648: INFO: Pod "pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801896ms
Apr 29 10:13:56.653: INFO: Pod "pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012732912s
STEP: Saw pod success
Apr 29 10:13:56.653: INFO: Pod "pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:13:56.657: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:13:56.678: INFO: Waiting for pod pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:13:56.681: INFO: Pod pod-projected-configmaps-7e666233-6a67-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:13:56.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9820" for this suite.
Apr 29 10:14:02.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:14:02.771: INFO: namespace projected-9820 deletion completed in 6.086896161s

• [SLOW TEST:8.167 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:14:02.772: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 29 10:14:05.339: INFO: Successfully updated pod "labelsupdate8344398d-6a67-11e9-aeaf-0a580af401c3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:14:09.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2151" for this suite.
Apr 29 10:14:31.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:14:31.465: INFO: namespace projected-2151 deletion completed in 22.095095963s

• [SLOW TEST:28.694 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:14:31.471: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 29 10:14:31.495: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 10:14:31.503: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 10:14:31.505: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-2 before test
Apr 29 10:14:31.512: INFO: kube-proxy-x8ptf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:14:31.512: INFO: 	Container kube-proxy ready: true, restart count 2
Apr 29 10:14:31.513: INFO: kube-flannel-ds-amd64-c4rpf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:14:31.513: INFO: 	Container kube-flannel ready: true, restart count 2
Apr 29 10:14:31.513: INFO: sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-ln6c4 from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 10:14:31.513: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 29 10:14:31.513: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 10:14:31.513: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-3 before test
Apr 29 10:14:31.522: INFO: kube-flannel-ds-amd64-5d5mf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:14:31.522: INFO: 	Container kube-flannel ready: true, restart count 2
Apr 29 10:14:31.522: INFO: kube-proxy-h9j6t from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:14:31.522: INFO: 	Container kube-proxy ready: true, restart count 2
Apr 29 10:14:31.522: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-29 10:13:51 +0000 UTC (1 container statuses recorded)
Apr 29 10:14:31.522: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 10:14:31.522: INFO: sonobuoy-e2e-job-225965689e404c5d from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 10:14:31.522: INFO: 	Container e2e ready: true, restart count 0
Apr 29 10:14:31.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 10:14:31.522: INFO: sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-km6th from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 10:14:31.522: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 29 10:14:31.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1599ea665be8754a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:14:32.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6365" for this suite.
Apr 29 10:14:38.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:14:38.628: INFO: namespace sched-pred-6365 deletion completed in 6.084844906s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.157 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:14:38.628: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 29 10:14:38.659: INFO: Waiting up to 5m0s for pod "pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3" in namespace "emptydir-7870" to be "success or failure"
Apr 29 10:14:38.662: INFO: Pod "pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076544ms
Apr 29 10:14:40.666: INFO: Pod "pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007021583s
STEP: Saw pod success
Apr 29 10:14:40.666: INFO: Pod "pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:14:40.668: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:14:40.689: INFO: Waiting for pod pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:14:40.691: INFO: Pod pod-98a31b4d-6a67-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:14:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7870" for this suite.
Apr 29 10:14:46.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:14:46.786: INFO: namespace emptydir-7870 deletion completed in 6.091133742s

• [SLOW TEST:8.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:14:46.786: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0429 10:14:47.427161      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 29 10:14:47.427: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:14:47.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7639" for this suite.
Apr 29 10:14:53.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:14:53.529: INFO: namespace gc-7639 deletion completed in 6.096732154s

• [SLOW TEST:6.743 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:14:53.531: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 10:15:17.606: INFO: DNS probes using dns-8278/dns-test-a185867a-6a67-11e9-aeaf-0a580af401c3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:15:17.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8278" for this suite.
Apr 29 10:15:23.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:15:23.702: INFO: namespace dns-8278 deletion completed in 6.08264325s

• [SLOW TEST:30.171 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:15:23.702: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1801
I0429 10:15:23.729002      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1801, replica count: 1
I0429 10:15:24.779984      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:15:25.780396      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 10:15:25.890: INFO: Created: latency-svc-xhtj2
Apr 29 10:15:25.896: INFO: Got endpoints: latency-svc-xhtj2 [15.435171ms]
Apr 29 10:15:25.941: INFO: Created: latency-svc-tsq8p
Apr 29 10:15:25.977: INFO: Got endpoints: latency-svc-tsq8p [81.005246ms]
Apr 29 10:15:25.991: INFO: Created: latency-svc-lb4g6
Apr 29 10:15:26.001: INFO: Got endpoints: latency-svc-lb4g6 [104.959524ms]
Apr 29 10:15:26.005: INFO: Created: latency-svc-657hv
Apr 29 10:15:26.009: INFO: Got endpoints: latency-svc-657hv [113.405507ms]
Apr 29 10:15:26.015: INFO: Created: latency-svc-6vjl5
Apr 29 10:15:26.019: INFO: Got endpoints: latency-svc-6vjl5 [122.322848ms]
Apr 29 10:15:26.024: INFO: Created: latency-svc-65c2f
Apr 29 10:15:26.027: INFO: Got endpoints: latency-svc-65c2f [130.844236ms]
Apr 29 10:15:26.034: INFO: Created: latency-svc-cm7w8
Apr 29 10:15:26.036: INFO: Got endpoints: latency-svc-cm7w8 [139.946204ms]
Apr 29 10:15:26.046: INFO: Created: latency-svc-p7z87
Apr 29 10:15:26.049: INFO: Got endpoints: latency-svc-p7z87 [152.614148ms]
Apr 29 10:15:26.057: INFO: Created: latency-svc-hslfg
Apr 29 10:15:26.066: INFO: Got endpoints: latency-svc-hslfg [169.445222ms]
Apr 29 10:15:26.087: INFO: Created: latency-svc-5bcn2
Apr 29 10:15:26.089: INFO: Got endpoints: latency-svc-5bcn2 [192.514886ms]
Apr 29 10:15:26.100: INFO: Created: latency-svc-87rlc
Apr 29 10:15:26.102: INFO: Got endpoints: latency-svc-87rlc [205.868062ms]
Apr 29 10:15:26.117: INFO: Created: latency-svc-4nqp6
Apr 29 10:15:26.118: INFO: Got endpoints: latency-svc-4nqp6 [221.273723ms]
Apr 29 10:15:26.126: INFO: Created: latency-svc-xpk5h
Apr 29 10:15:26.130: INFO: Got endpoints: latency-svc-xpk5h [233.781174ms]
Apr 29 10:15:26.137: INFO: Created: latency-svc-ngg28
Apr 29 10:15:26.140: INFO: Got endpoints: latency-svc-ngg28 [243.18784ms]
Apr 29 10:15:26.141: INFO: Created: latency-svc-dmhch
Apr 29 10:15:26.149: INFO: Got endpoints: latency-svc-dmhch [252.503294ms]
Apr 29 10:15:26.160: INFO: Created: latency-svc-pqfh4
Apr 29 10:15:26.161: INFO: Got endpoints: latency-svc-pqfh4 [264.493798ms]
Apr 29 10:15:26.171: INFO: Created: latency-svc-ctp75
Apr 29 10:15:26.173: INFO: Got endpoints: latency-svc-ctp75 [196.49264ms]
Apr 29 10:15:26.181: INFO: Created: latency-svc-fz522
Apr 29 10:15:26.185: INFO: Got endpoints: latency-svc-fz522 [183.99177ms]
Apr 29 10:15:26.202: INFO: Created: latency-svc-7fvtv
Apr 29 10:15:26.204: INFO: Got endpoints: latency-svc-7fvtv [194.904276ms]
Apr 29 10:15:26.215: INFO: Created: latency-svc-lxggl
Apr 29 10:15:26.220: INFO: Created: latency-svc-rhwsb
Apr 29 10:15:26.223: INFO: Got endpoints: latency-svc-rhwsb [195.688924ms]
Apr 29 10:15:26.229: INFO: Got endpoints: latency-svc-lxggl [210.185449ms]
Apr 29 10:15:26.236: INFO: Created: latency-svc-7gwrs
Apr 29 10:15:26.240: INFO: Got endpoints: latency-svc-7gwrs [203.43153ms]
Apr 29 10:15:26.262: INFO: Created: latency-svc-2lldk
Apr 29 10:15:26.272: INFO: Got endpoints: latency-svc-2lldk [223.36366ms]
Apr 29 10:15:26.285: INFO: Created: latency-svc-84kgs
Apr 29 10:15:26.286: INFO: Got endpoints: latency-svc-84kgs [220.332884ms]
Apr 29 10:15:26.311: INFO: Created: latency-svc-xtv59
Apr 29 10:15:26.318: INFO: Got endpoints: latency-svc-xtv59 [229.266216ms]
Apr 29 10:15:26.329: INFO: Created: latency-svc-khksn
Apr 29 10:15:26.329: INFO: Got endpoints: latency-svc-khksn [227.033899ms]
Apr 29 10:15:26.339: INFO: Created: latency-svc-gnjzz
Apr 29 10:15:26.345: INFO: Got endpoints: latency-svc-gnjzz [226.895484ms]
Apr 29 10:15:26.351: INFO: Created: latency-svc-bv6r4
Apr 29 10:15:26.355: INFO: Got endpoints: latency-svc-bv6r4 [224.992579ms]
Apr 29 10:15:26.370: INFO: Created: latency-svc-npj9b
Apr 29 10:15:26.370: INFO: Got endpoints: latency-svc-npj9b [230.221717ms]
Apr 29 10:15:26.377: INFO: Created: latency-svc-5pc29
Apr 29 10:15:26.382: INFO: Got endpoints: latency-svc-5pc29 [232.866368ms]
Apr 29 10:15:26.395: INFO: Created: latency-svc-s8tpw
Apr 29 10:15:26.395: INFO: Got endpoints: latency-svc-s8tpw [233.859873ms]
Apr 29 10:15:26.408: INFO: Created: latency-svc-pmg4r
Apr 29 10:15:26.412: INFO: Got endpoints: latency-svc-pmg4r [238.618704ms]
Apr 29 10:15:26.427: INFO: Created: latency-svc-dxpm9
Apr 29 10:15:26.432: INFO: Got endpoints: latency-svc-dxpm9 [247.138433ms]
Apr 29 10:15:26.440: INFO: Created: latency-svc-hs7bj
Apr 29 10:15:26.445: INFO: Got endpoints: latency-svc-hs7bj [240.622534ms]
Apr 29 10:15:26.461: INFO: Created: latency-svc-xjm8v
Apr 29 10:15:26.466: INFO: Got endpoints: latency-svc-xjm8v [242.778536ms]
Apr 29 10:15:26.493: INFO: Created: latency-svc-rrfpp
Apr 29 10:15:26.493: INFO: Created: latency-svc-6gqf5
Apr 29 10:15:26.508: INFO: Created: latency-svc-nwjts
Apr 29 10:15:26.512: INFO: Got endpoints: latency-svc-rrfpp [283.350719ms]
Apr 29 10:15:26.522: INFO: Got endpoints: latency-svc-nwjts [249.560947ms]
Apr 29 10:15:26.522: INFO: Got endpoints: latency-svc-6gqf5 [282.590095ms]
Apr 29 10:15:26.530: INFO: Created: latency-svc-plkzq
Apr 29 10:15:26.531: INFO: Got endpoints: latency-svc-plkzq [244.945917ms]
Apr 29 10:15:26.546: INFO: Created: latency-svc-hwfz4
Apr 29 10:15:26.546: INFO: Got endpoints: latency-svc-hwfz4 [227.967662ms]
Apr 29 10:15:26.559: INFO: Created: latency-svc-cghk4
Apr 29 10:15:26.562: INFO: Got endpoints: latency-svc-cghk4 [233.029147ms]
Apr 29 10:15:26.571: INFO: Created: latency-svc-7vccs
Apr 29 10:15:26.572: INFO: Got endpoints: latency-svc-7vccs [227.618992ms]
Apr 29 10:15:26.583: INFO: Created: latency-svc-bdktt
Apr 29 10:15:26.584: INFO: Got endpoints: latency-svc-bdktt [228.897671ms]
Apr 29 10:15:26.594: INFO: Created: latency-svc-t5lzm
Apr 29 10:15:26.602: INFO: Got endpoints: latency-svc-t5lzm [231.641608ms]
Apr 29 10:15:26.603: INFO: Created: latency-svc-mxpcz
Apr 29 10:15:26.628: INFO: Created: latency-svc-dx8gs
Apr 29 10:15:26.635: INFO: Created: latency-svc-hq96h
Apr 29 10:15:26.649: INFO: Created: latency-svc-kpbf6
Apr 29 10:15:26.649: INFO: Got endpoints: latency-svc-mxpcz [267.220967ms]
Apr 29 10:15:26.657: INFO: Created: latency-svc-pgzq2
Apr 29 10:15:26.717: INFO: Got endpoints: latency-svc-dx8gs [321.961072ms]
Apr 29 10:15:26.726: INFO: Created: latency-svc-zj7sv
Apr 29 10:15:26.734: INFO: Created: latency-svc-mjhlw
Apr 29 10:15:26.743: INFO: Created: latency-svc-8xcs8
Apr 29 10:15:26.752: INFO: Got endpoints: latency-svc-hq96h [339.429091ms]
Apr 29 10:15:26.755: INFO: Created: latency-svc-nvr7p
Apr 29 10:15:26.772: INFO: Created: latency-svc-skp5s
Apr 29 10:15:26.790: INFO: Created: latency-svc-b7cxd
Apr 29 10:15:26.799: INFO: Got endpoints: latency-svc-kpbf6 [366.804543ms]
Apr 29 10:15:26.801: INFO: Created: latency-svc-v256f
Apr 29 10:15:26.814: INFO: Created: latency-svc-kcswc
Apr 29 10:15:26.828: INFO: Created: latency-svc-qfnss
Apr 29 10:15:26.840: INFO: Created: latency-svc-w2b6s
Apr 29 10:15:26.847: INFO: Created: latency-svc-v7bns
Apr 29 10:15:26.850: INFO: Got endpoints: latency-svc-pgzq2 [405.350414ms]
Apr 29 10:15:26.859: INFO: Created: latency-svc-m6qvm
Apr 29 10:15:26.873: INFO: Created: latency-svc-zwhmk
Apr 29 10:15:26.880: INFO: Created: latency-svc-ldkg7
Apr 29 10:15:26.888: INFO: Created: latency-svc-4znfz
Apr 29 10:15:26.899: INFO: Got endpoints: latency-svc-zj7sv [433.556345ms]
Apr 29 10:15:26.910: INFO: Created: latency-svc-4sh9n
Apr 29 10:15:26.947: INFO: Got endpoints: latency-svc-mjhlw [435.030237ms]
Apr 29 10:15:26.970: INFO: Created: latency-svc-lkk6j
Apr 29 10:15:26.999: INFO: Got endpoints: latency-svc-8xcs8 [476.995246ms]
Apr 29 10:15:27.012: INFO: Created: latency-svc-x6scc
Apr 29 10:15:27.048: INFO: Got endpoints: latency-svc-nvr7p [526.070894ms]
Apr 29 10:15:27.059: INFO: Created: latency-svc-z7gtj
Apr 29 10:15:27.101: INFO: Got endpoints: latency-svc-skp5s [570.030208ms]
Apr 29 10:15:27.114: INFO: Created: latency-svc-dxw9j
Apr 29 10:15:27.148: INFO: Got endpoints: latency-svc-b7cxd [601.196214ms]
Apr 29 10:15:27.160: INFO: Created: latency-svc-pzld2
Apr 29 10:15:27.198: INFO: Got endpoints: latency-svc-v256f [635.284194ms]
Apr 29 10:15:27.222: INFO: Created: latency-svc-bngnv
Apr 29 10:15:27.250: INFO: Got endpoints: latency-svc-kcswc [677.780899ms]
Apr 29 10:15:27.264: INFO: Created: latency-svc-mh8jv
Apr 29 10:15:27.299: INFO: Got endpoints: latency-svc-qfnss [714.606251ms]
Apr 29 10:15:27.321: INFO: Created: latency-svc-d8xmd
Apr 29 10:15:27.351: INFO: Got endpoints: latency-svc-w2b6s [748.884293ms]
Apr 29 10:15:27.365: INFO: Created: latency-svc-c4sjm
Apr 29 10:15:27.403: INFO: Got endpoints: latency-svc-v7bns [753.736359ms]
Apr 29 10:15:27.419: INFO: Created: latency-svc-mv8x8
Apr 29 10:15:27.448: INFO: Got endpoints: latency-svc-m6qvm [730.840109ms]
Apr 29 10:15:27.463: INFO: Created: latency-svc-t88sb
Apr 29 10:15:27.498: INFO: Got endpoints: latency-svc-zwhmk [746.736135ms]
Apr 29 10:15:27.519: INFO: Created: latency-svc-fvdhq
Apr 29 10:15:27.550: INFO: Got endpoints: latency-svc-ldkg7 [750.874ms]
Apr 29 10:15:27.563: INFO: Created: latency-svc-fdk7v
Apr 29 10:15:27.598: INFO: Got endpoints: latency-svc-4znfz [747.612499ms]
Apr 29 10:15:27.615: INFO: Created: latency-svc-vbqkj
Apr 29 10:15:27.650: INFO: Got endpoints: latency-svc-4sh9n [750.503781ms]
Apr 29 10:15:27.664: INFO: Created: latency-svc-gcxqk
Apr 29 10:15:27.700: INFO: Got endpoints: latency-svc-lkk6j [752.393772ms]
Apr 29 10:15:27.714: INFO: Created: latency-svc-cvsbv
Apr 29 10:15:27.748: INFO: Got endpoints: latency-svc-x6scc [749.341573ms]
Apr 29 10:15:27.761: INFO: Created: latency-svc-qxh7l
Apr 29 10:15:27.798: INFO: Got endpoints: latency-svc-z7gtj [749.5273ms]
Apr 29 10:15:27.810: INFO: Created: latency-svc-7pvgx
Apr 29 10:15:27.847: INFO: Got endpoints: latency-svc-dxw9j [746.059198ms]
Apr 29 10:15:27.860: INFO: Created: latency-svc-lj2fc
Apr 29 10:15:27.899: INFO: Got endpoints: latency-svc-pzld2 [751.02804ms]
Apr 29 10:15:27.913: INFO: Created: latency-svc-z9x7v
Apr 29 10:15:27.950: INFO: Got endpoints: latency-svc-bngnv [752.125145ms]
Apr 29 10:15:27.965: INFO: Created: latency-svc-75nm4
Apr 29 10:15:27.998: INFO: Got endpoints: latency-svc-mh8jv [747.272743ms]
Apr 29 10:15:28.009: INFO: Created: latency-svc-md8cj
Apr 29 10:15:28.048: INFO: Got endpoints: latency-svc-d8xmd [748.44618ms]
Apr 29 10:15:28.057: INFO: Created: latency-svc-jh7wb
Apr 29 10:15:28.102: INFO: Got endpoints: latency-svc-c4sjm [751.300995ms]
Apr 29 10:15:28.120: INFO: Created: latency-svc-xw822
Apr 29 10:15:28.150: INFO: Got endpoints: latency-svc-mv8x8 [746.756396ms]
Apr 29 10:15:28.166: INFO: Created: latency-svc-5wqqf
Apr 29 10:15:28.198: INFO: Got endpoints: latency-svc-t88sb [749.772285ms]
Apr 29 10:15:28.214: INFO: Created: latency-svc-8wg64
Apr 29 10:15:28.251: INFO: Got endpoints: latency-svc-fvdhq [752.742492ms]
Apr 29 10:15:28.267: INFO: Created: latency-svc-jxsg5
Apr 29 10:15:28.299: INFO: Got endpoints: latency-svc-fdk7v [748.910642ms]
Apr 29 10:15:28.311: INFO: Created: latency-svc-l8mmd
Apr 29 10:15:28.351: INFO: Got endpoints: latency-svc-vbqkj [752.005642ms]
Apr 29 10:15:28.363: INFO: Created: latency-svc-g6rx2
Apr 29 10:15:28.399: INFO: Got endpoints: latency-svc-gcxqk [748.957276ms]
Apr 29 10:15:28.410: INFO: Created: latency-svc-6d92f
Apr 29 10:15:28.448: INFO: Got endpoints: latency-svc-cvsbv [747.43312ms]
Apr 29 10:15:28.459: INFO: Created: latency-svc-qdndl
Apr 29 10:15:28.499: INFO: Got endpoints: latency-svc-qxh7l [750.344089ms]
Apr 29 10:15:28.514: INFO: Created: latency-svc-sn4pn
Apr 29 10:15:28.548: INFO: Got endpoints: latency-svc-7pvgx [749.679672ms]
Apr 29 10:15:28.621: INFO: Got endpoints: latency-svc-lj2fc [773.55658ms]
Apr 29 10:15:28.624: INFO: Created: latency-svc-pwjjl
Apr 29 10:15:28.636: INFO: Created: latency-svc-r5c4r
Apr 29 10:15:28.650: INFO: Got endpoints: latency-svc-z9x7v [750.859969ms]
Apr 29 10:15:28.664: INFO: Created: latency-svc-gnw5w
Apr 29 10:15:28.699: INFO: Got endpoints: latency-svc-75nm4 [748.513421ms]
Apr 29 10:15:28.714: INFO: Created: latency-svc-q5hdn
Apr 29 10:15:28.748: INFO: Got endpoints: latency-svc-md8cj [750.08038ms]
Apr 29 10:15:28.758: INFO: Created: latency-svc-hjtwf
Apr 29 10:15:28.799: INFO: Got endpoints: latency-svc-jh7wb [750.672275ms]
Apr 29 10:15:28.814: INFO: Created: latency-svc-5vbpf
Apr 29 10:15:28.851: INFO: Got endpoints: latency-svc-xw822 [748.804997ms]
Apr 29 10:15:28.862: INFO: Created: latency-svc-24gkq
Apr 29 10:15:28.897: INFO: Got endpoints: latency-svc-5wqqf [747.399386ms]
Apr 29 10:15:28.914: INFO: Created: latency-svc-wxlsg
Apr 29 10:15:28.950: INFO: Got endpoints: latency-svc-8wg64 [751.454564ms]
Apr 29 10:15:28.970: INFO: Created: latency-svc-vlsx8
Apr 29 10:15:29.001: INFO: Got endpoints: latency-svc-jxsg5 [749.301373ms]
Apr 29 10:15:29.019: INFO: Created: latency-svc-hwntt
Apr 29 10:15:29.047: INFO: Got endpoints: latency-svc-l8mmd [748.430682ms]
Apr 29 10:15:29.075: INFO: Created: latency-svc-qhl95
Apr 29 10:15:29.099: INFO: Got endpoints: latency-svc-g6rx2 [748.387258ms]
Apr 29 10:15:29.109: INFO: Created: latency-svc-mr774
Apr 29 10:15:29.148: INFO: Got endpoints: latency-svc-6d92f [749.139022ms]
Apr 29 10:15:29.158: INFO: Created: latency-svc-ltvdh
Apr 29 10:15:29.199: INFO: Got endpoints: latency-svc-qdndl [751.54337ms]
Apr 29 10:15:29.211: INFO: Created: latency-svc-8hq2r
Apr 29 10:15:29.248: INFO: Got endpoints: latency-svc-sn4pn [749.234996ms]
Apr 29 10:15:29.266: INFO: Created: latency-svc-bxl4p
Apr 29 10:15:29.298: INFO: Got endpoints: latency-svc-pwjjl [749.951242ms]
Apr 29 10:15:29.312: INFO: Created: latency-svc-nqbp4
Apr 29 10:15:29.349: INFO: Got endpoints: latency-svc-r5c4r [727.370432ms]
Apr 29 10:15:29.364: INFO: Created: latency-svc-lpbjz
Apr 29 10:15:29.405: INFO: Got endpoints: latency-svc-gnw5w [755.472756ms]
Apr 29 10:15:29.421: INFO: Created: latency-svc-2w94b
Apr 29 10:15:29.448: INFO: Got endpoints: latency-svc-q5hdn [748.476197ms]
Apr 29 10:15:29.462: INFO: Created: latency-svc-w27vs
Apr 29 10:15:29.498: INFO: Got endpoints: latency-svc-hjtwf [749.965943ms]
Apr 29 10:15:29.510: INFO: Created: latency-svc-fl62k
Apr 29 10:15:29.548: INFO: Got endpoints: latency-svc-5vbpf [749.100395ms]
Apr 29 10:15:29.559: INFO: Created: latency-svc-ql86z
Apr 29 10:15:29.600: INFO: Got endpoints: latency-svc-24gkq [748.485495ms]
Apr 29 10:15:29.613: INFO: Created: latency-svc-6m5bx
Apr 29 10:15:29.647: INFO: Got endpoints: latency-svc-wxlsg [749.639371ms]
Apr 29 10:15:29.660: INFO: Created: latency-svc-sn86j
Apr 29 10:15:29.697: INFO: Got endpoints: latency-svc-vlsx8 [747.55389ms]
Apr 29 10:15:29.709: INFO: Created: latency-svc-gxxmp
Apr 29 10:15:29.748: INFO: Got endpoints: latency-svc-hwntt [747.77644ms]
Apr 29 10:15:29.762: INFO: Created: latency-svc-rv95k
Apr 29 10:15:29.797: INFO: Got endpoints: latency-svc-qhl95 [749.706607ms]
Apr 29 10:15:29.809: INFO: Created: latency-svc-4vnvb
Apr 29 10:15:29.847: INFO: Got endpoints: latency-svc-mr774 [748.001943ms]
Apr 29 10:15:29.858: INFO: Created: latency-svc-vz2gz
Apr 29 10:15:29.898: INFO: Got endpoints: latency-svc-ltvdh [749.261613ms]
Apr 29 10:15:29.907: INFO: Created: latency-svc-tpw8f
Apr 29 10:15:29.947: INFO: Got endpoints: latency-svc-8hq2r [747.447043ms]
Apr 29 10:15:29.957: INFO: Created: latency-svc-pltt2
Apr 29 10:15:29.998: INFO: Got endpoints: latency-svc-bxl4p [749.645347ms]
Apr 29 10:15:30.012: INFO: Created: latency-svc-5mdct
Apr 29 10:15:30.047: INFO: Got endpoints: latency-svc-nqbp4 [749.059739ms]
Apr 29 10:15:30.065: INFO: Created: latency-svc-48ss6
Apr 29 10:15:30.098: INFO: Got endpoints: latency-svc-lpbjz [749.343813ms]
Apr 29 10:15:30.108: INFO: Created: latency-svc-fd5x9
Apr 29 10:15:30.148: INFO: Got endpoints: latency-svc-2w94b [742.087414ms]
Apr 29 10:15:30.158: INFO: Created: latency-svc-bx8k7
Apr 29 10:15:30.197: INFO: Got endpoints: latency-svc-w27vs [749.711709ms]
Apr 29 10:15:30.208: INFO: Created: latency-svc-f7r4b
Apr 29 10:15:30.247: INFO: Got endpoints: latency-svc-fl62k [749.226407ms]
Apr 29 10:15:30.258: INFO: Created: latency-svc-jnqcv
Apr 29 10:15:30.298: INFO: Got endpoints: latency-svc-ql86z [749.826911ms]
Apr 29 10:15:30.308: INFO: Created: latency-svc-s44l2
Apr 29 10:15:30.349: INFO: Got endpoints: latency-svc-6m5bx [748.951201ms]
Apr 29 10:15:30.358: INFO: Created: latency-svc-z8x72
Apr 29 10:15:30.397: INFO: Got endpoints: latency-svc-sn86j [750.210131ms]
Apr 29 10:15:30.409: INFO: Created: latency-svc-h8zkk
Apr 29 10:15:30.451: INFO: Got endpoints: latency-svc-gxxmp [753.465809ms]
Apr 29 10:15:30.462: INFO: Created: latency-svc-v58vd
Apr 29 10:15:30.498: INFO: Got endpoints: latency-svc-rv95k [749.159991ms]
Apr 29 10:15:30.506: INFO: Created: latency-svc-2f4bl
Apr 29 10:15:30.548: INFO: Got endpoints: latency-svc-4vnvb [750.562099ms]
Apr 29 10:15:30.563: INFO: Created: latency-svc-7jvl8
Apr 29 10:15:30.603: INFO: Got endpoints: latency-svc-vz2gz [755.627542ms]
Apr 29 10:15:30.615: INFO: Created: latency-svc-4t6tl
Apr 29 10:15:30.650: INFO: Got endpoints: latency-svc-tpw8f [752.51491ms]
Apr 29 10:15:30.664: INFO: Created: latency-svc-knzt2
Apr 29 10:15:30.701: INFO: Got endpoints: latency-svc-pltt2 [754.190946ms]
Apr 29 10:15:30.713: INFO: Created: latency-svc-b5589
Apr 29 10:15:30.748: INFO: Got endpoints: latency-svc-5mdct [749.789355ms]
Apr 29 10:15:30.766: INFO: Created: latency-svc-nz5sr
Apr 29 10:15:30.798: INFO: Got endpoints: latency-svc-48ss6 [750.7006ms]
Apr 29 10:15:30.812: INFO: Created: latency-svc-mls6g
Apr 29 10:15:30.847: INFO: Got endpoints: latency-svc-fd5x9 [749.055487ms]
Apr 29 10:15:30.860: INFO: Created: latency-svc-xp67g
Apr 29 10:15:30.899: INFO: Got endpoints: latency-svc-bx8k7 [751.074725ms]
Apr 29 10:15:30.977: INFO: Got endpoints: latency-svc-f7r4b [779.873024ms]
Apr 29 10:15:30.980: INFO: Created: latency-svc-xxhbj
Apr 29 10:15:30.995: INFO: Created: latency-svc-8627h
Apr 29 10:15:30.998: INFO: Got endpoints: latency-svc-jnqcv [750.800254ms]
Apr 29 10:15:31.015: INFO: Created: latency-svc-qzzkz
Apr 29 10:15:31.050: INFO: Got endpoints: latency-svc-s44l2 [751.848663ms]
Apr 29 10:15:31.064: INFO: Created: latency-svc-7ldtl
Apr 29 10:15:31.103: INFO: Got endpoints: latency-svc-z8x72 [754.432221ms]
Apr 29 10:15:31.138: INFO: Created: latency-svc-zkkpx
Apr 29 10:15:31.148: INFO: Got endpoints: latency-svc-h8zkk [751.133394ms]
Apr 29 10:15:31.162: INFO: Created: latency-svc-p9b87
Apr 29 10:15:31.199: INFO: Got endpoints: latency-svc-v58vd [747.982567ms]
Apr 29 10:15:31.213: INFO: Created: latency-svc-768lb
Apr 29 10:15:31.248: INFO: Got endpoints: latency-svc-2f4bl [750.060185ms]
Apr 29 10:15:31.262: INFO: Created: latency-svc-lg9dk
Apr 29 10:15:31.298: INFO: Got endpoints: latency-svc-7jvl8 [750.258167ms]
Apr 29 10:15:31.314: INFO: Created: latency-svc-4bvq8
Apr 29 10:15:31.351: INFO: Got endpoints: latency-svc-4t6tl [747.885616ms]
Apr 29 10:15:31.370: INFO: Created: latency-svc-d7wcd
Apr 29 10:15:31.404: INFO: Got endpoints: latency-svc-knzt2 [753.169709ms]
Apr 29 10:15:31.417: INFO: Created: latency-svc-dqgpt
Apr 29 10:15:31.449: INFO: Got endpoints: latency-svc-b5589 [747.573493ms]
Apr 29 10:15:31.460: INFO: Created: latency-svc-g7r29
Apr 29 10:15:31.497: INFO: Got endpoints: latency-svc-nz5sr [749.302161ms]
Apr 29 10:15:31.513: INFO: Created: latency-svc-4swz2
Apr 29 10:15:31.553: INFO: Got endpoints: latency-svc-mls6g [754.843787ms]
Apr 29 10:15:31.563: INFO: Created: latency-svc-tn66d
Apr 29 10:15:31.599: INFO: Got endpoints: latency-svc-xp67g [751.576489ms]
Apr 29 10:15:31.610: INFO: Created: latency-svc-kslnv
Apr 29 10:15:31.655: INFO: Got endpoints: latency-svc-xxhbj [755.853047ms]
Apr 29 10:15:31.669: INFO: Created: latency-svc-g7szz
Apr 29 10:15:31.703: INFO: Got endpoints: latency-svc-8627h [725.600936ms]
Apr 29 10:15:31.716: INFO: Created: latency-svc-zsbc4
Apr 29 10:15:31.748: INFO: Got endpoints: latency-svc-qzzkz [749.686671ms]
Apr 29 10:15:31.764: INFO: Created: latency-svc-5nx4b
Apr 29 10:15:31.798: INFO: Got endpoints: latency-svc-7ldtl [748.327094ms]
Apr 29 10:15:31.814: INFO: Created: latency-svc-w9bmk
Apr 29 10:15:31.849: INFO: Got endpoints: latency-svc-zkkpx [745.95261ms]
Apr 29 10:15:31.864: INFO: Created: latency-svc-lgvj7
Apr 29 10:15:31.898: INFO: Got endpoints: latency-svc-p9b87 [749.262093ms]
Apr 29 10:15:31.910: INFO: Created: latency-svc-xnrhv
Apr 29 10:15:31.948: INFO: Got endpoints: latency-svc-768lb [748.594628ms]
Apr 29 10:15:31.961: INFO: Created: latency-svc-wbg26
Apr 29 10:15:31.998: INFO: Got endpoints: latency-svc-lg9dk [750.393669ms]
Apr 29 10:15:32.009: INFO: Created: latency-svc-vwczx
Apr 29 10:15:32.047: INFO: Got endpoints: latency-svc-4bvq8 [748.739269ms]
Apr 29 10:15:32.060: INFO: Created: latency-svc-x92js
Apr 29 10:15:32.105: INFO: Got endpoints: latency-svc-d7wcd [753.574595ms]
Apr 29 10:15:32.121: INFO: Created: latency-svc-fq4pv
Apr 29 10:15:32.151: INFO: Got endpoints: latency-svc-dqgpt [747.158629ms]
Apr 29 10:15:32.166: INFO: Created: latency-svc-7m67z
Apr 29 10:15:32.199: INFO: Got endpoints: latency-svc-g7r29 [750.426605ms]
Apr 29 10:15:32.213: INFO: Created: latency-svc-9p9vb
Apr 29 10:15:32.247: INFO: Got endpoints: latency-svc-4swz2 [749.737748ms]
Apr 29 10:15:32.280: INFO: Created: latency-svc-mp9lj
Apr 29 10:15:32.298: INFO: Got endpoints: latency-svc-tn66d [744.10897ms]
Apr 29 10:15:32.315: INFO: Created: latency-svc-6w8kf
Apr 29 10:15:32.349: INFO: Got endpoints: latency-svc-kslnv [749.717371ms]
Apr 29 10:15:32.364: INFO: Created: latency-svc-xg5rp
Apr 29 10:15:32.398: INFO: Got endpoints: latency-svc-g7szz [743.235001ms]
Apr 29 10:15:32.412: INFO: Created: latency-svc-lv4kz
Apr 29 10:15:32.449: INFO: Got endpoints: latency-svc-zsbc4 [746.098901ms]
Apr 29 10:15:32.464: INFO: Created: latency-svc-d2xs4
Apr 29 10:15:32.499: INFO: Got endpoints: latency-svc-5nx4b [751.508663ms]
Apr 29 10:15:32.515: INFO: Created: latency-svc-jd5bp
Apr 29 10:15:32.550: INFO: Got endpoints: latency-svc-w9bmk [751.648621ms]
Apr 29 10:15:32.563: INFO: Created: latency-svc-frk66
Apr 29 10:15:32.598: INFO: Got endpoints: latency-svc-lgvj7 [748.72584ms]
Apr 29 10:15:32.609: INFO: Created: latency-svc-hctmz
Apr 29 10:15:32.651: INFO: Got endpoints: latency-svc-xnrhv [752.897823ms]
Apr 29 10:15:32.671: INFO: Created: latency-svc-ksdn2
Apr 29 10:15:32.698: INFO: Got endpoints: latency-svc-wbg26 [749.755377ms]
Apr 29 10:15:32.709: INFO: Created: latency-svc-sflpb
Apr 29 10:15:32.749: INFO: Got endpoints: latency-svc-vwczx [750.151197ms]
Apr 29 10:15:32.775: INFO: Created: latency-svc-pnxhz
Apr 29 10:15:32.798: INFO: Got endpoints: latency-svc-x92js [750.291138ms]
Apr 29 10:15:32.808: INFO: Created: latency-svc-rq79z
Apr 29 10:15:32.853: INFO: Got endpoints: latency-svc-fq4pv [747.747617ms]
Apr 29 10:15:32.867: INFO: Created: latency-svc-24cvl
Apr 29 10:15:32.897: INFO: Got endpoints: latency-svc-7m67z [746.242933ms]
Apr 29 10:15:32.914: INFO: Created: latency-svc-qpbcn
Apr 29 10:15:32.949: INFO: Got endpoints: latency-svc-9p9vb [749.34117ms]
Apr 29 10:15:32.962: INFO: Created: latency-svc-4jvqd
Apr 29 10:15:33.006: INFO: Got endpoints: latency-svc-mp9lj [758.433172ms]
Apr 29 10:15:33.018: INFO: Created: latency-svc-jd8j8
Apr 29 10:15:33.055: INFO: Got endpoints: latency-svc-6w8kf [757.043746ms]
Apr 29 10:15:33.070: INFO: Created: latency-svc-8wh2t
Apr 29 10:15:33.109: INFO: Got endpoints: latency-svc-xg5rp [759.916661ms]
Apr 29 10:15:33.120: INFO: Created: latency-svc-m4kld
Apr 29 10:15:33.147: INFO: Got endpoints: latency-svc-lv4kz [749.07154ms]
Apr 29 10:15:33.160: INFO: Created: latency-svc-nfdmw
Apr 29 10:15:33.198: INFO: Got endpoints: latency-svc-d2xs4 [748.449038ms]
Apr 29 10:15:33.213: INFO: Created: latency-svc-pg2dp
Apr 29 10:15:33.248: INFO: Got endpoints: latency-svc-jd5bp [748.777245ms]
Apr 29 10:15:33.330: INFO: Got endpoints: latency-svc-frk66 [780.237532ms]
Apr 29 10:15:33.336: INFO: Created: latency-svc-zj979
Apr 29 10:15:33.355: INFO: Got endpoints: latency-svc-hctmz [756.894877ms]
Apr 29 10:15:33.373: INFO: Created: latency-svc-92tkq
Apr 29 10:15:33.383: INFO: Created: latency-svc-vs6d5
Apr 29 10:15:33.415: INFO: Got endpoints: latency-svc-ksdn2 [763.924602ms]
Apr 29 10:15:33.427: INFO: Created: latency-svc-v5g55
Apr 29 10:15:33.448: INFO: Got endpoints: latency-svc-sflpb [749.717985ms]
Apr 29 10:15:33.463: INFO: Created: latency-svc-hjvpk
Apr 29 10:15:33.498: INFO: Got endpoints: latency-svc-pnxhz [748.927752ms]
Apr 29 10:15:33.523: INFO: Created: latency-svc-49pjz
Apr 29 10:15:33.548: INFO: Got endpoints: latency-svc-rq79z [749.987581ms]
Apr 29 10:15:33.557: INFO: Created: latency-svc-j6fvk
Apr 29 10:15:33.598: INFO: Got endpoints: latency-svc-24cvl [744.836168ms]
Apr 29 10:15:33.611: INFO: Created: latency-svc-v8n4b
Apr 29 10:15:33.649: INFO: Got endpoints: latency-svc-qpbcn [751.980602ms]
Apr 29 10:15:33.669: INFO: Created: latency-svc-t2pt9
Apr 29 10:15:33.699: INFO: Got endpoints: latency-svc-4jvqd [749.767064ms]
Apr 29 10:15:33.711: INFO: Created: latency-svc-k7dmj
Apr 29 10:15:33.749: INFO: Got endpoints: latency-svc-jd8j8 [742.864274ms]
Apr 29 10:15:33.798: INFO: Got endpoints: latency-svc-8wh2t [743.179607ms]
Apr 29 10:15:33.848: INFO: Got endpoints: latency-svc-m4kld [739.138721ms]
Apr 29 10:15:33.899: INFO: Got endpoints: latency-svc-nfdmw [750.943442ms]
Apr 29 10:15:33.948: INFO: Got endpoints: latency-svc-pg2dp [750.214351ms]
Apr 29 10:15:33.998: INFO: Got endpoints: latency-svc-zj979 [749.333725ms]
Apr 29 10:15:34.064: INFO: Got endpoints: latency-svc-92tkq [734.074826ms]
Apr 29 10:15:34.107: INFO: Got endpoints: latency-svc-vs6d5 [751.641501ms]
Apr 29 10:15:34.151: INFO: Got endpoints: latency-svc-v5g55 [735.97205ms]
Apr 29 10:15:34.203: INFO: Got endpoints: latency-svc-hjvpk [755.058645ms]
Apr 29 10:15:34.251: INFO: Got endpoints: latency-svc-49pjz [752.594245ms]
Apr 29 10:15:34.299: INFO: Got endpoints: latency-svc-j6fvk [750.971511ms]
Apr 29 10:15:34.350: INFO: Got endpoints: latency-svc-v8n4b [751.770455ms]
Apr 29 10:15:34.398: INFO: Got endpoints: latency-svc-t2pt9 [748.757687ms]
Apr 29 10:15:34.449: INFO: Got endpoints: latency-svc-k7dmj [749.742775ms]
Apr 29 10:15:34.449: INFO: Latencies: [81.005246ms 104.959524ms 113.405507ms 122.322848ms 130.844236ms 139.946204ms 152.614148ms 169.445222ms 183.99177ms 192.514886ms 194.904276ms 195.688924ms 196.49264ms 203.43153ms 205.868062ms 210.185449ms 220.332884ms 221.273723ms 223.36366ms 224.992579ms 226.895484ms 227.033899ms 227.618992ms 227.967662ms 228.897671ms 229.266216ms 230.221717ms 231.641608ms 232.866368ms 233.029147ms 233.781174ms 233.859873ms 238.618704ms 240.622534ms 242.778536ms 243.18784ms 244.945917ms 247.138433ms 249.560947ms 252.503294ms 264.493798ms 267.220967ms 282.590095ms 283.350719ms 321.961072ms 339.429091ms 366.804543ms 405.350414ms 433.556345ms 435.030237ms 476.995246ms 526.070894ms 570.030208ms 601.196214ms 635.284194ms 677.780899ms 714.606251ms 725.600936ms 727.370432ms 730.840109ms 734.074826ms 735.97205ms 739.138721ms 742.087414ms 742.864274ms 743.179607ms 743.235001ms 744.10897ms 744.836168ms 745.95261ms 746.059198ms 746.098901ms 746.242933ms 746.736135ms 746.756396ms 747.158629ms 747.272743ms 747.399386ms 747.43312ms 747.447043ms 747.55389ms 747.573493ms 747.612499ms 747.747617ms 747.77644ms 747.885616ms 747.982567ms 748.001943ms 748.327094ms 748.387258ms 748.430682ms 748.44618ms 748.449038ms 748.476197ms 748.485495ms 748.513421ms 748.594628ms 748.72584ms 748.739269ms 748.757687ms 748.777245ms 748.804997ms 748.884293ms 748.910642ms 748.927752ms 748.951201ms 748.957276ms 749.055487ms 749.059739ms 749.07154ms 749.100395ms 749.139022ms 749.159991ms 749.226407ms 749.234996ms 749.261613ms 749.262093ms 749.301373ms 749.302161ms 749.333725ms 749.34117ms 749.341573ms 749.343813ms 749.5273ms 749.639371ms 749.645347ms 749.679672ms 749.686671ms 749.706607ms 749.711709ms 749.717371ms 749.717985ms 749.737748ms 749.742775ms 749.755377ms 749.767064ms 749.772285ms 749.789355ms 749.826911ms 749.951242ms 749.965943ms 749.987581ms 750.060185ms 750.08038ms 750.151197ms 750.210131ms 750.214351ms 750.258167ms 750.291138ms 750.344089ms 750.393669ms 750.426605ms 750.503781ms 750.562099ms 750.672275ms 750.7006ms 750.800254ms 750.859969ms 750.874ms 750.943442ms 750.971511ms 751.02804ms 751.074725ms 751.133394ms 751.300995ms 751.454564ms 751.508663ms 751.54337ms 751.576489ms 751.641501ms 751.648621ms 751.770455ms 751.848663ms 751.980602ms 752.005642ms 752.125145ms 752.393772ms 752.51491ms 752.594245ms 752.742492ms 752.897823ms 753.169709ms 753.465809ms 753.574595ms 753.736359ms 754.190946ms 754.432221ms 754.843787ms 755.058645ms 755.472756ms 755.627542ms 755.853047ms 756.894877ms 757.043746ms 758.433172ms 759.916661ms 763.924602ms 773.55658ms 779.873024ms 780.237532ms]
Apr 29 10:15:34.449: INFO: 50 %ile: 748.777245ms
Apr 29 10:15:34.449: INFO: 90 %ile: 752.897823ms
Apr 29 10:15:34.449: INFO: 99 %ile: 779.873024ms
Apr 29 10:15:34.449: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:15:34.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1801" for this suite.
Apr 29 10:15:46.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:15:46.541: INFO: namespace svc-latency-1801 deletion completed in 12.087902241s

• [SLOW TEST:22.839 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:15:46.541: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 10:15:46.591: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:46.594: INFO: Number of nodes with available pods: 0
Apr 29 10:15:46.594: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 10:15:47.598: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:47.600: INFO: Number of nodes with available pods: 1
Apr 29 10:15:47.600: INFO: Node essentialpks-conformance-3 is running more than one daemon pod
Apr 29 10:15:48.599: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:48.601: INFO: Number of nodes with available pods: 2
Apr 29 10:15:48.601: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 29 10:15:48.614: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:48.617: INFO: Number of nodes with available pods: 1
Apr 29 10:15:48.617: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 10:15:49.623: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:49.628: INFO: Number of nodes with available pods: 1
Apr 29 10:15:49.628: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 10:15:50.621: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:50.624: INFO: Number of nodes with available pods: 1
Apr 29 10:15:50.624: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 10:15:51.621: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:51.624: INFO: Number of nodes with available pods: 1
Apr 29 10:15:51.624: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 10:15:52.621: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 10:15:52.624: INFO: Number of nodes with available pods: 2
Apr 29 10:15:52.624: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3345, will wait for the garbage collector to delete the pods
Apr 29 10:15:52.686: INFO: Deleting DaemonSet.extensions daemon-set took: 4.934955ms
Apr 29 10:15:52.787: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.280949ms
Apr 29 10:15:58.790: INFO: Number of nodes with available pods: 0
Apr 29 10:15:58.790: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 10:15:58.797: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3345/daemonsets","resourceVersion":"31094"},"items":null}

Apr 29 10:15:58.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3345/pods","resourceVersion":"31094"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:15:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3345" for this suite.
Apr 29 10:16:04.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:16:04.907: INFO: namespace daemonsets-3345 deletion completed in 6.094386148s

• [SLOW TEST:18.366 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:16:04.908: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-cc106dfb-6a67-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:16:04.941: INFO: Waiting up to 5m0s for pod "pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3" in namespace "secrets-2111" to be "success or failure"
Apr 29 10:16:04.948: INFO: Pod "pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.40893ms
Apr 29 10:16:06.951: INFO: Pod "pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009935839s
STEP: Saw pod success
Apr 29 10:16:06.951: INFO: Pod "pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:16:06.954: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:16:06.972: INFO: Waiting for pod pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:16:06.975: INFO: Pod pod-secrets-cc10dd2e-6a67-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:16:06.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2111" for this suite.
Apr 29 10:16:12.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:16:13.071: INFO: namespace secrets-2111 deletion completed in 6.091851369s

• [SLOW TEST:8.163 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:16:13.071: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 29 10:16:13.103: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Apr 29 10:16:20.138: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:16:20.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8292" for this suite.
Apr 29 10:16:26.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:16:26.234: INFO: namespace pods-8292 deletion completed in 6.087695978s

• [SLOW TEST:13.163 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:16:26.234: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 29 10:16:26.285: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4560,SelfLink:/api/v1/namespaces/watch-4560/configmaps/e2e-watch-test-label-changed,UID:d8d082f5-6a67-11e9-9342-005056a45e5c,ResourceVersion:31218,Generation:0,CreationTimestamp:2019-04-29 10:16:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 10:16:26.286: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4560,SelfLink:/api/v1/namespaces/watch-4560/configmaps/e2e-watch-test-label-changed,UID:d8d082f5-6a67-11e9-9342-005056a45e5c,ResourceVersion:31219,Generation:0,CreationTimestamp:2019-04-29 10:16:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 29 10:16:26.286: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4560,SelfLink:/api/v1/namespaces/watch-4560/configmaps/e2e-watch-test-label-changed,UID:d8d082f5-6a67-11e9-9342-005056a45e5c,ResourceVersion:31220,Generation:0,CreationTimestamp:2019-04-29 10:16:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 29 10:16:36.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4560,SelfLink:/api/v1/namespaces/watch-4560/configmaps/e2e-watch-test-label-changed,UID:d8d082f5-6a67-11e9-9342-005056a45e5c,ResourceVersion:31238,Generation:0,CreationTimestamp:2019-04-29 10:16:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 10:16:36.313: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4560,SelfLink:/api/v1/namespaces/watch-4560/configmaps/e2e-watch-test-label-changed,UID:d8d082f5-6a67-11e9-9342-005056a45e5c,ResourceVersion:31239,Generation:0,CreationTimestamp:2019-04-29 10:16:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 29 10:16:36.314: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-4560,SelfLink:/api/v1/namespaces/watch-4560/configmaps/e2e-watch-test-label-changed,UID:d8d082f5-6a67-11e9-9342-005056a45e5c,ResourceVersion:31240,Generation:0,CreationTimestamp:2019-04-29 10:16:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:16:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4560" for this suite.
Apr 29 10:16:42.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:16:42.408: INFO: namespace watch-4560 deletion completed in 6.090469462s

• [SLOW TEST:16.174 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:16:42.409: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 10:16:42.450: INFO: Waiting up to 5m0s for pod "pod-e26c9891-6a67-11e9-aeaf-0a580af401c3" in namespace "emptydir-1945" to be "success or failure"
Apr 29 10:16:42.454: INFO: Pod "pod-e26c9891-6a67-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038034ms
Apr 29 10:16:44.458: INFO: Pod "pod-e26c9891-6a67-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007976396s
STEP: Saw pod success
Apr 29 10:16:44.458: INFO: Pod "pod-e26c9891-6a67-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:16:44.460: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-e26c9891-6a67-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:16:44.477: INFO: Waiting for pod pod-e26c9891-6a67-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:16:44.479: INFO: Pod pod-e26c9891-6a67-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:16:44.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1945" for this suite.
Apr 29 10:16:50.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:16:50.575: INFO: namespace emptydir-1945 deletion completed in 6.09158395s

• [SLOW TEST:8.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:16:50.577: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-e749bd2a-6a67-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:16:50.615: INFO: Waiting up to 5m0s for pod "pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3" in namespace "secrets-7539" to be "success or failure"
Apr 29 10:16:50.622: INFO: Pod "pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.64021ms
Apr 29 10:16:52.625: INFO: Pod "pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009915968s
STEP: Saw pod success
Apr 29 10:16:52.625: INFO: Pod "pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:16:52.627: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:16:52.644: INFO: Waiting for pod pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:16:52.646: INFO: Pod pod-secrets-e74a477a-6a67-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:16:52.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7539" for this suite.
Apr 29 10:16:58.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:16:58.745: INFO: namespace secrets-7539 deletion completed in 6.096379675s

• [SLOW TEST:8.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:16:58.746: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:16:58.778: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3" in namespace "projected-2528" to be "success or failure"
Apr 29 10:16:58.784: INFO: Pod "downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059626ms
Apr 29 10:17:00.788: INFO: Pod "downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009654977s
STEP: Saw pod success
Apr 29 10:17:00.788: INFO: Pod "downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:17:00.791: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:17:00.808: INFO: Waiting for pod downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:17:00.810: INFO: Pod downwardapi-volume-ec27f896-6a67-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:17:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2528" for this suite.
Apr 29 10:17:06.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:17:06.895: INFO: namespace projected-2528 deletion completed in 6.080559805s

• [SLOW TEST:8.149 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:17:06.895: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:17:12.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8824" for this suite.
Apr 29 10:17:18.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:17:19.085: INFO: namespace namespaces-8824 deletion completed in 6.103892628s
STEP: Destroying namespace "nsdeletetest-5262" for this suite.
Apr 29 10:17:19.089: INFO: Namespace nsdeletetest-5262 was already deleted
STEP: Destroying namespace "nsdeletetest-2314" for this suite.
Apr 29 10:17:25.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:17:25.177: INFO: namespace nsdeletetest-2314 deletion completed in 6.088046508s

• [SLOW TEST:18.282 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:17:25.179: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 29 10:17:25.263: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31427,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 10:17:25.264: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31427,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 29 10:17:35.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31443,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 29 10:17:35.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31443,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 29 10:17:45.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31459,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 10:17:45.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31459,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 29 10:17:55.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31476,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 10:17:55.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-a,UID:fbf9d771-6a67-11e9-9342-005056a45e5c,ResourceVersion:31476,Generation:0,CreationTimestamp:2019-04-29 10:17:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 29 10:18:05.297: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b,UID:13d631f3-6a68-11e9-9342-005056a45e5c,ResourceVersion:31491,Generation:0,CreationTimestamp:2019-04-29 10:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 10:18:05.298: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b,UID:13d631f3-6a68-11e9-9342-005056a45e5c,ResourceVersion:31491,Generation:0,CreationTimestamp:2019-04-29 10:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 29 10:18:15.308: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b,UID:13d631f3-6a68-11e9-9342-005056a45e5c,ResourceVersion:31507,Generation:0,CreationTimestamp:2019-04-29 10:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 10:18:15.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2644,SelfLink:/api/v1/namespaces/watch-2644/configmaps/e2e-watch-test-configmap-b,UID:13d631f3-6a68-11e9-9342-005056a45e5c,ResourceVersion:31507,Generation:0,CreationTimestamp:2019-04-29 10:18:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:18:25.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2644" for this suite.
Apr 29 10:18:31.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:18:31.412: INFO: namespace watch-2644 deletion completed in 6.096348795s

• [SLOW TEST:66.233 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:18:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-2363d0d0-6a68-11e9-aeaf-0a580af401c3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:18:31.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6642" for this suite.
Apr 29 10:18:37.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:18:37.540: INFO: namespace configmap-6642 deletion completed in 6.096576979s

• [SLOW TEST:6.128 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:18:37.542: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-7377/configmap-test-270b7e8f-6a68-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:18:37.581: INFO: Waiting up to 5m0s for pod "pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3" in namespace "configmap-7377" to be "success or failure"
Apr 29 10:18:37.588: INFO: Pod "pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.404607ms
Apr 29 10:18:39.593: INFO: Pod "pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011530426s
STEP: Saw pod success
Apr 29 10:18:39.593: INFO: Pod "pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:18:39.596: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3 container env-test: <nil>
STEP: delete the pod
Apr 29 10:18:39.618: INFO: Waiting for pod pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:18:39.622: INFO: Pod pod-configmaps-270bdff1-6a68-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:18:39.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7377" for this suite.
Apr 29 10:18:45.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:18:45.719: INFO: namespace configmap-7377 deletion completed in 6.092191946s

• [SLOW TEST:8.178 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:18:45.721: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9170
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-9170
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9170
Apr 29 10:18:45.790: INFO: Found 0 stateful pods, waiting for 1
Apr 29 10:18:55.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 29 10:18:55.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:18:56.019: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:18:56.019: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:18:56.019: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:18:56.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 10:19:06.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:19:06.029: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:19:06.050: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:06.050: INFO: ss-0  essentialpks-conformance-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:06.051: INFO: 
Apr 29 10:19:06.051: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 29 10:19:07.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991474261s
Apr 29 10:19:08.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9877249s
Apr 29 10:19:09.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984475637s
Apr 29 10:19:10.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980763792s
Apr 29 10:19:11.072: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975692641s
Apr 29 10:19:12.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97036854s
Apr 29 10:19:13.082: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963840058s
Apr 29 10:19:14.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959967518s
Apr 29 10:19:15.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.638316ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9170
Apr 29 10:19:16.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:19:16.297: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 10:19:16.297: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:19:16.297: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:19:16.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:19:16.494: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 10:19:16.494: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:19:16.494: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:19:16.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:19:16.717: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 10:19:16.717: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:19:16.718: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:19:16.721: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:19:16.721: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:19:16.721: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 29 10:19:16.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:19:16.920: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:19:16.920: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:19:16.920: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:19:16.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:19:17.109: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:19:17.109: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:19:17.109: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:19:17.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:19:17.293: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:19:17.293: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:19:17.293: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:19:17.293: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:19:17.295: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 29 10:19:27.304: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:19:27.304: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:19:27.304: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:19:27.332: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:27.332: INFO: ss-0  essentialpks-conformance-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:27.332: INFO: ss-1  essentialpks-conformance-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:27.332: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:27.332: INFO: 
Apr 29 10:19:27.332: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:28.338: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:28.338: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:28.338: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:28.338: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:28.339: INFO: 
Apr 29 10:19:28.339: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:29.342: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:29.342: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:29.342: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:29.342: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:29.342: INFO: 
Apr 29 10:19:29.342: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:30.349: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:30.349: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:30.349: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:30.349: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:30.349: INFO: 
Apr 29 10:19:30.349: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:31.356: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:31.356: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:31.357: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:31.357: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:31.357: INFO: 
Apr 29 10:19:31.357: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:32.363: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:32.363: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:32.363: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:32.363: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:32.363: INFO: 
Apr 29 10:19:32.363: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:33.367: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:33.367: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:33.367: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:33.368: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:33.368: INFO: 
Apr 29 10:19:33.368: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 10:19:34.372: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:34.372: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:34.372: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:34.372: INFO: 
Apr 29 10:19:34.372: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 29 10:19:35.378: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:35.378: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:35.378: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:35.378: INFO: 
Apr 29 10:19:35.378: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 29 10:19:36.382: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Apr 29 10:19:36.382: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:18:45 +0000 UTC  }]
Apr 29 10:19:36.383: INFO: ss-2  essentialpks-conformance-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:19:06 +0000 UTC  }]
Apr 29 10:19:36.383: INFO: 
Apr 29 10:19:36.383: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9170
Apr 29 10:19:37.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:19:37.511: INFO: rc: 1
Apr 29 10:19:37.511: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0011be450 exit status 1 <nil> <nil> true [0xc0024a4198 0xc0024a41b0 0xc0024a41c8] [0xc0024a4198 0xc0024a41b0 0xc0024a41c8] [0xc0024a41a8 0xc0024a41c0] [0x9bf9f0 0x9bf9f0] 0xc0018f55c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Apr 29 10:19:47.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:19:47.584: INFO: rc: 1
Apr 29 10:19:47.585: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011beb10 exit status 1 <nil> <nil> true [0xc0024a41d0 0xc0024a41e8 0xc0024a4200] [0xc0024a41d0 0xc0024a41e8 0xc0024a4200] [0xc0024a41e0 0xc0024a41f8] [0x9bf9f0 0x9bf9f0] 0xc0018f5920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:19:57.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:19:57.648: INFO: rc: 1
Apr 29 10:19:57.649: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7e630 exit status 1 <nil> <nil> true [0xc0023b81d0 0xc0023b8218 0xc0023b8240] [0xc0023b81d0 0xc0023b8218 0xc0023b8240] [0xc0023b81f8 0xc0023b8228] [0x9bf9f0 0x9bf9f0] 0xc00274b620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:20:07.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:20:07.729: INFO: rc: 1
Apr 29 10:20:07.730: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011bef00 exit status 1 <nil> <nil> true [0xc0024a4208 0xc0024a4220 0xc0024a4238] [0xc0024a4208 0xc0024a4220 0xc0024a4238] [0xc0024a4218 0xc0024a4230] [0x9bf9f0 0x9bf9f0] 0xc0018f5f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:20:17.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:20:17.802: INFO: rc: 1
Apr 29 10:20:17.802: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7e9f0 exit status 1 <nil> <nil> true [0xc0023b8260 0xc0023b8288 0xc0023b82d0] [0xc0023b8260 0xc0023b8288 0xc0023b82d0] [0xc0023b8278 0xc0023b82b8] [0x9bf9f0 0x9bf9f0] 0xc00274bc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:20:27.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:20:27.871: INFO: rc: 1
Apr 29 10:20:27.871: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0011bf590 exit status 1 <nil> <nil> true [0xc0024a4240 0xc0024a4258 0xc0024a4270] [0xc0024a4240 0xc0024a4258 0xc0024a4270] [0xc0024a4250 0xc0024a4268] [0x9bf9f0 0x9bf9f0] 0xc0011171a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:20:37.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:20:37.944: INFO: rc: 1
Apr 29 10:20:37.944: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7edb0 exit status 1 <nil> <nil> true [0xc0023b82e0 0xc0023b8318 0xc0023b8348] [0xc0023b82e0 0xc0023b8318 0xc0023b8348] [0xc0023b8300 0xc0023b8340] [0x9bf9f0 0x9bf9f0] 0xc001b43080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:20:47.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:20:48.019: INFO: rc: 1
Apr 29 10:20:48.019: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7f140 exit status 1 <nil> <nil> true [0xc0023b8368 0xc0023b83a8 0xc0023b83e8] [0xc0023b8368 0xc0023b83a8 0xc0023b83e8] [0xc0023b8398 0xc0023b83c8] [0x9bf9f0 0x9bf9f0] 0xc00156b620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:20:58.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:20:58.106: INFO: rc: 1
Apr 29 10:20:58.106: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7f4d0 exit status 1 <nil> <nil> true [0xc0023b8400 0xc0023b8438 0xc0023b8478] [0xc0023b8400 0xc0023b8438 0xc0023b8478] [0xc0023b8418 0xc0023b8468] [0x9bf9f0 0x9bf9f0] 0xc001a448a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:21:08.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:21:08.178: INFO: rc: 1
Apr 29 10:21:08.178: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7fb60 exit status 1 <nil> <nil> true [0xc0023b8480 0xc0023b84c8 0xc0023b84f8] [0xc0023b8480 0xc0023b84c8 0xc0023b84f8] [0xc0023b84b8 0xc0023b84e8] [0x9bf9f0 0x9bf9f0] 0xc000c39380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:21:18.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:21:18.253: INFO: rc: 1
Apr 29 10:21:18.253: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000f7fef0 exit status 1 <nil> <nil> true [0xc0023b8500 0xc0023b8518 0xc0023b8530] [0xc0023b8500 0xc0023b8518 0xc0023b8530] [0xc0023b8510 0xc0023b8528] [0x9bf9f0 0x9bf9f0] 0xc0019ffe00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:21:28.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:21:28.322: INFO: rc: 1
Apr 29 10:21:28.322: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8180 exit status 1 <nil> <nil> true [0xc0024a4008 0xc0024a4020 0xc0024a4038] [0xc0024a4008 0xc0024a4020 0xc0024a4038] [0xc0024a4018 0xc0024a4030] [0x9bf9f0 0x9bf9f0] 0xc000c38360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:21:38.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:21:38.407: INFO: rc: 1
Apr 29 10:21:38.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8510 exit status 1 <nil> <nil> true [0xc0024a4040 0xc0024a4058 0xc0024a4070] [0xc0024a4040 0xc0024a4058 0xc0024a4070] [0xc0024a4050 0xc0024a4068] [0x9bf9f0 0x9bf9f0] 0xc001a44e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:21:48.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:21:48.481: INFO: rc: 1
Apr 29 10:21:48.481: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b2330 exit status 1 <nil> <nil> true [0xc0023b8000 0xc0023b8030 0xc0023b8070] [0xc0023b8000 0xc0023b8030 0xc0023b8070] [0xc0023b8020 0xc0023b8068] [0x9bf9f0 0x9bf9f0] 0xc00156b620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:21:58.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:21:58.554: INFO: rc: 1
Apr 29 10:21:58.555: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b88d0 exit status 1 <nil> <nil> true [0xc0024a4078 0xc0024a4090 0xc0024a40a8] [0xc0024a4078 0xc0024a4090 0xc0024a40a8] [0xc0024a4088 0xc0024a40a0] [0x9bf9f0 0x9bf9f0] 0xc0018bf080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:22:08.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:22:08.622: INFO: rc: 1
Apr 29 10:22:08.622: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b26c0 exit status 1 <nil> <nil> true [0xc0023b8078 0xc0023b80b8 0xc0023b80e8] [0xc0023b8078 0xc0023b80b8 0xc0023b80e8] [0xc0023b80a0 0xc0023b80d8] [0x9bf9f0 0x9bf9f0] 0xc00190d800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:22:18.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:22:18.711: INFO: rc: 1
Apr 29 10:22:18.711: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8c60 exit status 1 <nil> <nil> true [0xc0024a40b0 0xc0024a40c8 0xc0024a40e0] [0xc0024a40b0 0xc0024a40c8 0xc0024a40e0] [0xc0024a40c0 0xc0024a40d8] [0x9bf9f0 0x9bf9f0] 0xc00274a1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:22:28.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:22:28.796: INFO: rc: 1
Apr 29 10:22:28.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b9020 exit status 1 <nil> <nil> true [0xc0024a40e8 0xc0024a4100 0xc0024a4118] [0xc0024a40e8 0xc0024a4100 0xc0024a4118] [0xc0024a40f8 0xc0024a4110] [0x9bf9f0 0x9bf9f0] 0xc00274a8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:22:38.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:22:38.886: INFO: rc: 1
Apr 29 10:22:38.887: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b93e0 exit status 1 <nil> <nil> true [0xc0024a4120 0xc0024a4138 0xc0024a4150] [0xc0024a4120 0xc0024a4138 0xc0024a4150] [0xc0024a4130 0xc0024a4148] [0x9bf9f0 0x9bf9f0] 0xc00274af60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:22:48.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:22:48.967: INFO: rc: 1
Apr 29 10:22:48.967: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b9770 exit status 1 <nil> <nil> true [0xc0024a4158 0xc0024a4170 0xc0024a4188] [0xc0024a4158 0xc0024a4170 0xc0024a4188] [0xc0024a4168 0xc0024a4180] [0x9bf9f0 0x9bf9f0] 0xc00274b620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:22:58.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:22:59.050: INFO: rc: 1
Apr 29 10:22:59.050: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b9b00 exit status 1 <nil> <nil> true [0xc0024a4190 0xc0024a41a8 0xc0024a41c0] [0xc0024a4190 0xc0024a41a8 0xc0024a41c0] [0xc0024a41a0 0xc0024a41b8] [0x9bf9f0 0x9bf9f0] 0xc00274bc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:23:09.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:23:09.130: INFO: rc: 1
Apr 29 10:23:09.130: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b2a50 exit status 1 <nil> <nil> true [0xc0023b80f8 0xc0023b8140 0xc0023b8158] [0xc0023b80f8 0xc0023b8140 0xc0023b8158] [0xc0023b8120 0xc0023b8150] [0x9bf9f0 0x9bf9f0] 0xc0018f4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:23:19.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:23:19.209: INFO: rc: 1
Apr 29 10:23:19.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b2de0 exit status 1 <nil> <nil> true [0xc0023b8178 0xc0023b81c0 0xc0023b81f8] [0xc0023b8178 0xc0023b81c0 0xc0023b81f8] [0xc0023b81b0 0xc0023b81e0] [0x9bf9f0 0x9bf9f0] 0xc0018f49c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:23:29.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:23:29.292: INFO: rc: 1
Apr 29 10:23:29.292: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b2180 exit status 1 <nil> <nil> true [0xc0023b8010 0xc0023b8048 0xc0023b8078] [0xc0023b8010 0xc0023b8048 0xc0023b8078] [0xc0023b8030 0xc0023b8070] [0x9bf9f0 0x9bf9f0] 0xc0011171a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:23:39.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:23:39.361: INFO: rc: 1
Apr 29 10:23:39.361: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8390 exit status 1 <nil> <nil> true [0xc0024a4000 0xc0024a4018 0xc0024a4030] [0xc0024a4000 0xc0024a4018 0xc0024a4030] [0xc0024a4010 0xc0024a4028] [0x9bf9f0 0x9bf9f0] 0xc0018bf5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:23:49.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:23:49.436: INFO: rc: 1
Apr 29 10:23:49.437: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8750 exit status 1 <nil> <nil> true [0xc0024a4038 0xc0024a4050 0xc0024a4068] [0xc0024a4038 0xc0024a4050 0xc0024a4068] [0xc0024a4048 0xc0024a4060] [0x9bf9f0 0x9bf9f0] 0xc001e5d2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:23:59.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:23:59.522: INFO: rc: 1
Apr 29 10:23:59.524: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b2510 exit status 1 <nil> <nil> true [0xc0023b8080 0xc0023b80c8 0xc0023b80f8] [0xc0023b8080 0xc0023b80c8 0xc0023b80f8] [0xc0023b80b8 0xc0023b80e8] [0x9bf9f0 0x9bf9f0] 0xc001a45800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:24:09.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:24:09.603: INFO: rc: 1
Apr 29 10:24:09.604: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8b40 exit status 1 <nil> <nil> true [0xc0024a4070 0xc0024a4088 0xc0024a40a0] [0xc0024a4070 0xc0024a4088 0xc0024a40a0] [0xc0024a4080 0xc0024a4098] [0x9bf9f0 0x9bf9f0] 0xc0019fe180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:24:19.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:24:19.685: INFO: rc: 1
Apr 29 10:24:19.686: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0025b8f30 exit status 1 <nil> <nil> true [0xc0024a40a8 0xc0024a40c0 0xc0024a40d8] [0xc0024a40a8 0xc0024a40c0 0xc0024a40d8] [0xc0024a40b8 0xc0024a40d0] [0x9bf9f0 0x9bf9f0] 0xc0018f41e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:24:29.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:24:29.770: INFO: rc: 1
Apr 29 10:24:29.770: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0030b28d0 exit status 1 <nil> <nil> true [0xc0023b8108 0xc0023b8148 0xc0023b8178] [0xc0023b8108 0xc0023b8148 0xc0023b8178] [0xc0023b8140 0xc0023b8158] [0x9bf9f0 0x9bf9f0] 0xc00274a5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Apr 29 10:24:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-9170 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:24:39.863: INFO: rc: 1
Apr 29 10:24:39.863: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Apr 29 10:24:39.863: INFO: Scaling statefulset ss to 0
Apr 29 10:24:39.873: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 29 10:24:39.876: INFO: Deleting all statefulset in ns statefulset-9170
Apr 29 10:24:39.879: INFO: Scaling statefulset ss to 0
Apr 29 10:24:39.886: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:24:39.889: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:24:39.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9170" for this suite.
Apr 29 10:24:45.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:24:46.023: INFO: namespace statefulset-9170 deletion completed in 6.111770776s

• [SLOW TEST:360.303 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:24:46.024: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:24:46.057: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3" in namespace "projected-6890" to be "success or failure"
Apr 29 10:24:46.062: INFO: Pod "downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.948813ms
Apr 29 10:24:48.067: INFO: Pod "downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009807604s
STEP: Saw pod success
Apr 29 10:24:48.067: INFO: Pod "downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:24:48.073: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:24:48.099: INFO: Waiting for pod downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:24:48.101: INFO: Pod downwardapi-volume-02ad2d74-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:24:48.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6890" for this suite.
Apr 29 10:24:54.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:24:54.200: INFO: namespace projected-6890 deletion completed in 6.094602908s

• [SLOW TEST:8.176 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:24:54.201: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-bfrd
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 10:24:54.289: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bfrd" in namespace "subpath-2615" to be "success or failure"
Apr 29 10:24:54.296: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.284483ms
Apr 29 10:24:56.301: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011985425s
Apr 29 10:24:58.304: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 4.015715165s
Apr 29 10:25:00.308: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 6.019578612s
Apr 29 10:25:02.312: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 8.023705316s
Apr 29 10:25:04.318: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 10.028793958s
Apr 29 10:25:06.322: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 12.033163591s
Apr 29 10:25:08.325: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 14.036743361s
Apr 29 10:25:10.329: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 16.04038602s
Apr 29 10:25:12.334: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 18.045395544s
Apr 29 10:25:14.338: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Running", Reason="", readiness=true. Elapsed: 20.048780303s
Apr 29 10:25:16.341: INFO: Pod "pod-subpath-test-secret-bfrd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052620613s
STEP: Saw pod success
Apr 29 10:25:16.341: INFO: Pod "pod-subpath-test-secret-bfrd" satisfied condition "success or failure"
Apr 29 10:25:16.344: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-secret-bfrd container test-container-subpath-secret-bfrd: <nil>
STEP: delete the pod
Apr 29 10:25:16.360: INFO: Waiting for pod pod-subpath-test-secret-bfrd to disappear
Apr 29 10:25:16.363: INFO: Pod pod-subpath-test-secret-bfrd no longer exists
STEP: Deleting pod pod-subpath-test-secret-bfrd
Apr 29 10:25:16.363: INFO: Deleting pod "pod-subpath-test-secret-bfrd" in namespace "subpath-2615"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:25:16.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2615" for this suite.
Apr 29 10:25:22.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:25:22.471: INFO: namespace subpath-2615 deletion completed in 6.101827225s

• [SLOW TEST:28.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:25:22.472: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 10:25:22.505: INFO: Waiting up to 5m0s for pod "pod-1866a20b-6a69-11e9-aeaf-0a580af401c3" in namespace "emptydir-261" to be "success or failure"
Apr 29 10:25:22.515: INFO: Pod "pod-1866a20b-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.482295ms
Apr 29 10:25:24.526: INFO: Pod "pod-1866a20b-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021595722s
STEP: Saw pod success
Apr 29 10:25:24.527: INFO: Pod "pod-1866a20b-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:25:24.531: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-1866a20b-6a69-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:25:24.550: INFO: Waiting for pod pod-1866a20b-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:25:24.552: INFO: Pod pod-1866a20b-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:25:24.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-261" for this suite.
Apr 29 10:25:30.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:25:30.648: INFO: namespace emptydir-261 deletion completed in 6.092118277s

• [SLOW TEST:8.176 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:25:30.649: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 29 10:25:30.731: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:25:33.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8943" for this suite.
Apr 29 10:25:39.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:25:39.667: INFO: namespace init-container-8943 deletion completed in 6.109389347s

• [SLOW TEST:9.018 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:25:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:25:39.708: INFO: (0) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.936407ms)
Apr 29 10:25:39.713: INFO: (1) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.237883ms)
Apr 29 10:25:39.716: INFO: (2) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.369124ms)
Apr 29 10:25:39.719: INFO: (3) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.009538ms)
Apr 29 10:25:39.724: INFO: (4) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.38387ms)
Apr 29 10:25:39.727: INFO: (5) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.895399ms)
Apr 29 10:25:39.730: INFO: (6) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.571977ms)
Apr 29 10:25:39.734: INFO: (7) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.592578ms)
Apr 29 10:25:39.737: INFO: (8) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.537101ms)
Apr 29 10:25:39.741: INFO: (9) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.960789ms)
Apr 29 10:25:39.745: INFO: (10) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.311302ms)
Apr 29 10:25:39.748: INFO: (11) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.298573ms)
Apr 29 10:25:39.753: INFO: (12) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.524702ms)
Apr 29 10:25:39.757: INFO: (13) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.850306ms)
Apr 29 10:25:39.760: INFO: (14) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.576867ms)
Apr 29 10:25:39.763: INFO: (15) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.87437ms)
Apr 29 10:25:39.767: INFO: (16) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.839466ms)
Apr 29 10:25:39.771: INFO: (17) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.614771ms)
Apr 29 10:25:39.774: INFO: (18) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.605871ms)
Apr 29 10:25:39.779: INFO: (19) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.474784ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:25:39.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7548" for this suite.
Apr 29 10:25:45.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:25:45.884: INFO: namespace proxy-7548 deletion completed in 6.102104106s

• [SLOW TEST:6.216 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:25:45.885: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Apr 29 10:25:45.916: INFO: Waiting up to 5m0s for pod "client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3" in namespace "containers-1548" to be "success or failure"
Apr 29 10:25:45.923: INFO: Pod "client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365378ms
Apr 29 10:25:47.928: INFO: Pod "client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011752725s
STEP: Saw pod success
Apr 29 10:25:47.928: INFO: Pod "client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:25:47.933: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:25:47.951: INFO: Waiting for pod client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:25:47.953: INFO: Pod client-containers-265a9ff2-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:25:47.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1548" for this suite.
Apr 29 10:25:53.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:25:54.067: INFO: namespace containers-1548 deletion completed in 6.11052378s

• [SLOW TEST:8.182 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:25:54.067: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:25:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3154" for this suite.
Apr 29 10:26:34.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:26:34.242: INFO: namespace kubelet-test-3154 deletion completed in 38.108417751s

• [SLOW TEST:40.175 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:26:34.243: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:26:34.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3" in namespace "downward-api-8502" to be "success or failure"
Apr 29 10:26:34.286: INFO: Pod "downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09706ms
Apr 29 10:26:36.292: INFO: Pod "downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010570841s
STEP: Saw pod success
Apr 29 10:26:36.292: INFO: Pod "downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:26:36.296: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:26:36.316: INFO: Waiting for pod downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:26:36.320: INFO: Pod downwardapi-volume-432eb129-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:26:36.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8502" for this suite.
Apr 29 10:26:42.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:26:42.441: INFO: namespace downward-api-8502 deletion completed in 6.116621881s

• [SLOW TEST:8.198 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:26:42.443: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:26:42.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3" in namespace "downward-api-3370" to be "success or failure"
Apr 29 10:26:42.481: INFO: Pod "downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.310966ms
Apr 29 10:26:44.485: INFO: Pod "downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006934619s
STEP: Saw pod success
Apr 29 10:26:44.485: INFO: Pod "downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:26:44.487: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:26:44.503: INFO: Waiting for pod downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:26:44.506: INFO: Pod downwardapi-volume-481192db-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:26:44.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3370" for this suite.
Apr 29 10:26:50.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:26:50.612: INFO: namespace downward-api-3370 deletion completed in 6.102033131s

• [SLOW TEST:8.169 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:26:50.612: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9023
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 10:26:50.638: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 10:27:12.706: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.189:8080/dial?request=hostName&protocol=http&host=10.244.1.198&port=8080&tries=1'] Namespace:pod-network-test-9023 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 10:27:12.706: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 10:27:12.845: INFO: Waiting for endpoints: map[]
Apr 29 10:27:12.850: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.189:8080/dial?request=hostName&protocol=http&host=10.244.2.188&port=8080&tries=1'] Namespace:pod-network-test-9023 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 10:27:12.850: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 10:27:12.975: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:27:12.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9023" for this suite.
Apr 29 10:27:34.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:27:35.072: INFO: namespace pod-network-test-9023 deletion completed in 22.092781593s

• [SLOW TEST:44.460 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:27:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 10:27:35.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7872'
Apr 29 10:27:35.321: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 10:27:35.321: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Apr 29 10:27:35.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete jobs e2e-test-nginx-job --namespace=kubectl-7872'
Apr 29 10:27:35.422: INFO: stderr: ""
Apr 29 10:27:35.422: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:27:35.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7872" for this suite.
Apr 29 10:27:41.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:27:41.529: INFO: namespace kubectl-7872 deletion completed in 6.099421091s

• [SLOW TEST:6.457 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:27:41.529: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6200
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6200
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6200
Apr 29 10:27:41.575: INFO: Found 0 stateful pods, waiting for 1
Apr 29 10:27:51.579: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 29 10:27:51.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:27:51.784: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:27:51.784: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:27:51.784: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:27:51.791: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 10:28:01.796: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:28:01.796: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:28:01.812: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99997229s
Apr 29 10:28:02.816: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996394333s
Apr 29 10:28:03.823: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992058561s
Apr 29 10:28:04.827: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985389362s
Apr 29 10:28:05.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981296934s
Apr 29 10:28:06.837: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974905289s
Apr 29 10:28:07.841: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971100104s
Apr 29 10:28:08.845: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967168548s
Apr 29 10:28:09.850: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962756502s
Apr 29 10:28:10.854: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.187319ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6200
Apr 29 10:28:11.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:28:12.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 10:28:12.047: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:28:12.047: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:28:12.051: INFO: Found 1 stateful pods, waiting for 3
Apr 29 10:28:22.055: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:28:22.055: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:28:22.055: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 29 10:28:22.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:28:22.304: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:28:22.304: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:28:22.304: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:28:22.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:28:22.519: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:28:22.519: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:28:22.519: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:28:22.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 10:28:22.738: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 10:28:22.738: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 10:28:22.738: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 10:28:22.738: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:28:22.742: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 29 10:28:32.752: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:28:32.752: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:28:32.752: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 10:28:32.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999728s
Apr 29 10:28:33.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991178301s
Apr 29 10:28:34.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985659143s
Apr 29 10:28:35.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981371972s
Apr 29 10:28:36.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977503869s
Apr 29 10:28:37.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973440725s
Apr 29 10:28:38.796: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969001414s
Apr 29 10:28:39.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964501068s
Apr 29 10:28:40.819: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9501206s
Apr 29 10:28:41.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 941.743582ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6200
Apr 29 10:28:42.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:28:43.009: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 10:28:43.009: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:28:43.009: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:28:43.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:28:43.175: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 10:28:43.175: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:28:43.175: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:28:43.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-6200 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 10:28:43.390: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 10:28:43.390: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 10:28:43.390: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 10:28:43.390: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 29 10:29:03.407: INFO: Deleting all statefulset in ns statefulset-6200
Apr 29 10:29:03.409: INFO: Scaling statefulset ss to 0
Apr 29 10:29:03.418: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:29:03.421: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:29:03.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6200" for this suite.
Apr 29 10:29:09.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:29:09.542: INFO: namespace statefulset-6200 deletion completed in 6.107169673s

• [SLOW TEST:88.013 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:29:09.544: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3
Apr 29 10:29:09.577: INFO: Pod name my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3: Found 0 pods out of 1
Apr 29 10:29:14.581: INFO: Pod name my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3: Found 1 pods out of 1
Apr 29 10:29:14.581: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3" are running
Apr 29 10:29:14.584: INFO: Pod "my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3-gb4ck" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-29 10:29:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-29 10:29:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-29 10:29:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-29 10:29:09 +0000 UTC Reason: Message:}])
Apr 29 10:29:14.584: INFO: Trying to dial the pod
Apr 29 10:29:19.599: INFO: Controller my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3: Got expected result from replica 1 [my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3-gb4ck]: "my-hostname-basic-9fbef2ce-6a69-11e9-aeaf-0a580af401c3-gb4ck", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:29:19.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7937" for this suite.
Apr 29 10:29:25.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:29:25.701: INFO: namespace replication-controller-7937 deletion completed in 6.094839324s

• [SLOW TEST:16.157 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:29:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a960987b-6a69-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:29:25.742: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3" in namespace "projected-6076" to be "success or failure"
Apr 29 10:29:25.747: INFO: Pod "pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.150321ms
Apr 29 10:29:27.751: INFO: Pod "pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009077078s
STEP: Saw pod success
Apr 29 10:29:27.751: INFO: Pod "pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:29:27.754: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:29:27.774: INFO: Waiting for pod pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:29:27.778: INFO: Pod pod-projected-configmaps-a961ad45-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:29:27.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6076" for this suite.
Apr 29 10:29:33.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:29:33.883: INFO: namespace projected-6076 deletion completed in 6.101492501s

• [SLOW TEST:8.181 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:29:33.883: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 10:29:33.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4494'
Apr 29 10:29:34.015: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 10:29:34.015: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Apr 29 10:29:34.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4494'
Apr 29 10:29:34.149: INFO: stderr: ""
Apr 29 10:29:34.149: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:29:34.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4494" for this suite.
Apr 29 10:29:40.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:29:40.263: INFO: namespace kubectl-4494 deletion completed in 6.106585833s

• [SLOW TEST:6.380 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:29:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:30:40.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4457" for this suite.
Apr 29 10:31:02.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:31:02.404: INFO: namespace container-probe-4457 deletion completed in 22.092445686s

• [SLOW TEST:82.141 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:31:02.404: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:31:02.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-81" for this suite.
Apr 29 10:31:08.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:31:08.524: INFO: namespace services-81 deletion completed in 6.086237222s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.120 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:31:08.524: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0429 10:31:18.622283      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 29 10:31:18.622: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:31:18.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7722" for this suite.
Apr 29 10:31:24.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:31:24.715: INFO: namespace gc-7722 deletion completed in 6.085307369s

• [SLOW TEST:16.191 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:31:24.715: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f04ffd9c-6a69-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:31:24.746: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3" in namespace "projected-7507" to be "success or failure"
Apr 29 10:31:24.751: INFO: Pod "pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.212869ms
Apr 29 10:31:26.754: INFO: Pod "pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007696966s
STEP: Saw pod success
Apr 29 10:31:26.754: INFO: Pod "pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:31:26.757: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:31:26.771: INFO: Waiting for pod pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:31:26.774: INFO: Pod pod-projected-secrets-f0506469-6a69-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:31:26.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7507" for this suite.
Apr 29 10:31:32.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:31:32.870: INFO: namespace projected-7507 deletion completed in 6.091845987s

• [SLOW TEST:8.154 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:31:32.870: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Apr 29 10:31:33.427: INFO: created pod pod-service-account-defaultsa
Apr 29 10:31:33.427: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 29 10:31:33.433: INFO: created pod pod-service-account-mountsa
Apr 29 10:31:33.433: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 29 10:31:33.444: INFO: created pod pod-service-account-nomountsa
Apr 29 10:31:33.444: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 29 10:31:33.452: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 29 10:31:33.452: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 29 10:31:33.462: INFO: created pod pod-service-account-mountsa-mountspec
Apr 29 10:31:33.462: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 29 10:31:33.465: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 29 10:31:33.465: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 29 10:31:33.471: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 29 10:31:33.471: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 29 10:31:33.477: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 29 10:31:33.477: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 29 10:31:33.485: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 29 10:31:33.485: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:31:33.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2936" for this suite.
Apr 29 10:31:39.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:31:39.579: INFO: namespace svcaccounts-2936 deletion completed in 6.088977893s

• [SLOW TEST:6.709 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:31:39.579: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:31:41.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-66" for this suite.
Apr 29 10:32:31.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:32:31.717: INFO: namespace kubelet-test-66 deletion completed in 50.082501276s

• [SLOW TEST:52.138 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:32:31.717: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:32:31.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3" in namespace "projected-6906" to be "success or failure"
Apr 29 10:32:31.753: INFO: Pod "downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.006467ms
Apr 29 10:32:33.757: INFO: Pod "downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006885965s
STEP: Saw pod success
Apr 29 10:32:33.757: INFO: Pod "downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:32:33.761: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:32:33.776: INFO: Waiting for pod downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:32:33.780: INFO: Pod downwardapi-volume-18401d9d-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:32:33.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6906" for this suite.
Apr 29 10:32:39.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:32:39.885: INFO: namespace projected-6906 deletion completed in 6.100099712s

• [SLOW TEST:8.167 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:32:39.885: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4tzxt in namespace proxy-8188
I0429 10:32:39.932594      15 runners.go:184] Created replication controller with name: proxy-service-4tzxt, namespace: proxy-8188, replica count: 1
I0429 10:32:40.983030      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:41.983375      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:42.983567      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:43.983720      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:44.984010      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:45.984228      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:46.984512      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:47.984860      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:48.985117      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:49.985448      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:50.985818      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:51.986145      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:52.986508      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 10:32:53.986937      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 10:32:54.987321      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 10:32:55.987824      15 runners.go:184] proxy-service-4tzxt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 10:32:55.990: INFO: setup took 16.076815441s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 29 10:32:56.006: INFO: (0) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 14.512811ms)
Apr 29 10:32:56.011: INFO: (0) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 20.583754ms)
Apr 29 10:32:56.011: INFO: (0) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 20.95214ms)
Apr 29 10:32:56.011: INFO: (0) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 20.701061ms)
Apr 29 10:32:56.011: INFO: (0) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 20.558248ms)
Apr 29 10:32:56.011: INFO: (0) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 20.812546ms)
Apr 29 10:32:56.015: INFO: (0) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 23.464174ms)
Apr 29 10:32:56.015: INFO: (0) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 24.704256ms)
Apr 29 10:32:56.017: INFO: (0) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 26.165376ms)
Apr 29 10:32:56.017: INFO: (0) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 26.813296ms)
Apr 29 10:32:56.017: INFO: (0) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 26.595847ms)
Apr 29 10:32:56.018: INFO: (0) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 27.769125ms)
Apr 29 10:32:56.018: INFO: (0) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 27.331081ms)
Apr 29 10:32:56.019: INFO: (0) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 28.250465ms)
Apr 29 10:32:56.019: INFO: (0) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 28.619407ms)
Apr 29 10:32:56.022: INFO: (0) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 30.600786ms)
Apr 29 10:32:56.030: INFO: (1) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 7.075242ms)
Apr 29 10:32:56.032: INFO: (1) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 9.453217ms)
Apr 29 10:32:56.032: INFO: (1) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 9.189228ms)
Apr 29 10:32:56.032: INFO: (1) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 10.243935ms)
Apr 29 10:32:56.032: INFO: (1) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 9.843557ms)
Apr 29 10:32:56.033: INFO: (1) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.378847ms)
Apr 29 10:32:56.033: INFO: (1) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 10.505545ms)
Apr 29 10:32:56.033: INFO: (1) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.503226ms)
Apr 29 10:32:56.036: INFO: (1) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 14.160228ms)
Apr 29 10:32:56.037: INFO: (1) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 13.648059ms)
Apr 29 10:32:56.038: INFO: (1) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 14.969759ms)
Apr 29 10:32:56.038: INFO: (1) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 15.085749ms)
Apr 29 10:32:56.038: INFO: (1) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 15.423954ms)
Apr 29 10:32:56.038: INFO: (1) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 16.220121ms)
Apr 29 10:32:56.039: INFO: (1) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 15.846347ms)
Apr 29 10:32:56.039: INFO: (1) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 17.08344ms)
Apr 29 10:32:56.048: INFO: (2) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 8.938528ms)
Apr 29 10:32:56.048: INFO: (2) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 9.042407ms)
Apr 29 10:32:56.049: INFO: (2) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 9.233092ms)
Apr 29 10:32:56.049: INFO: (2) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 9.525054ms)
Apr 29 10:32:56.049: INFO: (2) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 10.029249ms)
Apr 29 10:32:56.049: INFO: (2) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 9.950169ms)
Apr 29 10:32:56.049: INFO: (2) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 10.021741ms)
Apr 29 10:32:56.050: INFO: (2) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.580178ms)
Apr 29 10:32:56.050: INFO: (2) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 10.136793ms)
Apr 29 10:32:56.051: INFO: (2) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 11.796469ms)
Apr 29 10:32:56.051: INFO: (2) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 11.981971ms)
Apr 29 10:32:56.052: INFO: (2) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 12.579671ms)
Apr 29 10:32:56.052: INFO: (2) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 12.67166ms)
Apr 29 10:32:56.052: INFO: (2) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 13.003969ms)
Apr 29 10:32:56.054: INFO: (2) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 14.915259ms)
Apr 29 10:32:56.056: INFO: (2) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 17.130309ms)
Apr 29 10:32:56.064: INFO: (3) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 7.224372ms)
Apr 29 10:32:56.065: INFO: (3) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 7.679287ms)
Apr 29 10:32:56.065: INFO: (3) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 7.874706ms)
Apr 29 10:32:56.065: INFO: (3) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 8.00513ms)
Apr 29 10:32:56.065: INFO: (3) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 7.854605ms)
Apr 29 10:32:56.066: INFO: (3) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 9.02439ms)
Apr 29 10:32:56.066: INFO: (3) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 9.601835ms)
Apr 29 10:32:56.066: INFO: (3) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 9.504722ms)
Apr 29 10:32:56.067: INFO: (3) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 9.517303ms)
Apr 29 10:32:56.067: INFO: (3) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 9.997531ms)
Apr 29 10:32:56.067: INFO: (3) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 10.53035ms)
Apr 29 10:32:56.068: INFO: (3) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 11.002881ms)
Apr 29 10:32:56.068: INFO: (3) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 11.311198ms)
Apr 29 10:32:56.068: INFO: (3) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 11.468848ms)
Apr 29 10:32:56.068: INFO: (3) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 11.215216ms)
Apr 29 10:32:56.068: INFO: (3) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 11.396373ms)
Apr 29 10:32:56.081: INFO: (4) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 12.052131ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 13.189646ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.584963ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.711508ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 13.381236ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 14.05695ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 13.685472ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 13.641751ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 13.777208ms)
Apr 29 10:32:56.082: INFO: (4) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 13.941234ms)
Apr 29 10:32:56.085: INFO: (4) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 16.872044ms)
Apr 29 10:32:56.085: INFO: (4) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 16.618537ms)
Apr 29 10:32:56.085: INFO: (4) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 16.549434ms)
Apr 29 10:32:56.085: INFO: (4) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 16.880492ms)
Apr 29 10:32:56.085: INFO: (4) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 16.708789ms)
Apr 29 10:32:56.086: INFO: (4) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 17.855586ms)
Apr 29 10:32:56.094: INFO: (5) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 6.965125ms)
Apr 29 10:32:56.096: INFO: (5) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 9.239589ms)
Apr 29 10:32:56.097: INFO: (5) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 10.156672ms)
Apr 29 10:32:56.099: INFO: (5) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 11.640366ms)
Apr 29 10:32:56.099: INFO: (5) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 11.877062ms)
Apr 29 10:32:56.100: INFO: (5) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 13.369553ms)
Apr 29 10:32:56.102: INFO: (5) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 14.963044ms)
Apr 29 10:32:56.102: INFO: (5) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 15.535194ms)
Apr 29 10:32:56.103: INFO: (5) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 16.32041ms)
Apr 29 10:32:56.103: INFO: (5) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 15.71728ms)
Apr 29 10:32:56.103: INFO: (5) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 15.842986ms)
Apr 29 10:32:56.103: INFO: (5) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 16.150472ms)
Apr 29 10:32:56.104: INFO: (5) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 16.73053ms)
Apr 29 10:32:56.104: INFO: (5) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 16.635616ms)
Apr 29 10:32:56.104: INFO: (5) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 16.911363ms)
Apr 29 10:32:56.104: INFO: (5) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 16.872353ms)
Apr 29 10:32:56.111: INFO: (6) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 6.832054ms)
Apr 29 10:32:56.114: INFO: (6) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 8.641313ms)
Apr 29 10:32:56.114: INFO: (6) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 9.326778ms)
Apr 29 10:32:56.115: INFO: (6) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 10.712341ms)
Apr 29 10:32:56.115: INFO: (6) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.856044ms)
Apr 29 10:32:56.116: INFO: (6) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 11.178358ms)
Apr 29 10:32:56.116: INFO: (6) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 11.645053ms)
Apr 29 10:32:56.117: INFO: (6) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 11.325522ms)
Apr 29 10:32:56.117: INFO: (6) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 11.746575ms)
Apr 29 10:32:56.117: INFO: (6) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 11.640159ms)
Apr 29 10:32:56.117: INFO: (6) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 11.508202ms)
Apr 29 10:32:56.117: INFO: (6) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 12.350277ms)
Apr 29 10:32:56.120: INFO: (6) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 15.123711ms)
Apr 29 10:32:56.120: INFO: (6) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 15.415697ms)
Apr 29 10:32:56.120: INFO: (6) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 15.449065ms)
Apr 29 10:32:56.122: INFO: (6) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 17.272419ms)
Apr 29 10:32:56.128: INFO: (7) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 6.535852ms)
Apr 29 10:32:56.129: INFO: (7) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 6.700958ms)
Apr 29 10:32:56.129: INFO: (7) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 7.054233ms)
Apr 29 10:32:56.129: INFO: (7) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 7.045703ms)
Apr 29 10:32:56.129: INFO: (7) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 7.01548ms)
Apr 29 10:32:56.131: INFO: (7) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 9.108579ms)
Apr 29 10:32:56.131: INFO: (7) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 9.141676ms)
Apr 29 10:32:56.132: INFO: (7) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 8.946185ms)
Apr 29 10:32:56.132: INFO: (7) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 9.769074ms)
Apr 29 10:32:56.132: INFO: (7) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 9.858685ms)
Apr 29 10:32:56.133: INFO: (7) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.410505ms)
Apr 29 10:32:56.133: INFO: (7) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.707358ms)
Apr 29 10:32:56.135: INFO: (7) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 12.853196ms)
Apr 29 10:32:56.136: INFO: (7) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 13.356692ms)
Apr 29 10:32:56.136: INFO: (7) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 13.01282ms)
Apr 29 10:32:56.137: INFO: (7) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 14.287138ms)
Apr 29 10:32:56.142: INFO: (8) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 4.867777ms)
Apr 29 10:32:56.142: INFO: (8) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 5.158568ms)
Apr 29 10:32:56.142: INFO: (8) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 5.409988ms)
Apr 29 10:32:56.146: INFO: (8) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 8.693155ms)
Apr 29 10:32:56.146: INFO: (8) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 9.18256ms)
Apr 29 10:32:56.147: INFO: (8) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 9.488884ms)
Apr 29 10:32:56.147: INFO: (8) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 9.264726ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.546003ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 10.452677ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 10.59392ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 10.606235ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 10.816274ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.662953ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 10.744059ms)
Apr 29 10:32:56.148: INFO: (8) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 10.773315ms)
Apr 29 10:32:56.150: INFO: (8) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 12.342245ms)
Apr 29 10:32:56.158: INFO: (9) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 7.666455ms)
Apr 29 10:32:56.158: INFO: (9) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 7.831622ms)
Apr 29 10:32:56.158: INFO: (9) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 8.417008ms)
Apr 29 10:32:56.160: INFO: (9) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 9.761647ms)
Apr 29 10:32:56.161: INFO: (9) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 10.504ms)
Apr 29 10:32:56.161: INFO: (9) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 10.905891ms)
Apr 29 10:32:56.161: INFO: (9) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.662281ms)
Apr 29 10:32:56.163: INFO: (9) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 12.000312ms)
Apr 29 10:32:56.163: INFO: (9) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 12.42671ms)
Apr 29 10:32:56.163: INFO: (9) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 13.339023ms)
Apr 29 10:32:56.164: INFO: (9) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 12.411667ms)
Apr 29 10:32:56.165: INFO: (9) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 15.341982ms)
Apr 29 10:32:56.166: INFO: (9) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 14.343695ms)
Apr 29 10:32:56.166: INFO: (9) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 15.089053ms)
Apr 29 10:32:56.166: INFO: (9) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 15.228617ms)
Apr 29 10:32:56.167: INFO: (9) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 16.678275ms)
Apr 29 10:32:56.177: INFO: (10) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 9.749318ms)
Apr 29 10:32:56.177: INFO: (10) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 9.175382ms)
Apr 29 10:32:56.178: INFO: (10) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 10.240367ms)
Apr 29 10:32:56.178: INFO: (10) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 11.613062ms)
Apr 29 10:32:56.179: INFO: (10) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 11.857516ms)
Apr 29 10:32:56.179: INFO: (10) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 11.783282ms)
Apr 29 10:32:56.179: INFO: (10) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 12.460347ms)
Apr 29 10:32:56.180: INFO: (10) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 12.180152ms)
Apr 29 10:32:56.180: INFO: (10) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 12.633534ms)
Apr 29 10:32:56.180: INFO: (10) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 12.243891ms)
Apr 29 10:32:56.180: INFO: (10) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 13.6082ms)
Apr 29 10:32:56.180: INFO: (10) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 12.870783ms)
Apr 29 10:32:56.181: INFO: (10) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 13.584841ms)
Apr 29 10:32:56.181: INFO: (10) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.421369ms)
Apr 29 10:32:56.181: INFO: (10) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 13.255259ms)
Apr 29 10:32:56.181: INFO: (10) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 13.164188ms)
Apr 29 10:32:56.187: INFO: (11) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 6.145079ms)
Apr 29 10:32:56.187: INFO: (11) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 6.072532ms)
Apr 29 10:32:56.187: INFO: (11) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 6.14004ms)
Apr 29 10:32:56.187: INFO: (11) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 6.090148ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 15.34794ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 15.199116ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 15.365662ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 15.738051ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 15.906596ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 16.206527ms)
Apr 29 10:32:56.197: INFO: (11) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 16.1664ms)
Apr 29 10:32:56.198: INFO: (11) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 17.131356ms)
Apr 29 10:32:56.199: INFO: (11) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 17.119617ms)
Apr 29 10:32:56.199: INFO: (11) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 17.127105ms)
Apr 29 10:32:56.200: INFO: (11) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 18.697118ms)
Apr 29 10:32:56.201: INFO: (11) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 19.366312ms)
Apr 29 10:32:56.212: INFO: (12) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.860701ms)
Apr 29 10:32:56.218: INFO: (12) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 16.465631ms)
Apr 29 10:32:56.218: INFO: (12) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 17.128403ms)
Apr 29 10:32:56.219: INFO: (12) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 16.976521ms)
Apr 29 10:32:56.219: INFO: (12) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 17.066965ms)
Apr 29 10:32:56.220: INFO: (12) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 17.899834ms)
Apr 29 10:32:56.220: INFO: (12) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 17.889052ms)
Apr 29 10:32:56.220: INFO: (12) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 18.516715ms)
Apr 29 10:32:56.221: INFO: (12) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 19.713871ms)
Apr 29 10:32:56.221: INFO: (12) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 19.696974ms)
Apr 29 10:32:56.221: INFO: (12) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 20.23229ms)
Apr 29 10:32:56.221: INFO: (12) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 19.888257ms)
Apr 29 10:32:56.222: INFO: (12) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 19.863149ms)
Apr 29 10:32:56.222: INFO: (12) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 20.426354ms)
Apr 29 10:32:56.223: INFO: (12) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 20.821811ms)
Apr 29 10:32:56.223: INFO: (12) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 21.484214ms)
Apr 29 10:32:56.229: INFO: (13) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 6.111601ms)
Apr 29 10:32:56.230: INFO: (13) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 6.940969ms)
Apr 29 10:32:56.230: INFO: (13) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 7.107229ms)
Apr 29 10:32:56.235: INFO: (13) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 12.136413ms)
Apr 29 10:32:56.236: INFO: (13) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 12.067914ms)
Apr 29 10:32:56.238: INFO: (13) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.822502ms)
Apr 29 10:32:56.238: INFO: (13) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 14.349184ms)
Apr 29 10:32:56.238: INFO: (13) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 14.492211ms)
Apr 29 10:32:56.238: INFO: (13) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 14.787758ms)
Apr 29 10:32:56.238: INFO: (13) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 14.496039ms)
Apr 29 10:32:56.238: INFO: (13) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 14.87057ms)
Apr 29 10:32:56.239: INFO: (13) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 15.230591ms)
Apr 29 10:32:56.239: INFO: (13) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 15.399376ms)
Apr 29 10:32:56.239: INFO: (13) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 15.666662ms)
Apr 29 10:32:56.239: INFO: (13) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 16.204136ms)
Apr 29 10:32:56.240: INFO: (13) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 15.916536ms)
Apr 29 10:32:56.250: INFO: (14) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 9.441488ms)
Apr 29 10:32:56.251: INFO: (14) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 10.245677ms)
Apr 29 10:32:56.253: INFO: (14) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 12.968631ms)
Apr 29 10:32:56.253: INFO: (14) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 13.045335ms)
Apr 29 10:32:56.253: INFO: (14) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 12.636483ms)
Apr 29 10:32:56.253: INFO: (14) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 13.702623ms)
Apr 29 10:32:56.253: INFO: (14) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 12.402758ms)
Apr 29 10:32:56.254: INFO: (14) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 12.795168ms)
Apr 29 10:32:56.254: INFO: (14) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 12.543503ms)
Apr 29 10:32:56.254: INFO: (14) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 12.757666ms)
Apr 29 10:32:56.255: INFO: (14) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 14.911555ms)
Apr 29 10:32:56.255: INFO: (14) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 14.83895ms)
Apr 29 10:32:56.255: INFO: (14) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 14.59037ms)
Apr 29 10:32:56.255: INFO: (14) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 14.9191ms)
Apr 29 10:32:56.255: INFO: (14) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 14.96013ms)
Apr 29 10:32:56.255: INFO: (14) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 15.590903ms)
Apr 29 10:32:56.268: INFO: (15) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 11.611747ms)
Apr 29 10:32:56.268: INFO: (15) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 12.323775ms)
Apr 29 10:32:56.268: INFO: (15) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 12.112578ms)
Apr 29 10:32:56.268: INFO: (15) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 12.787695ms)
Apr 29 10:32:56.269: INFO: (15) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 12.881777ms)
Apr 29 10:32:56.269: INFO: (15) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.007919ms)
Apr 29 10:32:56.269: INFO: (15) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 13.401779ms)
Apr 29 10:32:56.270: INFO: (15) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.730968ms)
Apr 29 10:32:56.273: INFO: (15) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 17.587228ms)
Apr 29 10:32:56.273: INFO: (15) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 17.479473ms)
Apr 29 10:32:56.274: INFO: (15) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 17.719197ms)
Apr 29 10:32:56.274: INFO: (15) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 18.001324ms)
Apr 29 10:32:56.274: INFO: (15) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 17.716172ms)
Apr 29 10:32:56.274: INFO: (15) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 17.876464ms)
Apr 29 10:32:56.274: INFO: (15) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 17.860434ms)
Apr 29 10:32:56.274: INFO: (15) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 17.889949ms)
Apr 29 10:32:56.287: INFO: (16) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 12.455123ms)
Apr 29 10:32:56.287: INFO: (16) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 12.831702ms)
Apr 29 10:32:56.287: INFO: (16) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 13.026544ms)
Apr 29 10:32:56.288: INFO: (16) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 13.797016ms)
Apr 29 10:32:56.288: INFO: (16) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 14.128911ms)
Apr 29 10:32:56.289: INFO: (16) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 14.437702ms)
Apr 29 10:32:56.289: INFO: (16) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 14.644561ms)
Apr 29 10:32:56.290: INFO: (16) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 15.972821ms)
Apr 29 10:32:56.291: INFO: (16) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 16.464876ms)
Apr 29 10:32:56.291: INFO: (16) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 16.33099ms)
Apr 29 10:32:56.291: INFO: (16) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 16.600489ms)
Apr 29 10:32:56.291: INFO: (16) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 17.530744ms)
Apr 29 10:32:56.292: INFO: (16) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 17.912413ms)
Apr 29 10:32:56.292: INFO: (16) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 17.67098ms)
Apr 29 10:32:56.292: INFO: (16) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 17.569538ms)
Apr 29 10:32:56.292: INFO: (16) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 18.127209ms)
Apr 29 10:32:56.300: INFO: (17) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 7.539171ms)
Apr 29 10:32:56.302: INFO: (17) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 9.221795ms)
Apr 29 10:32:56.302: INFO: (17) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 9.408836ms)
Apr 29 10:32:56.302: INFO: (17) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 9.800991ms)
Apr 29 10:32:56.302: INFO: (17) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 9.581285ms)
Apr 29 10:32:56.303: INFO: (17) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 9.860199ms)
Apr 29 10:32:56.304: INFO: (17) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 10.931565ms)
Apr 29 10:32:56.304: INFO: (17) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 11.087364ms)
Apr 29 10:32:56.304: INFO: (17) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 11.294654ms)
Apr 29 10:32:56.304: INFO: (17) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 11.709052ms)
Apr 29 10:32:56.305: INFO: (17) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 12.209234ms)
Apr 29 10:32:56.306: INFO: (17) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 13.106469ms)
Apr 29 10:32:56.307: INFO: (17) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 14.221878ms)
Apr 29 10:32:56.307: INFO: (17) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 13.736901ms)
Apr 29 10:32:56.308: INFO: (17) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 14.787769ms)
Apr 29 10:32:56.308: INFO: (17) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 14.845555ms)
Apr 29 10:32:56.318: INFO: (18) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 9.459331ms)
Apr 29 10:32:56.318: INFO: (18) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 9.643943ms)
Apr 29 10:32:56.319: INFO: (18) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 9.86464ms)
Apr 29 10:32:56.319: INFO: (18) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.128783ms)
Apr 29 10:32:56.319: INFO: (18) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 10.309295ms)
Apr 29 10:32:56.319: INFO: (18) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 10.598929ms)
Apr 29 10:32:56.321: INFO: (18) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 13.011235ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 13.264386ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 13.257248ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 14.028553ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 13.331081ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 13.688736ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 14.202495ms)
Apr 29 10:32:56.322: INFO: (18) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 13.891614ms)
Apr 29 10:32:56.323: INFO: (18) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 14.19862ms)
Apr 29 10:32:56.323: INFO: (18) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 14.771408ms)
Apr 29 10:32:56.333: INFO: (19) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 9.577934ms)
Apr 29 10:32:56.333: INFO: (19) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:460/proxy/: tls baz (200; 10.170449ms)
Apr 29 10:32:56.333: INFO: (19) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:162/proxy/: bar (200; 10.125315ms)
Apr 29 10:32:56.334: INFO: (19) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.217817ms)
Apr 29 10:32:56.334: INFO: (19) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:443/proxy/tlsrewritem... (200; 10.48279ms)
Apr 29 10:32:56.334: INFO: (19) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:160/proxy/: foo (200; 10.451421ms)
Apr 29 10:32:56.337: INFO: (19) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">test<... (200; 14.00391ms)
Apr 29 10:32:56.337: INFO: (19) /api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/http:proxy-service-4tzxt-p89rd:1080/proxy/rewriteme">... (200; 14.09072ms)
Apr 29 10:32:56.337: INFO: (19) /api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/: <a href="/api/v1/namespaces/proxy-8188/pods/proxy-service-4tzxt-p89rd/proxy/rewriteme">test</a> (200; 14.231892ms)
Apr 29 10:32:56.340: INFO: (19) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname1/proxy/: foo (200; 16.401295ms)
Apr 29 10:32:56.342: INFO: (19) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname1/proxy/: foo (200; 18.953281ms)
Apr 29 10:32:56.342: INFO: (19) /api/v1/namespaces/proxy-8188/services/http:proxy-service-4tzxt:portname2/proxy/: bar (200; 19.144994ms)
Apr 29 10:32:56.343: INFO: (19) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname2/proxy/: tls qux (200; 19.73533ms)
Apr 29 10:32:56.343: INFO: (19) /api/v1/namespaces/proxy-8188/services/proxy-service-4tzxt:portname2/proxy/: bar (200; 19.741702ms)
Apr 29 10:32:56.346: INFO: (19) /api/v1/namespaces/proxy-8188/services/https:proxy-service-4tzxt:tlsportname1/proxy/: tls baz (200; 22.491539ms)
Apr 29 10:32:56.346: INFO: (19) /api/v1/namespaces/proxy-8188/pods/https:proxy-service-4tzxt-p89rd:462/proxy/: tls qux (200; 23.233967ms)
STEP: deleting ReplicationController proxy-service-4tzxt in namespace proxy-8188, will wait for the garbage collector to delete the pods
Apr 29 10:32:56.406: INFO: Deleting ReplicationController proxy-service-4tzxt took: 5.963813ms
Apr 29 10:32:56.706: INFO: Terminating ReplicationController proxy-service-4tzxt pods took: 300.315962ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:32:58.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8188" for this suite.
Apr 29 10:33:04.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:33:05.010: INFO: namespace proxy-8188 deletion completed in 6.097706759s

• [SLOW TEST:25.125 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:33:05.012: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:33:05.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3" in namespace "downward-api-4295" to be "success or failure"
Apr 29 10:33:05.049: INFO: Pod "downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026594ms
Apr 29 10:33:07.053: INFO: Pod "downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009681027s
STEP: Saw pod success
Apr 29 10:33:07.053: INFO: Pod "downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:33:07.056: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:33:07.071: INFO: Waiting for pod downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:33:07.073: INFO: Pod downwardapi-volume-2c181e7b-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:33:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4295" for this suite.
Apr 29 10:33:13.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:33:13.171: INFO: namespace downward-api-4295 deletion completed in 6.095302488s

• [SLOW TEST:8.160 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:33:13.172: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3226
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 10:33:13.198: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 10:33:33.272: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.215:8080/dial?request=hostName&protocol=udp&host=10.244.2.214&port=8081&tries=1'] Namespace:pod-network-test-3226 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 10:33:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 10:33:33.397: INFO: Waiting for endpoints: map[]
Apr 29 10:33:33.408: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.215:8080/dial?request=hostName&protocol=udp&host=10.244.1.207&port=8081&tries=1'] Namespace:pod-network-test-3226 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 10:33:33.408: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 10:33:33.531: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:33:33.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3226" for this suite.
Apr 29 10:33:55.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:33:55.635: INFO: namespace pod-network-test-3226 deletion completed in 22.096289547s

• [SLOW TEST:42.463 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:33:55.637: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 10:33:58.194: INFO: Successfully updated pod "pod-update-4a459c4d-6a6a-11e9-aeaf-0a580af401c3"
STEP: verifying the updated pod is in kubernetes
Apr 29 10:33:58.200: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:33:58.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3697" for this suite.
Apr 29 10:34:20.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:34:20.314: INFO: namespace pods-3697 deletion completed in 22.109993513s

• [SLOW TEST:24.676 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:34:20.314: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 29 10:34:20.353: INFO: Waiting up to 5m0s for pod "pod-58faef02-6a6a-11e9-aeaf-0a580af401c3" in namespace "emptydir-9462" to be "success or failure"
Apr 29 10:34:20.361: INFO: Pod "pod-58faef02-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.539039ms
Apr 29 10:34:22.364: INFO: Pod "pod-58faef02-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011133237s
STEP: Saw pod success
Apr 29 10:34:22.364: INFO: Pod "pod-58faef02-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:34:22.368: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-58faef02-6a6a-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:34:22.383: INFO: Waiting for pod pod-58faef02-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:34:22.385: INFO: Pod pod-58faef02-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:34:22.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9462" for this suite.
Apr 29 10:34:28.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:34:28.486: INFO: namespace emptydir-9462 deletion completed in 6.097753534s

• [SLOW TEST:8.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:34:28.487: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 10:34:28.520: INFO: Waiting up to 5m0s for pod "pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3" in namespace "emptydir-1010" to be "success or failure"
Apr 29 10:34:28.525: INFO: Pod "pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.077611ms
Apr 29 10:34:30.529: INFO: Pod "pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009313931s
STEP: Saw pod success
Apr 29 10:34:30.529: INFO: Pod "pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:34:30.536: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:34:30.556: INFO: Waiting for pod pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:34:30.558: INFO: Pod pod-5dd9fe20-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:34:30.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1010" for this suite.
Apr 29 10:34:36.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:34:36.650: INFO: namespace emptydir-1010 deletion completed in 6.087953093s

• [SLOW TEST:8.163 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:34:36.656: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 29 10:34:39.258: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6927 pod-service-account-630de7c3-6a6a-11e9-aeaf-0a580af401c3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 29 10:34:39.440: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6927 pod-service-account-630de7c3-6a6a-11e9-aeaf-0a580af401c3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 29 10:34:39.644: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6927 pod-service-account-630de7c3-6a6a-11e9-aeaf-0a580af401c3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:34:39.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6927" for this suite.
Apr 29 10:34:45.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:34:45.936: INFO: namespace svcaccounts-6927 deletion completed in 6.101732658s

• [SLOW TEST:9.287 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:34:45.943: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:34:49.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9308" for this suite.
Apr 29 10:35:11.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:35:11.108: INFO: namespace replication-controller-9308 deletion completed in 22.096700412s

• [SLOW TEST:25.169 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:35:11.113: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 29 10:35:11.158: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 29 10:35:16.163: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:35:16.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-645" for this suite.
Apr 29 10:35:22.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:35:22.290: INFO: namespace replication-controller-645 deletion completed in 6.108974696s

• [SLOW TEST:11.177 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:35:22.290: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 10:35:22.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7788'
Apr 29 10:35:22.400: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 10:35:22.401: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr 29 10:35:22.420: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-6hv4p]
Apr 29 10:35:22.420: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-6hv4p" in namespace "kubectl-7788" to be "running and ready"
Apr 29 10:35:22.424: INFO: Pod "e2e-test-nginx-rc-6hv4p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545986ms
Apr 29 10:35:24.427: INFO: Pod "e2e-test-nginx-rc-6hv4p": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109743s
Apr 29 10:35:24.427: INFO: Pod "e2e-test-nginx-rc-6hv4p" satisfied condition "running and ready"
Apr 29 10:35:24.427: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-6hv4p]
Apr 29 10:35:24.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 logs rc/e2e-test-nginx-rc --namespace=kubectl-7788'
Apr 29 10:35:24.531: INFO: stderr: ""
Apr 29 10:35:24.531: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Apr 29 10:35:24.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete rc e2e-test-nginx-rc --namespace=kubectl-7788'
Apr 29 10:35:24.632: INFO: stderr: ""
Apr 29 10:35:24.632: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:35:24.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7788" for this suite.
Apr 29 10:35:46.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:35:46.739: INFO: namespace kubectl-7788 deletion completed in 22.103244389s

• [SLOW TEST:24.449 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:35:46.739: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:35:46.768: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:35:47.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5364" for this suite.
Apr 29 10:35:53.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:35:53.921: INFO: namespace custom-resource-definition-5364 deletion completed in 6.088705823s

• [SLOW TEST:7.181 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:35:53.921: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 29 10:35:53.959: INFO: Waiting up to 5m0s for pod "pod-90c68539-6a6a-11e9-aeaf-0a580af401c3" in namespace "emptydir-3392" to be "success or failure"
Apr 29 10:35:53.962: INFO: Pod "pod-90c68539-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513096ms
Apr 29 10:35:55.966: INFO: Pod "pod-90c68539-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007336812s
STEP: Saw pod success
Apr 29 10:35:55.966: INFO: Pod "pod-90c68539-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:35:55.969: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-90c68539-6a6a-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:35:55.985: INFO: Waiting for pod pod-90c68539-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:35:55.987: INFO: Pod pod-90c68539-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:35:55.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3392" for this suite.
Apr 29 10:36:02.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:36:02.073: INFO: namespace emptydir-3392 deletion completed in 6.081787058s

• [SLOW TEST:8.152 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:36:02.073: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 29 10:36:02.099: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 10:36:02.105: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 10:36:02.108: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-2 before test
Apr 29 10:36:02.113: INFO: kube-proxy-x8ptf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:36:02.113: INFO: 	Container kube-proxy ready: true, restart count 2
Apr 29 10:36:02.113: INFO: kube-flannel-ds-amd64-c4rpf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:36:02.113: INFO: 	Container kube-flannel ready: true, restart count 2
Apr 29 10:36:02.113: INFO: sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-ln6c4 from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 10:36:02.113: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 29 10:36:02.113: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 10:36:02.113: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-3 before test
Apr 29 10:36:02.129: INFO: sonobuoy-e2e-job-225965689e404c5d from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 10:36:02.129: INFO: 	Container e2e ready: true, restart count 0
Apr 29 10:36:02.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 10:36:02.129: INFO: sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-km6th from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 10:36:02.129: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr 29 10:36:02.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 10:36:02.129: INFO: kube-flannel-ds-amd64-5d5mf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:36:02.129: INFO: 	Container kube-flannel ready: true, restart count 2
Apr 29 10:36:02.129: INFO: kube-proxy-h9j6t from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 10:36:02.129: INFO: 	Container kube-proxy ready: true, restart count 2
Apr 29 10:36:02.129: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-29 10:13:51 +0000 UTC (1 container statuses recorded)
Apr 29 10:36:02.129: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node essentialpks-conformance-2
STEP: verifying the node has the label node essentialpks-conformance-3
Apr 29 10:36:02.161: INFO: Pod sonobuoy requesting resource cpu=0m on Node essentialpks-conformance-3
Apr 29 10:36:02.161: INFO: Pod sonobuoy-e2e-job-225965689e404c5d requesting resource cpu=0m on Node essentialpks-conformance-3
Apr 29 10:36:02.161: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-km6th requesting resource cpu=0m on Node essentialpks-conformance-3
Apr 29 10:36:02.161: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-ln6c4 requesting resource cpu=0m on Node essentialpks-conformance-2
Apr 29 10:36:02.161: INFO: Pod kube-flannel-ds-amd64-5d5mf requesting resource cpu=100m on Node essentialpks-conformance-3
Apr 29 10:36:02.161: INFO: Pod kube-flannel-ds-amd64-c4rpf requesting resource cpu=100m on Node essentialpks-conformance-2
Apr 29 10:36:02.161: INFO: Pod kube-proxy-h9j6t requesting resource cpu=0m on Node essentialpks-conformance-3
Apr 29 10:36:02.161: INFO: Pod kube-proxy-x8ptf requesting resource cpu=0m on Node essentialpks-conformance-2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3.1599eb92dae3e127], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8472/filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3 to essentialpks-conformance-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3.1599eb930ccd04ef], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3.1599eb930f83373d], Reason = [Created], Message = [Created container filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3.1599eb9319baa67f], Reason = [Started], Message = [Started container filler-pod-95ab4864-6a6a-11e9-aeaf-0a580af401c3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3.1599eb92db4018b9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8472/filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3 to essentialpks-conformance-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3.1599eb92ff3678aa], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3.1599eb9301928198], Reason = [Created], Message = [Created container filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3.1599eb930a3cdef6], Reason = [Started], Message = [Started container filler-pod-95ac75b8-6a6a-11e9-aeaf-0a580af401c3]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1599eb935360aa37], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node essentialpks-conformance-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node essentialpks-conformance-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:36:05.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8472" for this suite.
Apr 29 10:36:11.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:36:11.343: INFO: namespace sched-pred-8472 deletion completed in 6.098150938s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.270 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:36:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Apr 29 10:36:11.378: INFO: Waiting up to 5m0s for pod "client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3" in namespace "containers-6145" to be "success or failure"
Apr 29 10:36:11.382: INFO: Pod "client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.390013ms
Apr 29 10:36:13.386: INFO: Pod "client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007957641s
STEP: Saw pod success
Apr 29 10:36:13.386: INFO: Pod "client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:36:13.389: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:36:13.405: INFO: Waiting for pod client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:36:13.408: INFO: Pod client-containers-9b28afe6-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:36:13.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6145" for this suite.
Apr 29 10:36:19.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:36:19.506: INFO: namespace containers-6145 deletion completed in 6.094188975s

• [SLOW TEST:8.163 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:36:19.506: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0429 10:36:29.556104      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 29 10:36:29.556: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:36:29.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1880" for this suite.
Apr 29 10:36:35.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:36:35.655: INFO: namespace gc-1880 deletion completed in 6.092110751s

• [SLOW TEST:16.149 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:36:35.656: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 29 10:36:35.692: INFO: Waiting up to 5m0s for pod "pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3" in namespace "emptydir-3554" to be "success or failure"
Apr 29 10:36:35.697: INFO: Pod "pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.133659ms
Apr 29 10:36:37.701: INFO: Pod "pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008728204s
STEP: Saw pod success
Apr 29 10:36:37.701: INFO: Pod "pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:36:37.704: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:36:37.719: INFO: Waiting for pod pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:36:37.721: INFO: Pod pod-a9a6c3c0-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:36:37.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3554" for this suite.
Apr 29 10:36:43.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:36:43.816: INFO: namespace emptydir-3554 deletion completed in 6.090795741s

• [SLOW TEST:8.160 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:36:43.816: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:36:43.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3" in namespace "projected-9368" to be "success or failure"
Apr 29 10:36:43.855: INFO: Pod "downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.413767ms
Apr 29 10:36:45.858: INFO: Pod "downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006379617s
STEP: Saw pod success
Apr 29 10:36:45.858: INFO: Pod "downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:36:45.860: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:36:45.878: INFO: Waiting for pod downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:36:45.883: INFO: Pod downwardapi-volume-ae839dd1-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:36:45.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9368" for this suite.
Apr 29 10:36:51.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:36:51.989: INFO: namespace projected-9368 deletion completed in 6.10195458s

• [SLOW TEST:8.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:36:51.990: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:36:52.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3" in namespace "downward-api-5557" to be "success or failure"
Apr 29 10:36:52.033: INFO: Pod "downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.744877ms
Apr 29 10:36:54.038: INFO: Pod "downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012377945s
STEP: Saw pod success
Apr 29 10:36:54.038: INFO: Pod "downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:36:54.042: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:36:54.061: INFO: Waiting for pod downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:36:54.063: INFO: Pod downwardapi-volume-b362e0b8-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:36:54.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5557" for this suite.
Apr 29 10:37:00.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:37:00.168: INFO: namespace downward-api-5557 deletion completed in 6.102366969s

• [SLOW TEST:8.179 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:37:00.170: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 10:37:00.209: INFO: Waiting up to 5m0s for pod "pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3" in namespace "emptydir-5414" to be "success or failure"
Apr 29 10:37:00.214: INFO: Pod "pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.60334ms
Apr 29 10:37:02.218: INFO: Pod "pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009441065s
STEP: Saw pod success
Apr 29 10:37:02.218: INFO: Pod "pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:37:02.222: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:37:02.240: INFO: Waiting for pod pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:37:02.243: INFO: Pod pod-b843d37d-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:37:02.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5414" for this suite.
Apr 29 10:37:08.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:37:08.344: INFO: namespace emptydir-5414 deletion completed in 6.095972172s

• [SLOW TEST:8.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:37:08.347: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-bd22cfb2-6a6a-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:37:08.384: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3" in namespace "configmap-7500" to be "success or failure"
Apr 29 10:37:08.388: INFO: Pod "pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219007ms
Apr 29 10:37:10.391: INFO: Pod "pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007475267s
STEP: Saw pod success
Apr 29 10:37:10.391: INFO: Pod "pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:37:10.394: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:37:10.409: INFO: Waiting for pod pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:37:10.411: INFO: Pod pod-configmaps-bd23409d-6a6a-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:37:10.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7500" for this suite.
Apr 29 10:37:16.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:37:16.530: INFO: namespace configmap-7500 deletion completed in 6.116057674s

• [SLOW TEST:8.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:37:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 29 10:37:18.579: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-c2032fbe-6a6a-11e9-aeaf-0a580af401c3,GenerateName:,Namespace:events-9370,SelfLink:/api/v1/namespaces/events-9370/pods/send-events-c2032fbe-6a6a-11e9-aeaf-0a580af401c3,UID:c2067413-6a6a-11e9-9342-005056a45e5c,ResourceVersion:35094,Generation:0,CreationTimestamp:2019-04-29 10:37:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 557720006,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vmhxm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vmhxm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-vmhxm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d83eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d83ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:37:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:37:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:37:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:37:16 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.233,StartTime:2019-04-29 10:37:16 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-29 10:37:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://6f9da706e8abbdb41bf3a13893b3c763fc83f69e01ff6fc7c1aabb257d1a7e13}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr 29 10:37:20.583: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 29 10:37:22.586: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:37:22.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9370" for this suite.
Apr 29 10:38:00.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:38:00.713: INFO: namespace events-9370 deletion completed in 38.113034098s

• [SLOW TEST:44.183 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:38:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Apr 29 10:38:00.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-9118'
Apr 29 10:38:01.117: INFO: stderr: ""
Apr 29 10:38:01.117: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Apr 29 10:38:02.121: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 10:38:02.121: INFO: Found 0 / 1
Apr 29 10:38:03.121: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 10:38:03.121: INFO: Found 1 / 1
Apr 29 10:38:03.121: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 10:38:03.125: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 10:38:03.125: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr 29 10:38:03.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 logs redis-master-6chxb redis-master --namespace=kubectl-9118'
Apr 29 10:38:03.231: INFO: stderr: ""
Apr 29 10:38:03.231: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Apr 10:38:02.048 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Apr 10:38:02.048 # Server started, Redis version 3.2.12\n1:M 29 Apr 10:38:02.048 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Apr 10:38:02.048 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr 29 10:38:03.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 log redis-master-6chxb redis-master --namespace=kubectl-9118 --tail=1'
Apr 29 10:38:03.338: INFO: stderr: ""
Apr 29 10:38:03.338: INFO: stdout: "1:M 29 Apr 10:38:02.048 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr 29 10:38:03.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 log redis-master-6chxb redis-master --namespace=kubectl-9118 --limit-bytes=1'
Apr 29 10:38:03.440: INFO: stderr: ""
Apr 29 10:38:03.440: INFO: stdout: " "
STEP: exposing timestamps
Apr 29 10:38:03.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 log redis-master-6chxb redis-master --namespace=kubectl-9118 --tail=1 --timestamps'
Apr 29 10:38:03.546: INFO: stderr: ""
Apr 29 10:38:03.546: INFO: stdout: "2019-04-29T10:38:02.050834701Z 1:M 29 Apr 10:38:02.048 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr 29 10:38:06.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 log redis-master-6chxb redis-master --namespace=kubectl-9118 --since=1s'
Apr 29 10:38:06.148: INFO: stderr: ""
Apr 29 10:38:06.148: INFO: stdout: ""
Apr 29 10:38:06.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 log redis-master-6chxb redis-master --namespace=kubectl-9118 --since=24h'
Apr 29 10:38:06.255: INFO: stderr: ""
Apr 29 10:38:06.255: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Apr 10:38:02.048 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Apr 10:38:02.048 # Server started, Redis version 3.2.12\n1:M 29 Apr 10:38:02.048 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Apr 10:38:02.048 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Apr 29 10:38:06.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-9118'
Apr 29 10:38:06.365: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:38:06.365: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr 29 10:38:06.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get rc,svc -l name=nginx --no-headers --namespace=kubectl-9118'
Apr 29 10:38:06.469: INFO: stderr: "No resources found.\n"
Apr 29 10:38:06.469: INFO: stdout: ""
Apr 29 10:38:06.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -l name=nginx --namespace=kubectl-9118 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 10:38:06.547: INFO: stderr: ""
Apr 29 10:38:06.547: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:38:06.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9118" for this suite.
Apr 29 10:38:28.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:38:28.658: INFO: namespace kubectl-9118 deletion completed in 22.104262171s

• [SLOW TEST:27.944 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:38:28.658: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4866
Apr 29 10:38:30.701: INFO: Started pod liveness-exec in namespace container-probe-4866
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 10:38:30.705: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:42:31.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4866" for this suite.
Apr 29 10:42:37.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:42:37.366: INFO: namespace container-probe-4866 deletion completed in 6.098706165s

• [SLOW TEST:248.708 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:42:37.375: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:42:37.404: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:42:39.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7780" for this suite.
Apr 29 10:43:19.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:43:19.545: INFO: namespace pods-7780 deletion completed in 40.09710523s

• [SLOW TEST:42.171 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:43:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Apr 29 10:43:19.579: INFO: Waiting up to 5m0s for pod "var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3" in namespace "var-expansion-2787" to be "success or failure"
Apr 29 10:43:19.583: INFO: Pod "var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071757ms
Apr 29 10:43:21.587: INFO: Pod "var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007738299s
STEP: Saw pod success
Apr 29 10:43:21.587: INFO: Pod "var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:43:21.590: INFO: Trying to get logs from node essentialpks-conformance-2 pod var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 10:43:21.605: INFO: Waiting for pod var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:43:21.607: INFO: Pod var-expansion-9a6317f5-6a6b-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:43:21.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2787" for this suite.
Apr 29 10:43:27.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:43:27.696: INFO: namespace var-expansion-2787 deletion completed in 6.086575858s

• [SLOW TEST:8.147 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:43:27.698: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8535
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 29 10:43:27.745: INFO: Found 0 stateful pods, waiting for 3
Apr 29 10:43:37.749: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:43:37.749: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:43:37.749: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 29 10:43:37.772: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 29 10:43:47.815: INFO: Updating stateful set ss2
Apr 29 10:43:47.833: INFO: Waiting for Pod statefulset-8535/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 29 10:43:57.843: INFO: Waiting for Pod statefulset-8535/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr 29 10:44:07.895: INFO: Found 2 stateful pods, waiting for 3
Apr 29 10:44:17.901: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:44:17.901: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 10:44:17.901: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 29 10:44:17.931: INFO: Updating stateful set ss2
Apr 29 10:44:17.936: INFO: Waiting for Pod statefulset-8535/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 29 10:44:27.960: INFO: Updating stateful set ss2
Apr 29 10:44:27.978: INFO: Waiting for StatefulSet statefulset-8535/ss2 to complete update
Apr 29 10:44:27.979: INFO: Waiting for Pod statefulset-8535/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 29 10:44:37.986: INFO: Deleting all statefulset in ns statefulset-8535
Apr 29 10:44:37.993: INFO: Scaling statefulset ss2 to 0
Apr 29 10:45:08.009: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:45:08.014: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:45:08.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8535" for this suite.
Apr 29 10:45:14.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:45:14.164: INFO: namespace statefulset-8535 deletion completed in 6.13215883s

• [SLOW TEST:106.466 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:45:14.166: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 29 10:45:18.268: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:18.271: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 10:45:20.272: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:20.275: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 10:45:22.272: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:22.275: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 10:45:24.272: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:24.274: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 10:45:26.272: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:26.275: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 10:45:28.272: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:28.276: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 10:45:30.272: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 10:45:30.276: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:45:30.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6686" for this suite.
Apr 29 10:45:52.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:45:52.390: INFO: namespace container-lifecycle-hook-6686 deletion completed in 22.101725272s

• [SLOW TEST:38.225 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:45:52.399: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-f57ea19b-6a6b-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:45:52.440: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3" in namespace "projected-8443" to be "success or failure"
Apr 29 10:45:52.444: INFO: Pod "pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016373ms
Apr 29 10:45:54.448: INFO: Pod "pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007792537s
STEP: Saw pod success
Apr 29 10:45:54.448: INFO: Pod "pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:45:54.450: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:45:54.467: INFO: Waiting for pod pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:45:54.469: INFO: Pod pod-projected-configmaps-f57f258f-6a6b-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:45:54.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8443" for this suite.
Apr 29 10:46:00.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:46:00.572: INFO: namespace projected-8443 deletion completed in 6.099662106s

• [SLOW TEST:8.173 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:46:00.573: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 29 10:46:03.131: INFO: Successfully updated pod "labelsupdatefa5d1722-6a6b-11e9-aeaf-0a580af401c3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:46:07.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7293" for this suite.
Apr 29 10:46:29.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:46:29.266: INFO: namespace downward-api-7293 deletion completed in 22.094063098s

• [SLOW TEST:28.693 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:46:29.266: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:46:29.305: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 29 10:46:34.309: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 10:46:34.309: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 29 10:46:36.313: INFO: Creating deployment "test-rollover-deployment"
Apr 29 10:46:36.323: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 29 10:46:38.335: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 29 10:46:38.344: INFO: Ensure that both replica sets have 1 created replica
Apr 29 10:46:38.350: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 29 10:46:38.360: INFO: Updating deployment test-rollover-deployment
Apr 29 10:46:38.360: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 29 10:46:40.371: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 29 10:46:40.377: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 29 10:46:40.383: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 10:46:40.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131600, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 10:46:42.390: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 10:46:42.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131600, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 10:46:44.391: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 10:46:44.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131600, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 10:46:46.391: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 10:46:46.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131600, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 10:46:48.390: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 10:46:48.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131600, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692131596, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 10:46:50.392: INFO: 
Apr 29 10:46:50.392: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 29 10:46:50.400: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-245,SelfLink:/apis/apps/v1/namespaces/deployment-245/deployments/test-rollover-deployment,UID:0fa52e92-6a6c-11e9-9342-005056a45e5c,ResourceVersion:36483,Generation:2,CreationTimestamp:2019-04-29 10:46:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-29 10:46:36 +0000 UTC 2019-04-29 10:46:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-29 10:46:50 +0000 UTC 2019-04-29 10:46:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 10:46:50.403: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-245,SelfLink:/apis/apps/v1/namespaces/deployment-245/replicasets/test-rollover-deployment-766b4d6c9d,UID:10dd02d4-6a6c-11e9-9342-005056a45e5c,ResourceVersion:36472,Generation:2,CreationTimestamp:2019-04-29 10:46:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0fa52e92-6a6c-11e9-9342-005056a45e5c 0xc0031937e7 0xc0031937e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 29 10:46:50.403: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 29 10:46:50.403: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-245,SelfLink:/apis/apps/v1/namespaces/deployment-245/replicasets/test-rollover-controller,UID:0b76a1c7-6a6c-11e9-9342-005056a45e5c,ResourceVersion:36482,Generation:2,CreationTimestamp:2019-04-29 10:46:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0fa52e92-6a6c-11e9-9342-005056a45e5c 0xc003193627 0xc003193628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 10:46:50.403: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-245,SelfLink:/apis/apps/v1/namespaces/deployment-245/replicasets/test-rollover-deployment-6455657675,UID:0fa7f59b-6a6c-11e9-9342-005056a45e5c,ResourceVersion:36435,Generation:2,CreationTimestamp:2019-04-29 10:46:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0fa52e92-6a6c-11e9-9342-005056a45e5c 0xc0031936f7 0xc0031936f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 10:46:50.406: INFO: Pod "test-rollover-deployment-766b4d6c9d-9r4dl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-9r4dl,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-245,SelfLink:/api/v1/namespaces/deployment-245/pods/test-rollover-deployment-766b4d6c9d-9r4dl,UID:10e1c38b-6a6c-11e9-9342-005056a45e5c,ResourceVersion:36453,Generation:0,CreationTimestamp:2019-04-29 10:46:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 10dd02d4-6a6c-11e9-9342-005056a45e5c 0xc002c97787 0xc002c97788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pxzzt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pxzzt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pxzzt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c977f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c97810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:46:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:46:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:46:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:46:38 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.250,StartTime:2019-04-29 10:46:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-29 10:46:39 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://993d41c50273dfa1678c2d40a87de86b44da2f59a16f0899c71b68a8b23603c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:46:50.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-245" for this suite.
Apr 29 10:46:56.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:46:56.511: INFO: namespace deployment-245 deletion completed in 6.102083024s

• [SLOW TEST:27.245 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:46:56.511: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:46:58.578: INFO: Waiting up to 5m0s for pod "client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3" in namespace "pods-5350" to be "success or failure"
Apr 29 10:46:58.585: INFO: Pod "client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.683363ms
Apr 29 10:47:00.590: INFO: Pod "client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012206002s
STEP: Saw pod success
Apr 29 10:47:00.590: INFO: Pod "client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:47:00.594: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3 container env3cont: <nil>
STEP: delete the pod
Apr 29 10:47:00.610: INFO: Waiting for pod client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:47:00.613: INFO: Pod client-envvars-1ceb4c54-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:47:00.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5350" for this suite.
Apr 29 10:47:38.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:47:38.712: INFO: namespace pods-5350 deletion completed in 38.095097739s

• [SLOW TEST:42.201 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:47:38.713: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 10:47:41.270: INFO: Successfully updated pod "pod-update-activedeadlineseconds-34dcaa07-6a6c-11e9-aeaf-0a580af401c3"
Apr 29 10:47:41.270: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-34dcaa07-6a6c-11e9-aeaf-0a580af401c3" in namespace "pods-9213" to be "terminated due to deadline exceeded"
Apr 29 10:47:41.274: INFO: Pod "pod-update-activedeadlineseconds-34dcaa07-6a6c-11e9-aeaf-0a580af401c3": Phase="Running", Reason="", readiness=true. Elapsed: 3.257182ms
Apr 29 10:47:43.279: INFO: Pod "pod-update-activedeadlineseconds-34dcaa07-6a6c-11e9-aeaf-0a580af401c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008316207s
Apr 29 10:47:45.282: INFO: Pod "pod-update-activedeadlineseconds-34dcaa07-6a6c-11e9-aeaf-0a580af401c3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.011733886s
Apr 29 10:47:45.282: INFO: Pod "pod-update-activedeadlineseconds-34dcaa07-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:47:45.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9213" for this suite.
Apr 29 10:47:51.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:47:51.394: INFO: namespace pods-9213 deletion completed in 6.107582395s

• [SLOW TEST:12.681 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:47:51.394: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 29 10:47:55.470: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:47:55.477: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:47:57.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:47:57.480: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:47:59.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:47:59.480: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:01.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:01.481: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:03.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:03.499: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:05.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:05.480: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:07.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:07.481: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:09.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:09.481: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:11.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:11.481: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:13.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:13.481: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:15.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:15.482: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:17.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:17.484: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 10:48:19.477: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 10:48:19.481: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:48:19.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9427" for this suite.
Apr 29 10:48:41.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:48:41.602: INFO: namespace container-lifecycle-hook-9427 deletion completed in 22.104054733s

• [SLOW TEST:50.208 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:48:41.607: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:48:41.637: INFO: Creating ReplicaSet my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3
Apr 29 10:48:41.651: INFO: Pod name my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3: Found 1 pods out of 1
Apr 29 10:48:41.651: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3" is running
Apr 29 10:48:43.662: INFO: Pod "my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3-zg75r" is running (conditions: [])
Apr 29 10:48:43.662: INFO: Trying to dial the pod
Apr 29 10:48:48.674: INFO: Controller my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3: Got expected result from replica 1 [my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3-zg75r]: "my-hostname-basic-5a5a01e2-6a6c-11e9-aeaf-0a580af401c3-zg75r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:48:48.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5364" for this suite.
Apr 29 10:48:54.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:48:54.782: INFO: namespace replicaset-5364 deletion completed in 6.103102332s

• [SLOW TEST:13.175 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:48:54.782: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:49:18.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-662" for this suite.
Apr 29 10:49:24.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:49:25.022: INFO: namespace namespaces-662 deletion completed in 6.094981456s
STEP: Destroying namespace "nsdeletetest-4172" for this suite.
Apr 29 10:49:25.025: INFO: Namespace nsdeletetest-4172 was already deleted
STEP: Destroying namespace "nsdeletetest-7556" for this suite.
Apr 29 10:49:31.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:49:31.133: INFO: namespace nsdeletetest-7556 deletion completed in 6.107917181s

• [SLOW TEST:36.353 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:49:31.139: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:49:31.175: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3" in namespace "projected-8688" to be "success or failure"
Apr 29 10:49:31.179: INFO: Pod "downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418255ms
Apr 29 10:49:33.184: INFO: Pod "downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009289044s
STEP: Saw pod success
Apr 29 10:49:33.184: INFO: Pod "downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:49:33.190: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:49:33.207: INFO: Waiting for pod downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:49:33.210: INFO: Pod downwardapi-volume-77df9691-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:49:33.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8688" for this suite.
Apr 29 10:49:39.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:49:39.305: INFO: namespace projected-8688 deletion completed in 6.091321198s

• [SLOW TEST:8.166 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:49:39.306: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7cbd96b1-6a6c-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:49:39.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3" in namespace "projected-1299" to be "success or failure"
Apr 29 10:49:39.343: INFO: Pod "pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.794747ms
Apr 29 10:49:41.347: INFO: Pod "pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007426782s
STEP: Saw pod success
Apr 29 10:49:41.347: INFO: Pod "pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:49:41.351: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:49:41.368: INFO: Waiting for pod pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:49:41.371: INFO: Pod pod-projected-configmaps-7cbe08a1-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:49:41.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1299" for this suite.
Apr 29 10:49:47.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:49:47.476: INFO: namespace projected-1299 deletion completed in 6.101827932s

• [SLOW TEST:8.170 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:49:47.476: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-819c31a3-6a6c-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:49:47.534: INFO: Waiting up to 5m0s for pod "pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3" in namespace "secrets-1216" to be "success or failure"
Apr 29 10:49:47.548: INFO: Pod "pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.115182ms
Apr 29 10:49:49.553: INFO: Pod "pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018774592s
STEP: Saw pod success
Apr 29 10:49:49.553: INFO: Pod "pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:49:49.556: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:49:49.576: INFO: Waiting for pod pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:49:49.579: INFO: Pod pod-secrets-81a01b38-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:49:49.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1216" for this suite.
Apr 29 10:49:55.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:49:55.674: INFO: namespace secrets-1216 deletion completed in 6.0913098s
STEP: Destroying namespace "secret-namespace-5043" for this suite.
Apr 29 10:50:01.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:50:01.783: INFO: namespace secret-namespace-5043 deletion completed in 6.108207436s

• [SLOW TEST:14.307 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:50:01.790: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:50:01.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 version'
Apr 29 10:50:01.895: INFO: stderr: ""
Apr 29 10:50:01.895: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1+vmware.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-09T21:17:38Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:50:01.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2593" for this suite.
Apr 29 10:50:07.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:50:07.984: INFO: namespace kubectl-2593 deletion completed in 6.086208012s

• [SLOW TEST:6.194 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:50:07.984: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 29 10:50:08.036: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:50:11.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2570" for this suite.
Apr 29 10:50:33.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:50:33.455: INFO: namespace init-container-2570 deletion completed in 22.100535914s

• [SLOW TEST:25.471 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:50:33.457: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Apr 29 10:50:33.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 --namespace=kubectl-6759 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 29 10:50:35.720: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 29 10:50:35.720: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:50:37.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6759" for this suite.
Apr 29 10:50:43.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:50:43.814: INFO: namespace kubectl-6759 deletion completed in 6.084093617s

• [SLOW TEST:10.357 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:50:43.814: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-a3319b1a-6a6c-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:50:43.856: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3" in namespace "projected-7999" to be "success or failure"
Apr 29 10:50:43.864: INFO: Pod "pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.830489ms
Apr 29 10:50:45.867: INFO: Pod "pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011208231s
STEP: Saw pod success
Apr 29 10:50:45.867: INFO: Pod "pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:50:45.871: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:50:45.891: INFO: Waiting for pod pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:50:45.894: INFO: Pod pod-projected-secrets-a3320a3d-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:50:45.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7999" for this suite.
Apr 29 10:50:51.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:50:51.994: INFO: namespace projected-7999 deletion completed in 6.095463259s

• [SLOW TEST:8.181 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:50:51.999: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:50:52.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3" in namespace "downward-api-6190" to be "success or failure"
Apr 29 10:50:52.045: INFO: Pod "downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090687ms
Apr 29 10:50:54.050: INFO: Pod "downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009389603s
Apr 29 10:50:56.053: INFO: Pod "downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012322718s
STEP: Saw pod success
Apr 29 10:50:56.053: INFO: Pod "downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:50:56.056: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:50:56.079: INFO: Waiting for pod downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:50:56.081: INFO: Pod downwardapi-volume-a812d7ab-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:50:56.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6190" for this suite.
Apr 29 10:51:02.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:51:02.169: INFO: namespace downward-api-6190 deletion completed in 6.08585158s

• [SLOW TEST:10.170 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:51:02.170: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:51:02.205: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 29 10:51:07.209: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 10:51:07.209: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 29 10:51:07.226: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8691,SelfLink:/apis/apps/v1/namespaces/deployment-8691/deployments/test-cleanup-deployment,UID:b11c5ee0-6a6c-11e9-9342-005056a45e5c,ResourceVersion:37322,Generation:1,CreationTimestamp:2019-04-29 10:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 10:51:07.231: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-8691,SelfLink:/apis/apps/v1/namespaces/deployment-8691/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:b11e4781-6a6c-11e9-9342-005056a45e5c,ResourceVersion:37324,Generation:1,CreationTimestamp:2019-04-29 10:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b11c5ee0-6a6c-11e9-9342-005056a45e5c 0xc002a83017 0xc002a83018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 10:51:07.231: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 29 10:51:07.231: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8691,SelfLink:/apis/apps/v1/namespaces/deployment-8691/replicasets/test-cleanup-controller,UID:ae1ed5e8-6a6c-11e9-9342-005056a45e5c,ResourceVersion:37323,Generation:1,CreationTimestamp:2019-04-29 10:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b11c5ee0-6a6c-11e9-9342-005056a45e5c 0xc002a82f47 0xc002a82f48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 29 10:51:07.254: INFO: Pod "test-cleanup-controller-bfx6k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-bfx6k,GenerateName:test-cleanup-controller-,Namespace:deployment-8691,SelfLink:/api/v1/namespaces/deployment-8691/pods/test-cleanup-controller-bfx6k,UID:ae1ffdfc-6a6c-11e9-9342-005056a45e5c,ResourceVersion:37314,Generation:0,CreationTimestamp:2019-04-29 10:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller ae1ed5e8-6a6c-11e9-9342-005056a45e5c 0xc002a83987 0xc002a83988}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rwmtn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rwmtn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rwmtn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a839f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a83a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:51:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:51:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:51:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:51:02 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.12,StartTime:2019-04-29 10:51:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 10:51:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://2d7cc797737c571d06e73137dbd7f920a059ffb20b5bf84b991ee58b2605df3b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 10:51:07.254: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-5c59s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-5c59s,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-8691,SelfLink:/api/v1/namespaces/deployment-8691/pods/test-cleanup-deployment-55cbfbc8f5-5c59s,UID:b11ee4ba-6a6c-11e9-9342-005056a45e5c,ResourceVersion:37325,Generation:0,CreationTimestamp:2019-04-29 10:51:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 b11e4781-6a6c-11e9-9342-005056a45e5c 0xc002a83ae7 0xc002a83ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rwmtn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rwmtn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rwmtn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a83b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a83b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:51:07.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8691" for this suite.
Apr 29 10:51:13.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:51:13.363: INFO: namespace deployment-8691 deletion completed in 6.097784837s

• [SLOW TEST:11.194 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:51:13.366: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-b4cdcb94-6a6c-11e9-aeaf-0a580af401c3
STEP: Creating secret with name secret-projected-all-test-volume-b4cdcb7c-6a6c-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 29 10:51:13.409: INFO: Waiting up to 5m0s for pod "projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3" in namespace "projected-4139" to be "success or failure"
Apr 29 10:51:13.413: INFO: Pod "projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658651ms
Apr 29 10:51:15.416: INFO: Pod "projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006608572s
STEP: Saw pod success
Apr 29 10:51:15.416: INFO: Pod "projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:51:15.419: INFO: Trying to get logs from node essentialpks-conformance-2 pod projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 29 10:51:15.433: INFO: Waiting for pod projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:51:15.435: INFO: Pod projected-volume-b4cdcb4d-6a6c-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:51:15.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4139" for this suite.
Apr 29 10:51:21.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:51:21.535: INFO: namespace projected-4139 deletion completed in 6.096929833s

• [SLOW TEST:8.169 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:51:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Apr 29 10:51:21.562: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 29 10:51:21.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4860'
Apr 29 10:51:21.729: INFO: stderr: ""
Apr 29 10:51:21.729: INFO: stdout: "service/redis-slave created\n"
Apr 29 10:51:21.729: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 29 10:51:21.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4860'
Apr 29 10:51:21.888: INFO: stderr: ""
Apr 29 10:51:21.888: INFO: stdout: "service/redis-master created\n"
Apr 29 10:51:21.888: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 29 10:51:21.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4860'
Apr 29 10:51:22.069: INFO: stderr: ""
Apr 29 10:51:22.069: INFO: stdout: "service/frontend created\n"
Apr 29 10:51:22.069: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 29 10:51:22.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4860'
Apr 29 10:51:22.238: INFO: stderr: ""
Apr 29 10:51:22.238: INFO: stdout: "deployment.apps/frontend created\n"
Apr 29 10:51:22.239: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 29 10:51:22.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4860'
Apr 29 10:51:22.411: INFO: stderr: ""
Apr 29 10:51:22.411: INFO: stdout: "deployment.apps/redis-master created\n"
Apr 29 10:51:22.411: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 29 10:51:22.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4860'
Apr 29 10:51:22.609: INFO: stderr: ""
Apr 29 10:51:22.609: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr 29 10:51:22.609: INFO: Waiting for all frontend pods to be Running.
Apr 29 10:51:27.660: INFO: Waiting for frontend to serve content.
Apr 29 10:51:27.706: INFO: Trying to add a new entry to the guestbook.
Apr 29 10:51:27.752: INFO: Verifying that added entry can be retrieved.
Apr 29 10:51:27.764: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:51:32.779: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:51:37.793: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:51:42.809: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:51:47.827: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:51:52.847: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:51:57.863: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:52:02.886: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:52:07.903: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:52:12.919: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:52:17.935: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Apr 29 10:52:22.960: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Apr 29 10:52:27.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4860'
Apr 29 10:52:28.100: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:52:28.100: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 10:52:28.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4860'
Apr 29 10:52:28.218: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:52:28.218: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 10:52:28.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4860'
Apr 29 10:52:28.348: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:52:28.348: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 10:52:28.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4860'
Apr 29 10:52:28.449: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:52:28.449: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 10:52:28.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4860'
Apr 29 10:52:28.542: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:52:28.542: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 10:52:28.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4860'
Apr 29 10:52:28.638: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 10:52:28.638: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:52:28.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4860" for this suite.
Apr 29 10:53:10.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:53:10.762: INFO: namespace kubectl-4860 deletion completed in 42.118859839s

• [SLOW TEST:109.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:53:10.765: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:53:10.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 version --client'
Apr 29 10:53:10.877: INFO: stderr: ""
Apr 29 10:53:10.877: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr 29 10:53:10.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4835'
Apr 29 10:53:11.066: INFO: stderr: ""
Apr 29 10:53:11.067: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 29 10:53:11.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4835'
Apr 29 10:53:11.266: INFO: stderr: ""
Apr 29 10:53:11.266: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 29 10:53:12.271: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 10:53:12.271: INFO: Found 0 / 1
Apr 29 10:53:13.270: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 10:53:13.270: INFO: Found 1 / 1
Apr 29 10:53:13.270: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 10:53:13.273: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 10:53:13.273: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 10:53:13.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 describe pod redis-master-zdpft --namespace=kubectl-4835'
Apr 29 10:53:13.373: INFO: stderr: ""
Apr 29 10:53:13.373: INFO: stdout: "Name:               redis-master-zdpft\nNamespace:          kubectl-4835\nPriority:           0\nPriorityClassName:  <none>\nNode:               essentialpks-conformance-2/192.168.102.3\nStart Time:         Mon, 29 Apr 2019 10:53:11 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.2.19\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://26439485edfebfca1300ef7f99b9ecda85bb03589bc13bb5095e23676cc06ece\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 29 Apr 2019 10:53:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wk6t7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wk6t7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wk6t7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                 Message\n  ----    ------     ----  ----                                 -------\n  Normal  Scheduled  2s    default-scheduler                    Successfully assigned kubectl-4835/redis-master-zdpft to essentialpks-conformance-2\n  Normal  Pulled     2s    kubelet, essentialpks-conformance-2  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, essentialpks-conformance-2  Created container redis-master\n  Normal  Started    2s    kubelet, essentialpks-conformance-2  Started container redis-master\n"
Apr 29 10:53:13.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 describe rc redis-master --namespace=kubectl-4835'
Apr 29 10:53:13.480: INFO: stderr: ""
Apr 29 10:53:13.480: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4835\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-zdpft\n"
Apr 29 10:53:13.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 describe service redis-master --namespace=kubectl-4835'
Apr 29 10:53:13.581: INFO: stderr: ""
Apr 29 10:53:13.581: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4835\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.102.63.166\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.2.19:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 29 10:53:13.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 describe node essentialpks-conformance-1'
Apr 29 10:53:13.704: INFO: stderr: ""
Apr 29 10:53:13.704: INFO: stdout: "Name:               essentialpks-conformance-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=essentialpks-conformance-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"66:0a:21:e8:08:da\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.102.2\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 19 Mar 2019 13:23:36 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 29 Apr 2019 10:52:51 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 29 Apr 2019 10:52:51 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 29 Apr 2019 10:52:51 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 29 Apr 2019 10:52:51 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.102.2\n  Hostname:    essentialpks-conformance-1\nCapacity:\n cpu:                2\n ephemeral-storage:  40168028Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8175132Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37018854544\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8072732Ki\n pods:               110\nSystem Info:\n Machine ID:                 a05031910e15e3003afad29e5c8f7b92\n System UUID:                B4732442-5A30-614F-A047-8F2023A2AFAD\n Boot ID:                    81227a72-9b24-4037-92f6-17192a32a52d\n Kernel Version:             4.4.0-116-generic\n OS Image:                   Ubuntu 16.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.14.1+vmware.1\n Kube-Proxy Version:         v1.14.1+vmware.1\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-dcgqh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  kube-system                coredns-59d4dcfccd-4tmr7                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     40d\n  kube-system                coredns-59d4dcfccd-gjs6z                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     40d\n  kube-system                etcd-essentialpks-conformance-1                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m\n  kube-system                kube-apiserver-essentialpks-conformance-1                  250m (12%)    0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                kube-controller-manager-essentialpks-conformance-1         200m (10%)    0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                kube-flannel-ds-amd64-cs5jp                                100m (5%)     100m (5%)   50Mi (0%)        50Mi (0%)      40d\n  kube-system                kube-proxy-xbbzv                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         40d\n  kube-system                kube-scheduler-essentialpks-conformance-1                  100m (5%)     0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (42%)  100m (5%)\n  memory             190Mi (2%)  390Mi (4%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Apr 29 10:53:13.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 describe namespace kubectl-4835'
Apr 29 10:53:13.813: INFO: stderr: ""
Apr 29 10:53:13.813: INFO: stdout: "Name:         kubectl-4835\nLabels:       e2e-framework=kubectl\n              e2e-run=7d8e90f8-6a67-11e9-aeaf-0a580af401c3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:53:13.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4835" for this suite.
Apr 29 10:53:35.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:53:35.922: INFO: namespace kubectl-4835 deletion completed in 22.104209419s

• [SLOW TEST:25.159 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:53:35.923: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-1257
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1257 to expose endpoints map[]
Apr 29 10:53:35.965: INFO: successfully validated that service endpoint-test2 in namespace services-1257 exposes endpoints map[] (6.108852ms elapsed)
STEP: Creating pod pod1 in namespace services-1257
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1257 to expose endpoints map[pod1:[80]]
Apr 29 10:53:37.992: INFO: successfully validated that service endpoint-test2 in namespace services-1257 exposes endpoints map[pod1:[80]] (2.021777194s elapsed)
STEP: Creating pod pod2 in namespace services-1257
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1257 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 29 10:53:39.015: INFO: successfully validated that service endpoint-test2 in namespace services-1257 exposes endpoints map[pod1:[80] pod2:[80]] (1.01656472s elapsed)
STEP: Deleting pod pod1 in namespace services-1257
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1257 to expose endpoints map[pod2:[80]]
Apr 29 10:53:39.027: INFO: successfully validated that service endpoint-test2 in namespace services-1257 exposes endpoints map[pod2:[80]] (5.455176ms elapsed)
STEP: Deleting pod pod2 in namespace services-1257
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1257 to expose endpoints map[]
Apr 29 10:53:39.041: INFO: successfully validated that service endpoint-test2 in namespace services-1257 exposes endpoints map[] (6.398717ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:53:39.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1257" for this suite.
Apr 29 10:54:01.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:54:01.170: INFO: namespace services-1257 deletion completed in 22.100694475s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:25.247 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:54:01.170: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6149.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6149.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6149.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6149.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6149.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6149.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 10:54:03.245: INFO: DNS probes using dns-6149/dns-test-18d36d64-6a6d-11e9-aeaf-0a580af401c3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:54:03.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6149" for this suite.
Apr 29 10:54:09.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:54:09.347: INFO: namespace dns-6149 deletion completed in 6.085084346s

• [SLOW TEST:8.177 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:54:09.348: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 10:54:09.391: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3" in namespace "projected-9099" to be "success or failure"
Apr 29 10:54:09.398: INFO: Pod "downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.602197ms
Apr 29 10:54:11.403: INFO: Pod "downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012672444s
STEP: Saw pod success
Apr 29 10:54:11.403: INFO: Pod "downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:54:11.407: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 10:54:11.429: INFO: Waiting for pod downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:54:11.432: INFO: Pod downwardapi-volume-1db4009d-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:54:11.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9099" for this suite.
Apr 29 10:54:17.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:54:17.560: INFO: namespace projected-9099 deletion completed in 6.123562158s

• [SLOW TEST:8.212 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:54:17.560: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:54:17.585: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:54:19.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1546" for this suite.
Apr 29 10:55:03.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:55:03.837: INFO: namespace pods-1546 deletion completed in 44.111743225s

• [SLOW TEST:46.277 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:55:03.843: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 29 10:55:03.873: INFO: PodSpec: initContainers in spec.initContainers
Apr 29 10:55:47.991: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3e2eab44-6a6d-11e9-aeaf-0a580af401c3", GenerateName:"", Namespace:"init-container-7392", SelfLink:"/api/v1/namespaces/init-container-7392/pods/pod-init-3e2eab44-6a6d-11e9-aeaf-0a580af401c3", UID:"3e2c95a4-6a6d-11e9-9342-005056a45e5c", ResourceVersion:"38203", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63692132103, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"873647776"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vkhcq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00276a4c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vkhcq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vkhcq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vkhcq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c96338), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"essentialpks-conformance-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002354480), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c963c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c963f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c963f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c963fc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692132103, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692132103, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692132103, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692132103, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.102.3", PodIP:"10.244.2.24", StartTime:(*v1.Time)(0xc0028ccb40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0026a8620)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0026a8690)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://86520d5142ad31728aecd72571dee753a80cd53a26fb50e578db9f7aa49e5bfe"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028ccc80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028ccbe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:55:47.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7392" for this suite.
Apr 29 10:56:10.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:56:10.084: INFO: namespace init-container-7392 deletion completed in 22.086808589s

• [SLOW TEST:66.242 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:56:10.086: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5350
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5350
STEP: Creating statefulset with conflicting port in namespace statefulset-5350
STEP: Waiting until pod test-pod will start running in namespace statefulset-5350
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5350
Apr 29 10:56:12.159: INFO: Observed stateful pod in namespace: statefulset-5350, name: ss-0, uid: 666536e0-6a6d-11e9-9342-005056a45e5c, status phase: Pending. Waiting for statefulset controller to delete.
Apr 29 10:56:12.343: INFO: Observed stateful pod in namespace: statefulset-5350, name: ss-0, uid: 666536e0-6a6d-11e9-9342-005056a45e5c, status phase: Failed. Waiting for statefulset controller to delete.
Apr 29 10:56:12.354: INFO: Observed stateful pod in namespace: statefulset-5350, name: ss-0, uid: 666536e0-6a6d-11e9-9342-005056a45e5c, status phase: Failed. Waiting for statefulset controller to delete.
Apr 29 10:56:12.357: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5350
STEP: Removing pod with conflicting port in namespace statefulset-5350
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5350 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 29 10:56:14.398: INFO: Deleting all statefulset in ns statefulset-5350
Apr 29 10:56:14.401: INFO: Scaling statefulset ss to 0
Apr 29 10:56:34.435: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 10:56:34.439: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:56:34.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5350" for this suite.
Apr 29 10:56:40.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:56:40.563: INFO: namespace statefulset-5350 deletion completed in 6.103842006s

• [SLOW TEST:30.478 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:56:40.565: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Apr 29 10:56:40.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 cluster-info'
Apr 29 10:56:40.680: INFO: stderr: ""
Apr 29 10:56:40.680: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:56:40.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4430" for this suite.
Apr 29 10:56:46.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:56:46.778: INFO: namespace kubectl-4430 deletion completed in 6.093678771s

• [SLOW TEST:6.213 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:56:46.778: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-7b891d7b-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7b891d7b-6a6d-11e9-aeaf-0a580af401c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:56:50.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9800" for this suite.
Apr 29 10:57:12.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:57:12.970: INFO: namespace configmap-9800 deletion completed in 22.101512801s

• [SLOW TEST:26.192 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:57:12.974: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:57:13.003: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 29 10:57:13.013: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 29 10:57:18.017: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 10:57:18.017: INFO: Creating deployment "test-rolling-update-deployment"
Apr 29 10:57:18.021: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 29 10:57:18.029: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 29 10:57:20.035: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 29 10:57:20.038: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 29 10:57:20.046: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4587,SelfLink:/apis/apps/v1/namespaces/deployment-4587/deployments/test-rolling-update-deployment,UID:8e210423-6a6d-11e9-9342-005056a45e5c,ResourceVersion:38546,Generation:1,CreationTimestamp:2019-04-29 10:57:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-29 10:57:18 +0000 UTC 2019-04-29 10:57:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-29 10:57:19 +0000 UTC 2019-04-29 10:57:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 10:57:20.049: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-4587,SelfLink:/apis/apps/v1/namespaces/deployment-4587/replicasets/test-rolling-update-deployment-67599b4d9,UID:8e229039-6a6d-11e9-9342-005056a45e5c,ResourceVersion:38536,Generation:1,CreationTimestamp:2019-04-29 10:57:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 8e210423-6a6d-11e9-9342-005056a45e5c 0xc0005883d0 0xc0005883d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr 29 10:57:20.049: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 29 10:57:20.049: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4587,SelfLink:/apis/apps/v1/namespaces/deployment-4587/replicasets/test-rolling-update-controller,UID:8b23f801-6a6d-11e9-9342-005056a45e5c,ResourceVersion:38545,Generation:2,CreationTimestamp:2019-04-29 10:57:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 8e210423-6a6d-11e9-9342-005056a45e5c 0xc0005882c7 0xc0005882c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 10:57:20.052: INFO: Pod "test-rolling-update-deployment-67599b4d9-n94b6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-n94b6,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-4587,SelfLink:/api/v1/namespaces/deployment-4587/pods/test-rolling-update-deployment-67599b4d9-n94b6,UID:8e232ba3-6a6d-11e9-9342-005056a45e5c,ResourceVersion:38535,Generation:0,CreationTimestamp:2019-04-29 10:57:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 8e229039-6a6d-11e9-9342-005056a45e5c 0xc0005895a0 0xc0005895a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7q7xw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7q7xw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7q7xw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0005896c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000589890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:57:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:57:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:57:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 10:57:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.29,StartTime:2019-04-29 10:57:18 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-29 10:57:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://d24826d9c45458471244846b1679b71f97a576d52fd87259ed94b3adbbe9a340}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:57:20.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4587" for this suite.
Apr 29 10:57:26.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:57:26.144: INFO: namespace deployment-4587 deletion completed in 6.088538533s

• [SLOW TEST:13.171 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:57:26.145: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:57:26.199: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"92ffd8f5-6a6d-11e9-9342-005056a45e5c", Controller:(*bool)(0xc0005aa406), BlockOwnerDeletion:(*bool)(0xc0005aa407)}}
Apr 29 10:57:26.204: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"92fdd031-6a6d-11e9-9342-005056a45e5c", Controller:(*bool)(0xc0003c6416), BlockOwnerDeletion:(*bool)(0xc0003c6417)}}
Apr 29 10:57:26.207: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"92fe4e42-6a6d-11e9-9342-005056a45e5c", Controller:(*bool)(0xc0005aa776), BlockOwnerDeletion:(*bool)(0xc0005aa777)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:57:31.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6825" for this suite.
Apr 29 10:57:37.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:57:37.308: INFO: namespace gc-6825 deletion completed in 6.08714605s

• [SLOW TEST:11.163 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:57:37.308: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 29 10:57:37.340: INFO: Waiting up to 5m0s for pod "downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3" in namespace "downward-api-2981" to be "success or failure"
Apr 29 10:57:37.346: INFO: Pod "downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.629472ms
Apr 29 10:57:39.352: INFO: Pod "downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011674764s
STEP: Saw pod success
Apr 29 10:57:39.352: INFO: Pod "downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:57:39.359: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 10:57:39.383: INFO: Waiting for pod downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:57:39.386: INFO: Pod downward-api-99a6fbd9-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:57:39.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2981" for this suite.
Apr 29 10:57:45.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:57:45.503: INFO: namespace downward-api-2981 deletion completed in 6.112520891s

• [SLOW TEST:8.195 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:57:45.504: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-9e89bfbc-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:57:45.540: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3" in namespace "projected-8513" to be "success or failure"
Apr 29 10:57:45.551: INFO: Pod "pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.278645ms
Apr 29 10:57:47.556: INFO: Pod "pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016446828s
STEP: Saw pod success
Apr 29 10:57:47.557: INFO: Pod "pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:57:47.561: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:57:47.579: INFO: Waiting for pod pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:57:47.582: INFO: Pod pod-projected-secrets-9e8a5547-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:57:47.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8513" for this suite.
Apr 29 10:57:53.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:57:53.666: INFO: namespace projected-8513 deletion completed in 6.081600157s

• [SLOW TEST:8.162 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:57:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 10:57:53.701: INFO: Waiting up to 5m0s for pod "pod-a367855e-6a6d-11e9-aeaf-0a580af401c3" in namespace "emptydir-4076" to be "success or failure"
Apr 29 10:57:53.708: INFO: Pod "pod-a367855e-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.384244ms
Apr 29 10:57:55.713: INFO: Pod "pod-a367855e-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011510865s
STEP: Saw pod success
Apr 29 10:57:55.713: INFO: Pod "pod-a367855e-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:57:55.716: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-a367855e-6a6d-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 10:57:55.733: INFO: Waiting for pod pod-a367855e-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:57:55.736: INFO: Pod pod-a367855e-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:57:55.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4076" for this suite.
Apr 29 10:58:01.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:58:01.825: INFO: namespace emptydir-4076 deletion completed in 6.085602919s

• [SLOW TEST:8.157 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:58:01.826: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a84529b0-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:58:01.869: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3" in namespace "projected-3193" to be "success or failure"
Apr 29 10:58:01.874: INFO: Pod "pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.2236ms
Apr 29 10:58:03.878: INFO: Pod "pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009117091s
STEP: Saw pod success
Apr 29 10:58:03.878: INFO: Pod "pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:58:03.883: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:58:03.907: INFO: Waiting for pod pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:58:03.911: INFO: Pod pod-projected-configmaps-a8458538-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:58:03.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3193" for this suite.
Apr 29 10:58:09.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:58:10.024: INFO: namespace projected-3193 deletion completed in 6.107812035s

• [SLOW TEST:8.198 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:58:10.024: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ad26cb28-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 10:58:10.056: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3" in namespace "configmap-2984" to be "success or failure"
Apr 29 10:58:10.067: INFO: Pod "pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.51252ms
Apr 29 10:58:12.071: INFO: Pod "pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014228058s
STEP: Saw pod success
Apr 29 10:58:12.071: INFO: Pod "pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:58:12.074: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 10:58:12.093: INFO: Waiting for pod pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:58:12.096: INFO: Pod pod-configmaps-ad273610-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:58:12.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2984" for this suite.
Apr 29 10:58:18.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:58:18.205: INFO: namespace configmap-2984 deletion completed in 6.104959005s

• [SLOW TEST:8.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:58:18.214: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 10:58:40.258: INFO: Container started at 2019-04-29 10:58:19 +0000 UTC, pod became ready at 2019-04-29 10:58:38 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:58:40.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-500" for this suite.
Apr 29 10:59:02.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:59:02.363: INFO: namespace container-probe-500 deletion completed in 22.10047476s

• [SLOW TEST:44.150 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:59:02.363: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 10:59:02.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4816'
Apr 29 10:59:02.547: INFO: stderr: ""
Apr 29 10:59:02.547: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Apr 29 10:59:02.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete pods e2e-test-nginx-pod --namespace=kubectl-4816'
Apr 29 10:59:08.974: INFO: stderr: ""
Apr 29 10:59:08.974: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:59:08.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4816" for this suite.
Apr 29 10:59:14.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:59:15.080: INFO: namespace kubectl-4816 deletion completed in 6.103486933s

• [SLOW TEST:12.718 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:59:15.082: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d3edc496-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 10:59:15.122: INFO: Waiting up to 5m0s for pod "pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3" in namespace "secrets-3011" to be "success or failure"
Apr 29 10:59:15.125: INFO: Pod "pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.000118ms
Apr 29 10:59:17.141: INFO: Pod "pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019277503s
STEP: Saw pod success
Apr 29 10:59:17.141: INFO: Pod "pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 10:59:17.144: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 10:59:17.162: INFO: Waiting for pod pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 10:59:17.165: INFO: Pod pod-secrets-d3ef2051-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:59:17.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3011" for this suite.
Apr 29 10:59:23.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 10:59:23.277: INFO: namespace secrets-3011 deletion completed in 6.107715325s

• [SLOW TEST:8.195 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 10:59:23.278: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Apr 29 10:59:23.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-7511'
Apr 29 10:59:23.479: INFO: stderr: ""
Apr 29 10:59:23.479: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 10:59:23.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7511'
Apr 29 10:59:23.578: INFO: stderr: ""
Apr 29 10:59:23.578: INFO: stdout: "update-demo-nautilus-7nxpw update-demo-nautilus-x6w5p "
Apr 29 10:59:23.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7nxpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:23.652: INFO: stderr: ""
Apr 29 10:59:23.652: INFO: stdout: ""
Apr 29 10:59:23.652: INFO: update-demo-nautilus-7nxpw is created but not running
Apr 29 10:59:28.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7511'
Apr 29 10:59:28.735: INFO: stderr: ""
Apr 29 10:59:28.735: INFO: stdout: "update-demo-nautilus-7nxpw update-demo-nautilus-x6w5p "
Apr 29 10:59:28.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7nxpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:28.827: INFO: stderr: ""
Apr 29 10:59:28.827: INFO: stdout: "true"
Apr 29 10:59:28.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7nxpw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:28.909: INFO: stderr: ""
Apr 29 10:59:28.909: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 10:59:28.909: INFO: validating pod update-demo-nautilus-7nxpw
Apr 29 10:59:28.917: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 10:59:28.917: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 10:59:28.917: INFO: update-demo-nautilus-7nxpw is verified up and running
Apr 29 10:59:28.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-x6w5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:29.005: INFO: stderr: ""
Apr 29 10:59:29.005: INFO: stdout: "true"
Apr 29 10:59:29.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-x6w5p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:29.108: INFO: stderr: ""
Apr 29 10:59:29.109: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 10:59:29.109: INFO: validating pod update-demo-nautilus-x6w5p
Apr 29 10:59:29.115: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 10:59:29.116: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 10:59:29.116: INFO: update-demo-nautilus-x6w5p is verified up and running
STEP: rolling-update to new replication controller
Apr 29 10:59:29.118: INFO: scanned /root for discovery docs: <nil>
Apr 29 10:59:29.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7511'
Apr 29 10:59:51.586: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 29 10:59:51.586: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 10:59:51.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7511'
Apr 29 10:59:51.665: INFO: stderr: ""
Apr 29 10:59:51.665: INFO: stdout: "update-demo-kitten-ncpqq update-demo-kitten-r2jr6 "
Apr 29 10:59:51.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-kitten-ncpqq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:51.744: INFO: stderr: ""
Apr 29 10:59:51.744: INFO: stdout: "true"
Apr 29 10:59:51.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-kitten-ncpqq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:51.840: INFO: stderr: ""
Apr 29 10:59:51.840: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 29 10:59:51.840: INFO: validating pod update-demo-kitten-ncpqq
Apr 29 10:59:51.847: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 29 10:59:51.847: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 29 10:59:51.847: INFO: update-demo-kitten-ncpqq is verified up and running
Apr 29 10:59:51.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-kitten-r2jr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:51.928: INFO: stderr: ""
Apr 29 10:59:51.928: INFO: stdout: "true"
Apr 29 10:59:51.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-kitten-r2jr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7511'
Apr 29 10:59:52.024: INFO: stderr: ""
Apr 29 10:59:52.024: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 29 10:59:52.024: INFO: validating pod update-demo-kitten-r2jr6
Apr 29 10:59:52.029: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 29 10:59:52.030: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 29 10:59:52.030: INFO: update-demo-kitten-r2jr6 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 10:59:52.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7511" for this suite.
Apr 29 11:00:14.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:00:14.145: INFO: namespace kubectl-7511 deletion completed in 22.110263412s

• [SLOW TEST:50.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:00:14.146: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 29 11:00:14.190: INFO: Waiting up to 5m0s for pod "downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3" in namespace "downward-api-4344" to be "success or failure"
Apr 29 11:00:14.194: INFO: Pod "downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.592836ms
Apr 29 11:00:16.198: INFO: Pod "downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008654286s
STEP: Saw pod success
Apr 29 11:00:16.198: INFO: Pod "downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:00:16.201: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 11:00:16.220: INFO: Waiting for pod downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:00:16.224: INFO: Pod downward-api-f7232190-6a6d-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:00:16.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4344" for this suite.
Apr 29 11:00:22.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:00:22.321: INFO: namespace downward-api-4344 deletion completed in 6.09259764s

• [SLOW TEST:8.175 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:00:22.321: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-fc0a20ca-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating configMap with name cm-test-opt-upd-fc0a2111-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fc0a20ca-6a6d-11e9-aeaf-0a580af401c3
STEP: Updating configmap cm-test-opt-upd-fc0a2111-6a6d-11e9-aeaf-0a580af401c3
STEP: Creating configMap with name cm-test-opt-create-fc0a2127-6a6d-11e9-aeaf-0a580af401c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:00:26.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4474" for this suite.
Apr 29 11:00:48.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:00:48.595: INFO: namespace configmap-4474 deletion completed in 22.104114575s

• [SLOW TEST:26.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:00:48.596: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-0bab6984-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:00:48.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3" in namespace "configmap-6173" to be "success or failure"
Apr 29 11:00:48.643: INFO: Pod "pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.97831ms
Apr 29 11:00:50.649: INFO: Pod "pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014055955s
STEP: Saw pod success
Apr 29 11:00:50.649: INFO: Pod "pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:00:50.655: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:00:50.672: INFO: Waiting for pod pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:00:50.675: INFO: Pod pod-configmaps-0bac1495-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:00:50.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6173" for this suite.
Apr 29 11:00:56.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:00:56.761: INFO: namespace configmap-6173 deletion completed in 6.081870823s

• [SLOW TEST:8.165 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:00:56.761: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:00:56.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3" in namespace "downward-api-9909" to be "success or failure"
Apr 29 11:00:56.801: INFO: Pod "downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.120473ms
Apr 29 11:00:58.805: INFO: Pod "downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009050052s
STEP: Saw pod success
Apr 29 11:00:58.805: INFO: Pod "downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:00:58.808: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:00:58.827: INFO: Waiting for pod downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:00:58.831: INFO: Pod downwardapi-volume-10898787-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:00:58.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9909" for this suite.
Apr 29 11:01:04.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:01:04.911: INFO: namespace downward-api-9909 deletion completed in 6.076413663s

• [SLOW TEST:8.149 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:01:04.911: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:01:04.942: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3" in namespace "downward-api-8150" to be "success or failure"
Apr 29 11:01:04.944: INFO: Pod "downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.66007ms
Apr 29 11:01:06.948: INFO: Pod "downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006678595s
STEP: Saw pod success
Apr 29 11:01:06.948: INFO: Pod "downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:01:06.952: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:01:06.967: INFO: Waiting for pod downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:01:06.970: INFO: Pod downwardapi-volume-15647135-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:01:06.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8150" for this suite.
Apr 29 11:01:12.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:01:13.073: INFO: namespace downward-api-8150 deletion completed in 6.099861387s

• [SLOW TEST:8.163 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:01:13.078: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:01:13.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7218" for this suite.
Apr 29 11:01:35.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:01:35.224: INFO: namespace pods-7218 deletion completed in 22.095281626s

• [SLOW TEST:22.147 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:01:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-2775e8f0-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:01:35.257: INFO: Waiting up to 5m0s for pod "pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3" in namespace "configmap-3825" to be "success or failure"
Apr 29 11:01:35.265: INFO: Pod "pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.34283ms
Apr 29 11:01:37.270: INFO: Pod "pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012755348s
STEP: Saw pod success
Apr 29 11:01:37.270: INFO: Pod "pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:01:37.273: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:01:37.290: INFO: Waiting for pod pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:01:37.293: INFO: Pod pod-configmaps-27765c76-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:01:37.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3825" for this suite.
Apr 29 11:01:43.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:01:43.376: INFO: namespace configmap-3825 deletion completed in 6.079800504s

• [SLOW TEST:8.152 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:01:43.377: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:01:43.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3" in namespace "downward-api-8162" to be "success or failure"
Apr 29 11:01:43.416: INFO: Pod "downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.8407ms
Apr 29 11:01:45.420: INFO: Pod "downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008582928s
STEP: Saw pod success
Apr 29 11:01:45.420: INFO: Pod "downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:01:45.422: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:01:45.439: INFO: Waiting for pod downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:01:45.441: INFO: Pod downwardapi-volume-2c5268f6-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:01:45.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8162" for this suite.
Apr 29 11:01:51.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:01:51.537: INFO: namespace downward-api-8162 deletion completed in 6.09329395s

• [SLOW TEST:8.161 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:01:51.538: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 11:01:51.577: INFO: (0) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.163786ms)
Apr 29 11:01:51.580: INFO: (1) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.881502ms)
Apr 29 11:01:51.583: INFO: (2) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.908231ms)
Apr 29 11:01:51.586: INFO: (3) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.089331ms)
Apr 29 11:01:51.589: INFO: (4) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.385674ms)
Apr 29 11:01:51.593: INFO: (5) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.414693ms)
Apr 29 11:01:51.596: INFO: (6) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.289897ms)
Apr 29 11:01:51.600: INFO: (7) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.157232ms)
Apr 29 11:01:51.604: INFO: (8) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.272386ms)
Apr 29 11:01:51.608: INFO: (9) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.920609ms)
Apr 29 11:01:51.612: INFO: (10) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.853654ms)
Apr 29 11:01:51.615: INFO: (11) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.509351ms)
Apr 29 11:01:51.619: INFO: (12) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.954154ms)
Apr 29 11:01:51.624: INFO: (13) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.559293ms)
Apr 29 11:01:51.627: INFO: (14) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.53139ms)
Apr 29 11:01:51.631: INFO: (15) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.847453ms)
Apr 29 11:01:51.635: INFO: (16) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.517646ms)
Apr 29 11:01:51.639: INFO: (17) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.823334ms)
Apr 29 11:01:51.642: INFO: (18) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.707257ms)
Apr 29 11:01:51.646: INFO: (19) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.446181ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:01:51.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8033" for this suite.
Apr 29 11:01:57.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:01:57.733: INFO: namespace proxy-8033 deletion completed in 6.083341656s

• [SLOW TEST:6.195 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:01:57.739: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-34e1168d-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:01:57.772: INFO: Waiting up to 5m0s for pod "pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3" in namespace "configmap-9020" to be "success or failure"
Apr 29 11:01:57.776: INFO: Pod "pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.847908ms
Apr 29 11:01:59.780: INFO: Pod "pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007798914s
STEP: Saw pod success
Apr 29 11:01:59.780: INFO: Pod "pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:01:59.783: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:01:59.801: INFO: Waiting for pod pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:01:59.803: INFO: Pod pod-configmaps-34e17c3a-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:01:59.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9020" for this suite.
Apr 29 11:02:05.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:02:05.904: INFO: namespace configmap-9020 deletion completed in 6.09737816s

• [SLOW TEST:8.165 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:02:05.904: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0429 11:02:45.960594      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 29 11:02:45.960: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:02:45.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5687" for this suite.
Apr 29 11:02:51.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:02:52.072: INFO: namespace gc-5687 deletion completed in 6.107942273s

• [SLOW TEST:46.168 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:02:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-mn5g
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 11:02:52.116: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mn5g" in namespace "subpath-5417" to be "success or failure"
Apr 29 11:02:52.118: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092345ms
Apr 29 11:02:54.122: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006183578s
Apr 29 11:02:56.125: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.009654517s
Apr 29 11:02:58.139: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 6.023085423s
Apr 29 11:03:00.143: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 8.027264921s
Apr 29 11:03:02.153: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.037414172s
Apr 29 11:03:04.156: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 12.04072703s
Apr 29 11:03:06.161: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.045100737s
Apr 29 11:03:08.165: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.049696284s
Apr 29 11:03:10.171: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.055578101s
Apr 29 11:03:12.175: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Running", Reason="", readiness=true. Elapsed: 20.059452294s
Apr 29 11:03:14.178: INFO: Pod "pod-subpath-test-downwardapi-mn5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.062724155s
STEP: Saw pod success
Apr 29 11:03:14.179: INFO: Pod "pod-subpath-test-downwardapi-mn5g" satisfied condition "success or failure"
Apr 29 11:03:14.183: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-downwardapi-mn5g container test-container-subpath-downwardapi-mn5g: <nil>
STEP: delete the pod
Apr 29 11:03:14.210: INFO: Waiting for pod pod-subpath-test-downwardapi-mn5g to disappear
Apr 29 11:03:14.214: INFO: Pod pod-subpath-test-downwardapi-mn5g no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mn5g
Apr 29 11:03:14.214: INFO: Deleting pod "pod-subpath-test-downwardapi-mn5g" in namespace "subpath-5417"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:03:14.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5417" for this suite.
Apr 29 11:03:20.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:03:20.336: INFO: namespace subpath-5417 deletion completed in 6.115939897s

• [SLOW TEST:28.263 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:03:20.337: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-661cc547-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:03:20.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3" in namespace "configmap-9374" to be "success or failure"
Apr 29 11:03:20.374: INFO: Pod "pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.507789ms
Apr 29 11:03:22.378: INFO: Pod "pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008136741s
STEP: Saw pod success
Apr 29 11:03:22.378: INFO: Pod "pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:03:22.381: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:03:22.400: INFO: Waiting for pod pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:03:22.402: INFO: Pod pod-configmaps-661d279e-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:03:22.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9374" for this suite.
Apr 29 11:03:28.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:03:28.505: INFO: namespace configmap-9374 deletion completed in 6.098853569s

• [SLOW TEST:8.168 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:03:28.506: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-m7b7
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 11:03:28.544: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-m7b7" in namespace "subpath-9300" to be "success or failure"
Apr 29 11:03:28.548: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.336967ms
Apr 29 11:03:30.553: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352636s
Apr 29 11:03:32.557: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012973908s
Apr 29 11:03:34.561: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 6.016615952s
Apr 29 11:03:36.565: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 8.020205048s
Apr 29 11:03:38.572: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 10.027400437s
Apr 29 11:03:40.578: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 12.033917799s
Apr 29 11:03:42.585: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 14.040387866s
Apr 29 11:03:44.589: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 16.04417773s
Apr 29 11:03:46.593: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 18.0485224s
Apr 29 11:03:48.597: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Running", Reason="", readiness=true. Elapsed: 20.052668275s
Apr 29 11:03:50.601: INFO: Pod "pod-subpath-test-projected-m7b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056317493s
STEP: Saw pod success
Apr 29 11:03:50.601: INFO: Pod "pod-subpath-test-projected-m7b7" satisfied condition "success or failure"
Apr 29 11:03:50.605: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-projected-m7b7 container test-container-subpath-projected-m7b7: <nil>
STEP: delete the pod
Apr 29 11:03:50.622: INFO: Waiting for pod pod-subpath-test-projected-m7b7 to disappear
Apr 29 11:03:50.628: INFO: Pod pod-subpath-test-projected-m7b7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-m7b7
Apr 29 11:03:50.628: INFO: Deleting pod "pod-subpath-test-projected-m7b7" in namespace "subpath-9300"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:03:50.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9300" for this suite.
Apr 29 11:03:56.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:03:56.721: INFO: namespace subpath-9300 deletion completed in 6.085823191s

• [SLOW TEST:28.215 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:03:56.723: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 11:03:56.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5555'
Apr 29 11:03:56.971: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 11:03:56.971: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Apr 29 11:03:56.979: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Apr 29 11:03:56.984: INFO: scanned /root for discovery docs: <nil>
Apr 29 11:03:56.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5555'
Apr 29 11:04:09.922: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 29 11:04:09.922: INFO: stdout: "Created e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a\nScaling up e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr 29 11:04:09.922: INFO: stdout: "Created e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a\nScaling up e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr 29 11:04:09.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5555'
Apr 29 11:04:10.010: INFO: stderr: ""
Apr 29 11:04:10.010: INFO: stdout: "e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a-vbwkp "
Apr 29 11:04:10.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a-vbwkp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5555'
Apr 29 11:04:10.103: INFO: stderr: ""
Apr 29 11:04:10.103: INFO: stdout: "true"
Apr 29 11:04:10.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a-vbwkp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5555'
Apr 29 11:04:10.188: INFO: stderr: ""
Apr 29 11:04:10.188: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr 29 11:04:10.188: INFO: e2e-test-nginx-rc-8924a486f6937a75506a8ce0415a4f1a-vbwkp is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Apr 29 11:04:10.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete rc e2e-test-nginx-rc --namespace=kubectl-5555'
Apr 29 11:04:10.280: INFO: stderr: ""
Apr 29 11:04:10.280: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:10.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5555" for this suite.
Apr 29 11:04:16.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:04:16.366: INFO: namespace kubectl-5555 deletion completed in 6.082706053s

• [SLOW TEST:19.643 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:04:16.368: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr 29 11:04:16.391: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:19.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6925" for this suite.
Apr 29 11:04:25.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:04:25.882: INFO: namespace init-container-6925 deletion completed in 6.094350752s

• [SLOW TEST:9.514 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:04:25.890: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 29 11:04:25.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7036,SelfLink:/api/v1/namespaces/watch-7036/configmaps/e2e-watch-test-resource-version,UID:8d2d39f6-6a6e-11e9-9342-005056a45e5c,ResourceVersion:40216,Generation:0,CreationTimestamp:2019-04-29 11:04:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 11:04:25.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7036,SelfLink:/api/v1/namespaces/watch-7036/configmaps/e2e-watch-test-resource-version,UID:8d2d39f6-6a6e-11e9-9342-005056a45e5c,ResourceVersion:40217,Generation:0,CreationTimestamp:2019-04-29 11:04:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:25.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7036" for this suite.
Apr 29 11:04:31.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:04:32.020: INFO: namespace watch-7036 deletion completed in 6.081262727s

• [SLOW TEST:6.130 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:04:32.020: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-90d691c3-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:04:32.052: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3" in namespace "projected-4217" to be "success or failure"
Apr 29 11:04:32.056: INFO: Pod "pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.545003ms
Apr 29 11:04:34.059: INFO: Pod "pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007606025s
STEP: Saw pod success
Apr 29 11:04:34.059: INFO: Pod "pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:04:34.062: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 11:04:34.082: INFO: Waiting for pod pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:04:34.086: INFO: Pod pod-projected-secrets-90d70a24-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:34.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4217" for this suite.
Apr 29 11:04:40.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:04:40.195: INFO: namespace projected-4217 deletion completed in 6.106769506s

• [SLOW TEST:8.176 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:04:40.196: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Apr 29 11:04:40.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-4068'
Apr 29 11:04:40.386: INFO: stderr: ""
Apr 29 11:04:40.386: INFO: stdout: "pod/pause created\n"
Apr 29 11:04:40.386: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 29 11:04:40.386: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4068" to be "running and ready"
Apr 29 11:04:40.391: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.608741ms
Apr 29 11:04:42.394: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00797469s
Apr 29 11:04:42.394: INFO: Pod "pause" satisfied condition "running and ready"
Apr 29 11:04:42.394: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 29 11:04:42.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 label pods pause testing-label=testing-label-value --namespace=kubectl-4068'
Apr 29 11:04:42.468: INFO: stderr: ""
Apr 29 11:04:42.468: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 29 11:04:42.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pod pause -L testing-label --namespace=kubectl-4068'
Apr 29 11:04:42.546: INFO: stderr: ""
Apr 29 11:04:42.546: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 29 11:04:42.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 label pods pause testing-label- --namespace=kubectl-4068'
Apr 29 11:04:42.624: INFO: stderr: ""
Apr 29 11:04:42.624: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 29 11:04:42.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pod pause -L testing-label --namespace=kubectl-4068'
Apr 29 11:04:42.705: INFO: stderr: ""
Apr 29 11:04:42.705: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Apr 29 11:04:42.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-4068'
Apr 29 11:04:42.784: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 11:04:42.784: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 29 11:04:42.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get rc,svc -l name=pause --no-headers --namespace=kubectl-4068'
Apr 29 11:04:42.865: INFO: stderr: "No resources found.\n"
Apr 29 11:04:42.865: INFO: stdout: ""
Apr 29 11:04:42.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -l name=pause --namespace=kubectl-4068 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 11:04:42.953: INFO: stderr: ""
Apr 29 11:04:42.953: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:42.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4068" for this suite.
Apr 29 11:04:48.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:04:49.053: INFO: namespace kubectl-4068 deletion completed in 6.096511398s

• [SLOW TEST:8.857 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:04:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Apr 29 11:04:49.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 api-versions'
Apr 29 11:04:49.176: INFO: stderr: ""
Apr 29 11:04:49.176: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:49.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-285" for this suite.
Apr 29 11:04:55.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:04:55.281: INFO: namespace kubectl-285 deletion completed in 6.097901702s

• [SLOW TEST:6.228 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:04:55.286: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:04:55.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3" in namespace "projected-8119" to be "success or failure"
Apr 29 11:04:55.325: INFO: Pod "downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904351ms
Apr 29 11:04:57.331: INFO: Pod "downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010609794s
STEP: Saw pod success
Apr 29 11:04:57.331: INFO: Pod "downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:04:57.334: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:04:57.352: INFO: Waiting for pod downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:04:57.354: INFO: Pod downwardapi-volume-9eb56ca5-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:04:57.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8119" for this suite.
Apr 29 11:05:03.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:05:03.477: INFO: namespace projected-8119 deletion completed in 6.119497872s

• [SLOW TEST:8.192 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:05:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Apr 29 11:05:03.514: INFO: Waiting up to 5m0s for pod "var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3" in namespace "var-expansion-2538" to be "success or failure"
Apr 29 11:05:03.519: INFO: Pod "var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325605ms
Apr 29 11:05:05.523: INFO: Pod "var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009265774s
STEP: Saw pod success
Apr 29 11:05:05.523: INFO: Pod "var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:05:05.527: INFO: Trying to get logs from node essentialpks-conformance-2 pod var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 11:05:05.542: INFO: Waiting for pod var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:05:05.544: INFO: Pod var-expansion-a3979cab-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:05:05.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2538" for this suite.
Apr 29 11:05:11.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:05:11.651: INFO: namespace var-expansion-2538 deletion completed in 6.103550862s

• [SLOW TEST:8.173 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:05:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Apr 29 11:05:11.690: INFO: Waiting up to 5m0s for pod "client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3" in namespace "containers-3140" to be "success or failure"
Apr 29 11:05:11.696: INFO: Pod "client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.095453ms
Apr 29 11:05:13.699: INFO: Pod "client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008829163s
STEP: Saw pod success
Apr 29 11:05:13.699: INFO: Pod "client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:05:13.703: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:05:13.719: INFO: Waiting for pod client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:05:13.721: INFO: Pod client-containers-a8773fe1-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:05:13.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3140" for this suite.
Apr 29 11:05:19.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:05:19.815: INFO: namespace containers-3140 deletion completed in 6.090796151s

• [SLOW TEST:8.164 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:05:19.818: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 29 11:05:22.378: INFO: Successfully updated pod "annotationupdatead545c9a-6a6e-11e9-aeaf-0a580af401c3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:05:26.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3011" for this suite.
Apr 29 11:05:48.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:05:48.521: INFO: namespace downward-api-3011 deletion completed in 22.106975092s

• [SLOW TEST:28.703 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:05:48.522: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 29 11:05:48.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-2883'
Apr 29 11:05:48.739: INFO: stderr: ""
Apr 29 11:05:48.740: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 29 11:05:49.743: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:05:49.744: INFO: Found 0 / 1
Apr 29 11:05:50.744: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:05:50.744: INFO: Found 1 / 1
Apr 29 11:05:50.744: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 29 11:05:50.747: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:05:50.747: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 11:05:50.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 patch pod redis-master-lj8bp --namespace=kubectl-2883 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 29 11:05:50.838: INFO: stderr: ""
Apr 29 11:05:50.838: INFO: stdout: "pod/redis-master-lj8bp patched\n"
STEP: checking annotations
Apr 29 11:05:50.845: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:05:50.845: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:05:50.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2883" for this suite.
Apr 29 11:06:12.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:06:12.953: INFO: namespace kubectl-2883 deletion completed in 22.10403024s

• [SLOW TEST:24.432 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:06:12.953: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 29 11:06:12.993: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-watch-closed,UID:ccfe8fe1-6a6e-11e9-9342-005056a45e5c,ResourceVersion:40579,Generation:0,CreationTimestamp:2019-04-29 11:06:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 11:06:12.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-watch-closed,UID:ccfe8fe1-6a6e-11e9-9342-005056a45e5c,ResourceVersion:40580,Generation:0,CreationTimestamp:2019-04-29 11:06:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 29 11:06:13.005: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-watch-closed,UID:ccfe8fe1-6a6e-11e9-9342-005056a45e5c,ResourceVersion:40581,Generation:0,CreationTimestamp:2019-04-29 11:06:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 11:06:13.005: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-watch-closed,UID:ccfe8fe1-6a6e-11e9-9342-005056a45e5c,ResourceVersion:40582,Generation:0,CreationTimestamp:2019-04-29 11:06:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:06:13.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2716" for this suite.
Apr 29 11:06:19.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:06:19.103: INFO: namespace watch-2716 deletion completed in 6.092712442s

• [SLOW TEST:6.149 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:06:19.103: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-d0ab9b8e-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:06:19.145: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3" in namespace "projected-4781" to be "success or failure"
Apr 29 11:06:19.149: INFO: Pod "pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978041ms
Apr 29 11:06:21.152: INFO: Pod "pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00774636s
Apr 29 11:06:23.156: INFO: Pod "pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011586427s
STEP: Saw pod success
Apr 29 11:06:23.156: INFO: Pod "pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:06:23.159: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 11:06:23.183: INFO: Waiting for pod pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:06:23.186: INFO: Pod pod-projected-secrets-d0ac157c-6a6e-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:06:23.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4781" for this suite.
Apr 29 11:06:29.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:06:29.301: INFO: namespace projected-4781 deletion completed in 6.111264954s

• [SLOW TEST:10.198 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:06:29.306: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-d6c19457-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating secret with name s-test-opt-upd-d6c194dc-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d6c19457-6a6e-11e9-aeaf-0a580af401c3
STEP: Updating secret s-test-opt-upd-d6c194dc-6a6e-11e9-aeaf-0a580af401c3
STEP: Creating secret with name s-test-opt-create-d6c19516-6a6e-11e9-aeaf-0a580af401c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:07:41.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2968" for this suite.
Apr 29 11:08:03.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:08:03.944: INFO: namespace secrets-2968 deletion completed in 22.090650379s

• [SLOW TEST:94.639 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:08:03.945: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 29 11:08:12.012: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.012: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.140: INFO: Exec stderr: ""
Apr 29 11:08:12.140: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.140: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.253: INFO: Exec stderr: ""
Apr 29 11:08:12.253: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.254: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.368: INFO: Exec stderr: ""
Apr 29 11:08:12.368: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.368: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.470: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 29 11:08:12.470: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.470: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.573: INFO: Exec stderr: ""
Apr 29 11:08:12.573: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.673: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 29 11:08:12.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.673: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.796: INFO: Exec stderr: ""
Apr 29 11:08:12.796: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.796: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:12.924: INFO: Exec stderr: ""
Apr 29 11:08:12.925: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:12.925: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:13.032: INFO: Exec stderr: ""
Apr 29 11:08:13.033: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3658 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:08:13.033: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:08:13.140: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:08:13.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3658" for this suite.
Apr 29 11:09:03.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:09:03.241: INFO: namespace e2e-kubelet-etc-hosts-3658 deletion completed in 50.096163216s

• [SLOW TEST:59.296 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:09:03.242: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 29 11:09:03.513: INFO: Pod name wrapped-volume-race-32a33cc2-6a6f-11e9-aeaf-0a580af401c3: Found 0 pods out of 5
Apr 29 11:09:08.520: INFO: Pod name wrapped-volume-race-32a33cc2-6a6f-11e9-aeaf-0a580af401c3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-32a33cc2-6a6f-11e9-aeaf-0a580af401c3 in namespace emptydir-wrapper-1629, will wait for the garbage collector to delete the pods
Apr 29 11:09:18.623: INFO: Deleting ReplicationController wrapped-volume-race-32a33cc2-6a6f-11e9-aeaf-0a580af401c3 took: 13.486644ms
Apr 29 11:09:18.923: INFO: Terminating ReplicationController wrapped-volume-race-32a33cc2-6a6f-11e9-aeaf-0a580af401c3 pods took: 300.305739ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 11:09:59.041: INFO: Pod name wrapped-volume-race-53bbc001-6a6f-11e9-aeaf-0a580af401c3: Found 0 pods out of 5
Apr 29 11:10:04.051: INFO: Pod name wrapped-volume-race-53bbc001-6a6f-11e9-aeaf-0a580af401c3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-53bbc001-6a6f-11e9-aeaf-0a580af401c3 in namespace emptydir-wrapper-1629, will wait for the garbage collector to delete the pods
Apr 29 11:10:14.139: INFO: Deleting ReplicationController wrapped-volume-race-53bbc001-6a6f-11e9-aeaf-0a580af401c3 took: 14.447848ms
Apr 29 11:10:14.439: INFO: Terminating ReplicationController wrapped-volume-race-53bbc001-6a6f-11e9-aeaf-0a580af401c3 pods took: 300.463607ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 11:10:59.161: INFO: Pod name wrapped-volume-race-7790c448-6a6f-11e9-aeaf-0a580af401c3: Found 0 pods out of 5
Apr 29 11:11:04.180: INFO: Pod name wrapped-volume-race-7790c448-6a6f-11e9-aeaf-0a580af401c3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7790c448-6a6f-11e9-aeaf-0a580af401c3 in namespace emptydir-wrapper-1629, will wait for the garbage collector to delete the pods
Apr 29 11:11:14.269: INFO: Deleting ReplicationController wrapped-volume-race-7790c448-6a6f-11e9-aeaf-0a580af401c3 took: 6.994097ms
Apr 29 11:11:14.569: INFO: Terminating ReplicationController wrapped-volume-race-7790c448-6a6f-11e9-aeaf-0a580af401c3 pods took: 300.345108ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:12:00.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1629" for this suite.
Apr 29 11:12:06.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:12:06.373: INFO: namespace emptydir-wrapper-1629 deletion completed in 6.088062827s

• [SLOW TEST:183.131 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:12:06.373: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Apr 29 11:12:06.400: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-008546913 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:12:06.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2349" for this suite.
Apr 29 11:12:12.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:12:12.566: INFO: namespace kubectl-2349 deletion completed in 6.092392048s

• [SLOW TEST:6.193 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:12:12.567: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 11:12:12.605: INFO: Waiting up to 5m0s for pod "pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3" in namespace "emptydir-9357" to be "success or failure"
Apr 29 11:12:12.608: INFO: Pod "pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531491ms
Apr 29 11:12:14.613: INFO: Pod "pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007904165s
STEP: Saw pod success
Apr 29 11:12:14.613: INFO: Pod "pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:12:14.616: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:12:14.635: INFO: Waiting for pod pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:12:14.637: INFO: Pod pod-a3592c7e-6a6f-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:12:14.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9357" for this suite.
Apr 29 11:12:20.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:12:20.756: INFO: namespace emptydir-9357 deletion completed in 6.11497247s

• [SLOW TEST:8.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:12:20.763: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 11:12:20.797: INFO: Creating deployment "nginx-deployment"
Apr 29 11:12:20.801: INFO: Waiting for observed generation 1
Apr 29 11:12:22.810: INFO: Waiting for all required pods to come up
Apr 29 11:12:22.814: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 29 11:12:24.828: INFO: Waiting for deployment "nginx-deployment" to complete
Apr 29 11:12:24.834: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr 29 11:12:24.844: INFO: Updating deployment nginx-deployment
Apr 29 11:12:24.844: INFO: Waiting for observed generation 2
Apr 29 11:12:26.855: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 29 11:12:26.858: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 29 11:12:26.861: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 29 11:12:26.870: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 29 11:12:26.870: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 29 11:12:26.873: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr 29 11:12:26.877: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr 29 11:12:26.877: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr 29 11:12:26.882: INFO: Updating deployment nginx-deployment
Apr 29 11:12:26.882: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr 29 11:12:26.891: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 29 11:12:26.918: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 29 11:12:26.945: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-40,SelfLink:/apis/apps/v1/namespaces/deployment-40/deployments/nginx-deployment,UID:a839a915-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42339,Generation:3,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-04-29 11:12:24 +0000 UTC 2019-04-29 11:12:20 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-04-29 11:12:26 +0000 UTC 2019-04-29 11:12:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr 29 11:12:26.949: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-40,SelfLink:/apis/apps/v1/namespaces/deployment-40/replicasets/nginx-deployment-5f9595f595,UID:aaa2f2a8-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42328,Generation:3,CreationTimestamp:2019-04-29 11:12:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a839a915-6a6f-11e9-9342-005056a45e5c 0xc003084497 0xc003084498}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 11:12:26.949: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr 29 11:12:26.949: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-40,SelfLink:/apis/apps/v1/namespaces/deployment-40/replicasets/nginx-deployment-6f478d8d8,UID:a83a4b9a-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42326,Generation:3,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment a839a915-6a6f-11e9-9342-005056a45e5c 0xc003084567 0xc003084568}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr 29 11:12:26.972: INFO: Pod "nginx-deployment-5f9595f595-79pzv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-79pzv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-79pzv,UID:abe16e5e-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42345,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc003084e47 0xc003084e48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003084eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003084ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.972: INFO: Pod "nginx-deployment-5f9595f595-96zpn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-96zpn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-96zpn,UID:aaa3b9bd-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42276,Generation:0,CreationTimestamp:2019-04-29 11:12:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc003084f37 0xc003084f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003084fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003084fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-04-29 11:12:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-5f9595f595-cf4m6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cf4m6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-cf4m6,UID:abe1697f-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42346,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc003085090 0xc003085091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-5f9595f595-dnfvd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-dnfvd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-dnfvd,UID:aaadaca3-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42307,Generation:0,CreationTimestamp:2019-04-29 11:12:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc003085187 0xc003085188}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030851f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:,StartTime:2019-04-29 11:12:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-5f9595f595-j667r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j667r,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-j667r,UID:abdd33b4-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42342,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc0030852e0 0xc0030852e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-5f9595f595-jdbkn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jdbkn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-jdbkn,UID:aaa54524-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42292,Generation:0,CreationTimestamp:2019-04-29 11:12:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc0030853f0 0xc0030853f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:,StartTime:2019-04-29 11:12:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-5f9595f595-nkn4t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nkn4t,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-nkn4t,UID:aaaf18b5-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42311,Generation:0,CreationTimestamp:2019-04-29 11:12:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc003085570 0xc003085571}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030855f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-04-29 11:12:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-5f9595f595-w48xc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w48xc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-5f9595f595-w48xc,UID:aaa53843-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42294,Generation:0,CreationTimestamp:2019-04-29 11:12:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 aaa2f2a8-6a6f-11e9-9342-005056a45e5c 0xc0030856e0 0xc0030856e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:24 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-04-29 11:12:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-6f478d8d8-29kgb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-29kgb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-29kgb,UID:abe034a6-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42340,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085890 0xc003085891}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030858f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-6f478d8d8-4xw4p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4xw4p,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-4xw4p,UID:abdb389d-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42332,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085977 0xc003085978}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030859e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.973: INFO: Pod "nginx-deployment-6f478d8d8-6gnxh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6gnxh,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-6gnxh,UID:a842146a-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42252,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085a80 0xc003085a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085ae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085b00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.92,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://e822e7645b19ca2753ec3d35303dc15ed586139e03f0e957ff4f4c7f4ef394c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-6xmp2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6xmp2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-6xmp2,UID:abe04b94-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42341,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085bd0 0xc003085bd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-7ws6j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7ws6j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-7ws6j,UID:abe05761-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42343,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085cb7 0xc003085cb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-hw9zr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hw9zr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-hw9zr,UID:a83d85c4-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42220,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085da7 0xc003085da8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.221,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://1f2ea591215c5b85a94fed98091e291ba0ee32063f5bc39391b8fb4294fe9b0f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-j95l9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-j95l9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-j95l9,UID:abdc6f00-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42337,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc003085f17 0xc003085f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003085f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003085fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-m6hbs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-m6hbs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-m6hbs,UID:a83c3e9a-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42247,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f4030 0xc0025f4031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f4090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f40b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.91,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://b3bdfc952a5a1f4dd1307bd03b198dd69472e47b015865fe7f03f5c330ecc867}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-p57q9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-p57q9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-p57q9,UID:a84004bb-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42256,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f4180 0xc0025f4181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f41e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f4200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.88,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://df809e7803099a57279b42d17b7de0d3d3d77220518ba0dd28ba9a97ac053461}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-qkjkm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qkjkm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-qkjkm,UID:a841fe40-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42244,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f42d0 0xc0025f42d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f4330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f4350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.90,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://78cf473c2ab3cc4592ea9be80894e4a294f014eeef581e7364cb60efb1dcec0b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.974: INFO: Pod "nginx-deployment-6f478d8d8-t2xp2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-t2xp2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-t2xp2,UID:a83fc168-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42227,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f4420 0xc0025f4421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f4480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f44a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.222,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://103c8fe4f7b8a48cd352386a87a05e7d2aa6dafdae03f6eae66c6ce2d6a0b958}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.975: INFO: Pod "nginx-deployment-6f478d8d8-v8wh9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-v8wh9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-v8wh9,UID:a8422627-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42223,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f4577 0xc0025f4578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f45e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f4600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.223,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://aeb7c71396c223f1bd367dd7e48b52b9b7dd2b91a75b4966228a63ebf432d09b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.975: INFO: Pod "nginx-deployment-6f478d8d8-wz6hx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wz6hx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-wz6hx,UID:a83e5a37-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42232,Generation:0,CreationTimestamp:2019-04-29 11:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f46d7 0xc0025f46d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f4740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f4760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:20 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.87,StartTime:2019-04-29 11:12:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-29 11:12:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://12ed3fb7fd06bbee31a7f2d29c8fab56893915fb2afd3beeaf535a0c957b316d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.975: INFO: Pod "nginx-deployment-6f478d8d8-xs5lq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xs5lq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-xs5lq,UID:abe05d14-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42344,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f4830 0xc0025f4831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f4890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f48b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr 29 11:12:26.975: INFO: Pod "nginx-deployment-6f478d8d8-z9t4p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z9t4p,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-40,SelfLink:/api/v1/namespaces/deployment-40/pods/nginx-deployment-6f478d8d8-z9t4p,UID:abdc3241-6a6f-11e9-9342-005056a45e5c,ResourceVersion:42335,Generation:0,CreationTimestamp:2019-04-29 11:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 a83a4b9a-6a6f-11e9-9342-005056a45e5c 0xc0025f4917 0xc0025f4918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7vwbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7vwbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7vwbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f4980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f49a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:12:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:12:26.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-40" for this suite.
Apr 29 11:12:33.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:12:33.110: INFO: namespace deployment-40 deletion completed in 6.109169106s

• [SLOW TEST:12.347 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:12:33.111: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-af9750d2-6a6f-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:12:33.145: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3" in namespace "projected-814" to be "success or failure"
Apr 29 11:12:33.151: INFO: Pod "pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071734ms
Apr 29 11:12:35.155: INFO: Pod "pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009640346s
Apr 29 11:12:37.158: INFO: Pod "pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013059243s
Apr 29 11:12:39.162: INFO: Pod "pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017242265s
STEP: Saw pod success
Apr 29 11:12:39.162: INFO: Pod "pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:12:39.165: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:12:39.184: INFO: Waiting for pod pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:12:39.188: INFO: Pod pod-projected-configmaps-af97fa8f-6a6f-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:12:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-814" for this suite.
Apr 29 11:12:45.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:12:45.294: INFO: namespace projected-814 deletion completed in 6.102746493s

• [SLOW TEST:12.185 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:12:45.299: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-b6db560c-6a6f-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:12:45.335: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3" in namespace "configmap-8271" to be "success or failure"
Apr 29 11:12:45.338: INFO: Pod "pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271612ms
Apr 29 11:12:47.341: INFO: Pod "pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006746293s
STEP: Saw pod success
Apr 29 11:12:47.341: INFO: Pod "pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:12:47.344: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:12:47.362: INFO: Waiting for pod pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:12:47.364: INFO: Pod pod-configmaps-b6dbcbaa-6a6f-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:12:47.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8271" for this suite.
Apr 29 11:12:53.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:12:53.455: INFO: namespace configmap-8271 deletion completed in 6.087764744s

• [SLOW TEST:8.156 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:12:53.456: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 29 11:12:53.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-6339'
Apr 29 11:12:53.657: INFO: stderr: ""
Apr 29 11:12:53.657: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 11:12:53.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:12:53.749: INFO: stderr: ""
Apr 29 11:12:53.749: INFO: stdout: "update-demo-nautilus-7spdv update-demo-nautilus-8j6bk "
Apr 29 11:12:53.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7spdv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:12:53.836: INFO: stderr: ""
Apr 29 11:12:53.836: INFO: stdout: ""
Apr 29 11:12:53.837: INFO: update-demo-nautilus-7spdv is created but not running
Apr 29 11:12:58.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:12:58.928: INFO: stderr: ""
Apr 29 11:12:58.928: INFO: stdout: "update-demo-nautilus-7spdv update-demo-nautilus-8j6bk "
Apr 29 11:12:58.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7spdv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:12:59.018: INFO: stderr: ""
Apr 29 11:12:59.018: INFO: stdout: "true"
Apr 29 11:12:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7spdv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:12:59.118: INFO: stderr: ""
Apr 29 11:12:59.118: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:12:59.118: INFO: validating pod update-demo-nautilus-7spdv
Apr 29 11:12:59.128: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:12:59.128: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:12:59.128: INFO: update-demo-nautilus-7spdv is verified up and running
Apr 29 11:12:59.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-8j6bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:12:59.227: INFO: stderr: ""
Apr 29 11:12:59.227: INFO: stdout: "true"
Apr 29 11:12:59.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-8j6bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:12:59.324: INFO: stderr: ""
Apr 29 11:12:59.325: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:12:59.325: INFO: validating pod update-demo-nautilus-8j6bk
Apr 29 11:12:59.329: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:12:59.329: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:12:59.330: INFO: update-demo-nautilus-8j6bk is verified up and running
STEP: scaling down the replication controller
Apr 29 11:12:59.331: INFO: scanned /root for discovery docs: <nil>
Apr 29 11:12:59.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6339'
Apr 29 11:13:00.445: INFO: stderr: ""
Apr 29 11:13:00.445: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 11:13:00.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:13:00.537: INFO: stderr: ""
Apr 29 11:13:00.537: INFO: stdout: "update-demo-nautilus-7spdv update-demo-nautilus-8j6bk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 29 11:13:05.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:13:05.627: INFO: stderr: ""
Apr 29 11:13:05.627: INFO: stdout: "update-demo-nautilus-7spdv update-demo-nautilus-8j6bk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 29 11:13:10.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:13:10.706: INFO: stderr: ""
Apr 29 11:13:10.706: INFO: stdout: "update-demo-nautilus-8j6bk "
Apr 29 11:13:10.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-8j6bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:10.795: INFO: stderr: ""
Apr 29 11:13:10.795: INFO: stdout: "true"
Apr 29 11:13:10.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-8j6bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:10.895: INFO: stderr: ""
Apr 29 11:13:10.895: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:13:10.895: INFO: validating pod update-demo-nautilus-8j6bk
Apr 29 11:13:10.898: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:13:10.898: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:13:10.898: INFO: update-demo-nautilus-8j6bk is verified up and running
STEP: scaling up the replication controller
Apr 29 11:13:10.900: INFO: scanned /root for discovery docs: <nil>
Apr 29 11:13:10.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6339'
Apr 29 11:13:12.039: INFO: stderr: ""
Apr 29 11:13:12.039: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 11:13:12.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:13:12.134: INFO: stderr: ""
Apr 29 11:13:12.134: INFO: stdout: "update-demo-nautilus-7fkwf update-demo-nautilus-8j6bk "
Apr 29 11:13:12.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7fkwf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:12.227: INFO: stderr: ""
Apr 29 11:13:12.227: INFO: stdout: ""
Apr 29 11:13:12.227: INFO: update-demo-nautilus-7fkwf is created but not running
Apr 29 11:13:17.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6339'
Apr 29 11:13:17.320: INFO: stderr: ""
Apr 29 11:13:17.320: INFO: stdout: "update-demo-nautilus-7fkwf update-demo-nautilus-8j6bk "
Apr 29 11:13:17.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7fkwf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:17.410: INFO: stderr: ""
Apr 29 11:13:17.410: INFO: stdout: "true"
Apr 29 11:13:17.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7fkwf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:17.518: INFO: stderr: ""
Apr 29 11:13:17.518: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:13:17.518: INFO: validating pod update-demo-nautilus-7fkwf
Apr 29 11:13:17.524: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:13:17.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:13:17.524: INFO: update-demo-nautilus-7fkwf is verified up and running
Apr 29 11:13:17.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-8j6bk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:17.605: INFO: stderr: ""
Apr 29 11:13:17.605: INFO: stdout: "true"
Apr 29 11:13:17.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-8j6bk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6339'
Apr 29 11:13:17.696: INFO: stderr: ""
Apr 29 11:13:17.696: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:13:17.696: INFO: validating pod update-demo-nautilus-8j6bk
Apr 29 11:13:17.701: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:13:17.701: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:13:17.701: INFO: update-demo-nautilus-8j6bk is verified up and running
STEP: using delete to clean up resources
Apr 29 11:13:17.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-6339'
Apr 29 11:13:17.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 11:13:17.802: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 11:13:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6339'
Apr 29 11:13:17.902: INFO: stderr: "No resources found.\n"
Apr 29 11:13:17.902: INFO: stdout: ""
Apr 29 11:13:17.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -l name=update-demo --namespace=kubectl-6339 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 11:13:17.989: INFO: stderr: ""
Apr 29 11:13:17.989: INFO: stdout: "update-demo-nautilus-7fkwf\nupdate-demo-nautilus-8j6bk\n"
Apr 29 11:13:18.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6339'
Apr 29 11:13:18.574: INFO: stderr: "No resources found.\n"
Apr 29 11:13:18.574: INFO: stdout: ""
Apr 29 11:13:18.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -l name=update-demo --namespace=kubectl-6339 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 11:13:18.650: INFO: stderr: ""
Apr 29 11:13:18.650: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:13:18.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6339" for this suite.
Apr 29 11:13:24.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:13:24.740: INFO: namespace kubectl-6339 deletion completed in 6.08503514s

• [SLOW TEST:31.284 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:13:24.741: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 29 11:13:27.812: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:13:27.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4947" for this suite.
Apr 29 11:13:49.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:13:49.924: INFO: namespace replicaset-4947 deletion completed in 22.090471357s

• [SLOW TEST:25.183 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:13:49.924: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0429 11:14:20.010267      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 29 11:14:20.010: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:14:20.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6136" for this suite.
Apr 29 11:14:26.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:14:26.124: INFO: namespace gc-6136 deletion completed in 6.105826399s

• [SLOW TEST:36.201 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:14:26.126: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 11:14:26.161: INFO: Waiting up to 5m0s for pod "pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3" in namespace "emptydir-7446" to be "success or failure"
Apr 29 11:14:26.166: INFO: Pod "pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.550315ms
Apr 29 11:14:28.169: INFO: Pod "pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008392417s
STEP: Saw pod success
Apr 29 11:14:28.169: INFO: Pod "pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:14:28.175: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:14:28.191: INFO: Waiting for pod pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:14:28.193: INFO: Pod pod-f2f46f6d-6a6f-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:14:28.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7446" for this suite.
Apr 29 11:14:34.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:14:34.303: INFO: namespace emptydir-7446 deletion completed in 6.106415306s

• [SLOW TEST:8.177 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:14:34.308: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-f7d554c9-6a6f-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:14:36.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3943" for this suite.
Apr 29 11:14:50.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:14:50.479: INFO: namespace configmap-3943 deletion completed in 14.102407605s

• [SLOW TEST:16.171 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:14:50.479: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Apr 29 11:14:50.511: INFO: Waiting up to 5m0s for pod "var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3" in namespace "var-expansion-5931" to be "success or failure"
Apr 29 11:14:50.515: INFO: Pod "var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426028ms
Apr 29 11:14:52.518: INFO: Pod "var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007248982s
STEP: Saw pod success
Apr 29 11:14:52.518: INFO: Pod "var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:14:52.521: INFO: Trying to get logs from node essentialpks-conformance-2 pod var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 11:14:52.536: INFO: Waiting for pod var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:14:52.538: INFO: Pod var-expansion-0178889b-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:14:52.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5931" for this suite.
Apr 29 11:14:58.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:14:58.640: INFO: namespace var-expansion-5931 deletion completed in 6.098915609s

• [SLOW TEST:8.161 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:14:58.640: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:14:58.680: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3" in namespace "downward-api-739" to be "success or failure"
Apr 29 11:14:58.687: INFO: Pod "downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.561871ms
Apr 29 11:15:00.693: INFO: Pod "downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013725013s
STEP: Saw pod success
Apr 29 11:15:00.694: INFO: Pod "downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:15:00.699: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:15:00.719: INFO: Waiting for pod downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:15:00.721: INFO: Pod downwardapi-volume-0656b266-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:15:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-739" for this suite.
Apr 29 11:15:06.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:15:06.816: INFO: namespace downward-api-739 deletion completed in 6.091436845s

• [SLOW TEST:8.177 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:15:06.817: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Apr 29 11:15:06.847: INFO: Waiting up to 5m0s for pod "client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3" in namespace "containers-9240" to be "success or failure"
Apr 29 11:15:06.851: INFO: Pod "client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868761ms
Apr 29 11:15:08.857: INFO: Pod "client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009270365s
STEP: Saw pod success
Apr 29 11:15:08.857: INFO: Pod "client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:15:08.863: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:15:08.882: INFO: Waiting for pod client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:15:08.885: INFO: Pod client-containers-0b34fc58-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:15:08.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9240" for this suite.
Apr 29 11:15:14.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:15:14.978: INFO: namespace containers-9240 deletion completed in 6.0887749s

• [SLOW TEST:8.161 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:15:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 11:15:15.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8345'
Apr 29 11:15:15.224: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 11:15:15.224: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Apr 29 11:15:17.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8345'
Apr 29 11:15:17.353: INFO: stderr: ""
Apr 29 11:15:17.353: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:15:17.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8345" for this suite.
Apr 29 11:15:39.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:15:39.457: INFO: namespace kubectl-8345 deletion completed in 22.099273405s

• [SLOW TEST:24.478 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:15:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1365
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1365 to expose endpoints map[]
Apr 29 11:15:39.510: INFO: Get endpoints failed (5.43144ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Apr 29 11:15:40.513: INFO: successfully validated that service multi-endpoint-test in namespace services-1365 exposes endpoints map[] (1.008404325s elapsed)
STEP: Creating pod pod1 in namespace services-1365
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1365 to expose endpoints map[pod1:[100]]
Apr 29 11:15:42.543: INFO: successfully validated that service multi-endpoint-test in namespace services-1365 exposes endpoints map[pod1:[100]] (2.022746344s elapsed)
STEP: Creating pod pod2 in namespace services-1365
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1365 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 29 11:15:44.575: INFO: successfully validated that service multi-endpoint-test in namespace services-1365 exposes endpoints map[pod1:[100] pod2:[101]] (2.027349405s elapsed)
STEP: Deleting pod pod1 in namespace services-1365
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1365 to expose endpoints map[pod2:[101]]
Apr 29 11:15:44.591: INFO: successfully validated that service multi-endpoint-test in namespace services-1365 exposes endpoints map[pod2:[101]] (9.127291ms elapsed)
STEP: Deleting pod pod2 in namespace services-1365
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1365 to expose endpoints map[]
Apr 29 11:15:45.601: INFO: successfully validated that service multi-endpoint-test in namespace services-1365 exposes endpoints map[] (1.005929032s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:15:45.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1365" for this suite.
Apr 29 11:16:07.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:16:07.714: INFO: namespace services-1365 deletion completed in 22.092809132s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:28.258 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:16:07.715: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8968
Apr 29 11:16:09.757: INFO: Started pod liveness-exec in namespace container-probe-8968
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 11:16:09.762: INFO: Initial restart count of pod liveness-exec is 0
Apr 29 11:16:57.867: INFO: Restart count of pod container-probe-8968/liveness-exec is now 1 (48.105461349s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:16:57.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8968" for this suite.
Apr 29 11:17:03.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:17:03.971: INFO: namespace container-probe-8968 deletion completed in 6.091564342s

• [SLOW TEST:56.256 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:17:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-1006/configmap-test-5109a4fe-6a70-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:17:04.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3" in namespace "configmap-1006" to be "success or failure"
Apr 29 11:17:04.018: INFO: Pod "pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.541996ms
Apr 29 11:17:06.022: INFO: Pod "pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007388695s
STEP: Saw pod success
Apr 29 11:17:06.022: INFO: Pod "pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:17:06.025: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3 container env-test: <nil>
STEP: delete the pod
Apr 29 11:17:06.040: INFO: Waiting for pod pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:17:06.042: INFO: Pod pod-configmaps-510a2145-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:17:06.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1006" for this suite.
Apr 29 11:17:12.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:17:12.135: INFO: namespace configmap-1006 deletion completed in 6.089231576s

• [SLOW TEST:8.164 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:17:12.135: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr 29 11:17:12.161: INFO: namespace kubectl-2123
Apr 29 11:17:12.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-2123'
Apr 29 11:17:12.347: INFO: stderr: ""
Apr 29 11:17:12.347: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 29 11:17:13.351: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:17:13.351: INFO: Found 0 / 1
Apr 29 11:17:14.351: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:17:14.351: INFO: Found 1 / 1
Apr 29 11:17:14.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 11:17:14.353: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 11:17:14.353: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 11:17:14.353: INFO: wait on redis-master startup in kubectl-2123 
Apr 29 11:17:14.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 logs redis-master-jt6kh redis-master --namespace=kubectl-2123'
Apr 29 11:17:14.453: INFO: stderr: ""
Apr 29 11:17:14.453: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Apr 11:17:13.125 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Apr 11:17:13.125 # Server started, Redis version 3.2.12\n1:M 29 Apr 11:17:13.125 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Apr 11:17:13.125 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr 29 11:17:14.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2123'
Apr 29 11:17:14.559: INFO: stderr: ""
Apr 29 11:17:14.559: INFO: stdout: "service/rm2 exposed\n"
Apr 29 11:17:14.563: INFO: Service rm2 in namespace kubectl-2123 found.
STEP: exposing service
Apr 29 11:17:16.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2123'
Apr 29 11:17:16.657: INFO: stderr: ""
Apr 29 11:17:16.657: INFO: stdout: "service/rm3 exposed\n"
Apr 29 11:17:16.660: INFO: Service rm3 in namespace kubectl-2123 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:17:18.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2123" for this suite.
Apr 29 11:17:40.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:17:40.782: INFO: namespace kubectl-2123 deletion completed in 22.11338501s

• [SLOW TEST:28.647 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:17:40.788: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-66fd1dc2-6a70-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume configMaps
Apr 29 11:17:40.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3" in namespace "projected-88" to be "success or failure"
Apr 29 11:17:40.841: INFO: Pod "pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691203ms
Apr 29 11:17:42.846: INFO: Pod "pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010452222s
STEP: Saw pod success
Apr 29 11:17:42.847: INFO: Pod "pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:17:42.850: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 11:17:42.869: INFO: Waiting for pod pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:17:42.872: INFO: Pod pod-projected-configmaps-66fdb9f8-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:17:42.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-88" for this suite.
Apr 29 11:17:48.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:17:48.959: INFO: namespace projected-88 deletion completed in 6.08424475s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:17:48.960: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Apr 29 11:17:51.002: INFO: Pod pod-hostip-6bda2214-6a70-11e9-aeaf-0a580af401c3 has hostIP: 192.168.102.3
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:17:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3082" for this suite.
Apr 29 11:18:13.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:18:13.096: INFO: namespace pods-3082 deletion completed in 22.088570936s

• [SLOW TEST:24.136 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:18:13.097: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 11:18:17.182: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:17.185: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 11:18:19.185: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:19.189: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 11:18:21.185: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:21.190: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 11:18:23.185: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:23.189: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 11:18:25.185: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:25.189: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 11:18:27.185: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:27.190: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 11:18:29.185: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 11:18:29.189: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:18:29.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3720" for this suite.
Apr 29 11:18:51.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:18:51.292: INFO: namespace container-lifecycle-hook-3720 deletion completed in 22.095694248s

• [SLOW TEST:38.195 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:18:51.293: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr 29 11:18:51.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 create -f - --namespace=kubectl-9251'
Apr 29 11:18:51.526: INFO: stderr: ""
Apr 29 11:18:51.526: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 11:18:51.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9251'
Apr 29 11:18:51.624: INFO: stderr: ""
Apr 29 11:18:51.624: INFO: stdout: "update-demo-nautilus-7sgts update-demo-nautilus-sq2rl "
Apr 29 11:18:51.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7sgts -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9251'
Apr 29 11:18:51.720: INFO: stderr: ""
Apr 29 11:18:51.720: INFO: stdout: ""
Apr 29 11:18:51.720: INFO: update-demo-nautilus-7sgts is created but not running
Apr 29 11:18:56.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9251'
Apr 29 11:18:56.803: INFO: stderr: ""
Apr 29 11:18:56.803: INFO: stdout: "update-demo-nautilus-7sgts update-demo-nautilus-sq2rl "
Apr 29 11:18:56.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7sgts -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9251'
Apr 29 11:18:56.889: INFO: stderr: ""
Apr 29 11:18:56.889: INFO: stdout: "true"
Apr 29 11:18:56.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-7sgts -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9251'
Apr 29 11:18:56.975: INFO: stderr: ""
Apr 29 11:18:56.975: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:18:56.975: INFO: validating pod update-demo-nautilus-7sgts
Apr 29 11:18:56.983: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:18:56.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:18:56.983: INFO: update-demo-nautilus-7sgts is verified up and running
Apr 29 11:18:56.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-sq2rl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9251'
Apr 29 11:18:57.067: INFO: stderr: ""
Apr 29 11:18:57.067: INFO: stdout: "true"
Apr 29 11:18:57.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods update-demo-nautilus-sq2rl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9251'
Apr 29 11:18:57.147: INFO: stderr: ""
Apr 29 11:18:57.147: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 11:18:57.147: INFO: validating pod update-demo-nautilus-sq2rl
Apr 29 11:18:57.154: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 11:18:57.154: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 11:18:57.154: INFO: update-demo-nautilus-sq2rl is verified up and running
STEP: using delete to clean up resources
Apr 29 11:18:57.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete --grace-period=0 --force -f - --namespace=kubectl-9251'
Apr 29 11:18:57.242: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 11:18:57.242: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 11:18:57.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9251'
Apr 29 11:18:57.325: INFO: stderr: "No resources found.\n"
Apr 29 11:18:57.325: INFO: stdout: ""
Apr 29 11:18:57.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -l name=update-demo --namespace=kubectl-9251 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 11:18:57.406: INFO: stderr: ""
Apr 29 11:18:57.406: INFO: stdout: "update-demo-nautilus-7sgts\nupdate-demo-nautilus-sq2rl\n"
Apr 29 11:18:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9251'
Apr 29 11:18:58.002: INFO: stderr: "No resources found.\n"
Apr 29 11:18:58.002: INFO: stdout: ""
Apr 29 11:18:58.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pods -l name=update-demo --namespace=kubectl-9251 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 11:18:58.093: INFO: stderr: ""
Apr 29 11:18:58.093: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:18:58.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9251" for this suite.
Apr 29 11:19:20.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:19:20.220: INFO: namespace kubectl-9251 deletion completed in 22.12324458s

• [SLOW TEST:28.927 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:19:20.221: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-454
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-454
STEP: Deleting pre-stop pod
Apr 29 11:19:43.293: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:19:43.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-454" for this suite.
Apr 29 11:20:21.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:20:21.407: INFO: namespace prestop-454 deletion completed in 38.099687881s

• [SLOW TEST:61.186 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:20:21.409: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-c6b92c1b-6a70-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:20:21.458: INFO: Waiting up to 5m0s for pod "pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3" in namespace "secrets-5479" to be "success or failure"
Apr 29 11:20:21.477: INFO: Pod "pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.650375ms
Apr 29 11:20:23.485: INFO: Pod "pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026226414s
STEP: Saw pod success
Apr 29 11:20:23.485: INFO: Pod "pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:20:23.488: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 11:20:23.506: INFO: Waiting for pod pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:20:23.508: INFO: Pod pod-secrets-c6b9c05a-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:20:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5479" for this suite.
Apr 29 11:20:29.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:20:29.613: INFO: namespace secrets-5479 deletion completed in 6.100998122s

• [SLOW TEST:8.204 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:20:29.613: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:20:33.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3058" for this suite.
Apr 29 11:20:39.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:20:39.767: INFO: namespace kubelet-test-3058 deletion completed in 6.105853074s

• [SLOW TEST:10.155 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:20:39.773: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 11:20:39.809: INFO: Waiting up to 5m0s for pod "pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3" in namespace "emptydir-2957" to be "success or failure"
Apr 29 11:20:39.813: INFO: Pod "pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.463132ms
Apr 29 11:20:41.818: INFO: Pod "pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009181132s
STEP: Saw pod success
Apr 29 11:20:41.818: INFO: Pod "pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:20:41.822: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:20:41.839: INFO: Waiting for pod pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:20:41.843: INFO: Pod pod-d1aaaddf-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:20:41.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2957" for this suite.
Apr 29 11:20:47.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:20:47.938: INFO: namespace emptydir-2957 deletion completed in 6.090985876s

• [SLOW TEST:8.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:20:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d688f8bd-6a70-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-d688f8bd-6a70-11e9-aeaf-0a580af401c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:20:52.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-934" for this suite.
Apr 29 11:21:14.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:21:14.163: INFO: namespace projected-934 deletion completed in 22.133762785s

• [SLOW TEST:26.225 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:21:14.163: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-e62ab82d-6a70-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:21:14.206: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3" in namespace "projected-2587" to be "success or failure"
Apr 29 11:21:14.209: INFO: Pod "pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.806276ms
Apr 29 11:21:16.213: INFO: Pod "pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006485732s
STEP: Saw pod success
Apr 29 11:21:16.213: INFO: Pod "pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:21:16.216: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 11:21:16.233: INFO: Waiting for pod pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:21:16.235: INFO: Pod pod-projected-secrets-e62b5344-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:21:16.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2587" for this suite.
Apr 29 11:21:22.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:21:22.334: INFO: namespace projected-2587 deletion completed in 6.095622956s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:21:22.339: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 29 11:21:22.372: INFO: Waiting up to 5m0s for pod "downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3" in namespace "downward-api-9858" to be "success or failure"
Apr 29 11:21:22.375: INFO: Pod "downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.009308ms
Apr 29 11:21:24.378: INFO: Pod "downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006359367s
STEP: Saw pod success
Apr 29 11:21:24.378: INFO: Pod "downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:21:24.381: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 11:21:24.400: INFO: Waiting for pod downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:21:24.403: INFO: Pod downward-api-eb099ecc-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:21:24.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9858" for this suite.
Apr 29 11:21:30.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:21:30.506: INFO: namespace downward-api-9858 deletion completed in 6.098675422s

• [SLOW TEST:8.166 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:21:30.510: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:21:30.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3" in namespace "projected-7843" to be "success or failure"
Apr 29 11:21:30.558: INFO: Pod "downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.99282ms
Apr 29 11:21:32.564: INFO: Pod "downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008863523s
STEP: Saw pod success
Apr 29 11:21:32.564: INFO: Pod "downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:21:32.568: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:21:32.585: INFO: Waiting for pod downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:21:32.587: INFO: Pod downwardapi-volume-efe946f6-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:21:32.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7843" for this suite.
Apr 29 11:21:38.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:21:38.678: INFO: namespace projected-7843 deletion completed in 6.085771672s

• [SLOW TEST:8.170 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:21:38.679: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f4cea987-6a70-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:21:38.765: INFO: Waiting up to 5m0s for pod "pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3" in namespace "secrets-3030" to be "success or failure"
Apr 29 11:21:38.769: INFO: Pod "pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.459135ms
Apr 29 11:21:40.773: INFO: Pod "pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008272302s
STEP: Saw pod success
Apr 29 11:21:40.773: INFO: Pod "pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:21:40.777: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3 container secret-env-test: <nil>
STEP: delete the pod
Apr 29 11:21:40.792: INFO: Waiting for pod pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:21:40.795: INFO: Pod pod-secrets-f4cf1220-6a70-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:21:40.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3030" for this suite.
Apr 29 11:21:46.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:21:46.897: INFO: namespace secrets-3030 deletion completed in 6.098887398s

• [SLOW TEST:8.218 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:21:46.898: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3342
Apr 29 11:21:48.993: INFO: Started pod liveness-http in namespace container-probe-3342
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 11:21:48.996: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:25:49.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3342" for this suite.
Apr 29 11:25:55.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:25:55.641: INFO: namespace container-probe-3342 deletion completed in 6.081971939s

• [SLOW TEST:248.744 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:25:55.645: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr 29 11:25:58.209: INFO: Successfully updated pod "annotationupdate8df059f7-6a71-11e9-aeaf-0a580af401c3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:26:02.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4195" for this suite.
Apr 29 11:26:24.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:26:24.330: INFO: namespace projected-4195 deletion completed in 22.084208531s

• [SLOW TEST:28.686 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:26:24.331: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 11:26:28.402: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:28.405: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:30.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:30.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:32.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:32.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:34.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:34.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:36.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:36.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:38.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:38.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:40.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:40.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:42.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:42.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:44.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:44.411: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:46.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:46.410: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 11:26:48.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 11:26:48.410: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:26:48.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3372" for this suite.
Apr 29 11:27:10.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:27:10.524: INFO: namespace container-lifecycle-hook-3372 deletion completed in 22.109513127s

• [SLOW TEST:46.193 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:27:10.524: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-ba9335a6-6a71-11e9-aeaf-0a580af401c3
STEP: Creating configMap with name cm-test-opt-upd-ba9335f6-6a71-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ba9335a6-6a71-11e9-aeaf-0a580af401c3
STEP: Updating configmap cm-test-opt-upd-ba9335f6-6a71-11e9-aeaf-0a580af401c3
STEP: Creating configMap with name cm-test-opt-create-ba93360e-6a71-11e9-aeaf-0a580af401c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:27:14.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2354" for this suite.
Apr 29 11:27:36.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:27:36.768: INFO: namespace projected-2354 deletion completed in 22.103061321s

• [SLOW TEST:26.244 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:27:36.770: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 11:27:36.805: INFO: Waiting up to 5m0s for pod "pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3" in namespace "emptydir-338" to be "success or failure"
Apr 29 11:27:36.813: INFO: Pod "pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261124ms
Apr 29 11:27:38.820: INFO: Pod "pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014310033s
STEP: Saw pod success
Apr 29 11:27:38.820: INFO: Pod "pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:27:38.823: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:27:38.843: INFO: Waiting for pod pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:27:38.849: INFO: Pod pod-ca37c1f2-6a71-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:27:38.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-338" for this suite.
Apr 29 11:27:44.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:27:44.956: INFO: namespace emptydir-338 deletion completed in 6.100215947s

• [SLOW TEST:8.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:27:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-bnl8
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 11:27:44.996: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bnl8" in namespace "subpath-2550" to be "success or failure"
Apr 29 11:27:45.001: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489021ms
Apr 29 11:27:47.004: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008110907s
Apr 29 11:27:49.010: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 4.014026149s
Apr 29 11:27:51.014: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 6.018159183s
Apr 29 11:27:53.018: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 8.022073349s
Apr 29 11:27:55.026: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 10.030018722s
Apr 29 11:27:57.032: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 12.0356398s
Apr 29 11:27:59.037: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 14.040668186s
Apr 29 11:28:01.041: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.045004002s
Apr 29 11:28:03.047: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 18.050455693s
Apr 29 11:28:05.051: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Running", Reason="", readiness=true. Elapsed: 20.055134799s
Apr 29 11:28:07.056: INFO: Pod "pod-subpath-test-configmap-bnl8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059608631s
STEP: Saw pod success
Apr 29 11:28:07.056: INFO: Pod "pod-subpath-test-configmap-bnl8" satisfied condition "success or failure"
Apr 29 11:28:07.059: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-configmap-bnl8 container test-container-subpath-configmap-bnl8: <nil>
STEP: delete the pod
Apr 29 11:28:07.091: INFO: Waiting for pod pod-subpath-test-configmap-bnl8 to disappear
Apr 29 11:28:07.094: INFO: Pod pod-subpath-test-configmap-bnl8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bnl8
Apr 29 11:28:07.094: INFO: Deleting pod "pod-subpath-test-configmap-bnl8" in namespace "subpath-2550"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:28:07.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2550" for this suite.
Apr 29 11:28:13.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:28:13.199: INFO: namespace subpath-2550 deletion completed in 6.09766066s

• [SLOW TEST:28.244 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:28:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-282/secret-test-dfeeb553-6a71-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:28:13.241: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3" in namespace "secrets-282" to be "success or failure"
Apr 29 11:28:13.244: INFO: Pod "pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758444ms
Apr 29 11:28:15.247: INFO: Pod "pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006153069s
STEP: Saw pod success
Apr 29 11:28:15.247: INFO: Pod "pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:28:15.250: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3 container env-test: <nil>
STEP: delete the pod
Apr 29 11:28:15.265: INFO: Waiting for pod pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:28:15.268: INFO: Pod pod-configmaps-dfef686d-6a71-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:28:15.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-282" for this suite.
Apr 29 11:28:21.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:28:21.370: INFO: namespace secrets-282 deletion completed in 6.09857309s

• [SLOW TEST:8.166 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:28:21.371: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 11:28:21.417: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 11:28:21.428: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:21.430: INFO: Number of nodes with available pods: 0
Apr 29 11:28:21.430: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:22.435: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:22.437: INFO: Number of nodes with available pods: 0
Apr 29 11:28:22.437: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:23.435: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:23.439: INFO: Number of nodes with available pods: 2
Apr 29 11:28:23.439: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 29 11:28:23.488: INFO: Wrong image for pod: daemon-set-26v6l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:23.488: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:23.495: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:24.499: INFO: Wrong image for pod: daemon-set-26v6l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:24.499: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:24.505: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:25.500: INFO: Wrong image for pod: daemon-set-26v6l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:25.500: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:25.505: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:26.500: INFO: Wrong image for pod: daemon-set-26v6l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:26.500: INFO: Pod daemon-set-26v6l is not available
Apr 29 11:28:26.500: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:26.507: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:27.500: INFO: Pod daemon-set-2cfm6 is not available
Apr 29 11:28:27.500: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:27.504: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:28.499: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:28.504: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:29.500: INFO: Wrong image for pod: daemon-set-7nwd5. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr 29 11:28:29.500: INFO: Pod daemon-set-7nwd5 is not available
Apr 29 11:28:29.503: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:30.500: INFO: Pod daemon-set-hf55w is not available
Apr 29 11:28:30.504: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 29 11:28:30.508: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:30.511: INFO: Number of nodes with available pods: 1
Apr 29 11:28:30.511: INFO: Node essentialpks-conformance-3 is running more than one daemon pod
Apr 29 11:28:31.516: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:28:31.518: INFO: Number of nodes with available pods: 2
Apr 29 11:28:31.518: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6758, will wait for the garbage collector to delete the pods
Apr 29 11:28:31.587: INFO: Deleting DaemonSet.extensions daemon-set took: 6.923615ms
Apr 29 11:28:31.887: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.163093ms
Apr 29 11:28:44.090: INFO: Number of nodes with available pods: 0
Apr 29 11:28:44.090: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 11:28:44.093: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6758/daemonsets","resourceVersion":"45302"},"items":null}

Apr 29 11:28:44.095: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6758/pods","resourceVersion":"45302"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:28:44.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6758" for this suite.
Apr 29 11:28:50.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:28:50.214: INFO: namespace daemonsets-6758 deletion completed in 6.105109177s

• [SLOW TEST:28.843 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:28:50.214: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 11:28:50.257: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 29 11:28:50.268: INFO: Number of nodes with available pods: 0
Apr 29 11:28:50.268: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 29 11:28:50.288: INFO: Number of nodes with available pods: 0
Apr 29 11:28:50.288: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:51.293: INFO: Number of nodes with available pods: 0
Apr 29 11:28:51.293: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:52.291: INFO: Number of nodes with available pods: 1
Apr 29 11:28:52.291: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 29 11:28:52.343: INFO: Number of nodes with available pods: 0
Apr 29 11:28:52.343: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 29 11:28:52.352: INFO: Number of nodes with available pods: 0
Apr 29 11:28:52.352: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:53.356: INFO: Number of nodes with available pods: 0
Apr 29 11:28:53.356: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:54.357: INFO: Number of nodes with available pods: 0
Apr 29 11:28:54.357: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:55.356: INFO: Number of nodes with available pods: 0
Apr 29 11:28:55.356: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:56.356: INFO: Number of nodes with available pods: 0
Apr 29 11:28:56.356: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:28:57.357: INFO: Number of nodes with available pods: 1
Apr 29 11:28:57.357: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9512, will wait for the garbage collector to delete the pods
Apr 29 11:28:57.424: INFO: Deleting DaemonSet.extensions daemon-set took: 6.112945ms
Apr 29 11:28:57.724: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.321444ms
Apr 29 11:29:09.027: INFO: Number of nodes with available pods: 0
Apr 29 11:29:09.027: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 11:29:09.032: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9512/daemonsets","resourceVersion":"45424"},"items":null}

Apr 29 11:29:09.035: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9512/pods","resourceVersion":"45424"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:29:09.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9512" for this suite.
Apr 29 11:29:15.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:29:15.148: INFO: namespace daemonsets-9512 deletion completed in 6.088641028s

• [SLOW TEST:24.934 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:29:15.149: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:29:15.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6012" for this suite.
Apr 29 11:29:21.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:29:21.300: INFO: namespace kubelet-test-6012 deletion completed in 6.105068037s

• [SLOW TEST:6.151 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:29:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 29 11:29:21.341: INFO: Waiting up to 5m0s for pod "downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3" in namespace "downward-api-1737" to be "success or failure"
Apr 29 11:29:21.345: INFO: Pod "downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958323ms
Apr 29 11:29:23.350: INFO: Pod "downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008801077s
STEP: Saw pod success
Apr 29 11:29:23.350: INFO: Pod "downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:29:23.356: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 11:29:23.377: INFO: Waiting for pod downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:29:23.380: INFO: Pod downward-api-08863ed8-6a72-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:29:23.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1737" for this suite.
Apr 29 11:29:29.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:29:29.499: INFO: namespace downward-api-1737 deletion completed in 6.115261392s

• [SLOW TEST:8.197 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:29:29.500: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3617
Apr 29 11:29:45.543: INFO: Started pod liveness-http in namespace container-probe-3617
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 11:29:45.549: INFO: Initial restart count of pod liveness-http is 0
Apr 29 11:29:59.578: INFO: Restart count of pod container-probe-3617/liveness-http is now 1 (14.028890388s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:29:59.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3617" for this suite.
Apr 29 11:30:05.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:30:05.694: INFO: namespace container-probe-3617 deletion completed in 6.10427722s

• [SLOW TEST:36.194 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:30:05.694: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 11:30:05.783: INFO: Create a RollingUpdate DaemonSet
Apr 29 11:30:05.787: INFO: Check that daemon pods launch on every node of the cluster
Apr 29 11:30:05.790: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:30:05.792: INFO: Number of nodes with available pods: 0
Apr 29 11:30:05.792: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:30:06.797: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:30:06.799: INFO: Number of nodes with available pods: 0
Apr 29 11:30:06.800: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:30:07.797: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:30:07.800: INFO: Number of nodes with available pods: 2
Apr 29 11:30:07.800: INFO: Number of running nodes: 2, number of available pods: 2
Apr 29 11:30:07.800: INFO: Update the DaemonSet to trigger a rollout
Apr 29 11:30:07.818: INFO: Updating DaemonSet daemon-set
Apr 29 11:30:11.832: INFO: Roll back the DaemonSet before rollout is complete
Apr 29 11:30:11.838: INFO: Updating DaemonSet daemon-set
Apr 29 11:30:11.838: INFO: Make sure DaemonSet rollback is complete
Apr 29 11:30:11.842: INFO: Wrong image for pod: daemon-set-dmfh9. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 29 11:30:11.842: INFO: Pod daemon-set-dmfh9 is not available
Apr 29 11:30:11.846: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:30:12.854: INFO: Wrong image for pod: daemon-set-dmfh9. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr 29 11:30:12.854: INFO: Pod daemon-set-dmfh9 is not available
Apr 29 11:30:12.859: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:30:13.854: INFO: Pod daemon-set-67dgm is not available
Apr 29 11:30:13.861: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8381, will wait for the garbage collector to delete the pods
Apr 29 11:30:13.934: INFO: Deleting DaemonSet.extensions daemon-set took: 13.039664ms
Apr 29 11:30:14.234: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.328416ms
Apr 29 11:30:29.037: INFO: Number of nodes with available pods: 0
Apr 29 11:30:29.037: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 11:30:29.040: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8381/daemonsets","resourceVersion":"45717"},"items":null}

Apr 29 11:30:29.043: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8381/pods","resourceVersion":"45717"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:30:29.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8381" for this suite.
Apr 29 11:30:35.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:30:35.169: INFO: namespace daemonsets-8381 deletion completed in 6.110905615s

• [SLOW TEST:29.476 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:30:35.170: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7110
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr 29 11:30:35.209: INFO: Found 0 stateful pods, waiting for 3
Apr 29 11:30:45.215: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 11:30:45.215: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 11:30:45.215: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 11:30:45.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-7110 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 11:30:45.444: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 11:30:45.444: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 11:30:45.444: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr 29 11:30:55.481: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 29 11:31:05.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-7110 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 11:31:05.701: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 11:31:05.701: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 11:31:05.701: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 11:31:15.720: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:31:15.720: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 29 11:31:15.720: INFO: Waiting for Pod statefulset-7110/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 29 11:31:15.720: INFO: Waiting for Pod statefulset-7110/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr 29 11:31:25.728: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:31:25.728: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Apr 29 11:31:35.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-7110 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr 29 11:31:35.962: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr 29 11:31:35.962: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr 29 11:31:35.963: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr 29 11:31:45.991: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 29 11:31:56.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 exec --namespace=statefulset-7110 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr 29 11:31:56.253: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr 29 11:31:56.254: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr 29 11:31:56.254: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr 29 11:31:56.267: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:31:56.267: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:31:56.267: INFO: Waiting for Pod statefulset-7110/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:31:56.267: INFO: Waiting for Pod statefulset-7110/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:06.276: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:32:06.276: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:06.276: INFO: Waiting for Pod statefulset-7110/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:06.276: INFO: Waiting for Pod statefulset-7110/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:16.275: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:32:16.275: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:16.275: INFO: Waiting for Pod statefulset-7110/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:26.274: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:32:26.274: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr 29 11:32:36.275: INFO: Waiting for StatefulSet statefulset-7110/ss2 to complete update
Apr 29 11:32:36.275: INFO: Waiting for Pod statefulset-7110/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr 29 11:32:46.273: INFO: Deleting all statefulset in ns statefulset-7110
Apr 29 11:32:46.277: INFO: Scaling statefulset ss2 to 0
Apr 29 11:33:16.292: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 11:33:16.296: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:33:16.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7110" for this suite.
Apr 29 11:33:22.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:33:22.412: INFO: namespace statefulset-7110 deletion completed in 6.099946618s

• [SLOW TEST:167.243 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:33:22.419: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr 29 11:33:22.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3" in namespace "projected-3050" to be "success or failure"
Apr 29 11:33:22.460: INFO: Pod "downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.381464ms
Apr 29 11:33:24.464: INFO: Pod "downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006861961s
STEP: Saw pod success
Apr 29 11:33:24.464: INFO: Pod "downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:33:24.467: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3 container client-container: <nil>
STEP: delete the pod
Apr 29 11:33:24.492: INFO: Waiting for pod downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:33:24.494: INFO: Pod downwardapi-volume-983d8c81-6a72-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:33:24.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3050" for this suite.
Apr 29 11:33:30.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:33:30.595: INFO: namespace projected-3050 deletion completed in 6.096036312s

• [SLOW TEST:8.177 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:33:30.602: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:33:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9956" for this suite.
Apr 29 11:33:58.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:33:58.909: INFO: namespace container-runtime-9956 deletion completed in 6.072847336s

• [SLOW TEST:28.307 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:33:58.909: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr 29 11:33:58.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8836'
Apr 29 11:33:59.147: INFO: stderr: ""
Apr 29 11:33:59.148: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr 29 11:34:04.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 get pod e2e-test-nginx-pod --namespace=kubectl-8836 -o json'
Apr 29 11:34:04.284: INFO: stderr: ""
Apr 29 11:34:04.284: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-04-29T11:33:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8836\",\n        \"resourceVersion\": \"46497\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8836/pods/e2e-test-nginx-pod\",\n        \"uid\": \"ae19760c-6a72-11e9-9342-005056a45e5c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kqhqg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"essentialpks-conformance-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kqhqg\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kqhqg\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-29T11:33:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-29T11:34:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-29T11:34:01Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-29T11:33:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://05a59a60f2a20d09139324d8b10b81c48e0f95dddde3831441a294442e7bfdc5\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-29T11:33:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.102.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.162\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-29T11:33:59Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 29 11:34:04.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 replace -f - --namespace=kubectl-8836'
Apr 29 11:34:04.467: INFO: stderr: ""
Apr 29 11:34:04.467: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Apr 29 11:34:04.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-008546913 delete pods e2e-test-nginx-pod --namespace=kubectl-8836'
Apr 29 11:34:08.988: INFO: stderr: ""
Apr 29 11:34:08.988: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:34:08.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8836" for this suite.
Apr 29 11:34:15.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:34:15.086: INFO: namespace kubectl-8836 deletion completed in 6.093743406s

• [SLOW TEST:16.176 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:34:15.086: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 11:34:15.133: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:34:15.142: INFO: Number of nodes with available pods: 0
Apr 29 11:34:15.142: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:34:16.147: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:34:16.150: INFO: Number of nodes with available pods: 0
Apr 29 11:34:16.150: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Apr 29 11:34:17.147: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:34:17.150: INFO: Number of nodes with available pods: 2
Apr 29 11:34:17.150: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 29 11:34:17.166: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 11:34:17.175: INFO: Number of nodes with available pods: 2
Apr 29 11:34:17.175: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1266, will wait for the garbage collector to delete the pods
Apr 29 11:34:18.248: INFO: Deleting DaemonSet.extensions daemon-set took: 7.432499ms
Apr 29 11:34:18.548: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.228423ms
Apr 29 11:34:29.051: INFO: Number of nodes with available pods: 0
Apr 29 11:34:29.051: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 11:34:29.055: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1266/daemonsets","resourceVersion":"46614"},"items":null}

Apr 29 11:34:29.058: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1266/pods","resourceVersion":"46614"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:34:29.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1266" for this suite.
Apr 29 11:34:35.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:34:35.148: INFO: namespace daemonsets-1266 deletion completed in 6.076221805s

• [SLOW TEST:20.061 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:34:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-c3976682-6a72-11e9-aeaf-0a580af401c3
STEP: Creating secret with name s-test-opt-upd-c39766db-6a72-11e9-aeaf-0a580af401c3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c3976682-6a72-11e9-aeaf-0a580af401c3
STEP: Updating secret s-test-opt-upd-c39766db-6a72-11e9-aeaf-0a580af401c3
STEP: Creating secret with name s-test-opt-create-c39766f4-6a72-11e9-aeaf-0a580af401c3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:35:45.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3234" for this suite.
Apr 29 11:36:07.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:36:07.777: INFO: namespace projected-3234 deletion completed in 22.110315792s

• [SLOW TEST:92.629 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:36:07.784: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0429 11:36:13.852287      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 29 11:36:13.852: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:36:13.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7601" for this suite.
Apr 29 11:36:19.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:36:19.941: INFO: namespace gc-7601 deletion completed in 6.081485668s

• [SLOW TEST:12.159 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:36:19.942: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4350.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4350.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 68.187.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.187.68_udp@PTR;check="$$(dig +tcp +noall +answer +search 68.187.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.187.68_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4350.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4350.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 68.187.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.187.68_udp@PTR;check="$$(dig +tcp +noall +answer +search 68.187.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.187.68_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 11:36:22.018: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local from pod dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3: the server could not find the requested resource (get pods dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3)
Apr 29 11:36:22.022: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local from pod dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3: the server could not find the requested resource (get pods dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3)
Apr 29 11:36:22.043: INFO: Unable to read jessie_udp@dns-test-service.dns-4350.svc.cluster.local from pod dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3: the server could not find the requested resource (get pods dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3)
Apr 29 11:36:22.047: INFO: Unable to read jessie_tcp@dns-test-service.dns-4350.svc.cluster.local from pod dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3: the server could not find the requested resource (get pods dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3)
Apr 29 11:36:22.050: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local from pod dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3: the server could not find the requested resource (get pods dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3)
Apr 29 11:36:22.056: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local from pod dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3: the server could not find the requested resource (get pods dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3)
Apr 29 11:36:22.076: INFO: Lookups using dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local jessie_udp@dns-test-service.dns-4350.svc.cluster.local jessie_tcp@dns-test-service.dns-4350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4350.svc.cluster.local]

Apr 29 11:36:27.146: INFO: DNS probes using dns-4350/dns-test-020f4b47-6a73-11e9-aeaf-0a580af401c3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:36:27.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4350" for this suite.
Apr 29 11:36:33.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:36:33.338: INFO: namespace dns-4350 deletion completed in 6.096103889s

• [SLOW TEST:13.396 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:36:33.338: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr 29 11:36:33.366: INFO: Creating deployment "test-recreate-deployment"
Apr 29 11:36:33.372: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 29 11:36:33.382: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 29 11:36:35.387: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 29 11:36:35.390: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 29 11:36:35.398: INFO: Updating deployment test-recreate-deployment
Apr 29 11:36:35.398: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr 29 11:36:35.460: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7232,SelfLink:/apis/apps/v1/namespaces/deployment-7232/deployments/test-recreate-deployment,UID:0a0764da-6a73-11e9-9342-005056a45e5c,ResourceVersion:47189,Generation:2,CreationTimestamp:2019-04-29 11:36:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-29 11:36:35 +0000 UTC 2019-04-29 11:36:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-29 11:36:35 +0000 UTC 2019-04-29 11:36:33 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 11:36:35.464: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-7232,SelfLink:/apis/apps/v1/namespaces/deployment-7232/replicasets/test-recreate-deployment-c9cbd8684,UID:0b414619-6a73-11e9-9342-005056a45e5c,ResourceVersion:47188,Generation:1,CreationTimestamp:2019-04-29 11:36:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0a0764da-6a73-11e9-9342-005056a45e5c 0xc001e983a0 0xc001e983a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 11:36:35.464: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 29 11:36:35.464: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-7232,SelfLink:/apis/apps/v1/namespaces/deployment-7232/replicasets/test-recreate-deployment-7d57d5ff7c,UID:0a08213c-6a73-11e9-9342-005056a45e5c,ResourceVersion:47179,Generation:2,CreationTimestamp:2019-04-29 11:36:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0a0764da-6a73-11e9-9342-005056a45e5c 0xc001e982d7 0xc001e982d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr 29 11:36:35.468: INFO: Pod "test-recreate-deployment-c9cbd8684-vz4kb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-vz4kb,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-7232,SelfLink:/api/v1/namespaces/deployment-7232/pods/test-recreate-deployment-c9cbd8684-vz4kb,UID:0b41a985-6a73-11e9-9342-005056a45e5c,ResourceVersion:47190,Generation:0,CreationTimestamp:2019-04-29 11:36:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 0b414619-6a73-11e9-9342-005056a45e5c 0xc003114a80 0xc003114a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qj7jp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qj7jp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qj7jp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003114ae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003114b00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:36:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:36:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:36:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-29 11:36:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-04-29 11:36:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:36:35.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7232" for this suite.
Apr 29 11:36:41.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:36:41.568: INFO: namespace deployment-7232 deletion completed in 6.096770898s

• [SLOW TEST:8.230 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:36:41.568: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr 29 11:36:41.595: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 11:36:41.606: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 11:36:41.609: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-2 before test
Apr 29 11:36:41.616: INFO: kube-flannel-ds-amd64-c4rpf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 11:36:41.616: INFO: 	Container kube-flannel ready: true, restart count 2
Apr 29 11:36:41.616: INFO: sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-ln6c4 from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 11:36:41.616: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Apr 29 11:36:41.616: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 29 11:36:41.616: INFO: kube-proxy-x8ptf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 11:36:41.616: INFO: 	Container kube-proxy ready: true, restart count 2
Apr 29 11:36:41.616: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-3 before test
Apr 29 11:36:41.624: INFO: kube-flannel-ds-amd64-5d5mf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 11:36:41.625: INFO: 	Container kube-flannel ready: true, restart count 2
Apr 29 11:36:41.625: INFO: kube-proxy-h9j6t from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Apr 29 11:36:41.625: INFO: 	Container kube-proxy ready: true, restart count 2
Apr 29 11:36:41.625: INFO: sonobuoy-e2e-job-225965689e404c5d from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 11:36:41.625: INFO: 	Container e2e ready: true, restart count 0
Apr 29 11:36:41.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 11:36:41.625: INFO: sonobuoy-systemd-logs-daemon-set-fc73758218e94e5b-km6th from heptio-sonobuoy started at 2019-04-29 10:13:52 +0000 UTC (2 container statuses recorded)
Apr 29 11:36:41.625: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 1
Apr 29 11:36:41.625: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 29 11:36:41.625: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-29 10:13:51 +0000 UTC (1 container statuses recorded)
Apr 29 11:36:41.625: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-102a9397-6a73-11e9-aeaf-0a580af401c3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-102a9397-6a73-11e9-aeaf-0a580af401c3 off the node essentialpks-conformance-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-102a9397-6a73-11e9-aeaf-0a580af401c3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:36:45.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8010" for this suite.
Apr 29 11:37:03.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:37:03.773: INFO: namespace sched-pred-8010 deletion completed in 18.075785168s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:22.204 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:37:03.773: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-1c2d2666-6a73-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:37:03.810: INFO: Waiting up to 5m0s for pod "pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3" in namespace "secrets-2104" to be "success or failure"
Apr 29 11:37:03.819: INFO: Pod "pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.100591ms
Apr 29 11:37:05.824: INFO: Pod "pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014294025s
STEP: Saw pod success
Apr 29 11:37:05.824: INFO: Pod "pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:37:05.827: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 11:37:05.844: INFO: Waiting for pod pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:37:05.863: INFO: Pod pod-secrets-1c2d9a23-6a73-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:37:05.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2104" for this suite.
Apr 29 11:37:11.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:37:11.973: INFO: namespace secrets-2104 deletion completed in 6.10304782s

• [SLOW TEST:8.199 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:37:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9452
Apr 29 11:37:14.026: INFO: Started pod liveness-http in namespace container-probe-9452
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 11:37:14.029: INFO: Initial restart count of pod liveness-http is 0
Apr 29 11:37:32.068: INFO: Restart count of pod container-probe-9452/liveness-http is now 1 (18.039303311s elapsed)
Apr 29 11:37:52.109: INFO: Restart count of pod container-probe-9452/liveness-http is now 2 (38.079645008s elapsed)
Apr 29 11:38:12.159: INFO: Restart count of pod container-probe-9452/liveness-http is now 3 (58.130395639s elapsed)
Apr 29 11:38:32.208: INFO: Restart count of pod container-probe-9452/liveness-http is now 4 (1m18.17911493s elapsed)
Apr 29 11:39:40.361: INFO: Restart count of pod container-probe-9452/liveness-http is now 5 (2m26.332160431s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:39:40.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9452" for this suite.
Apr 29 11:39:46.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:39:46.464: INFO: namespace container-probe-9452 deletion completed in 6.087434306s

• [SLOW TEST:154.491 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:39:46.469: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr 29 11:39:46.499: INFO: Waiting up to 5m0s for pod "downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3" in namespace "downward-api-308" to be "success or failure"
Apr 29 11:39:46.506: INFO: Pod "downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.413321ms
Apr 29 11:39:48.511: INFO: Pod "downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012028905s
STEP: Saw pod success
Apr 29 11:39:48.511: INFO: Pod "downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:39:48.515: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 11:39:48.536: INFO: Waiting for pod downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:39:48.539: INFO: Pod downward-api-7d25e8fe-6a73-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:39:48.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-308" for this suite.
Apr 29 11:39:54.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:39:54.630: INFO: namespace downward-api-308 deletion completed in 6.085271313s

• [SLOW TEST:8.161 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:39:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2439
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 11:39:54.663: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 11:40:10.734: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.248 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2439 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:40:10.734: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:40:11.846: INFO: Found all expected endpoints: [netserver-0]
Apr 29 11:40:11.850: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.182 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2439 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:40:11.850: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:40:12.956: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:40:12.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2439" for this suite.
Apr 29 11:40:34.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:40:35.068: INFO: namespace pod-network-test-2439 deletion completed in 22.103234502s

• [SLOW TEST:40.437 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:40:35.071: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Apr 29 11:40:35.099: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-008546913 proxy --unix-socket=/tmp/kubectl-proxy-unix632581196/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:40:35.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5842" for this suite.
Apr 29 11:40:41.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:40:41.270: INFO: namespace kubectl-5842 deletion completed in 6.100299013s

• [SLOW TEST:6.200 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:40:41.271: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Apr 29 11:40:41.887: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 29 11:40:43.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:45.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:47.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:49.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:51.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:53.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:55.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:57.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:40:59.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:01.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:03.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:05.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:07.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:09.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:11.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:13.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692134841, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 11:41:20.387: INFO: Waited 4.436124766s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:41:20.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5152" for this suite.
Apr 29 11:41:26.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:41:27.003: INFO: namespace aggregator-5152 deletion completed in 6.184918152s

• [SLOW TEST:45.732 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:41:27.005: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:41:29.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7235" for this suite.
Apr 29 11:41:35.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:41:35.259: INFO: namespace emptydir-wrapper-7235 deletion completed in 6.107412541s

• [SLOW TEST:8.254 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:41:35.259: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 11:41:35.297: INFO: Waiting up to 5m0s for pod "pod-bdff6237-6a73-11e9-aeaf-0a580af401c3" in namespace "emptydir-8030" to be "success or failure"
Apr 29 11:41:35.303: INFO: Pod "pod-bdff6237-6a73-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742346ms
Apr 29 11:41:37.306: INFO: Pod "pod-bdff6237-6a73-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008805252s
STEP: Saw pod success
Apr 29 11:41:37.306: INFO: Pod "pod-bdff6237-6a73-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:41:37.310: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-bdff6237-6a73-11e9-aeaf-0a580af401c3 container test-container: <nil>
STEP: delete the pod
Apr 29 11:41:37.329: INFO: Waiting for pod pod-bdff6237-6a73-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:41:37.332: INFO: Pod pod-bdff6237-6a73-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:41:37.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8030" for this suite.
Apr 29 11:41:43.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:41:43.425: INFO: namespace emptydir-8030 deletion completed in 6.089958896s

• [SLOW TEST:8.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:41:43.426: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Apr 29 11:41:43.485: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1705" to be "success or failure"
Apr 29 11:41:43.489: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174294ms
Apr 29 11:41:45.493: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007472802s
STEP: Saw pod success
Apr 29 11:41:45.493: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 29 11:41:45.495: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 29 11:41:45.516: INFO: Waiting for pod pod-host-path-test to disappear
Apr 29 11:41:45.518: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:41:45.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1705" for this suite.
Apr 29 11:41:51.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:41:51.613: INFO: namespace hostpath-1705 deletion completed in 6.092178864s

• [SLOW TEST:8.187 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:41:51.614: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2872
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 11:41:51.637: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 11:42:09.700: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.188:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2872 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:42:09.700: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:42:09.815: INFO: Found all expected endpoints: [netserver-0]
Apr 29 11:42:09.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.249:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2872 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 11:42:09.818: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
Apr 29 11:42:09.931: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:42:09.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2872" for this suite.
Apr 29 11:42:31.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:42:32.041: INFO: namespace pod-network-test-2872 deletion completed in 22.100867057s

• [SLOW TEST:40.427 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:42:32.042: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:42:34.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3522" for this suite.
Apr 29 11:43:12.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:43:12.231: INFO: namespace kubelet-test-3522 deletion completed in 38.102177215s

• [SLOW TEST:40.189 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:43:12.231: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f7caa4e0-6a73-11e9-aeaf-0a580af401c3
STEP: Creating a pod to test consume secrets
Apr 29 11:43:12.263: INFO: Waiting up to 5m0s for pod "pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3" in namespace "secrets-9585" to be "success or failure"
Apr 29 11:43:12.268: INFO: Pod "pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227061ms
Apr 29 11:43:14.273: INFO: Pod "pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009317713s
STEP: Saw pod success
Apr 29 11:43:14.273: INFO: Pod "pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3" satisfied condition "success or failure"
Apr 29 11:43:14.277: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 11:43:14.292: INFO: Waiting for pod pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3 to disappear
Apr 29 11:43:14.294: INFO: Pod pod-secrets-f7cb021b-6a73-11e9-aeaf-0a580af401c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:43:14.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9585" for this suite.
Apr 29 11:43:20.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:43:20.395: INFO: namespace secrets-9585 deletion completed in 6.097552301s

• [SLOW TEST:8.165 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr 29 11:43:20.399: INFO: >>> kubeConfig: /tmp/kubeconfig-008546913
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-hvfp
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 11:43:20.444: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hvfp" in namespace "subpath-8072" to be "success or failure"
Apr 29 11:43:20.448: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.221027ms
Apr 29 11:43:22.452: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 2.008325277s
Apr 29 11:43:24.456: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 4.012731893s
Apr 29 11:43:26.460: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 6.016716387s
Apr 29 11:43:28.465: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 8.021376847s
Apr 29 11:43:30.469: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 10.025010644s
Apr 29 11:43:32.472: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 12.028738787s
Apr 29 11:43:34.476: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 14.03237864s
Apr 29 11:43:36.480: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 16.036218402s
Apr 29 11:43:38.484: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 18.040246206s
Apr 29 11:43:40.488: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Running", Reason="", readiness=true. Elapsed: 20.044827199s
Apr 29 11:43:42.496: INFO: Pod "pod-subpath-test-configmap-hvfp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.052848508s
STEP: Saw pod success
Apr 29 11:43:42.497: INFO: Pod "pod-subpath-test-configmap-hvfp" satisfied condition "success or failure"
Apr 29 11:43:42.503: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-configmap-hvfp container test-container-subpath-configmap-hvfp: <nil>
STEP: delete the pod
Apr 29 11:43:42.524: INFO: Waiting for pod pod-subpath-test-configmap-hvfp to disappear
Apr 29 11:43:42.527: INFO: Pod pod-subpath-test-configmap-hvfp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hvfp
Apr 29 11:43:42.527: INFO: Deleting pod "pod-subpath-test-configmap-hvfp" in namespace "subpath-8072"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr 29 11:43:42.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8072" for this suite.
Apr 29 11:43:48.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 11:43:48.628: INFO: namespace subpath-8072 deletion completed in 6.093100255s

• [SLOW TEST:28.231 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSApr 29 11:43:48.635: INFO: Running AfterSuite actions on all nodes
Apr 29 11:43:48.635: INFO: Running AfterSuite actions on node 1
Apr 29 11:43:48.635: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5394.097 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h29m55.54237804s
Test Suite Passed
