I0301 09:01:30.162986      17 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-062681313
I0301 09:01:30.278747      17 e2e.go:243] Starting e2e run "5cec14c5-3760-4c4a-a03f-44baba2fb195" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1583053283 - Will randomize all specs
Will run 215 of 4412 specs

Mar  1 09:01:30.689: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:01:30.740: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  1 09:01:30.772: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  1 09:01:30.821: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  1 09:01:30.821: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar  1 09:01:30.821: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  1 09:01:30.837: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Mar  1 09:01:30.837: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Mar  1 09:01:30.837: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Mar  1 09:01:30.837: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Mar  1 09:01:30.837: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Mar  1 09:01:30.837: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  1 09:01:30.837: INFO: e2e test version: v1.15.10
Mar  1 09:01:30.839: INFO: kube-apiserver version: v1.15.10
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:01:30.839: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
Mar  1 09:01:31.013: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  1 09:01:31.028: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-e1fbbb38-a0dd-4372-b435-60d21091ed7a
STEP: Creating a pod to test consume configMaps
Mar  1 09:01:31.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b" in namespace "projected-1789" to be "success or failure"
Mar  1 09:01:31.238: INFO: Pod "pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.703099ms
Mar  1 09:01:33.246: INFO: Pod "pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014000636s
Mar  1 09:01:35.279: INFO: Pod "pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047164702s
STEP: Saw pod success
Mar  1 09:01:35.279: INFO: Pod "pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b" satisfied condition "success or failure"
Mar  1 09:01:35.285: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:01:35.399: INFO: Waiting for pod pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b to disappear
Mar  1 09:01:35.404: INFO: Pod pod-projected-configmaps-78ffdd27-510b-47a3-b4a6-741c1a81540b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:01:35.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1789" for this suite.
Mar  1 09:01:41.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:01:41.775: INFO: namespace projected-1789 deletion completed in 6.360550015s

• [SLOW TEST:10.936 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:01:41.776: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:01:42.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8498" for this suite.
Mar  1 09:01:48.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:01:48.659: INFO: namespace services-8498 deletion completed in 6.543921164s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.883 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:01:48.659: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:01:48.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 version'
Mar  1 09:01:49.244: INFO: stderr: ""
Mar  1 09:01:49.244: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.10\", GitCommit:\"1bea6c00a7055edef03f1d4bb58b773fa8917f11\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T20:13:57Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.10\", GitCommit:\"1bea6c00a7055edef03f1d4bb58b773fa8917f11\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T20:05:26Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:01:49.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5701" for this suite.
Mar  1 09:01:55.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:01:55.525: INFO: namespace kubectl-5701 deletion completed in 6.270008933s

• [SLOW TEST:6.866 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:01:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3462
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:01:55.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59" in namespace "projected-3462" to be "success or failure"
Mar  1 09:01:56.033: INFO: Pod "downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59": Phase="Pending", Reason="", readiness=false. Elapsed: 39.466058ms
Mar  1 09:01:58.041: INFO: Pod "downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047424825s
Mar  1 09:02:00.049: INFO: Pod "downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055641165s
STEP: Saw pod success
Mar  1 09:02:00.050: INFO: Pod "downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59" satisfied condition "success or failure"
Mar  1 09:02:00.055: INFO: Trying to get logs from node worker02 pod downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59 container client-container: <nil>
STEP: delete the pod
Mar  1 09:02:00.143: INFO: Waiting for pod downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59 to disappear
Mar  1 09:02:00.149: INFO: Pod downwardapi-volume-2cf55d3c-c890-4097-8982-aec88e1cfa59 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:02:00.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3462" for this suite.
Mar  1 09:02:06.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:02:06.602: INFO: namespace projected-3462 deletion completed in 6.445175099s

• [SLOW TEST:11.076 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:02:06.603: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-84c9df23-41fa-4f8b-b329-a045e590cae8
STEP: Creating a pod to test consume secrets
Mar  1 09:02:06.957: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b" in namespace "projected-7165" to be "success or failure"
Mar  1 09:02:06.963: INFO: Pod "pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.403963ms
Mar  1 09:02:08.971: INFO: Pod "pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014104734s
Mar  1 09:02:10.978: INFO: Pod "pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021426024s
Mar  1 09:02:13.121: INFO: Pod "pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.163797939s
STEP: Saw pod success
Mar  1 09:02:13.121: INFO: Pod "pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b" satisfied condition "success or failure"
Mar  1 09:02:13.146: INFO: Trying to get logs from node worker02 pod pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:02:13.265: INFO: Waiting for pod pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b to disappear
Mar  1 09:02:13.274: INFO: Pod pod-projected-secrets-ffd8962a-f301-48a0-a844-2690eaf3099b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:02:13.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7165" for this suite.
Mar  1 09:02:19.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:02:19.548: INFO: namespace projected-7165 deletion completed in 6.263853393s

• [SLOW TEST:12.945 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:02:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-b554e944-934e-4c78-9709-b952a6e468ee in namespace container-probe-971
Mar  1 09:02:23.894: INFO: Started pod liveness-b554e944-934e-4c78-9709-b952a6e468ee in namespace container-probe-971
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 09:02:23.901: INFO: Initial restart count of pod liveness-b554e944-934e-4c78-9709-b952a6e468ee is 0
Mar  1 09:02:40.001: INFO: Restart count of pod container-probe-971/liveness-b554e944-934e-4c78-9709-b952a6e468ee is now 1 (16.100272187s elapsed)
Mar  1 09:03:00.087: INFO: Restart count of pod container-probe-971/liveness-b554e944-934e-4c78-9709-b952a6e468ee is now 2 (36.186889536s elapsed)
Mar  1 09:03:20.195: INFO: Restart count of pod container-probe-971/liveness-b554e944-934e-4c78-9709-b952a6e468ee is now 3 (56.294357734s elapsed)
Mar  1 09:03:40.620: INFO: Restart count of pod container-probe-971/liveness-b554e944-934e-4c78-9709-b952a6e468ee is now 4 (1m16.719571063s elapsed)
Mar  1 09:04:43.108: INFO: Restart count of pod container-probe-971/liveness-b554e944-934e-4c78-9709-b952a6e468ee is now 5 (2m19.207063079s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:04:43.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-971" for this suite.
Mar  1 09:04:49.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:04:49.394: INFO: namespace container-probe-971 deletion completed in 6.220153536s

• [SLOW TEST:149.845 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:04:49.395: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8179
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:04:49.850: INFO: (0) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 120.704366ms)
Mar  1 09:04:49.859: INFO: (1) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.241482ms)
Mar  1 09:04:49.867: INFO: (2) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.42494ms)
Mar  1 09:04:49.874: INFO: (3) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.36108ms)
Mar  1 09:04:49.881: INFO: (4) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.257976ms)
Mar  1 09:04:49.888: INFO: (5) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.983436ms)
Mar  1 09:04:49.895: INFO: (6) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.68565ms)
Mar  1 09:04:49.903: INFO: (7) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.741339ms)
Mar  1 09:04:49.914: INFO: (8) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.686539ms)
Mar  1 09:04:49.920: INFO: (9) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.40151ms)
Mar  1 09:04:49.927: INFO: (10) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.697232ms)
Mar  1 09:04:49.933: INFO: (11) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.337923ms)
Mar  1 09:04:49.939: INFO: (12) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.967243ms)
Mar  1 09:04:49.945: INFO: (13) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.007797ms)
Mar  1 09:04:49.952: INFO: (14) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.725987ms)
Mar  1 09:04:49.959: INFO: (15) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.585467ms)
Mar  1 09:04:49.965: INFO: (16) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.227167ms)
Mar  1 09:04:49.985: INFO: (17) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 19.926039ms)
Mar  1 09:04:50.020: INFO: (18) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 34.904038ms)
Mar  1 09:04:50.027: INFO: (19) /api/v1/nodes/worker01/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.912729ms)
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:04:50.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8179" for this suite.
Mar  1 09:04:56.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:04:56.411: INFO: namespace proxy-8179 deletion completed in 6.37460624s

• [SLOW TEST:7.016 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:04:56.411: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0301 09:05:06.797922      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  1 09:05:06.798: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:05:06.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-935" for this suite.
Mar  1 09:05:14.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:05:15.403: INFO: namespace gc-935 deletion completed in 8.595288185s

• [SLOW TEST:18.991 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:05:15.403: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  1 09:05:15.680: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:05:22.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-995" for this suite.
Mar  1 09:05:30.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:05:31.138: INFO: namespace init-container-995 deletion completed in 8.482514455s

• [SLOW TEST:15.735 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:05:31.139: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-7362a96d-ac5f-477b-8172-4f205b074c63 in namespace container-probe-3199
Mar  1 09:05:37.524: INFO: Started pod busybox-7362a96d-ac5f-477b-8172-4f205b074c63 in namespace container-probe-3199
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 09:05:37.529: INFO: Initial restart count of pod busybox-7362a96d-ac5f-477b-8172-4f205b074c63 is 0
Mar  1 09:06:24.077: INFO: Restart count of pod container-probe-3199/busybox-7362a96d-ac5f-477b-8172-4f205b074c63 is now 1 (46.547669255s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:06:24.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3199" for this suite.
Mar  1 09:06:32.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:06:32.424: INFO: namespace container-probe-3199 deletion completed in 8.282315397s

• [SLOW TEST:61.286 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:06:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-c310f016-74e1-4e71-b8e9-9f02ab6284a2
STEP: Creating a pod to test consume secrets
Mar  1 09:06:32.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a" in namespace "projected-8003" to be "success or failure"
Mar  1 09:06:32.747: INFO: Pod "pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.580916ms
Mar  1 09:06:34.755: INFO: Pod "pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01546697s
Mar  1 09:06:36.763: INFO: Pod "pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023498873s
Mar  1 09:06:38.772: INFO: Pod "pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032297357s
STEP: Saw pod success
Mar  1 09:06:38.772: INFO: Pod "pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a" satisfied condition "success or failure"
Mar  1 09:06:38.779: INFO: Trying to get logs from node worker02 pod pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:06:38.863: INFO: Waiting for pod pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a to disappear
Mar  1 09:06:38.868: INFO: Pod pod-projected-secrets-55fb9bdf-88ff-44ea-93e9-80baab95139a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:06:38.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8003" for this suite.
Mar  1 09:06:44.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:06:45.139: INFO: namespace projected-8003 deletion completed in 6.25844824s

• [SLOW TEST:12.715 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:06:45.140: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  1 09:06:45.525: INFO: Waiting up to 5m0s for pod "pod-ff49f626-44fc-4bd4-83d8-02a929601d41" in namespace "emptydir-7720" to be "success or failure"
Mar  1 09:06:45.531: INFO: Pod "pod-ff49f626-44fc-4bd4-83d8-02a929601d41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060063ms
Mar  1 09:06:47.539: INFO: Pod "pod-ff49f626-44fc-4bd4-83d8-02a929601d41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01407231s
Mar  1 09:06:49.547: INFO: Pod "pod-ff49f626-44fc-4bd4-83d8-02a929601d41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02175448s
Mar  1 09:06:51.556: INFO: Pod "pod-ff49f626-44fc-4bd4-83d8-02a929601d41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03042662s
STEP: Saw pod success
Mar  1 09:06:51.556: INFO: Pod "pod-ff49f626-44fc-4bd4-83d8-02a929601d41" satisfied condition "success or failure"
Mar  1 09:06:51.562: INFO: Trying to get logs from node worker02 pod pod-ff49f626-44fc-4bd4-83d8-02a929601d41 container test-container: <nil>
STEP: delete the pod
Mar  1 09:06:51.687: INFO: Waiting for pod pod-ff49f626-44fc-4bd4-83d8-02a929601d41 to disappear
Mar  1 09:06:51.692: INFO: Pod pod-ff49f626-44fc-4bd4-83d8-02a929601d41 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:06:51.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7720" for this suite.
Mar  1 09:06:57.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:06:57.966: INFO: namespace emptydir-7720 deletion completed in 6.26456034s

• [SLOW TEST:12.827 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:06:57.967: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-324
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar  1 09:06:58.362: INFO: Found 0 stateful pods, waiting for 3
Mar  1 09:07:08.372: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:07:08.372: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:07:08.372: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  1 09:07:18.371: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:07:18.371: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:07:18.371: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:07:18.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-324 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 09:07:22.522: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 09:07:22.522: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 09:07:22.522: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from 172.20.8.7/library/nginx:1.14-alpine to 172.20.8.7/library/nginx:1.15-alpine
Mar  1 09:07:32.582: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  1 09:07:42.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-324 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 09:07:43.207: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  1 09:07:43.208: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 09:07:43.208: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 09:08:03.288: INFO: Waiting for StatefulSet statefulset-324/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  1 09:08:13.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-324 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 09:08:13.814: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 09:08:13.814: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 09:08:13.814: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 09:08:23.876: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  1 09:08:33.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-324 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 09:08:34.399: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  1 09:08:34.399: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 09:08:34.399: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 09:08:44.481: INFO: Waiting for StatefulSet statefulset-324/ss2 to complete update
Mar  1 09:08:44.481: INFO: Waiting for Pod statefulset-324/ss2-0 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
Mar  1 09:08:44.481: INFO: Waiting for Pod statefulset-324/ss2-1 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
Mar  1 09:08:54.497: INFO: Waiting for StatefulSet statefulset-324/ss2 to complete update
Mar  1 09:08:54.498: INFO: Waiting for Pod statefulset-324/ss2-0 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
Mar  1 09:08:54.498: INFO: Waiting for Pod statefulset-324/ss2-1 to have revision ss2-6458d64cb update revision ss2-b889cd5b5
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  1 09:09:14.495: INFO: Deleting all statefulset in ns statefulset-324
Mar  1 09:09:14.502: INFO: Scaling statefulset ss2 to 0
Mar  1 09:09:34.555: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 09:09:34.618: INFO: Waiting for stateful set status.replicas to become 0, currently 1
Mar  1 09:09:44.626: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:09:44.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-324" for this suite.
Mar  1 09:09:52.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:09:52.940: INFO: namespace statefulset-324 deletion completed in 8.236412863s

• [SLOW TEST:174.973 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:09:52.941: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:09:59.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2531" for this suite.
Mar  1 09:10:07.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:10:07.380: INFO: namespace watch-2531 deletion completed in 8.326784571s

• [SLOW TEST:14.439 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:10:07.380: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d11bcca4-2e24-4778-863a-ef2ae2a226c3
STEP: Creating a pod to test consume configMaps
Mar  1 09:10:07.861: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7" in namespace "projected-149" to be "success or failure"
Mar  1 09:10:07.866: INFO: Pod "pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.296457ms
Mar  1 09:10:09.879: INFO: Pod "pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018846377s
Mar  1 09:10:11.891: INFO: Pod "pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030255154s
Mar  1 09:10:13.899: INFO: Pod "pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038890794s
STEP: Saw pod success
Mar  1 09:10:13.900: INFO: Pod "pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7" satisfied condition "success or failure"
Mar  1 09:10:13.906: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:10:13.971: INFO: Waiting for pod pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7 to disappear
Mar  1 09:10:13.976: INFO: Pod pod-projected-configmaps-f3a86cb0-6dc7-4d8e-bcaa-3ae275085bd7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:10:13.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-149" for this suite.
Mar  1 09:10:20.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:10:20.296: INFO: namespace projected-149 deletion completed in 6.310639844s

• [SLOW TEST:12.916 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:10:20.298: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:10:20.865: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c" in namespace "projected-3676" to be "success or failure"
Mar  1 09:10:20.925: INFO: Pod "downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c": Phase="Pending", Reason="", readiness=false. Elapsed: 60.48818ms
Mar  1 09:10:22.933: INFO: Pod "downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068452664s
Mar  1 09:10:24.941: INFO: Pod "downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076732914s
STEP: Saw pod success
Mar  1 09:10:24.942: INFO: Pod "downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c" satisfied condition "success or failure"
Mar  1 09:10:24.947: INFO: Trying to get logs from node worker02 pod downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c container client-container: <nil>
STEP: delete the pod
Mar  1 09:10:25.040: INFO: Waiting for pod downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c to disappear
Mar  1 09:10:25.046: INFO: Pod downwardapi-volume-3d62bf34-395f-4640-b846-4968a190457c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:10:25.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3676" for this suite.
Mar  1 09:10:31.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:10:31.431: INFO: namespace projected-3676 deletion completed in 6.36450874s

• [SLOW TEST:11.134 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:10:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:10:31.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416" in namespace "downward-api-1465" to be "success or failure"
Mar  1 09:10:31.819: INFO: Pod "downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122353ms
Mar  1 09:10:33.827: INFO: Pod "downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01364743s
Mar  1 09:10:35.835: INFO: Pod "downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021414004s
Mar  1 09:10:37.843: INFO: Pod "downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030063861s
STEP: Saw pod success
Mar  1 09:10:37.843: INFO: Pod "downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416" satisfied condition "success or failure"
Mar  1 09:10:37.849: INFO: Trying to get logs from node worker02 pod downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416 container client-container: <nil>
STEP: delete the pod
Mar  1 09:10:37.903: INFO: Waiting for pod downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416 to disappear
Mar  1 09:10:37.909: INFO: Pod downwardapi-volume-fc38622e-de3b-4145-89c7-4cbcee0b0416 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:10:37.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1465" for this suite.
Mar  1 09:10:43.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:10:44.134: INFO: namespace downward-api-1465 deletion completed in 6.216826796s

• [SLOW TEST:12.702 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:10:44.134: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1081
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  1 09:10:44.621: INFO: Waiting up to 5m0s for pod "pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e" in namespace "emptydir-1081" to be "success or failure"
Mar  1 09:10:44.628: INFO: Pod "pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54549ms
Mar  1 09:10:46.637: INFO: Pod "pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015214054s
Mar  1 09:10:48.646: INFO: Pod "pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024905267s
STEP: Saw pod success
Mar  1 09:10:48.646: INFO: Pod "pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e" satisfied condition "success or failure"
Mar  1 09:10:48.654: INFO: Trying to get logs from node worker02 pod pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e container test-container: <nil>
STEP: delete the pod
Mar  1 09:10:48.787: INFO: Waiting for pod pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e to disappear
Mar  1 09:10:48.793: INFO: Pod pod-d62a16c3-9ce7-4e5d-a26c-3819566e370e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:10:48.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1081" for this suite.
Mar  1 09:10:54.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:10:56.034: INFO: namespace emptydir-1081 deletion completed in 7.202760907s

• [SLOW TEST:11.900 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:10:56.034: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9095
STEP: Creating secret with name secret-test-69630602-3186-4991-a8c9-b2e6ae147b6b
STEP: Creating a pod to test consume secrets
Mar  1 09:10:56.655: INFO: Waiting up to 5m0s for pod "pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa" in namespace "secrets-5195" to be "success or failure"
Mar  1 09:10:56.661: INFO: Pod "pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239369ms
Mar  1 09:10:58.669: INFO: Pod "pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01447793s
Mar  1 09:11:00.677: INFO: Pod "pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02277787s
Mar  1 09:11:02.685: INFO: Pod "pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030192827s
STEP: Saw pod success
Mar  1 09:11:02.685: INFO: Pod "pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa" satisfied condition "success or failure"
Mar  1 09:11:02.691: INFO: Trying to get logs from node worker02 pod pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:11:02.776: INFO: Waiting for pod pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa to disappear
Mar  1 09:11:02.782: INFO: Pod pod-secrets-9de47ddc-689a-4629-b1b9-e988603238fa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:11:02.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5195" for this suite.
Mar  1 09:11:08.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:11:09.094: INFO: namespace secrets-5195 deletion completed in 6.303578694s
STEP: Destroying namespace "secret-namespace-9095" for this suite.
Mar  1 09:11:15.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:11:15.419: INFO: namespace secret-namespace-9095 deletion completed in 6.32439726s

• [SLOW TEST:19.385 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:11:15.420: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:11:15.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae" in namespace "projected-2753" to be "success or failure"
Mar  1 09:11:15.713: INFO: Pod "downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28116ms
Mar  1 09:11:17.721: INFO: Pod "downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012989697s
Mar  1 09:11:19.761: INFO: Pod "downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053295379s
Mar  1 09:11:21.770: INFO: Pod "downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061743702s
STEP: Saw pod success
Mar  1 09:11:21.770: INFO: Pod "downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae" satisfied condition "success or failure"
Mar  1 09:11:21.776: INFO: Trying to get logs from node worker02 pod downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae container client-container: <nil>
STEP: delete the pod
Mar  1 09:11:21.830: INFO: Waiting for pod downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae to disappear
Mar  1 09:11:21.838: INFO: Pod downwardapi-volume-344c3ab5-9f57-441a-ac87-4ab8fd75b3ae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:11:21.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2753" for this suite.
Mar  1 09:11:27.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:11:28.115: INFO: namespace projected-2753 deletion completed in 6.260703107s

• [SLOW TEST:12.695 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:11:28.115: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  1 09:11:28.422: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  1 09:11:39.567: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:11:39.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8744" for this suite.
Mar  1 09:11:45.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:11:45.801: INFO: namespace pods-8744 deletion completed in 6.217478363s

• [SLOW TEST:17.686 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:11:45.802: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-2367f038-6dcb-4d04-8c81-7a78fb117d07
STEP: Creating a pod to test consume configMaps
Mar  1 09:11:46.233: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a" in namespace "projected-7794" to be "success or failure"
Mar  1 09:11:46.239: INFO: Pod "pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.506093ms
Mar  1 09:11:48.246: INFO: Pod "pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013396273s
Mar  1 09:11:50.255: INFO: Pod "pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021604557s
Mar  1 09:11:52.265: INFO: Pod "pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03173419s
STEP: Saw pod success
Mar  1 09:11:52.265: INFO: Pod "pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a" satisfied condition "success or failure"
Mar  1 09:11:52.272: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:11:52.380: INFO: Waiting for pod pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a to disappear
Mar  1 09:11:52.386: INFO: Pod pod-projected-configmaps-1bdc3221-d0b9-4aae-b101-0ee47c23d69a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:11:52.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7794" for this suite.
Mar  1 09:11:58.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:11:58.651: INFO: namespace projected-7794 deletion completed in 6.255658087s

• [SLOW TEST:12.849 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:11:58.652: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-354060c2-aca8-4ab7-8bc8-f62c5bfa365f
STEP: Creating a pod to test consume configMaps
Mar  1 09:11:59.030: INFO: Waiting up to 5m0s for pod "pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9" in namespace "configmap-886" to be "success or failure"
Mar  1 09:11:59.067: INFO: Pod "pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.835475ms
Mar  1 09:12:01.075: INFO: Pod "pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044956722s
Mar  1 09:12:03.083: INFO: Pod "pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053608679s
STEP: Saw pod success
Mar  1 09:12:03.083: INFO: Pod "pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9" satisfied condition "success or failure"
Mar  1 09:12:03.089: INFO: Trying to get logs from node worker02 pod pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:12:03.195: INFO: Waiting for pod pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9 to disappear
Mar  1 09:12:03.201: INFO: Pod pod-configmaps-c60b49eb-b92a-40e6-ae65-25a0575f83d9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:12:03.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-886" for this suite.
Mar  1 09:12:09.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:12:09.568: INFO: namespace configmap-886 deletion completed in 6.358261934s

• [SLOW TEST:10.917 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:12:09.569: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-64009bf6-b39c-4f59-be0d-6add7d767980
STEP: Creating a pod to test consume secrets
Mar  1 09:12:09.916: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b" in namespace "projected-5228" to be "success or failure"
Mar  1 09:12:09.923: INFO: Pod "pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59726ms
Mar  1 09:12:11.930: INFO: Pod "pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014350917s
Mar  1 09:12:13.939: INFO: Pod "pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022558727s
Mar  1 09:12:15.946: INFO: Pod "pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030145078s
STEP: Saw pod success
Mar  1 09:12:15.946: INFO: Pod "pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b" satisfied condition "success or failure"
Mar  1 09:12:15.952: INFO: Trying to get logs from node worker02 pod pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:12:16.065: INFO: Waiting for pod pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b to disappear
Mar  1 09:12:16.094: INFO: Pod pod-projected-secrets-22f75e18-8a54-4f0a-80c4-a0b77cf02d7b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:12:16.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5228" for this suite.
Mar  1 09:12:22.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:12:22.371: INFO: namespace projected-5228 deletion completed in 6.262625956s

• [SLOW TEST:12.802 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:12:22.371: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-7aea0f83-29b8-48c8-a7ac-3f6b1fbdfc0c
STEP: Creating a pod to test consume secrets
Mar  1 09:12:22.769: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f" in namespace "projected-5381" to be "success or failure"
Mar  1 09:12:22.775: INFO: Pod "pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54ms
Mar  1 09:12:24.785: INFO: Pod "pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015870606s
Mar  1 09:12:26.794: INFO: Pod "pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024962867s
Mar  1 09:12:28.801: INFO: Pod "pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032209214s
STEP: Saw pod success
Mar  1 09:12:28.801: INFO: Pod "pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f" satisfied condition "success or failure"
Mar  1 09:12:28.807: INFO: Trying to get logs from node worker02 pod pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:12:28.873: INFO: Waiting for pod pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f to disappear
Mar  1 09:12:28.878: INFO: Pod pod-projected-secrets-b91c3f26-3398-4b22-8e64-2af8e0b1b99f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:12:28.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5381" for this suite.
Mar  1 09:12:36.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:12:37.130: INFO: namespace projected-5381 deletion completed in 8.235180226s

• [SLOW TEST:14.759 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:12:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  1 09:12:37.677: INFO: Waiting up to 5m0s for pod "downward-api-237920fb-4350-4035-8021-002e4c712ff3" in namespace "downward-api-1782" to be "success or failure"
Mar  1 09:12:37.685: INFO: Pod "downward-api-237920fb-4350-4035-8021-002e4c712ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.832313ms
Mar  1 09:12:39.693: INFO: Pod "downward-api-237920fb-4350-4035-8021-002e4c712ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016336784s
Mar  1 09:12:41.701: INFO: Pod "downward-api-237920fb-4350-4035-8021-002e4c712ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024653367s
Mar  1 09:12:43.710: INFO: Pod "downward-api-237920fb-4350-4035-8021-002e4c712ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033355374s
STEP: Saw pod success
Mar  1 09:12:43.710: INFO: Pod "downward-api-237920fb-4350-4035-8021-002e4c712ff3" satisfied condition "success or failure"
Mar  1 09:12:43.716: INFO: Trying to get logs from node worker02 pod downward-api-237920fb-4350-4035-8021-002e4c712ff3 container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:12:43.793: INFO: Waiting for pod downward-api-237920fb-4350-4035-8021-002e4c712ff3 to disappear
Mar  1 09:12:43.799: INFO: Pod downward-api-237920fb-4350-4035-8021-002e4c712ff3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:12:43.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1782" for this suite.
Mar  1 09:12:49.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:12:50.196: INFO: namespace downward-api-1782 deletion completed in 6.386682483s

• [SLOW TEST:13.065 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:12:50.196: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Mar  1 09:12:50.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 cluster-info'
Mar  1 09:12:50.972: INFO: stderr: ""
Mar  1 09:12:50.972: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:12:50.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5084" for this suite.
Mar  1 09:12:57.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:12:57.206: INFO: namespace kubectl-5084 deletion completed in 6.224039546s

• [SLOW TEST:7.010 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:12:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-4fa436bb-8bff-4bdc-8580-bb86d74b20ee
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:12:57.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6766" for this suite.
Mar  1 09:13:03.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:13:03.702: INFO: namespace secrets-6766 deletion completed in 6.216338866s

• [SLOW TEST:6.495 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:13:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3273
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-8306fa7c-0764-4acd-b9a0-4c8b41d4aef4
STEP: Creating secret with name s-test-opt-upd-5e88f4ab-8336-4924-affb-491c9ce52a32
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8306fa7c-0764-4acd-b9a0-4c8b41d4aef4
STEP: Updating secret s-test-opt-upd-5e88f4ab-8336-4924-affb-491c9ce52a32
STEP: Creating secret with name s-test-opt-create-a23356a2-df2b-46dd-a6bf-007d1ee5ff2e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:13:12.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3273" for this suite.
Mar  1 09:13:36.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:13:36.781: INFO: namespace secrets-3273 deletion completed in 24.224559456s

• [SLOW TEST:33.079 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:13:36.781: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Mar  1 09:13:37.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-5231'
Mar  1 09:13:37.965: INFO: stderr: ""
Mar  1 09:13:37.965: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Mar  1 09:13:38.973: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:13:38.973: INFO: Found 0 / 1
Mar  1 09:13:39.973: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:13:39.973: INFO: Found 0 / 1
Mar  1 09:13:40.976: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:13:40.976: INFO: Found 0 / 1
Mar  1 09:13:41.974: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:13:41.974: INFO: Found 1 / 1
Mar  1 09:13:41.974: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 09:13:41.980: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:13:41.980: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Mar  1 09:13:41.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-wqhw8 redis-master --namespace=kubectl-5231'
Mar  1 09:13:42.195: INFO: stderr: ""
Mar  1 09:13:42.195: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 01 Mar 09:13:41.035 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Mar 09:13:41.035 # Server started, Redis version 3.2.12\n1:M 01 Mar 09:13:41.036 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Mar 09:13:41.036 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Mar  1 09:13:42.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-wqhw8 redis-master --namespace=kubectl-5231 --tail=1'
Mar  1 09:13:42.497: INFO: stderr: ""
Mar  1 09:13:42.498: INFO: stdout: "1:M 01 Mar 09:13:41.036 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Mar  1 09:13:42.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-wqhw8 redis-master --namespace=kubectl-5231 --limit-bytes=1'
Mar  1 09:13:42.731: INFO: stderr: ""
Mar  1 09:13:42.731: INFO: stdout: " "
STEP: exposing timestamps
Mar  1 09:13:42.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-wqhw8 redis-master --namespace=kubectl-5231 --tail=1 --timestamps'
Mar  1 09:13:42.993: INFO: stderr: ""
Mar  1 09:13:42.993: INFO: stdout: "2020-03-01T09:13:41.036333469Z 1:M 01 Mar 09:13:41.036 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Mar  1 09:13:45.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-wqhw8 redis-master --namespace=kubectl-5231 --since=1s'
Mar  1 09:13:45.712: INFO: stderr: ""
Mar  1 09:13:45.712: INFO: stdout: ""
Mar  1 09:13:45.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-wqhw8 redis-master --namespace=kubectl-5231 --since=24h'
Mar  1 09:13:45.928: INFO: stderr: ""
Mar  1 09:13:45.928: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 01 Mar 09:13:41.035 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Mar 09:13:41.035 # Server started, Redis version 3.2.12\n1:M 01 Mar 09:13:41.036 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Mar 09:13:41.036 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Mar  1 09:13:45.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-5231'
Mar  1 09:13:46.169: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 09:13:46.169: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar  1 09:13:46.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get rc,svc -l name=nginx --no-headers --namespace=kubectl-5231'
Mar  1 09:13:46.395: INFO: stderr: "No resources found.\n"
Mar  1 09:13:46.395: INFO: stdout: ""
Mar  1 09:13:46.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -l name=nginx --namespace=kubectl-5231 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 09:13:46.582: INFO: stderr: ""
Mar  1 09:13:46.582: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:13:46.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5231" for this suite.
Mar  1 09:14:10.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:14:10.931: INFO: namespace kubectl-5231 deletion completed in 24.314296984s

• [SLOW TEST:34.150 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:14:10.931: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c7aedc7c-3f42-44b3-94f9-5a095f029171
STEP: Creating a pod to test consume secrets
Mar  1 09:14:11.278: INFO: Waiting up to 5m0s for pod "pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e" in namespace "secrets-2881" to be "success or failure"
Mar  1 09:14:11.285: INFO: Pod "pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335249ms
Mar  1 09:14:13.292: INFO: Pod "pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013896523s
Mar  1 09:14:15.301: INFO: Pod "pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022361277s
Mar  1 09:14:17.309: INFO: Pod "pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030550323s
STEP: Saw pod success
Mar  1 09:14:17.309: INFO: Pod "pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e" satisfied condition "success or failure"
Mar  1 09:14:17.315: INFO: Trying to get logs from node worker02 pod pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:14:17.448: INFO: Waiting for pod pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e to disappear
Mar  1 09:14:17.454: INFO: Pod pod-secrets-cdea4e4e-789d-4ce1-98f5-c172db7dc08e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:14:17.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2881" for this suite.
Mar  1 09:14:23.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:14:23.776: INFO: namespace secrets-2881 deletion completed in 6.312490537s

• [SLOW TEST:12.845 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:14:23.778: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  1 09:15:04.187: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0301 09:15:04.187375      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:15:04.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7973" for this suite.
Mar  1 09:15:14.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:15:14.455: INFO: namespace gc-7973 deletion completed in 10.259325652s

• [SLOW TEST:50.677 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:15:14.456: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-4584/secret-test-c37c3685-5239-482e-9d94-748803612e55
STEP: Creating a pod to test consume secrets
Mar  1 09:15:14.790: INFO: Waiting up to 5m0s for pod "pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb" in namespace "secrets-4584" to be "success or failure"
Mar  1 09:15:14.848: INFO: Pod "pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb": Phase="Pending", Reason="", readiness=false. Elapsed: 57.708727ms
Mar  1 09:15:16.856: INFO: Pod "pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065818532s
Mar  1 09:15:18.864: INFO: Pod "pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074057674s
Mar  1 09:15:20.871: INFO: Pod "pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081113558s
STEP: Saw pod success
Mar  1 09:15:20.871: INFO: Pod "pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb" satisfied condition "success or failure"
Mar  1 09:15:20.877: INFO: Trying to get logs from node worker02 pod pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb container env-test: <nil>
STEP: delete the pod
Mar  1 09:15:21.016: INFO: Waiting for pod pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb to disappear
Mar  1 09:15:21.022: INFO: Pod pod-configmaps-055e67d1-3f5a-444b-aeca-b89a874b69cb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:15:21.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4584" for this suite.
Mar  1 09:15:29.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:15:29.439: INFO: namespace secrets-4584 deletion completed in 8.408540893s

• [SLOW TEST:14.983 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:15:29.439: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar  1 09:15:29.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8587'
Mar  1 09:15:30.962: INFO: stderr: ""
Mar  1 09:15:30.962: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 09:15:30.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:31.167: INFO: stderr: ""
Mar  1 09:15:31.167: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vns69 "
Mar  1 09:15:31.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:32.429: INFO: stderr: ""
Mar  1 09:15:32.429: INFO: stdout: ""
Mar  1 09:15:32.429: INFO: update-demo-nautilus-6bnvv is created but not running
Mar  1 09:15:37.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:37.855: INFO: stderr: ""
Mar  1 09:15:37.855: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vns69 "
Mar  1 09:15:37.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:38.041: INFO: stderr: ""
Mar  1 09:15:38.041: INFO: stdout: "true"
Mar  1 09:15:38.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:38.217: INFO: stderr: ""
Mar  1 09:15:38.217: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:15:38.217: INFO: validating pod update-demo-nautilus-6bnvv
Mar  1 09:15:38.231: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:15:38.231: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:15:38.231: INFO: update-demo-nautilus-6bnvv is verified up and running
Mar  1 09:15:38.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vns69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:38.406: INFO: stderr: ""
Mar  1 09:15:38.406: INFO: stdout: "true"
Mar  1 09:15:38.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vns69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:38.584: INFO: stderr: ""
Mar  1 09:15:38.584: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:15:38.584: INFO: validating pod update-demo-nautilus-vns69
Mar  1 09:15:38.624: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:15:38.624: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:15:38.624: INFO: update-demo-nautilus-vns69 is verified up and running
STEP: scaling down the replication controller
Mar  1 09:15:38.628: INFO: scanned /root for discovery docs: <nil>
Mar  1 09:15:38.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8587'
Mar  1 09:15:40.303: INFO: stderr: ""
Mar  1 09:15:40.303: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 09:15:40.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:40.494: INFO: stderr: ""
Mar  1 09:15:40.494: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vns69 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  1 09:15:45.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:45.687: INFO: stderr: ""
Mar  1 09:15:45.687: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vns69 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  1 09:15:50.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:51.146: INFO: stderr: ""
Mar  1 09:15:51.147: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vns69 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  1 09:15:56.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:56.545: INFO: stderr: ""
Mar  1 09:15:56.545: INFO: stdout: "update-demo-nautilus-6bnvv "
Mar  1 09:15:56.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:56.733: INFO: stderr: ""
Mar  1 09:15:56.733: INFO: stdout: "true"
Mar  1 09:15:56.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:56.982: INFO: stderr: ""
Mar  1 09:15:56.982: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:15:56.982: INFO: validating pod update-demo-nautilus-6bnvv
Mar  1 09:15:56.991: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:15:56.991: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:15:56.991: INFO: update-demo-nautilus-6bnvv is verified up and running
STEP: scaling up the replication controller
Mar  1 09:15:56.995: INFO: scanned /root for discovery docs: <nil>
Mar  1 09:15:56.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8587'
Mar  1 09:15:58.268: INFO: stderr: ""
Mar  1 09:15:58.268: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 09:15:58.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:15:58.460: INFO: stderr: ""
Mar  1 09:15:58.460: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vsb7r "
Mar  1 09:15:58.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:58.644: INFO: stderr: ""
Mar  1 09:15:58.644: INFO: stdout: "true"
Mar  1 09:15:58.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:58.826: INFO: stderr: ""
Mar  1 09:15:58.826: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:15:58.826: INFO: validating pod update-demo-nautilus-6bnvv
Mar  1 09:15:58.833: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:15:58.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:15:58.833: INFO: update-demo-nautilus-6bnvv is verified up and running
Mar  1 09:15:58.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vsb7r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:15:59.018: INFO: stderr: ""
Mar  1 09:15:59.018: INFO: stdout: ""
Mar  1 09:15:59.018: INFO: update-demo-nautilus-vsb7r is created but not running
Mar  1 09:16:04.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8587'
Mar  1 09:16:04.219: INFO: stderr: ""
Mar  1 09:16:04.219: INFO: stdout: "update-demo-nautilus-6bnvv update-demo-nautilus-vsb7r "
Mar  1 09:16:04.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:16:04.405: INFO: stderr: ""
Mar  1 09:16:04.405: INFO: stdout: "true"
Mar  1 09:16:04.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-6bnvv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:16:04.616: INFO: stderr: ""
Mar  1 09:16:04.616: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:16:04.616: INFO: validating pod update-demo-nautilus-6bnvv
Mar  1 09:16:04.623: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:16:04.624: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:16:04.624: INFO: update-demo-nautilus-6bnvv is verified up and running
Mar  1 09:16:04.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vsb7r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:16:05.033: INFO: stderr: ""
Mar  1 09:16:05.033: INFO: stdout: "true"
Mar  1 09:16:05.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vsb7r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8587'
Mar  1 09:16:05.224: INFO: stderr: ""
Mar  1 09:16:05.224: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:16:05.224: INFO: validating pod update-demo-nautilus-vsb7r
Mar  1 09:16:05.234: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:16:05.234: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:16:05.234: INFO: update-demo-nautilus-vsb7r is verified up and running
STEP: using delete to clean up resources
Mar  1 09:16:05.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8587'
Mar  1 09:16:05.436: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 09:16:05.437: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 09:16:05.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8587'
Mar  1 09:16:05.766: INFO: stderr: "No resources found.\n"
Mar  1 09:16:05.767: INFO: stdout: ""
Mar  1 09:16:05.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -l name=update-demo --namespace=kubectl-8587 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 09:16:05.960: INFO: stderr: ""
Mar  1 09:16:05.961: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:16:05.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8587" for this suite.
Mar  1 09:16:30.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:16:30.363: INFO: namespace kubectl-8587 deletion completed in 24.39305586s

• [SLOW TEST:60.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:16:30.363: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:16:30.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-6306'
Mar  1 09:16:32.334: INFO: stderr: ""
Mar  1 09:16:32.335: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar  1 09:16:32.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-6306'
Mar  1 09:16:34.127: INFO: stderr: ""
Mar  1 09:16:34.127: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  1 09:16:35.351: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:16:35.351: INFO: Found 0 / 1
Mar  1 09:16:36.136: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:16:36.136: INFO: Found 0 / 1
Mar  1 09:16:37.136: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:16:37.137: INFO: Found 0 / 1
Mar  1 09:16:38.136: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:16:38.136: INFO: Found 1 / 1
Mar  1 09:16:38.136: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 09:16:38.169: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 09:16:38.170: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 09:16:38.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 describe pod redis-master-4627v --namespace=kubectl-6306'
Mar  1 09:16:38.396: INFO: stderr: ""
Mar  1 09:16:38.396: INFO: stdout: "Name:           redis-master-4627v\nNamespace:      kubectl-6306\nPriority:       0\nNode:           worker02/172.20.8.6\nStart Time:     Sun, 01 Mar 2020 09:16:32 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.4.226\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://8c6c39518c37558b2c3c1d9e89c300cd58e52db4158bcfbfaaa9efe4501814c7\n    Image:          172.20.8.7/library/redis:1.0\n    Image ID:       docker-pullable://172.20.8.7/library/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 01 Mar 2020 09:16:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rp7cn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rp7cn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rp7cn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  6s    default-scheduler  Successfully assigned kubectl-6306/redis-master-4627v to worker02\n  Normal  Pulled     3s    kubelet, worker02  Container image \"172.20.8.7/library/redis:1.0\" already present on machine\n  Normal  Created    3s    kubelet, worker02  Created container redis-master\n  Normal  Started    2s    kubelet, worker02  Started container redis-master\n"
Mar  1 09:16:38.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 describe rc redis-master --namespace=kubectl-6306'
Mar  1 09:16:38.628: INFO: stderr: ""
Mar  1 09:16:38.628: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6306\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        172.20.8.7/library/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  6s    replication-controller  Created pod: redis-master-4627v\n"
Mar  1 09:16:38.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 describe service redis-master --namespace=kubectl-6306'
Mar  1 09:16:38.842: INFO: stderr: ""
Mar  1 09:16:38.842: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6306\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.108.228.211\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.4.226:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  1 09:16:38.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 describe node centos02'
Mar  1 09:16:39.101: INFO: stderr: ""
Mar  1 09:16:39.101: INFO: stdout: "Name:               centos02\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=centos02\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"b6:7b:72:09:7c:87\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.20.8.2\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Feb 2020 08:52:55 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sun, 01 Mar 2020 09:16:33 +0000   Sat, 29 Feb 2020 08:52:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sun, 01 Mar 2020 09:16:33 +0000   Sat, 29 Feb 2020 08:52:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sun, 01 Mar 2020 09:16:33 +0000   Sat, 29 Feb 2020 08:52:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sun, 01 Mar 2020 09:16:33 +0000   Sat, 29 Feb 2020 08:54:28 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.20.8.2\n  Hostname:    centos02\nCapacity:\n cpu:                8\n ephemeral-storage:  51175Mi\n hugepages-2Mi:      0\n memory:             3878044Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  48294789041\n hugepages-2Mi:      0\n memory:             3775644Ki\n pods:               110\nSystem Info:\n Machine ID:                 c1e2a7e37ea041feb133fd939fc56629\n System UUID:                564D17FA-35A3-5FA5-545D-CA0C20E0EA19\n Boot ID:                    48f91885-5686-44a7-9da4-0df39d306f91\n Kernel Version:             3.10.0-693.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.9\n Kubelet Version:            v1.15.10\n Kube-Proxy Version:         v1.15.10\nPodCIDR:                     10.244.1.0/24\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                coredns-fb476c9dc-jm9qv                                    100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     24h\n  kube-system                coredns-fb476c9dc-mjxq8                                    100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     24h\n  kube-system                kube-apiserver-centos02                                    250m (3%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                kube-controller-manager-centos02                           200m (2%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                kube-flannel-ds-amd64-fs4m6                                100m (1%)     100m (1%)   50Mi (1%)        50Mi (1%)      24h\n  kube-system                kube-proxy-9rs92                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                kube-scheduler-centos02                                    100m (1%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kubernetes-dashboard       dashboard-metrics-scraper-7869bdd8c5-ct7n4                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kubernetes-dashboard       kubernetes-dashboard-58b84dcf86-s7n7s                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-m9db4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (10%)  100m (1%)\n  memory             190Mi (5%)  390Mi (10%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar  1 09:16:39.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 describe namespace kubectl-6306'
Mar  1 09:16:39.320: INFO: stderr: ""
Mar  1 09:16:39.320: INFO: stdout: "Name:         kubectl-6306\nLabels:       e2e-framework=kubectl\n              e2e-run=5cec14c5-3760-4c4a-a03f-44baba2fb195\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:16:39.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6306" for this suite.
Mar  1 09:17:03.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:17:03.623: INFO: namespace kubectl-6306 deletion completed in 24.293306596s

• [SLOW TEST:33.260 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:17:03.623: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-d2197483-cb61-4933-8edd-a83173e2ea21
STEP: Creating a pod to test consume configMaps
Mar  1 09:17:04.475: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b" in namespace "projected-8118" to be "success or failure"
Mar  1 09:17:04.482: INFO: Pod "pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05455ms
Mar  1 09:17:06.490: INFO: Pod "pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014702733s
Mar  1 09:17:08.498: INFO: Pod "pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022629234s
Mar  1 09:17:10.505: INFO: Pod "pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030417831s
STEP: Saw pod success
Mar  1 09:17:10.505: INFO: Pod "pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b" satisfied condition "success or failure"
Mar  1 09:17:10.512: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:17:10.575: INFO: Waiting for pod pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b to disappear
Mar  1 09:17:10.581: INFO: Pod pod-projected-configmaps-8e1a702c-bfd2-4bd0-abc9-e01107cf187b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:17:10.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8118" for this suite.
Mar  1 09:17:18.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:17:18.815: INFO: namespace projected-8118 deletion completed in 8.224250073s

• [SLOW TEST:15.192 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:17:18.816: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Mar  1 09:17:19.174: INFO: Waiting up to 5m0s for pod "var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8" in namespace "var-expansion-7" to be "success or failure"
Mar  1 09:17:19.210: INFO: Pod "var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.231978ms
Mar  1 09:17:21.218: INFO: Pod "var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043891235s
Mar  1 09:17:23.225: INFO: Pod "var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051150445s
Mar  1 09:17:25.233: INFO: Pod "var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059134229s
STEP: Saw pod success
Mar  1 09:17:25.233: INFO: Pod "var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8" satisfied condition "success or failure"
Mar  1 09:17:25.240: INFO: Trying to get logs from node worker02 pod var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8 container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:17:25.345: INFO: Waiting for pod var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8 to disappear
Mar  1 09:17:25.352: INFO: Pod var-expansion-2b2dedda-bda7-4bb1-9c42-73b0c73c7fa8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:17:25.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7" for this suite.
Mar  1 09:17:33.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:17:34.251: INFO: namespace var-expansion-7 deletion completed in 8.890819687s

• [SLOW TEST:15.436 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:17:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  1 09:17:34.777: INFO: Waiting up to 5m0s for pod "pod-f1f72f9d-389f-4432-a199-5b5e62bff640" in namespace "emptydir-7733" to be "success or failure"
Mar  1 09:17:34.786: INFO: Pod "pod-f1f72f9d-389f-4432-a199-5b5e62bff640": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051467ms
Mar  1 09:17:36.795: INFO: Pod "pod-f1f72f9d-389f-4432-a199-5b5e62bff640": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01705436s
Mar  1 09:17:38.803: INFO: Pod "pod-f1f72f9d-389f-4432-a199-5b5e62bff640": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025023064s
Mar  1 09:17:40.811: INFO: Pod "pod-f1f72f9d-389f-4432-a199-5b5e62bff640": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032973954s
STEP: Saw pod success
Mar  1 09:17:40.811: INFO: Pod "pod-f1f72f9d-389f-4432-a199-5b5e62bff640" satisfied condition "success or failure"
Mar  1 09:17:40.817: INFO: Trying to get logs from node worker02 pod pod-f1f72f9d-389f-4432-a199-5b5e62bff640 container test-container: <nil>
STEP: delete the pod
Mar  1 09:17:40.874: INFO: Waiting for pod pod-f1f72f9d-389f-4432-a199-5b5e62bff640 to disappear
Mar  1 09:17:40.887: INFO: Pod pod-f1f72f9d-389f-4432-a199-5b5e62bff640 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:17:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7733" for this suite.
Mar  1 09:17:46.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:17:47.300: INFO: namespace emptydir-7733 deletion completed in 6.403448035s

• [SLOW TEST:13.048 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:17:47.300: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-254
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-254
STEP: Deleting pre-stop pod
Mar  1 09:18:02.709: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:18:02.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-254" for this suite.
Mar  1 09:18:42.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:18:43.034: INFO: namespace prestop-254 deletion completed in 40.299106055s

• [SLOW TEST:55.734 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:18:43.035: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-632.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-632.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 09:18:49.455: INFO: DNS probes using dns-632/dns-test-86c804df-c223-4d28-be12-9532835b8baa succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:18:49.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-632" for this suite.
Mar  1 09:18:57.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:18:57.743: INFO: namespace dns-632 deletion completed in 8.21217362s

• [SLOW TEST:14.709 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:18:57.744: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5272
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-8f8b60ae-ad7d-459a-a25f-f4d72073f3a2
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-8f8b60ae-ad7d-459a-a25f-f4d72073f3a2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:19:04.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5272" for this suite.
Mar  1 09:19:28.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:19:28.665: INFO: namespace configmap-5272 deletion completed in 24.255213704s

• [SLOW TEST:30.921 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:19:28.666: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f62ac82c-d0f4-4185-b07f-3ce2e872f0d4
STEP: Creating a pod to test consume secrets
Mar  1 09:19:29.111: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810" in namespace "projected-9733" to be "success or failure"
Mar  1 09:19:29.117: INFO: Pod "pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810": Phase="Pending", Reason="", readiness=false. Elapsed: 6.168233ms
Mar  1 09:19:31.123: INFO: Pod "pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012420923s
Mar  1 09:19:33.132: INFO: Pod "pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020925893s
Mar  1 09:19:35.140: INFO: Pod "pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028919534s
STEP: Saw pod success
Mar  1 09:19:35.140: INFO: Pod "pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810" satisfied condition "success or failure"
Mar  1 09:19:35.145: INFO: Trying to get logs from node worker02 pod pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:19:35.258: INFO: Waiting for pod pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810 to disappear
Mar  1 09:19:35.264: INFO: Pod pod-projected-secrets-08127b1f-098d-49ad-a46b-96cb19fe7810 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:19:35.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9733" for this suite.
Mar  1 09:19:43.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:19:43.744: INFO: namespace projected-9733 deletion completed in 8.470201079s

• [SLOW TEST:15.078 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:19:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:20:04.109: INFO: Container started at 2020-03-01 09:19:47 +0000 UTC, pod became ready at 2020-03-01 09:20:02 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:20:04.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6872" for this suite.
Mar  1 09:20:28.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:20:28.477: INFO: namespace container-probe-6872 deletion completed in 24.358328348s

• [SLOW TEST:44.731 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:20:28.477: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1740
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-84ffbb86-cd30-4b92-9f39-bdb72b72fc64
STEP: Creating secret with name s-test-opt-upd-524d1f71-505f-4b11-9a1e-30357b7a9719
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-84ffbb86-cd30-4b92-9f39-bdb72b72fc64
STEP: Updating secret s-test-opt-upd-524d1f71-505f-4b11-9a1e-30357b7a9719
STEP: Creating secret with name s-test-opt-create-ae8912c0-7a70-4b99-9b56-47059adb39f3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:22:08.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1740" for this suite.
Mar  1 09:22:32.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:22:33.129: INFO: namespace projected-1740 deletion completed in 24.570655126s

• [SLOW TEST:124.653 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:22:33.130: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 09:22:33.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-deployment --image=172.20.8.7/library/nginx:1.14-alpine --namespace=kubectl-8762'
Mar  1 09:22:39.758: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 09:22:39.758: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Mar  1 09:22:41.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8762'
Mar  1 09:22:41.997: INFO: stderr: ""
Mar  1 09:22:41.997: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:22:41.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8762" for this suite.
Mar  1 09:22:48.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:22:48.513: INFO: namespace kubectl-8762 deletion completed in 6.505815322s

• [SLOW TEST:15.383 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:22:48.514: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  1 09:22:48.806: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 09:22:48.824: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 09:22:48.829: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Mar  1 09:22:48.846: INFO: metrics-server-5848cb774b-bw57f from kube-system started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 09:22:48.846: INFO: kube-flannel-ds-amd64-dsrkh from kube-system started at 2020-02-29 08:55:44 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  1 09:22:48.846: INFO: sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-dlz68 from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 09:22:48.846: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 09:22:48.846: INFO: contour-certgen-6w5mk from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container contour ready: false, restart count 0
Mar  1 09:22:48.846: INFO: kuard-55c79684d4-pfs2p from default started at 2020-02-29 08:57:18 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container kuard ready: true, restart count 0
Mar  1 09:22:48.846: INFO: contour-6f89bf4d5f-szsh8 from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container contour ready: false, restart count 0
Mar  1 09:22:48.846: INFO: kuard-55c79684d4-bf6hv from default started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container kuard ready: true, restart count 0
Mar  1 09:22:48.846: INFO: contour-6f89bf4d5f-cbp6b from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container contour ready: false, restart count 0
Mar  1 09:22:48.846: INFO: kuard-55c79684d4-mtkkk from default started at 2020-02-29 08:57:18 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container kuard ready: true, restart count 0
Mar  1 09:22:48.846: INFO: kube-proxy-s67df from kube-system started at 2020-02-29 08:55:43 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 09:22:48.846: INFO: envoy-fgn4v from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.846: INFO: 	Container envoy ready: false, restart count 0
Mar  1 09:22:48.846: INFO: 
Logging pods the kubelet thinks is on node worker02 before test
Mar  1 09:22:48.862: INFO: envoy-zlkbx from projectcontour started at 2020-02-29 08:57:26 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.862: INFO: 	Container envoy ready: false, restart count 0
Mar  1 09:22:48.862: INFO: kube-flannel-ds-amd64-5brn5 from kube-system started at 2020-02-29 08:55:44 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.862: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  1 09:22:48.862: INFO: sonobuoy from sonobuoy started at 2020-03-01 09:01:02 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.862: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 09:22:48.862: INFO: sonobuoy-e2e-job-9a6dafb81c774c83 from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 09:22:48.862: INFO: 	Container e2e ready: true, restart count 0
Mar  1 09:22:48.862: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 09:22:48.862: INFO: sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-zbtnp from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 09:22:48.862: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 09:22:48.862: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 09:22:48.862: INFO: kube-proxy-wxn7v from kube-system started at 2020-02-29 08:55:43 +0000 UTC (1 container statuses recorded)
Mar  1 09:22:48.862: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c34ca097-40e8-4e84-af76-997f993fc8fd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c34ca097-40e8-4e84-af76-997f993fc8fd off the node worker02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c34ca097-40e8-4e84-af76-997f993fc8fd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:22:59.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7919" for this suite.
Mar  1 09:23:17.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:23:17.755: INFO: namespace sched-pred-7919 deletion completed in 18.237179749s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:29.241 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:23:17.755: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:23:18.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62" in namespace "projected-2457" to be "success or failure"
Mar  1 09:23:18.140: INFO: Pod "downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62": Phase="Pending", Reason="", readiness=false. Elapsed: 5.723956ms
Mar  1 09:23:20.148: INFO: Pod "downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013905726s
Mar  1 09:23:22.156: INFO: Pod "downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02173237s
Mar  1 09:23:24.165: INFO: Pod "downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031029877s
STEP: Saw pod success
Mar  1 09:23:24.165: INFO: Pod "downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62" satisfied condition "success or failure"
Mar  1 09:23:24.172: INFO: Trying to get logs from node worker02 pod downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62 container client-container: <nil>
STEP: delete the pod
Mar  1 09:23:24.277: INFO: Waiting for pod downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62 to disappear
Mar  1 09:23:24.283: INFO: Pod downwardapi-volume-cd036077-d3cf-459f-a09c-3bb6d16d3e62 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:23:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2457" for this suite.
Mar  1 09:23:32.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:23:32.572: INFO: namespace projected-2457 deletion completed in 8.235772293s

• [SLOW TEST:14.817 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:23:32.573: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:23:33.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd" in namespace "downward-api-1768" to be "success or failure"
Mar  1 09:23:33.247: INFO: Pod "downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.141993ms
Mar  1 09:23:35.255: INFO: Pod "downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014447474s
Mar  1 09:23:37.265: INFO: Pod "downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024132511s
Mar  1 09:23:39.274: INFO: Pod "downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033147434s
STEP: Saw pod success
Mar  1 09:23:39.274: INFO: Pod "downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd" satisfied condition "success or failure"
Mar  1 09:23:39.280: INFO: Trying to get logs from node worker02 pod downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd container client-container: <nil>
STEP: delete the pod
Mar  1 09:23:39.337: INFO: Waiting for pod downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd to disappear
Mar  1 09:23:39.343: INFO: Pod downwardapi-volume-5eb70e55-00d8-4919-af2d-8d5f9591d8dd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:23:39.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1768" for this suite.
Mar  1 09:23:45.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:23:45.658: INFO: namespace downward-api-1768 deletion completed in 6.283931965s

• [SLOW TEST:13.086 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:23:45.659: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-981
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Mar  1 09:23:46.159: INFO: Found 0 stateful pods, waiting for 3
Mar  1 09:23:56.169: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:23:56.169: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:23:56.169: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  1 09:24:06.169: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:24:06.169: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:24:06.169: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from 172.20.8.7/library/nginx:1.14-alpine to 172.20.8.7/library/nginx:1.15-alpine
Mar  1 09:24:06.222: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  1 09:24:16.393: INFO: Updating stateful set ss2
Mar  1 09:24:16.440: INFO: Waiting for Pod statefulset-981/ss2-2 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
STEP: Restoring Pods to the correct revision when they are deleted
Mar  1 09:24:26.792: INFO: Found 2 stateful pods, waiting for 3
Mar  1 09:24:36.871: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:24:36.871: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:24:36.871: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  1 09:24:36.968: INFO: Updating stateful set ss2
Mar  1 09:24:36.994: INFO: Waiting for Pod statefulset-981/ss2-1 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  1 09:24:47.048: INFO: Updating stateful set ss2
Mar  1 09:24:47.075: INFO: Waiting for StatefulSet statefulset-981/ss2 to complete update
Mar  1 09:24:47.075: INFO: Waiting for Pod statefulset-981/ss2-0 to have revision ss2-b889cd5b5 update revision ss2-6458d64cb
Mar  1 09:24:57.093: INFO: Waiting for StatefulSet statefulset-981/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  1 09:25:07.090: INFO: Deleting all statefulset in ns statefulset-981
Mar  1 09:25:07.096: INFO: Scaling statefulset ss2 to 0
Mar  1 09:25:27.181: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 09:25:27.187: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:25:27.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-981" for this suite.
Mar  1 09:25:35.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:25:35.583: INFO: namespace statefulset-981 deletion completed in 8.328688707s

• [SLOW TEST:109.924 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:25:35.583: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  1 09:25:35.930: INFO: Waiting up to 5m0s for pod "downward-api-86383562-7b76-49ee-bdd7-b195af10e26d" in namespace "downward-api-7741" to be "success or failure"
Mar  1 09:25:35.976: INFO: Pod "downward-api-86383562-7b76-49ee-bdd7-b195af10e26d": Phase="Pending", Reason="", readiness=false. Elapsed: 45.308777ms
Mar  1 09:25:37.984: INFO: Pod "downward-api-86383562-7b76-49ee-bdd7-b195af10e26d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053360391s
Mar  1 09:25:39.992: INFO: Pod "downward-api-86383562-7b76-49ee-bdd7-b195af10e26d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061162825s
Mar  1 09:25:42.000: INFO: Pod "downward-api-86383562-7b76-49ee-bdd7-b195af10e26d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069487462s
STEP: Saw pod success
Mar  1 09:25:42.000: INFO: Pod "downward-api-86383562-7b76-49ee-bdd7-b195af10e26d" satisfied condition "success or failure"
Mar  1 09:25:42.006: INFO: Trying to get logs from node worker02 pod downward-api-86383562-7b76-49ee-bdd7-b195af10e26d container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:25:42.078: INFO: Waiting for pod downward-api-86383562-7b76-49ee-bdd7-b195af10e26d to disappear
Mar  1 09:25:42.083: INFO: Pod downward-api-86383562-7b76-49ee-bdd7-b195af10e26d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:25:42.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7741" for this suite.
Mar  1 09:25:48.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:25:48.431: INFO: namespace downward-api-7741 deletion completed in 6.338564155s

• [SLOW TEST:12.848 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:25:48.431: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1539
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1539
STEP: Creating statefulset with conflicting port in namespace statefulset-1539
STEP: Waiting until pod test-pod will start running in namespace statefulset-1539
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1539
Mar  1 09:25:52.854: INFO: Observed stateful pod in namespace: statefulset-1539, name: ss-0, uid: 975ae43a-ad2f-4ffc-bf34-5c058f8a9413, status phase: Pending. Waiting for statefulset controller to delete.
Mar  1 09:25:53.066: INFO: Observed stateful pod in namespace: statefulset-1539, name: ss-0, uid: 975ae43a-ad2f-4ffc-bf34-5c058f8a9413, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 09:25:53.118: INFO: Observed stateful pod in namespace: statefulset-1539, name: ss-0, uid: 975ae43a-ad2f-4ffc-bf34-5c058f8a9413, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 09:25:53.137: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1539
STEP: Removing pod with conflicting port in namespace statefulset-1539
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1539 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  1 09:25:57.349: INFO: Deleting all statefulset in ns statefulset-1539
Mar  1 09:25:57.354: INFO: Scaling statefulset ss to 0
Mar  1 09:26:07.438: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 09:26:07.443: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:26:07.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1539" for this suite.
Mar  1 09:26:15.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:26:15.947: INFO: namespace statefulset-1539 deletion completed in 8.414385112s

• [SLOW TEST:27.515 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:26:15.947: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  1 09:26:16.341: INFO: Waiting up to 5m0s for pod "downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4" in namespace "downward-api-5044" to be "success or failure"
Mar  1 09:26:16.348: INFO: Pod "downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.057703ms
Mar  1 09:26:18.357: INFO: Pod "downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016125677s
Mar  1 09:26:20.365: INFO: Pod "downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023718517s
Mar  1 09:26:22.373: INFO: Pod "downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032366327s
STEP: Saw pod success
Mar  1 09:26:22.373: INFO: Pod "downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4" satisfied condition "success or failure"
Mar  1 09:26:22.379: INFO: Trying to get logs from node worker02 pod downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4 container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:26:22.440: INFO: Waiting for pod downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4 to disappear
Mar  1 09:26:22.479: INFO: Pod downward-api-9c9e38ca-0264-47af-be14-25a9f3fe09a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:26:22.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5044" for this suite.
Mar  1 09:26:28.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:26:28.824: INFO: namespace downward-api-5044 deletion completed in 6.335400129s

• [SLOW TEST:12.877 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:26:28.824: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:26:29.266: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  1 09:26:29.304: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:29.304: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:29.304: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:29.314: INFO: Number of nodes with available pods: 0
Mar  1 09:26:29.314: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:26:30.326: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:30.326: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:30.326: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:30.333: INFO: Number of nodes with available pods: 0
Mar  1 09:26:30.334: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:26:31.325: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:31.325: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:31.325: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:31.332: INFO: Number of nodes with available pods: 0
Mar  1 09:26:31.332: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:26:32.325: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:32.325: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:32.326: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:32.345: INFO: Number of nodes with available pods: 1
Mar  1 09:26:32.345: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:26:33.338: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:33.338: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:33.338: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:33.357: INFO: Number of nodes with available pods: 2
Mar  1 09:26:33.357: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  1 09:26:33.453: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:33.453: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:33.481: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:33.481: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:33.481: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:34.492: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:34.492: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:34.502: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:34.502: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:34.503: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:35.489: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:35.489: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:35.489: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:35.498: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:35.498: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:35.498: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:36.489: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:36.489: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:36.489: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:36.499: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:36.499: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:36.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:37.490: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:37.490: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:37.490: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:37.499: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:37.499: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:37.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:38.489: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:38.490: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:38.490: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:38.499: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:38.499: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:38.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:39.489: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:39.489: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:39.489: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:39.498: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:39.498: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:39.498: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:40.489: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:40.489: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:40.489: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:40.498: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:40.499: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:40.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:41.489: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:41.489: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:41.489: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:41.499: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:41.499: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:41.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:42.506: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:42.506: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:42.506: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:42.515: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:42.515: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:42.515: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:43.490: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:43.491: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:43.491: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:43.499: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:43.499: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:43.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:44.488: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:44.488: INFO: Wrong image for pod: daemon-set-dz55r. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:44.488: INFO: Pod daemon-set-dz55r is not available
Mar  1 09:26:44.497: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:44.497: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:44.497: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:45.490: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:45.490: INFO: Pod daemon-set-rckkr is not available
Mar  1 09:26:45.498: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:45.498: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:45.499: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:46.506: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:46.506: INFO: Pod daemon-set-rckkr is not available
Mar  1 09:26:46.544: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:46.544: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:46.544: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:47.512: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:47.522: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:47.522: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:47.522: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:48.535: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:48.545: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:48.545: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:48.545: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:49.490: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:49.501: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:49.501: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:49.501: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:50.490: INFO: Wrong image for pod: daemon-set-4klhs. Expected: 172.20.8.7/library/redis:1.0, got: 172.20.8.7/library/nginx:1.14-alpine.
Mar  1 09:26:50.490: INFO: Pod daemon-set-4klhs is not available
Mar  1 09:26:50.500: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:50.500: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:50.500: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:51.489: INFO: Pod daemon-set-j7v97 is not available
Mar  1 09:26:51.497: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:51.497: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:51.497: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  1 09:26:51.506: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:51.506: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:51.506: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:51.512: INFO: Number of nodes with available pods: 1
Mar  1 09:26:51.512: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:26:52.522: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:52.522: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:52.522: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:52.529: INFO: Number of nodes with available pods: 1
Mar  1 09:26:52.529: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:26:53.534: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:53.534: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:53.534: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:53.541: INFO: Number of nodes with available pods: 1
Mar  1 09:26:53.541: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:26:54.535: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:54.536: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:54.536: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:26:54.569: INFO: Number of nodes with available pods: 2
Mar  1 09:26:54.569: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8154, will wait for the garbage collector to delete the pods
Mar  1 09:26:54.724: INFO: Deleting DaemonSet.extensions daemon-set took: 69.537026ms
Mar  1 09:26:55.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.548394ms
Mar  1 09:27:04.932: INFO: Number of nodes with available pods: 0
Mar  1 09:27:04.932: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 09:27:04.937: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8154/daemonsets","resourceVersion":"191903"},"items":null}

Mar  1 09:27:04.943: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8154/pods","resourceVersion":"191903"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:27:04.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8154" for this suite.
Mar  1 09:27:13.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:27:13.351: INFO: namespace daemonsets-8154 deletion completed in 8.355515818s

• [SLOW TEST:44.527 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:27:13.352: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  1 09:27:13.702: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  1 09:27:18.710: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:27:19.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8644" for this suite.
Mar  1 09:27:27.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:27:28.043: INFO: namespace replication-controller-8644 deletion completed in 8.245957391s

• [SLOW TEST:14.691 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:27:28.044: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  1 09:27:28.376: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 09:27:28.394: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 09:27:28.399: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Mar  1 09:27:28.478: INFO: kube-flannel-ds-amd64-dsrkh from kube-system started at 2020-02-29 08:55:44 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  1 09:27:28.478: INFO: sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-dlz68 from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 09:27:28.478: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 09:27:28.478: INFO: contour-certgen-6w5mk from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container contour ready: false, restart count 0
Mar  1 09:27:28.478: INFO: kuard-55c79684d4-pfs2p from default started at 2020-02-29 08:57:18 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container kuard ready: true, restart count 0
Mar  1 09:27:28.478: INFO: contour-6f89bf4d5f-szsh8 from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container contour ready: false, restart count 0
Mar  1 09:27:28.478: INFO: kuard-55c79684d4-bf6hv from default started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container kuard ready: true, restart count 0
Mar  1 09:27:28.478: INFO: contour-6f89bf4d5f-cbp6b from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container contour ready: false, restart count 0
Mar  1 09:27:28.478: INFO: kube-proxy-s67df from kube-system started at 2020-02-29 08:55:43 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 09:27:28.478: INFO: envoy-fgn4v from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container envoy ready: false, restart count 0
Mar  1 09:27:28.478: INFO: kuard-55c79684d4-mtkkk from default started at 2020-02-29 08:57:18 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container kuard ready: true, restart count 0
Mar  1 09:27:28.478: INFO: metrics-server-5848cb774b-bw57f from kube-system started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.478: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 09:27:28.478: INFO: 
Logging pods the kubelet thinks is on node worker02 before test
Mar  1 09:27:28.502: INFO: envoy-zlkbx from projectcontour started at 2020-02-29 08:57:26 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.502: INFO: 	Container envoy ready: false, restart count 0
Mar  1 09:27:28.502: INFO: sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-zbtnp from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 09:27:28.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 09:27:28.502: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 09:27:28.502: INFO: kube-flannel-ds-amd64-5brn5 from kube-system started at 2020-02-29 08:55:44 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.502: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  1 09:27:28.502: INFO: sonobuoy from sonobuoy started at 2020-03-01 09:01:02 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.502: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 09:27:28.502: INFO: sonobuoy-e2e-job-9a6dafb81c774c83 from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 09:27:28.502: INFO: 	Container e2e ready: true, restart count 0
Mar  1 09:27:28.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 09:27:28.502: INFO: kube-proxy-wxn7v from kube-system started at 2020-02-29 08:55:43 +0000 UTC (1 container statuses recorded)
Mar  1 09:27:28.502: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node worker01
STEP: verifying the node has the label node worker02
Mar  1 09:27:28.644: INFO: Pod kuard-55c79684d4-bf6hv requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod kuard-55c79684d4-mtkkk requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod kuard-55c79684d4-pfs2p requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod kube-flannel-ds-amd64-5brn5 requesting resource cpu=100m on Node worker02
Mar  1 09:27:28.644: INFO: Pod kube-flannel-ds-amd64-dsrkh requesting resource cpu=100m on Node worker01
Mar  1 09:27:28.644: INFO: Pod kube-proxy-s67df requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod kube-proxy-wxn7v requesting resource cpu=0m on Node worker02
Mar  1 09:27:28.644: INFO: Pod metrics-server-5848cb774b-bw57f requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod contour-6f89bf4d5f-cbp6b requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod contour-6f89bf4d5f-szsh8 requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod contour-certgen-6w5mk requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod envoy-fgn4v requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod envoy-zlkbx requesting resource cpu=0m on Node worker02
Mar  1 09:27:28.644: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker02
Mar  1 09:27:28.644: INFO: Pod sonobuoy-e2e-job-9a6dafb81c774c83 requesting resource cpu=0m on Node worker02
Mar  1 09:27:28.644: INFO: Pod sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-dlz68 requesting resource cpu=0m on Node worker01
Mar  1 09:27:28.644: INFO: Pod sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-zbtnp requesting resource cpu=0m on Node worker02
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6.15f823ff60bd8a85], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6609/filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6 to worker01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6.15f823ffbb76c0fe], Reason = [Pulled], Message = [Container image "172.20.8.7/library/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6.15f823ffc3a588b4], Reason = [Created], Message = [Created container filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6.15f823ffdf2cb3a2], Reason = [Started], Message = [Started container filler-pod-176c9e08-e6dc-461e-8fe4-6a99f00b4db6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2.15f823ff682ba2b5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6609/filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2 to worker02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2.15f823fff000ad79], Reason = [Pulled], Message = [Container image "172.20.8.7/library/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2.15f8240005b641c0], Reason = [Created], Message = [Created container filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2.15f8240028e242d9], Reason = [Started], Message = [Started container filler-pod-a559744b-c860-495c-bc01-5f076b17fcb2]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f82400d2b94fc3], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node worker01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker02
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:27:36.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6609" for this suite.
Mar  1 09:27:44.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:27:44.558: INFO: namespace sched-pred-6609 deletion completed in 8.268263024s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.514 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:27:44.558: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:27:44.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692" in namespace "projected-9684" to be "success or failure"
Mar  1 09:27:44.872: INFO: Pod "downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692": Phase="Pending", Reason="", readiness=false. Elapsed: 6.38348ms
Mar  1 09:27:46.880: INFO: Pod "downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014250397s
Mar  1 09:27:48.896: INFO: Pod "downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02999802s
Mar  1 09:27:50.904: INFO: Pod "downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038232187s
STEP: Saw pod success
Mar  1 09:27:50.904: INFO: Pod "downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692" satisfied condition "success or failure"
Mar  1 09:27:50.910: INFO: Trying to get logs from node worker02 pod downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692 container client-container: <nil>
STEP: delete the pod
Mar  1 09:27:51.022: INFO: Waiting for pod downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692 to disappear
Mar  1 09:27:51.028: INFO: Pod downwardapi-volume-23063cd0-bf5c-4cc3-928b-374beb86d692 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:27:51.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9684" for this suite.
Mar  1 09:27:57.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:27:57.313: INFO: namespace projected-9684 deletion completed in 6.276859226s

• [SLOW TEST:12.755 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:27:57.313: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:27:57.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d" in namespace "projected-2595" to be "success or failure"
Mar  1 09:27:57.619: INFO: Pod "downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.207277ms
Mar  1 09:27:59.627: INFO: Pod "downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014969317s
Mar  1 09:28:01.636: INFO: Pod "downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023714407s
Mar  1 09:28:03.645: INFO: Pod "downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032332854s
STEP: Saw pod success
Mar  1 09:28:03.645: INFO: Pod "downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d" satisfied condition "success or failure"
Mar  1 09:28:03.650: INFO: Trying to get logs from node worker02 pod downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d container client-container: <nil>
STEP: delete the pod
Mar  1 09:28:03.741: INFO: Waiting for pod downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d to disappear
Mar  1 09:28:03.747: INFO: Pod downwardapi-volume-b8dd62ae-096f-4ddc-855a-4784da65c59d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:28:03.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2595" for this suite.
Mar  1 09:28:09.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:28:10.153: INFO: namespace projected-2595 deletion completed in 6.376945976s

• [SLOW TEST:12.839 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:28:10.153: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Mar  1 09:28:10.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 --namespace=kubectl-3618 run e2e-test-rm-busybox-job --image=172.20.8.7/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar  1 09:28:15.550: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar  1 09:28:15.550: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:28:17.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3618" for this suite.
Mar  1 09:28:25.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:28:25.795: INFO: namespace kubectl-3618 deletion completed in 8.224964037s

• [SLOW TEST:15.642 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:28:25.796: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3436
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  1 09:28:26.120: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  1 09:28:46.474: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3436 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:28:46.474: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:28:48.009: INFO: Found all expected endpoints: [netserver-0]
Mar  1 09:28:48.016: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.133 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3436 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:28:48.016: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:28:49.269: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:28:49.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3436" for this suite.
Mar  1 09:29:13.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:29:13.538: INFO: namespace pod-network-test-3436 deletion completed in 24.258173061s

• [SLOW TEST:47.742 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:29:13.539: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9215/configmap-test-c6adbd3d-5f88-49c5-81d1-04c4b879fc7f
STEP: Creating a pod to test consume configMaps
Mar  1 09:29:13.898: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971" in namespace "configmap-9215" to be "success or failure"
Mar  1 09:29:13.904: INFO: Pod "pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971": Phase="Pending", Reason="", readiness=false. Elapsed: 5.87903ms
Mar  1 09:29:15.912: INFO: Pod "pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014027144s
Mar  1 09:29:17.919: INFO: Pod "pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021253877s
Mar  1 09:29:19.928: INFO: Pod "pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029602801s
STEP: Saw pod success
Mar  1 09:29:19.928: INFO: Pod "pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971" satisfied condition "success or failure"
Mar  1 09:29:19.933: INFO: Trying to get logs from node worker02 pod pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971 container env-test: <nil>
STEP: delete the pod
Mar  1 09:29:20.000: INFO: Waiting for pod pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971 to disappear
Mar  1 09:29:20.032: INFO: Pod pod-configmaps-dfa4a04c-c538-49b5-9e4d-814838420971 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:29:20.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9215" for this suite.
Mar  1 09:29:28.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:29:28.286: INFO: namespace configmap-9215 deletion completed in 8.234135006s

• [SLOW TEST:14.747 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:29:28.286: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:30:28.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6235" for this suite.
Mar  1 09:30:52.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:30:52.990: INFO: namespace container-probe-6235 deletion completed in 24.297924871s

• [SLOW TEST:84.703 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:30:52.990: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:30:53.360: INFO: Creating ReplicaSet my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4
Mar  1 09:30:53.430: INFO: Pod name my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4: Found 0 pods out of 1
Mar  1 09:30:58.440: INFO: Pod name my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4: Found 1 pods out of 1
Mar  1 09:30:58.440: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4" is running
Mar  1 09:30:58.446: INFO: Pod "my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4-dndrc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 09:30:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 09:30:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 09:30:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 09:30:53 +0000 UTC Reason: Message:}])
Mar  1 09:30:58.446: INFO: Trying to dial the pod
Mar  1 09:31:03.501: INFO: Controller my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4: Got expected result from replica 1 [my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4-dndrc]: "my-hostname-basic-101eeba5-dd44-425d-a8a3-84e6220d73f4-dndrc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:31:03.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3541" for this suite.
Mar  1 09:31:11.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:31:12.067: INFO: namespace replicaset-3541 deletion completed in 8.557191042s

• [SLOW TEST:19.077 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:31:12.068: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-358
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-7d2c37cb-2712-46c9-a4af-ec193958fcc2
STEP: Creating configMap with name cm-test-opt-upd-da4c2b2a-ef33-4a5e-8ef7-3a6ee616edf5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7d2c37cb-2712-46c9-a4af-ec193958fcc2
STEP: Updating configmap cm-test-opt-upd-da4c2b2a-ef33-4a5e-8ef7-3a6ee616edf5
STEP: Creating configMap with name cm-test-opt-create-3ad93d35-9ab6-4de5-9be4-3b71ca7278b3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:31:26.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-358" for this suite.
Mar  1 09:31:50.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:31:50.898: INFO: namespace projected-358 deletion completed in 24.589278261s

• [SLOW TEST:38.830 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:31:50.898: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:31:51.437: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5b1190ec-63d9-4234-80c6-c3be1162758c", Controller:(*bool)(0xc003b18b32), BlockOwnerDeletion:(*bool)(0xc003b18b33)}}
Mar  1 09:31:51.503: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3a567037-c0d1-4c40-8a9f-7d92c305207d", Controller:(*bool)(0xc003da0cfa), BlockOwnerDeletion:(*bool)(0xc003da0cfb)}}
Mar  1 09:31:51.522: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a0b5c8ec-c307-45f5-8e27-c7c671b95f51", Controller:(*bool)(0xc003b18d2a), BlockOwnerDeletion:(*bool)(0xc003b18d2b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:31:56.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5451" for this suite.
Mar  1 09:32:04.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:32:05.994: INFO: namespace gc-5451 deletion completed in 9.328211701s

• [SLOW TEST:15.096 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:32:05.994: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:32:06.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708" in namespace "downward-api-6308" to be "success or failure"
Mar  1 09:32:06.347: INFO: Pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964589ms
Mar  1 09:32:08.355: INFO: Pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014479346s
Mar  1 09:32:10.363: INFO: Pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021771777s
Mar  1 09:32:12.370: INFO: Pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02920472s
Mar  1 09:32:14.379: INFO: Pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038102007s
STEP: Saw pod success
Mar  1 09:32:14.379: INFO: Pod "downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708" satisfied condition "success or failure"
Mar  1 09:32:14.385: INFO: Trying to get logs from node worker02 pod downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708 container client-container: <nil>
STEP: delete the pod
Mar  1 09:32:14.494: INFO: Waiting for pod downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708 to disappear
Mar  1 09:32:14.500: INFO: Pod downwardapi-volume-2a936582-0647-41f9-9143-a1d1401ef708 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:32:14.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6308" for this suite.
Mar  1 09:32:22.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:32:22.830: INFO: namespace downward-api-6308 deletion completed in 8.303028231s

• [SLOW TEST:16.836 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:32:22.830: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:32:23.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-550" for this suite.
Mar  1 09:32:29.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:32:30.609: INFO: namespace kubelet-test-550 deletion completed in 7.148911071s

• [SLOW TEST:7.779 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:32:30.610: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9310
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Mar  1 09:32:30.943: INFO: Waiting up to 5m0s for pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1" in namespace "containers-9310" to be "success or failure"
Mar  1 09:32:30.949: INFO: Pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.40442ms
Mar  1 09:32:32.957: INFO: Pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014237937s
Mar  1 09:32:34.966: INFO: Pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022801584s
Mar  1 09:32:36.973: INFO: Pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030130947s
Mar  1 09:32:39.020: INFO: Pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.077306689s
STEP: Saw pod success
Mar  1 09:32:39.020: INFO: Pod "client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1" satisfied condition "success or failure"
Mar  1 09:32:39.026: INFO: Trying to get logs from node worker02 pod client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1 container test-container: <nil>
STEP: delete the pod
Mar  1 09:32:39.144: INFO: Waiting for pod client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1 to disappear
Mar  1 09:32:39.149: INFO: Pod client-containers-0013cecc-7f6e-4cc8-82fa-c12392fae2d1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:32:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9310" for this suite.
Mar  1 09:32:45.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:32:45.472: INFO: namespace containers-9310 deletion completed in 6.31435042s

• [SLOW TEST:14.862 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:32:45.473: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-db64
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 09:32:46.250: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-db64" in namespace "subpath-7743" to be "success or failure"
Mar  1 09:32:46.256: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892863ms
Mar  1 09:32:48.264: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013435523s
Mar  1 09:32:50.272: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021713186s
Mar  1 09:32:52.282: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031643261s
Mar  1 09:32:54.290: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039768228s
Mar  1 09:32:56.299: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 10.048477718s
Mar  1 09:32:58.307: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 12.056178468s
Mar  1 09:33:00.314: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 14.063882965s
Mar  1 09:33:02.323: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 16.072917452s
Mar  1 09:33:04.699: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 18.448945644s
Mar  1 09:33:06.708: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 20.457213881s
Mar  1 09:33:08.715: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 22.464616401s
Mar  1 09:33:10.722: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 24.471758555s
Mar  1 09:33:12.730: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Running", Reason="", readiness=true. Elapsed: 26.479724651s
Mar  1 09:33:14.739: INFO: Pod "pod-subpath-test-configmap-db64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.488031635s
STEP: Saw pod success
Mar  1 09:33:14.739: INFO: Pod "pod-subpath-test-configmap-db64" satisfied condition "success or failure"
Mar  1 09:33:14.763: INFO: Trying to get logs from node worker02 pod pod-subpath-test-configmap-db64 container test-container-subpath-configmap-db64: <nil>
STEP: delete the pod
Mar  1 09:33:14.840: INFO: Waiting for pod pod-subpath-test-configmap-db64 to disappear
Mar  1 09:33:14.846: INFO: Pod pod-subpath-test-configmap-db64 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-db64
Mar  1 09:33:14.846: INFO: Deleting pod "pod-subpath-test-configmap-db64" in namespace "subpath-7743"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:33:14.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7743" for this suite.
Mar  1 09:33:24.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:33:25.627: INFO: namespace subpath-7743 deletion completed in 10.753158836s

• [SLOW TEST:40.154 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:33:25.627: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  1 09:33:38.216: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:38.216: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:38.474: INFO: Exec stderr: ""
Mar  1 09:33:38.475: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:38.475: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:38.724: INFO: Exec stderr: ""
Mar  1 09:33:38.724: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:39.037: INFO: Exec stderr: ""
Mar  1 09:33:39.037: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:39.037: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:39.282: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  1 09:33:39.282: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:39.282: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:39.598: INFO: Exec stderr: ""
Mar  1 09:33:39.599: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:39.599: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:39.853: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  1 09:33:39.853: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:39.853: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:40.121: INFO: Exec stderr: ""
Mar  1 09:33:40.121: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:40.121: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:40.409: INFO: Exec stderr: ""
Mar  1 09:33:40.409: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:40.409: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:40.663: INFO: Exec stderr: ""
Mar  1 09:33:40.663: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9660 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:33:40.663: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:33:40.925: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:33:40.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9660" for this suite.
Mar  1 09:34:27.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:34:27.428: INFO: namespace e2e-kubelet-etc-hosts-9660 deletion completed in 46.492566475s

• [SLOW TEST:61.801 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:34:27.428: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  1 09:34:34.698: INFO: Successfully updated pod "pod-update-45f9c128-5b08-42ca-94da-c77fbb1154b2"
STEP: verifying the updated pod is in kubernetes
Mar  1 09:34:34.748: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:34:34.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4698" for this suite.
Mar  1 09:34:58.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:34:59.036: INFO: namespace pods-4698 deletion completed in 24.258360151s

• [SLOW TEST:31.608 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:34:59.037: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1740
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:34:59.340: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:35:05.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1740" for this suite.
Mar  1 09:35:14.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:35:14.267: INFO: namespace custom-resource-definition-1740 deletion completed in 8.299799949s

• [SLOW TEST:15.230 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:35:14.267: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:35:14.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3" in namespace "downward-api-3843" to be "success or failure"
Mar  1 09:35:14.709: INFO: Pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665393ms
Mar  1 09:35:16.717: INFO: Pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014952704s
Mar  1 09:35:18.725: INFO: Pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022844624s
Mar  1 09:35:20.733: INFO: Pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030551924s
Mar  1 09:35:22.742: INFO: Pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039729098s
STEP: Saw pod success
Mar  1 09:35:22.742: INFO: Pod "downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3" satisfied condition "success or failure"
Mar  1 09:35:22.747: INFO: Trying to get logs from node worker02 pod downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3 container client-container: <nil>
STEP: delete the pod
Mar  1 09:35:22.826: INFO: Waiting for pod downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3 to disappear
Mar  1 09:35:22.831: INFO: Pod downwardapi-volume-d5f720f6-5094-43aa-a460-8a7c025434f3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:35:22.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3843" for this suite.
Mar  1 09:35:30.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:35:31.057: INFO: namespace downward-api-3843 deletion completed in 8.215688027s

• [SLOW TEST:16.790 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:35:31.058: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  1 09:35:31.375: INFO: Waiting up to 5m0s for pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6" in namespace "emptydir-3895" to be "success or failure"
Mar  1 09:35:31.382: INFO: Pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.728873ms
Mar  1 09:35:33.390: INFO: Pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015061246s
Mar  1 09:35:35.399: INFO: Pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023982023s
Mar  1 09:35:37.407: INFO: Pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03227967s
Mar  1 09:35:39.419: INFO: Pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.043665903s
STEP: Saw pod success
Mar  1 09:35:39.419: INFO: Pod "pod-853cd4ff-802b-4410-b1bf-c9a2194037f6" satisfied condition "success or failure"
Mar  1 09:35:39.426: INFO: Trying to get logs from node worker02 pod pod-853cd4ff-802b-4410-b1bf-c9a2194037f6 container test-container: <nil>
STEP: delete the pod
Mar  1 09:35:39.509: INFO: Waiting for pod pod-853cd4ff-802b-4410-b1bf-c9a2194037f6 to disappear
Mar  1 09:35:39.515: INFO: Pod pod-853cd4ff-802b-4410-b1bf-c9a2194037f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:35:39.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3895" for this suite.
Mar  1 09:35:47.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:35:47.779: INFO: namespace emptydir-3895 deletion completed in 8.255138324s

• [SLOW TEST:16.722 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:35:47.780: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7565
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  1 09:36:02.239: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 09:36:02.246: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 09:36:04.246: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 09:36:04.254: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 09:36:06.246: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 09:36:06.256: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 09:36:08.246: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 09:36:08.253: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:36:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7565" for this suite.
Mar  1 09:36:32.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:36:32.509: INFO: namespace container-lifecycle-hook-7565 deletion completed in 24.247206148s

• [SLOW TEST:44.729 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:36:32.509: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-67
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d547e720-ffdd-4099-ab7d-9e5fc6edb01c
STEP: Creating a pod to test consume configMaps
Mar  1 09:36:32.896: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1" in namespace "projected-67" to be "success or failure"
Mar  1 09:36:32.903: INFO: Pod "pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57008ms
Mar  1 09:36:34.937: INFO: Pod "pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041004668s
Mar  1 09:36:36.947: INFO: Pod "pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051255665s
Mar  1 09:36:38.956: INFO: Pod "pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059640509s
STEP: Saw pod success
Mar  1 09:36:38.956: INFO: Pod "pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1" satisfied condition "success or failure"
Mar  1 09:36:38.963: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:36:39.064: INFO: Waiting for pod pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1 to disappear
Mar  1 09:36:39.099: INFO: Pod pod-projected-configmaps-13d8acd0-b125-44a7-acd8-6278c68c35a1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:36:39.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-67" for this suite.
Mar  1 09:36:47.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:36:47.326: INFO: namespace projected-67 deletion completed in 8.218090791s

• [SLOW TEST:14.817 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:36:47.327: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  1 09:36:47.711: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:47.711: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:47.711: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:47.746: INFO: Number of nodes with available pods: 0
Mar  1 09:36:47.746: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:36:48.757: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:48.758: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:48.758: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:48.766: INFO: Number of nodes with available pods: 0
Mar  1 09:36:48.766: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:36:49.756: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:49.757: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:49.757: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:49.779: INFO: Number of nodes with available pods: 0
Mar  1 09:36:49.779: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:36:50.773: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:50.773: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:50.773: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:50.788: INFO: Number of nodes with available pods: 1
Mar  1 09:36:50.788: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:36:51.757: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:51.757: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:51.757: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:51.765: INFO: Number of nodes with available pods: 1
Mar  1 09:36:51.765: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:36:52.757: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:52.757: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:52.757: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:52.764: INFO: Number of nodes with available pods: 2
Mar  1 09:36:52.764: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  1 09:36:52.829: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:52.829: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:52.829: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:36:52.837: INFO: Number of nodes with available pods: 2
Mar  1 09:36:52.837: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6889, will wait for the garbage collector to delete the pods
Mar  1 09:36:54.002: INFO: Deleting DaemonSet.extensions daemon-set took: 43.230511ms
Mar  1 09:36:54.602: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.396901ms
Mar  1 09:37:04.952: INFO: Number of nodes with available pods: 0
Mar  1 09:37:04.952: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 09:37:04.958: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6889/daemonsets","resourceVersion":"193872"},"items":null}

Mar  1 09:37:04.964: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6889/pods","resourceVersion":"193872"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:37:04.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6889" for this suite.
Mar  1 09:37:13.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:37:13.243: INFO: namespace daemonsets-6889 deletion completed in 8.242779746s

• [SLOW TEST:25.916 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:37:13.244: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Mar  1 09:37:13.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-6340'
Mar  1 09:37:19.476: INFO: stderr: ""
Mar  1 09:37:19.476: INFO: stdout: "pod/pause created\n"
Mar  1 09:37:19.476: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  1 09:37:19.476: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6340" to be "running and ready"
Mar  1 09:37:19.492: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.621556ms
Mar  1 09:37:21.521: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044817475s
Mar  1 09:37:23.529: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.052705309s
Mar  1 09:37:23.529: INFO: Pod "pause" satisfied condition "running and ready"
Mar  1 09:37:23.529: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  1 09:37:23.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 label pods pause testing-label=testing-label-value --namespace=kubectl-6340'
Mar  1 09:37:24.183: INFO: stderr: ""
Mar  1 09:37:24.183: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  1 09:37:24.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pod pause -L testing-label --namespace=kubectl-6340'
Mar  1 09:37:24.372: INFO: stderr: ""
Mar  1 09:37:24.372: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  1 09:37:24.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 label pods pause testing-label- --namespace=kubectl-6340'
Mar  1 09:37:24.664: INFO: stderr: ""
Mar  1 09:37:24.664: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  1 09:37:24.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pod pause -L testing-label --namespace=kubectl-6340'
Mar  1 09:37:24.863: INFO: stderr: ""
Mar  1 09:37:24.863: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Mar  1 09:37:24.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-6340'
Mar  1 09:37:25.087: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 09:37:25.087: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  1 09:37:25.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get rc,svc -l name=pause --no-headers --namespace=kubectl-6340'
Mar  1 09:37:25.290: INFO: stderr: "No resources found.\n"
Mar  1 09:37:25.290: INFO: stdout: ""
Mar  1 09:37:25.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -l name=pause --namespace=kubectl-6340 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 09:37:25.472: INFO: stderr: ""
Mar  1 09:37:25.472: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:37:25.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6340" for this suite.
Mar  1 09:37:33.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:37:33.699: INFO: namespace kubectl-6340 deletion completed in 8.21673119s

• [SLOW TEST:20.455 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:37:33.700: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6927
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  1 09:37:34.030: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  1 09:37:58.658: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.26:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6927 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:37:58.658: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:37:59.044: INFO: Found all expected endpoints: [netserver-0]
Mar  1 09:37:59.051: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.135:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6927 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:37:59.051: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:37:59.308: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:37:59.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6927" for this suite.
Mar  1 09:38:23.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:38:23.575: INFO: namespace pod-network-test-6927 deletion completed in 24.256746304s

• [SLOW TEST:49.876 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:38:23.576: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2770
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  1 09:38:24.064: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  1 09:38:52.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.29:8080/dial?request=hostName&protocol=http&host=10.244.4.28&port=8080&tries=1'] Namespace:pod-network-test-2770 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:38:52.485: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:38:52.843: INFO: Waiting for endpoints: map[]
Mar  1 09:38:52.850: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.29:8080/dial?request=hostName&protocol=http&host=10.244.3.136&port=8080&tries=1'] Namespace:pod-network-test-2770 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:38:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:38:53.149: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:38:53.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2770" for this suite.
Mar  1 09:39:17.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:39:17.455: INFO: namespace pod-network-test-2770 deletion completed in 24.295112262s

• [SLOW TEST:53.878 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:39:17.455: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  1 09:39:17.839: INFO: Waiting up to 5m0s for pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e" in namespace "downward-api-9029" to be "success or failure"
Mar  1 09:39:17.910: INFO: Pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 70.794285ms
Mar  1 09:39:19.920: INFO: Pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081529582s
Mar  1 09:39:21.928: INFO: Pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088938179s
Mar  1 09:39:23.936: INFO: Pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097452056s
Mar  1 09:39:25.945: INFO: Pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.1058357s
STEP: Saw pod success
Mar  1 09:39:25.945: INFO: Pod "downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e" satisfied condition "success or failure"
Mar  1 09:39:25.951: INFO: Trying to get logs from node worker02 pod downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:39:26.082: INFO: Waiting for pod downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e to disappear
Mar  1 09:39:26.087: INFO: Pod downward-api-c5b0a0b4-f067-445e-8b16-25f684e7ca3e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:39:26.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9029" for this suite.
Mar  1 09:39:34.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:39:34.413: INFO: namespace downward-api-9029 deletion completed in 8.317201871s

• [SLOW TEST:16.958 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:39:34.414: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:39:34.822: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a" in namespace "downward-api-6718" to be "success or failure"
Mar  1 09:39:34.829: INFO: Pod "downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.140496ms
Mar  1 09:39:36.837: INFO: Pod "downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014137893s
Mar  1 09:39:39.059: INFO: Pod "downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236385644s
Mar  1 09:39:41.067: INFO: Pod "downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.244665147s
STEP: Saw pod success
Mar  1 09:39:41.067: INFO: Pod "downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a" satisfied condition "success or failure"
Mar  1 09:39:41.074: INFO: Trying to get logs from node worker02 pod downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a container client-container: <nil>
STEP: delete the pod
Mar  1 09:39:41.164: INFO: Waiting for pod downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a to disappear
Mar  1 09:39:41.171: INFO: Pod downwardapi-volume-f0a686d0-6132-473f-8d60-83b72419cf8a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:39:41.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6718" for this suite.
Mar  1 09:39:49.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:39:49.472: INFO: namespace downward-api-6718 deletion completed in 8.291775279s

• [SLOW TEST:15.058 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:39:49.472: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:39:55.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2765" for this suite.
Mar  1 09:40:48.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:40:48.326: INFO: namespace kubelet-test-2765 deletion completed in 52.328185507s

• [SLOW TEST:58.854 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:40:48.326: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:40:48.746: INFO: Create a RollingUpdate DaemonSet
Mar  1 09:40:48.874: INFO: Check that daemon pods launch on every node of the cluster
Mar  1 09:40:48.882: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:48.883: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:48.883: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:48.930: INFO: Number of nodes with available pods: 0
Mar  1 09:40:48.930: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:40:49.939: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:49.939: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:49.939: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:49.946: INFO: Number of nodes with available pods: 0
Mar  1 09:40:49.946: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:40:50.940: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:50.940: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:50.940: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:50.947: INFO: Number of nodes with available pods: 0
Mar  1 09:40:50.948: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:40:51.940: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:51.941: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:51.941: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:52.139: INFO: Number of nodes with available pods: 1
Mar  1 09:40:52.139: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:40:52.941: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:52.942: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:52.942: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:52.949: INFO: Number of nodes with available pods: 1
Mar  1 09:40:52.949: INFO: Node worker02 is running more than one daemon pod
Mar  1 09:40:53.940: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:53.941: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:53.941: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:53.948: INFO: Number of nodes with available pods: 2
Mar  1 09:40:53.948: INFO: Number of running nodes: 2, number of available pods: 2
Mar  1 09:40:53.948: INFO: Update the DaemonSet to trigger a rollout
Mar  1 09:40:53.975: INFO: Updating DaemonSet daemon-set
Mar  1 09:40:58.075: INFO: Roll back the DaemonSet before rollout is complete
Mar  1 09:40:58.107: INFO: Updating DaemonSet daemon-set
Mar  1 09:40:58.107: INFO: Make sure DaemonSet rollback is complete
Mar  1 09:40:58.115: INFO: Wrong image for pod: daemon-set-bjvkl. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  1 09:40:58.115: INFO: Pod daemon-set-bjvkl is not available
Mar  1 09:40:58.153: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:58.153: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:58.153: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:59.162: INFO: Wrong image for pod: daemon-set-bjvkl. Expected: 172.20.8.7/library/nginx:1.14-alpine, got: foo:non-existent.
Mar  1 09:40:59.162: INFO: Pod daemon-set-bjvkl is not available
Mar  1 09:40:59.172: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:59.172: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:40:59.172: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:41:00.161: INFO: Pod daemon-set-7bc9v is not available
Mar  1 09:41:00.170: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:41:00.171: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 09:41:00.171: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2394, will wait for the garbage collector to delete the pods
Mar  1 09:41:00.342: INFO: Deleting DaemonSet.extensions daemon-set took: 103.756954ms
Mar  1 09:41:00.942: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.288498ms
Mar  1 09:42:07.451: INFO: Number of nodes with available pods: 0
Mar  1 09:42:07.451: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 09:42:07.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2394/daemonsets","resourceVersion":"194786"},"items":null}

Mar  1 09:42:07.464: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2394/pods","resourceVersion":"194786"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:42:07.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2394" for this suite.
Mar  1 09:42:15.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:42:15.887: INFO: namespace daemonsets-2394 deletion completed in 8.368202611s

• [SLOW TEST:87.561 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:42:15.887: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-l5hz
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 09:42:16.370: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-l5hz" in namespace "subpath-4447" to be "success or failure"
Mar  1 09:42:16.427: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Pending", Reason="", readiness=false. Elapsed: 56.881073ms
Mar  1 09:42:18.436: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06508947s
Mar  1 09:42:20.445: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07452585s
Mar  1 09:42:22.453: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 6.082680707s
Mar  1 09:42:24.462: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 8.090976031s
Mar  1 09:42:26.477: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 10.106049734s
Mar  1 09:42:28.485: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 12.114269581s
Mar  1 09:42:30.492: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 14.121180155s
Mar  1 09:42:32.499: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 16.128637679s
Mar  1 09:42:34.508: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 18.137158506s
Mar  1 09:42:36.538: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 20.167082381s
Mar  1 09:42:38.547: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Running", Reason="", readiness=true. Elapsed: 22.176899781s
Mar  1 09:42:40.564: INFO: Pod "pod-subpath-test-secret-l5hz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.193726648s
STEP: Saw pod success
Mar  1 09:42:40.564: INFO: Pod "pod-subpath-test-secret-l5hz" satisfied condition "success or failure"
Mar  1 09:42:40.571: INFO: Trying to get logs from node worker02 pod pod-subpath-test-secret-l5hz container test-container-subpath-secret-l5hz: <nil>
STEP: delete the pod
Mar  1 09:42:40.654: INFO: Waiting for pod pod-subpath-test-secret-l5hz to disappear
Mar  1 09:42:40.660: INFO: Pod pod-subpath-test-secret-l5hz no longer exists
STEP: Deleting pod pod-subpath-test-secret-l5hz
Mar  1 09:42:40.660: INFO: Deleting pod "pod-subpath-test-secret-l5hz" in namespace "subpath-4447"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:42:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4447" for this suite.
Mar  1 09:42:48.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:42:49.041: INFO: namespace subpath-4447 deletion completed in 8.363086955s

• [SLOW TEST:33.154 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:42:49.042: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-16da3b65-5ae7-42e0-8de2-e11cf1ca4ce1
STEP: Creating a pod to test consume secrets
Mar  1 09:42:49.408: INFO: Waiting up to 5m0s for pod "pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c" in namespace "secrets-5485" to be "success or failure"
Mar  1 09:42:49.415: INFO: Pod "pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.563686ms
Mar  1 09:42:51.422: INFO: Pod "pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014272793s
Mar  1 09:42:53.430: INFO: Pod "pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021653997s
Mar  1 09:42:55.438: INFO: Pod "pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029611694s
STEP: Saw pod success
Mar  1 09:42:55.438: INFO: Pod "pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c" satisfied condition "success or failure"
Mar  1 09:42:55.444: INFO: Trying to get logs from node worker02 pod pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:42:55.532: INFO: Waiting for pod pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c to disappear
Mar  1 09:42:55.537: INFO: Pod pod-secrets-010aac9f-af2b-4396-9add-4f73287da41c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:42:55.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5485" for this suite.
Mar  1 09:43:01.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:43:01.852: INFO: namespace secrets-5485 deletion completed in 6.305021264s

• [SLOW TEST:12.811 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:43:01.853: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8494
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  1 09:43:08.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec pod-sharedvolume-30e6b192-7a6e-46a2-9c55-1edaacea65bb -c busybox-main-container --namespace=emptydir-8494 -- cat /usr/share/volumeshare/shareddata.txt'
Mar  1 09:43:08.749: INFO: stderr: ""
Mar  1 09:43:08.749: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:43:08.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8494" for this suite.
Mar  1 09:43:14.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:43:15.146: INFO: namespace emptydir-8494 deletion completed in 6.389204636s

• [SLOW TEST:13.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:43:15.146: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Mar  1 09:43:15.467: INFO: Waiting up to 5m0s for pod "client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc" in namespace "containers-9808" to be "success or failure"
Mar  1 09:43:15.472: INFO: Pod "client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.03162ms
Mar  1 09:43:17.479: INFO: Pod "client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012203094s
Mar  1 09:43:19.486: INFO: Pod "client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019446764s
Mar  1 09:43:21.493: INFO: Pod "client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026249671s
STEP: Saw pod success
Mar  1 09:43:21.493: INFO: Pod "client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc" satisfied condition "success or failure"
Mar  1 09:43:21.499: INFO: Trying to get logs from node worker02 pod client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc container test-container: <nil>
STEP: delete the pod
Mar  1 09:43:21.597: INFO: Waiting for pod client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc to disappear
Mar  1 09:43:21.602: INFO: Pod client-containers-e85cdb78-7cf3-44b9-b575-3b960e968efc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:43:21.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9808" for this suite.
Mar  1 09:43:27.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:43:27.970: INFO: namespace containers-9808 deletion completed in 6.359788351s

• [SLOW TEST:12.824 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:43:27.971: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 09:43:28.366: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  1 09:43:28.423: INFO: Number of nodes with available pods: 0
Mar  1 09:43:28.423: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  1 09:43:28.535: INFO: Number of nodes with available pods: 0
Mar  1 09:43:28.535: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:29.542: INFO: Number of nodes with available pods: 0
Mar  1 09:43:29.542: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:30.542: INFO: Number of nodes with available pods: 0
Mar  1 09:43:30.542: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:31.542: INFO: Number of nodes with available pods: 1
Mar  1 09:43:31.542: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  1 09:43:31.633: INFO: Number of nodes with available pods: 1
Mar  1 09:43:31.633: INFO: Number of running nodes: 0, number of available pods: 1
Mar  1 09:43:32.638: INFO: Number of nodes with available pods: 0
Mar  1 09:43:32.638: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  1 09:43:32.660: INFO: Number of nodes with available pods: 0
Mar  1 09:43:32.660: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:33.666: INFO: Number of nodes with available pods: 0
Mar  1 09:43:33.666: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:34.666: INFO: Number of nodes with available pods: 0
Mar  1 09:43:34.666: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:35.727: INFO: Number of nodes with available pods: 0
Mar  1 09:43:35.727: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:36.668: INFO: Number of nodes with available pods: 0
Mar  1 09:43:36.668: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:37.668: INFO: Number of nodes with available pods: 0
Mar  1 09:43:37.668: INFO: Node worker01 is running more than one daemon pod
Mar  1 09:43:38.668: INFO: Number of nodes with available pods: 1
Mar  1 09:43:38.668: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7702, will wait for the garbage collector to delete the pods
Mar  1 09:43:38.758: INFO: Deleting DaemonSet.extensions daemon-set took: 25.240432ms
Mar  1 09:43:39.258: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.260823ms
Mar  1 09:43:43.464: INFO: Number of nodes with available pods: 0
Mar  1 09:43:43.465: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 09:43:43.469: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7702/daemonsets","resourceVersion":"195159"},"items":null}

Mar  1 09:43:43.474: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7702/pods","resourceVersion":"195159"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:43:43.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7702" for this suite.
Mar  1 09:43:51.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:43:51.738: INFO: namespace daemonsets-7702 deletion completed in 8.173495393s

• [SLOW TEST:23.767 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:43:51.738: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-tl9v
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 09:43:52.239: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-tl9v" in namespace "subpath-4206" to be "success or failure"
Mar  1 09:43:52.255: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Pending", Reason="", readiness=false. Elapsed: 15.484403ms
Mar  1 09:43:54.262: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022610813s
Mar  1 09:43:56.270: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031011663s
Mar  1 09:43:58.278: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 6.038444547s
Mar  1 09:44:00.286: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 8.046276081s
Mar  1 09:44:02.293: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 10.053617711s
Mar  1 09:44:04.305: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.065652495s
Mar  1 09:44:06.331: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.09121916s
Mar  1 09:44:08.337: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.097212451s
Mar  1 09:44:10.344: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.104158738s
Mar  1 09:44:12.349: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.109320685s
Mar  1 09:44:14.358: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Running", Reason="", readiness=true. Elapsed: 22.119016739s
Mar  1 09:44:16.369: INFO: Pod "pod-subpath-test-projected-tl9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.129887249s
STEP: Saw pod success
Mar  1 09:44:16.369: INFO: Pod "pod-subpath-test-projected-tl9v" satisfied condition "success or failure"
Mar  1 09:44:16.375: INFO: Trying to get logs from node worker02 pod pod-subpath-test-projected-tl9v container test-container-subpath-projected-tl9v: <nil>
STEP: delete the pod
Mar  1 09:44:16.686: INFO: Waiting for pod pod-subpath-test-projected-tl9v to disappear
Mar  1 09:44:16.690: INFO: Pod pod-subpath-test-projected-tl9v no longer exists
STEP: Deleting pod pod-subpath-test-projected-tl9v
Mar  1 09:44:16.690: INFO: Deleting pod "pod-subpath-test-projected-tl9v" in namespace "subpath-4206"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:44:16.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4206" for this suite.
Mar  1 09:44:24.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:44:25.070: INFO: namespace subpath-4206 deletion completed in 8.337495834s

• [SLOW TEST:33.332 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:44:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-448
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 09:44:30.515: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:44:30.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-448" for this suite.
Mar  1 09:44:38.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:44:38.814: INFO: namespace container-runtime-448 deletion completed in 8.185180892s

• [SLOW TEST:13.743 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:44:38.814: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-b1793bc4-22c3-4a5f-941d-ad1731dab664
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:44:39.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2053" for this suite.
Mar  1 09:44:45.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:44:45.500: INFO: namespace configmap-2053 deletion completed in 6.342321702s

• [SLOW TEST:6.687 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:44:45.501: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-dba050ba-9bc9-4b2d-bb37-a730e60f8ed5
STEP: Creating a pod to test consume secrets
Mar  1 09:44:45.937: INFO: Waiting up to 5m0s for pod "pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c" in namespace "secrets-6153" to be "success or failure"
Mar  1 09:44:45.943: INFO: Pod "pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141041ms
Mar  1 09:44:47.950: INFO: Pod "pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012052751s
Mar  1 09:44:49.956: INFO: Pod "pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018731145s
Mar  1 09:44:51.962: INFO: Pod "pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024566979s
STEP: Saw pod success
Mar  1 09:44:51.962: INFO: Pod "pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c" satisfied condition "success or failure"
Mar  1 09:44:51.968: INFO: Trying to get logs from node worker02 pod pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:44:52.039: INFO: Waiting for pod pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c to disappear
Mar  1 09:44:52.043: INFO: Pod pod-secrets-78591a5c-6786-4ae1-b6e1-14761d20786c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:44:52.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6153" for this suite.
Mar  1 09:44:58.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:44:58.239: INFO: namespace secrets-6153 deletion completed in 6.189326555s

• [SLOW TEST:12.738 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:44:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  1 09:45:08.969: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:45:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0301 09:45:08.969900      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4095" for this suite.
Mar  1 09:45:17.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:45:17.329: INFO: namespace gc-4095 deletion completed in 8.352834609s

• [SLOW TEST:19.089 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:45:17.330: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  1 09:45:17.888: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195659,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 09:45:17.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195659,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  1 09:45:28.030: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195678,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  1 09:45:28.030: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195678,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  1 09:45:38.073: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195699,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 09:45:38.074: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195699,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  1 09:45:48.157: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195720,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 09:45:48.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-a,UID:fa4d29f2-cc78-46c7-86a0-b79470533030,ResourceVersion:195720,Generation:0,CreationTimestamp:2020-03-01 09:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  1 09:45:58.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-b,UID:e70c8c05-88d2-43f3-96de-0d1ac7f1c455,ResourceVersion:195739,Generation:0,CreationTimestamp:2020-03-01 09:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 09:45:58.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-b,UID:e70c8c05-88d2-43f3-96de-0d1ac7f1c455,ResourceVersion:195739,Generation:0,CreationTimestamp:2020-03-01 09:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  1 09:46:08.359: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-b,UID:e70c8c05-88d2-43f3-96de-0d1ac7f1c455,ResourceVersion:195758,Generation:0,CreationTimestamp:2020-03-01 09:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 09:46:08.359: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7743,SelfLink:/api/v1/namespaces/watch-7743/configmaps/e2e-watch-test-configmap-b,UID:e70c8c05-88d2-43f3-96de-0d1ac7f1c455,ResourceVersion:195758,Generation:0,CreationTimestamp:2020-03-01 09:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:46:18.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7743" for this suite.
Mar  1 09:46:24.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:46:24.633: INFO: namespace watch-7743 deletion completed in 6.264699124s

• [SLOW TEST:67.304 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:46:24.634: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  1 09:46:24.975: INFO: Waiting up to 5m0s for pod "pod-c4100b7a-4f43-4984-9a0b-640796b00111" in namespace "emptydir-3145" to be "success or failure"
Mar  1 09:46:25.010: INFO: Pod "pod-c4100b7a-4f43-4984-9a0b-640796b00111": Phase="Pending", Reason="", readiness=false. Elapsed: 34.534871ms
Mar  1 09:46:27.017: INFO: Pod "pod-c4100b7a-4f43-4984-9a0b-640796b00111": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041597981s
Mar  1 09:46:29.024: INFO: Pod "pod-c4100b7a-4f43-4984-9a0b-640796b00111": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048840275s
Mar  1 09:46:31.030: INFO: Pod "pod-c4100b7a-4f43-4984-9a0b-640796b00111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054908985s
STEP: Saw pod success
Mar  1 09:46:31.031: INFO: Pod "pod-c4100b7a-4f43-4984-9a0b-640796b00111" satisfied condition "success or failure"
Mar  1 09:46:31.035: INFO: Trying to get logs from node worker02 pod pod-c4100b7a-4f43-4984-9a0b-640796b00111 container test-container: <nil>
STEP: delete the pod
Mar  1 09:46:31.104: INFO: Waiting for pod pod-c4100b7a-4f43-4984-9a0b-640796b00111 to disappear
Mar  1 09:46:31.109: INFO: Pod pod-c4100b7a-4f43-4984-9a0b-640796b00111 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:46:31.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3145" for this suite.
Mar  1 09:46:39.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:46:39.308: INFO: namespace emptydir-3145 deletion completed in 8.191570508s

• [SLOW TEST:14.674 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:46:39.308: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-4892206d-c0c5-4fac-9e6f-5c6109728d42
STEP: Creating a pod to test consume configMaps
Mar  1 09:46:39.658: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae" in namespace "configmap-3594" to be "success or failure"
Mar  1 09:46:39.665: INFO: Pod "pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.330546ms
Mar  1 09:46:41.674: INFO: Pod "pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01586629s
Mar  1 09:46:43.681: INFO: Pod "pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02272638s
Mar  1 09:46:45.688: INFO: Pod "pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029887574s
STEP: Saw pod success
Mar  1 09:46:45.688: INFO: Pod "pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae" satisfied condition "success or failure"
Mar  1 09:46:45.692: INFO: Trying to get logs from node worker02 pod pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:46:45.757: INFO: Waiting for pod pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae to disappear
Mar  1 09:46:45.762: INFO: Pod pod-configmaps-f4c4f74b-ab88-45a5-a019-519986eeabae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:46:45.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3594" for this suite.
Mar  1 09:46:51.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:46:51.954: INFO: namespace configmap-3594 deletion completed in 6.183975748s

• [SLOW TEST:12.645 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:46:51.956: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Mar  1 09:46:52.927: INFO: created pod pod-service-account-defaultsa
Mar  1 09:46:52.927: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  1 09:46:52.952: INFO: created pod pod-service-account-mountsa
Mar  1 09:46:52.952: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  1 09:46:53.087: INFO: created pod pod-service-account-nomountsa
Mar  1 09:46:53.087: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  1 09:46:53.169: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  1 09:46:53.169: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  1 09:46:53.238: INFO: created pod pod-service-account-mountsa-mountspec
Mar  1 09:46:53.238: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  1 09:46:53.393: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  1 09:46:53.393: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  1 09:46:53.458: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  1 09:46:53.458: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  1 09:46:53.511: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  1 09:46:53.511: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  1 09:46:53.638: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  1 09:46:53.638: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:46:53.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9178" for this suite.
Mar  1 09:47:17.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:47:18.139: INFO: namespace svcaccounts-9178 deletion completed in 24.409310155s

• [SLOW TEST:26.183 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:47:18.139: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar  1 09:47:48.791: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:47:48.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0301 09:47:48.791038      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1978" for this suite.
Mar  1 09:47:56.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:47:56.975: INFO: namespace gc-1978 deletion completed in 8.177386912s

• [SLOW TEST:38.836 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:47:56.976: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5277
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5277
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5277
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5277
Mar  1 09:47:57.419: INFO: Found 0 stateful pods, waiting for 1
Mar  1 09:48:07.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  1 09:48:07.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 09:48:13.733: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 09:48:13.734: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 09:48:13.734: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 09:48:13.740: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 09:48:23.750: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 09:48:23.750: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 09:48:23.777: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:23.777: INFO: ss-0  worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:23.777: INFO: 
Mar  1 09:48:23.777: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  1 09:48:24.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994739373s
Mar  1 09:48:25.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986174193s
Mar  1 09:48:26.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97897654s
Mar  1 09:48:27.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968364257s
Mar  1 09:48:28.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96134328s
Mar  1 09:48:29.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95342931s
Mar  1 09:48:30.834: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945705507s
Mar  1 09:48:31.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.937632642s
Mar  1 09:48:32.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.968643ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5277
Mar  1 09:48:33.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 09:48:34.384: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  1 09:48:34.384: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 09:48:34.384: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 09:48:34.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 09:48:34.923: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 09:48:34.923: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 09:48:34.923: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 09:48:34.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 09:48:35.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 09:48:35.311: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 09:48:35.311: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 09:48:35.318: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:48:35.318: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 09:48:35.318: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  1 09:48:35.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 09:48:35.724: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 09:48:35.724: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 09:48:35.724: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 09:48:35.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 09:48:36.144: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 09:48:36.144: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 09:48:36.144: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 09:48:36.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-5277 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 09:48:36.594: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 09:48:36.594: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 09:48:36.594: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 09:48:36.594: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 09:48:36.805: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  1 09:48:46.843: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 09:48:46.843: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 09:48:46.843: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 09:48:46.919: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:46.919: INFO: ss-0  worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:46.919: INFO: ss-1  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:46.919: INFO: ss-2  worker02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:46.919: INFO: 
Mar  1 09:48:46.919: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:47.927: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:47.927: INFO: ss-0  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:47.928: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:47.928: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:47.928: INFO: 
Mar  1 09:48:47.928: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:49.005: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:49.006: INFO: ss-0  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:49.006: INFO: ss-1  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:49.006: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:49.006: INFO: 
Mar  1 09:48:49.006: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:50.013: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:50.013: INFO: ss-0  worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:50.013: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:50.013: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:50.013: INFO: 
Mar  1 09:48:50.013: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:51.031: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:51.031: INFO: ss-0  worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:51.031: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:51.031: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:51.032: INFO: 
Mar  1 09:48:51.032: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:52.038: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:52.038: INFO: ss-0  worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:52.038: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:52.038: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:52.038: INFO: 
Mar  1 09:48:52.038: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:53.066: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:53.066: INFO: ss-0  worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:53.066: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:53.066: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:53.066: INFO: 
Mar  1 09:48:53.066: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:54.072: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Mar  1 09:48:54.072: INFO: ss-0  worker02  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:47:57 +0000 UTC  }]
Mar  1 09:48:54.072: INFO: ss-1  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:54.072: INFO: ss-2  worker02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:48:23 +0000 UTC  }]
Mar  1 09:48:54.072: INFO: 
Mar  1 09:48:54.072: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 09:48:55.121: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.83891122s
Mar  1 09:48:56.127: INFO: Verifying statefulset ss doesn't scale past 0 for another 790.601202ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5277
Mar  1 09:48:57.133: INFO: Scaling statefulset ss to 0
Mar  1 09:48:57.145: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  1 09:48:57.148: INFO: Deleting all statefulset in ns statefulset-5277
Mar  1 09:48:57.153: INFO: Scaling statefulset ss to 0
Mar  1 09:48:57.164: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 09:48:57.167: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:48:57.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5277" for this suite.
Mar  1 09:49:05.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:49:05.607: INFO: namespace statefulset-5277 deletion completed in 8.372504338s

• [SLOW TEST:68.631 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:49:05.607: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:49:09.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9735" for this suite.
Mar  1 09:49:55.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:49:56.132: INFO: namespace kubelet-test-9735 deletion completed in 46.177564761s

• [SLOW TEST:50.525 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:49:56.133: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Mar  1 09:50:00.433: INFO: Pod pod-hostip-5ae218b0-20fe-4d56-8819-2ef53c30cafc has hostIP: 172.20.8.6
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:50:00.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-274" for this suite.
Mar  1 09:50:22.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:50:22.663: INFO: namespace pods-274 deletion completed in 22.221786169s

• [SLOW TEST:26.530 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:50:22.663: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-4997d3de-bb93-46e8-8fc0-8fb031e1ea31
STEP: Creating secret with name secret-projected-all-test-volume-a9267ea9-1537-4196-8615-84b358b95471
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  1 09:50:23.151: INFO: Waiting up to 5m0s for pod "projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2" in namespace "projected-6259" to be "success or failure"
Mar  1 09:50:23.229: INFO: Pod "projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2": Phase="Pending", Reason="", readiness=false. Elapsed: 77.854425ms
Mar  1 09:50:25.237: INFO: Pod "projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085457946s
Mar  1 09:50:27.244: INFO: Pod "projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092567689s
Mar  1 09:50:29.250: INFO: Pod "projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.09910927s
STEP: Saw pod success
Mar  1 09:50:29.251: INFO: Pod "projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2" satisfied condition "success or failure"
Mar  1 09:50:29.256: INFO: Trying to get logs from node worker02 pod projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  1 09:50:29.463: INFO: Waiting for pod projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2 to disappear
Mar  1 09:50:29.469: INFO: Pod projected-volume-4ad38b8a-4749-45c6-b556-8864767373b2 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:50:29.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6259" for this suite.
Mar  1 09:50:35.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:50:36.072: INFO: namespace projected-6259 deletion completed in 6.594104277s

• [SLOW TEST:13.409 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:50:36.073: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Mar  1 09:50:36.408: INFO: Waiting up to 5m0s for pod "var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d" in namespace "var-expansion-7156" to be "success or failure"
Mar  1 09:50:36.412: INFO: Pod "var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.465617ms
Mar  1 09:50:38.419: INFO: Pod "var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011387504s
Mar  1 09:50:40.433: INFO: Pod "var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025891857s
Mar  1 09:50:42.440: INFO: Pod "var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032703908s
STEP: Saw pod success
Mar  1 09:50:42.440: INFO: Pod "var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d" satisfied condition "success or failure"
Mar  1 09:50:42.445: INFO: Trying to get logs from node worker02 pod var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:50:42.547: INFO: Waiting for pod var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d to disappear
Mar  1 09:50:42.551: INFO: Pod var-expansion-16a7f9f8-adae-416a-addd-517bb1df755d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:50:42.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7156" for this suite.
Mar  1 09:50:48.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:50:48.976: INFO: namespace var-expansion-7156 deletion completed in 6.405899579s

• [SLOW TEST:12.904 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:50:48.977: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  1 09:50:59.480: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:50:59.492: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:01.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:01.498: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:03.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:03.499: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:05.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:05.542: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:07.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:07.500: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:09.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:09.498: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:11.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:11.499: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:13.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:13.500: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:15.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:15.498: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 09:51:17.492: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 09:51:17.500: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:51:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1826" for this suite.
Mar  1 09:51:41.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:51:41.980: INFO: namespace container-lifecycle-hook-1826 deletion completed in 24.470772181s

• [SLOW TEST:53.002 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:51:41.980: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4637
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  1 09:51:42.379: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar  1 09:52:06.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.65:8080/dial?request=hostName&protocol=udp&host=10.244.3.150&port=8081&tries=1'] Namespace:pod-network-test-4637 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:52:06.881: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:52:07.643: INFO: Waiting for endpoints: map[]
Mar  1 09:52:07.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.4.65:8080/dial?request=hostName&protocol=udp&host=10.244.4.64&port=8081&tries=1'] Namespace:pod-network-test-4637 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  1 09:52:07.671: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
Mar  1 09:52:07.943: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:52:07.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4637" for this suite.
Mar  1 09:52:31.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:52:32.117: INFO: namespace pod-network-test-4637 deletion completed in 24.165771387s

• [SLOW TEST:50.137 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:52:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 09:52:38.264: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:52:38.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7809" for this suite.
Mar  1 09:52:44.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:52:44.550: INFO: namespace container-runtime-7809 deletion completed in 6.197379274s

• [SLOW TEST:12.433 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:52:44.551: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9351
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Mar  1 09:52:44.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-9351'
Mar  1 09:52:45.578: INFO: stderr: ""
Mar  1 09:52:45.578: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 09:52:45.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9351'
Mar  1 09:52:46.075: INFO: stderr: ""
Mar  1 09:52:46.075: INFO: stdout: "update-demo-nautilus-fb7zf update-demo-nautilus-z6jsr "
Mar  1 09:52:46.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-fb7zf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:52:46.256: INFO: stderr: ""
Mar  1 09:52:46.256: INFO: stdout: ""
Mar  1 09:52:46.256: INFO: update-demo-nautilus-fb7zf is created but not running
Mar  1 09:52:51.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9351'
Mar  1 09:52:51.443: INFO: stderr: ""
Mar  1 09:52:51.443: INFO: stdout: "update-demo-nautilus-fb7zf update-demo-nautilus-z6jsr "
Mar  1 09:52:51.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-fb7zf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:52:51.629: INFO: stderr: ""
Mar  1 09:52:51.629: INFO: stdout: "true"
Mar  1 09:52:51.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-fb7zf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:52:51.812: INFO: stderr: ""
Mar  1 09:52:51.812: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:52:51.812: INFO: validating pod update-demo-nautilus-fb7zf
Mar  1 09:52:51.820: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:52:51.820: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:52:51.820: INFO: update-demo-nautilus-fb7zf is verified up and running
Mar  1 09:52:51.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-z6jsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:52:52.000: INFO: stderr: ""
Mar  1 09:52:52.000: INFO: stdout: "true"
Mar  1 09:52:52.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-z6jsr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:52:52.187: INFO: stderr: ""
Mar  1 09:52:52.187: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 09:52:52.187: INFO: validating pod update-demo-nautilus-z6jsr
Mar  1 09:52:52.196: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 09:52:52.196: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 09:52:52.196: INFO: update-demo-nautilus-z6jsr is verified up and running
STEP: rolling-update to new replication controller
Mar  1 09:52:52.201: INFO: scanned /root for discovery docs: <nil>
Mar  1 09:52:52.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9351'
Mar  1 09:53:16.833: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  1 09:53:16.833: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 09:53:16.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9351'
Mar  1 09:53:17.032: INFO: stderr: ""
Mar  1 09:53:17.032: INFO: stdout: "update-demo-kitten-b4c9j update-demo-kitten-wsggb "
Mar  1 09:53:17.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-kitten-b4c9j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:53:17.221: INFO: stderr: ""
Mar  1 09:53:17.221: INFO: stdout: "true"
Mar  1 09:53:17.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-kitten-b4c9j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:53:17.515: INFO: stderr: ""
Mar  1 09:53:17.515: INFO: stdout: "172.20.8.7/library/kitten:1.0"
Mar  1 09:53:17.515: INFO: validating pod update-demo-kitten-b4c9j
Mar  1 09:53:17.524: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  1 09:53:17.524: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  1 09:53:17.524: INFO: update-demo-kitten-b4c9j is verified up and running
Mar  1 09:53:17.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-kitten-wsggb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:53:17.711: INFO: stderr: ""
Mar  1 09:53:17.711: INFO: stdout: "true"
Mar  1 09:53:17.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-kitten-wsggb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9351'
Mar  1 09:53:17.911: INFO: stderr: ""
Mar  1 09:53:17.911: INFO: stdout: "172.20.8.7/library/kitten:1.0"
Mar  1 09:53:17.911: INFO: validating pod update-demo-kitten-wsggb
Mar  1 09:53:17.919: INFO: got data: {
  "image": "kitten.jpg"
}

Mar  1 09:53:17.919: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar  1 09:53:17.919: INFO: update-demo-kitten-wsggb is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:53:17.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9351" for this suite.
Mar  1 09:53:41.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:53:42.119: INFO: namespace kubectl-9351 deletion completed in 24.194142691s

• [SLOW TEST:57.569 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:53:42.120: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  1 09:53:42.453: INFO: Waiting up to 5m0s for pod "pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1" in namespace "emptydir-6480" to be "success or failure"
Mar  1 09:53:42.458: INFO: Pod "pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983299ms
Mar  1 09:53:44.464: INFO: Pod "pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01077538s
Mar  1 09:53:46.470: INFO: Pod "pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016386194s
Mar  1 09:53:48.477: INFO: Pod "pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023733517s
STEP: Saw pod success
Mar  1 09:53:48.477: INFO: Pod "pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1" satisfied condition "success or failure"
Mar  1 09:53:48.482: INFO: Trying to get logs from node worker02 pod pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1 container test-container: <nil>
STEP: delete the pod
Mar  1 09:53:48.586: INFO: Waiting for pod pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1 to disappear
Mar  1 09:53:48.591: INFO: Pod pod-8c081c08-5d44-4a6f-9d6a-ffd615d456c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:53:48.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6480" for this suite.
Mar  1 09:53:54.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:53:54.867: INFO: namespace emptydir-6480 deletion completed in 6.244647861s

• [SLOW TEST:12.747 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:53:54.867: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  1 09:53:55.179: INFO: PodSpec: initContainers in spec.initContainers
Mar  1 09:54:47.762: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b65ff526-ca3f-485e-a493-b61c0f9afb9a", GenerateName:"", Namespace:"init-container-1273", SelfLink:"/api/v1/namespaces/init-container-1273/pods/pod-init-b65ff526-ca3f-485e-a493-b61c0f9afb9a", UID:"96435cff-c528-4a37-bba5-ca972c70477c", ResourceVersion:"197544", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718653235, loc:(*time.Location)(0x7ed4a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"179534065"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wzkfn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0023c6500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"172.20.8.7/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wzkfn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"172.20.8.7/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wzkfn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"172.20.8.7/library/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wzkfn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000ee7cd8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001ba4ba0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000ee7d60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000ee7d80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000ee7d88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000ee7d8c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718653235, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718653235, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718653235, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718653235, loc:(*time.Location)(0x7ed4a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.20.8.6", PodIP:"10.244.4.70", StartTime:(*v1.Time)(0xc003f72b60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001efbce0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:3, Image:"172.20.8.7/library/busybox:1.29", ImageID:"docker-pullable://172.20.8.7/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://a51f57b7e4aab9608515afa4fefc22282f76caa63796f1f688213fef9774d745"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003f72bc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"172.20.8.7/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003f72ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"172.20.8.7/library/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:54:47.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1273" for this suite.
Mar  1 09:55:11.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:55:12.007: INFO: namespace init-container-1273 deletion completed in 24.236464452s

• [SLOW TEST:77.140 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:55:12.008: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  1 09:55:12.348: INFO: Waiting up to 5m0s for pod "pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd" in namespace "emptydir-7678" to be "success or failure"
Mar  1 09:55:12.355: INFO: Pod "pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328166ms
Mar  1 09:55:14.365: INFO: Pod "pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01660433s
Mar  1 09:55:16.377: INFO: Pod "pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028246853s
Mar  1 09:55:18.383: INFO: Pod "pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034226333s
STEP: Saw pod success
Mar  1 09:55:18.383: INFO: Pod "pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd" satisfied condition "success or failure"
Mar  1 09:55:18.386: INFO: Trying to get logs from node worker02 pod pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd container test-container: <nil>
STEP: delete the pod
Mar  1 09:55:18.475: INFO: Waiting for pod pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd to disappear
Mar  1 09:55:18.479: INFO: Pod pod-72df1d46-5a4d-4298-b8c5-5af22a39d1bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:55:18.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7678" for this suite.
Mar  1 09:55:24.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:55:24.948: INFO: namespace emptydir-7678 deletion completed in 6.412540564s

• [SLOW TEST:12.940 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:55:24.948: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7598
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  1 09:55:25.232: INFO: Waiting up to 5m0s for pod "pod-e968a23a-1334-4f8a-8919-23e5e1e746ac" in namespace "emptydir-7598" to be "success or failure"
Mar  1 09:55:25.289: INFO: Pod "pod-e968a23a-1334-4f8a-8919-23e5e1e746ac": Phase="Pending", Reason="", readiness=false. Elapsed: 57.042076ms
Mar  1 09:55:27.294: INFO: Pod "pod-e968a23a-1334-4f8a-8919-23e5e1e746ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062032364s
Mar  1 09:55:29.300: INFO: Pod "pod-e968a23a-1334-4f8a-8919-23e5e1e746ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067887747s
Mar  1 09:55:31.307: INFO: Pod "pod-e968a23a-1334-4f8a-8919-23e5e1e746ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074606234s
STEP: Saw pod success
Mar  1 09:55:31.307: INFO: Pod "pod-e968a23a-1334-4f8a-8919-23e5e1e746ac" satisfied condition "success or failure"
Mar  1 09:55:31.310: INFO: Trying to get logs from node worker02 pod pod-e968a23a-1334-4f8a-8919-23e5e1e746ac container test-container: <nil>
STEP: delete the pod
Mar  1 09:55:31.389: INFO: Waiting for pod pod-e968a23a-1334-4f8a-8919-23e5e1e746ac to disappear
Mar  1 09:55:31.393: INFO: Pod pod-e968a23a-1334-4f8a-8919-23e5e1e746ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:55:31.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7598" for this suite.
Mar  1 09:55:37.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:55:37.591: INFO: namespace emptydir-7598 deletion completed in 6.191452167s

• [SLOW TEST:12.643 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:55:37.592: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Mar  1 09:55:38.016: INFO: Waiting up to 5m0s for pod "client-containers-08898351-50f8-4079-8f1c-461312fd406d" in namespace "containers-5425" to be "success or failure"
Mar  1 09:55:38.021: INFO: Pod "client-containers-08898351-50f8-4079-8f1c-461312fd406d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.06963ms
Mar  1 09:55:40.028: INFO: Pod "client-containers-08898351-50f8-4079-8f1c-461312fd406d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012826397s
Mar  1 09:55:42.036: INFO: Pod "client-containers-08898351-50f8-4079-8f1c-461312fd406d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020414447s
Mar  1 09:55:44.043: INFO: Pod "client-containers-08898351-50f8-4079-8f1c-461312fd406d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027406244s
STEP: Saw pod success
Mar  1 09:55:44.043: INFO: Pod "client-containers-08898351-50f8-4079-8f1c-461312fd406d" satisfied condition "success or failure"
Mar  1 09:55:44.047: INFO: Trying to get logs from node worker02 pod client-containers-08898351-50f8-4079-8f1c-461312fd406d container test-container: <nil>
STEP: delete the pod
Mar  1 09:55:44.158: INFO: Waiting for pod client-containers-08898351-50f8-4079-8f1c-461312fd406d to disappear
Mar  1 09:55:44.162: INFO: Pod client-containers-08898351-50f8-4079-8f1c-461312fd406d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:55:44.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5425" for this suite.
Mar  1 09:55:50.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:55:50.414: INFO: namespace containers-5425 deletion completed in 6.214567696s

• [SLOW TEST:12.822 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:55:50.415: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-612
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-a1ef5858-1533-407a-b64b-fa66aca32983
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:55:56.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-612" for this suite.
Mar  1 09:56:20.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:56:21.204: INFO: namespace configmap-612 deletion completed in 24.261289624s

• [SLOW TEST:30.790 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:56:21.205: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-97105bd6-b69a-44d2-a736-eda87f42f2d0
STEP: Creating a pod to test consume secrets
Mar  1 09:56:21.515: INFO: Waiting up to 5m0s for pod "pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da" in namespace "secrets-7111" to be "success or failure"
Mar  1 09:56:21.580: INFO: Pod "pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da": Phase="Pending", Reason="", readiness=false. Elapsed: 64.45211ms
Mar  1 09:56:23.587: INFO: Pod "pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071085643s
Mar  1 09:56:25.594: INFO: Pod "pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078217087s
Mar  1 09:56:27.600: INFO: Pod "pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084163611s
STEP: Saw pod success
Mar  1 09:56:27.600: INFO: Pod "pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da" satisfied condition "success or failure"
Mar  1 09:56:27.604: INFO: Trying to get logs from node worker02 pod pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 09:56:27.681: INFO: Waiting for pod pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da to disappear
Mar  1 09:56:27.684: INFO: Pod pod-secrets-194bb64c-2f6f-4940-b189-ee36316611da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:56:27.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7111" for this suite.
Mar  1 09:56:33.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:56:34.121: INFO: namespace secrets-7111 deletion completed in 6.428283366s

• [SLOW TEST:12.917 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:56:34.122: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8026f4d7-47ab-4888-bf3e-9298eb112a33
STEP: Creating a pod to test consume configMaps
Mar  1 09:56:34.468: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc" in namespace "projected-2166" to be "success or failure"
Mar  1 09:56:34.473: INFO: Pod "pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.04426ms
Mar  1 09:56:36.480: INFO: Pod "pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01198016s
Mar  1 09:56:38.485: INFO: Pod "pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016821478s
Mar  1 09:56:40.492: INFO: Pod "pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023645078s
STEP: Saw pod success
Mar  1 09:56:40.492: INFO: Pod "pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc" satisfied condition "success or failure"
Mar  1 09:56:40.496: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 09:56:40.591: INFO: Waiting for pod pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc to disappear
Mar  1 09:56:40.596: INFO: Pod pod-projected-configmaps-fec41070-fdf9-444d-a43b-b3c1f98501bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:56:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2166" for this suite.
Mar  1 09:56:48.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:56:48.816: INFO: namespace projected-2166 deletion completed in 8.203224721s

• [SLOW TEST:14.694 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:56:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Mar  1 09:56:49.392: INFO: Waiting up to 5m0s for pod "var-expansion-34340abf-0e15-480d-8e7f-ced205de4447" in namespace "var-expansion-7750" to be "success or failure"
Mar  1 09:56:49.396: INFO: Pod "var-expansion-34340abf-0e15-480d-8e7f-ced205de4447": Phase="Pending", Reason="", readiness=false. Elapsed: 4.178067ms
Mar  1 09:56:51.403: INFO: Pod "var-expansion-34340abf-0e15-480d-8e7f-ced205de4447": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010898947s
Mar  1 09:56:53.409: INFO: Pod "var-expansion-34340abf-0e15-480d-8e7f-ced205de4447": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017132591s
Mar  1 09:56:55.414: INFO: Pod "var-expansion-34340abf-0e15-480d-8e7f-ced205de4447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022356558s
STEP: Saw pod success
Mar  1 09:56:55.414: INFO: Pod "var-expansion-34340abf-0e15-480d-8e7f-ced205de4447" satisfied condition "success or failure"
Mar  1 09:56:55.418: INFO: Trying to get logs from node worker02 pod var-expansion-34340abf-0e15-480d-8e7f-ced205de4447 container dapi-container: <nil>
STEP: delete the pod
Mar  1 09:56:55.511: INFO: Waiting for pod var-expansion-34340abf-0e15-480d-8e7f-ced205de4447 to disappear
Mar  1 09:56:55.515: INFO: Pod var-expansion-34340abf-0e15-480d-8e7f-ced205de4447 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:56:55.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7750" for this suite.
Mar  1 09:57:01.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:57:01.726: INFO: namespace var-expansion-7750 deletion completed in 6.203151417s

• [SLOW TEST:12.909 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:57:01.726: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  1 09:57:02.086: INFO: Waiting up to 5m0s for pod "pod-55922825-327e-4b81-8ad4-522f9cac78c6" in namespace "emptydir-861" to be "success or failure"
Mar  1 09:57:02.109: INFO: Pod "pod-55922825-327e-4b81-8ad4-522f9cac78c6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.383056ms
Mar  1 09:57:04.114: INFO: Pod "pod-55922825-327e-4b81-8ad4-522f9cac78c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027724586s
Mar  1 09:57:06.120: INFO: Pod "pod-55922825-327e-4b81-8ad4-522f9cac78c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033366496s
Mar  1 09:57:08.153: INFO: Pod "pod-55922825-327e-4b81-8ad4-522f9cac78c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066598542s
STEP: Saw pod success
Mar  1 09:57:08.153: INFO: Pod "pod-55922825-327e-4b81-8ad4-522f9cac78c6" satisfied condition "success or failure"
Mar  1 09:57:08.157: INFO: Trying to get logs from node worker02 pod pod-55922825-327e-4b81-8ad4-522f9cac78c6 container test-container: <nil>
STEP: delete the pod
Mar  1 09:57:08.244: INFO: Waiting for pod pod-55922825-327e-4b81-8ad4-522f9cac78c6 to disappear
Mar  1 09:57:08.249: INFO: Pod pod-55922825-327e-4b81-8ad4-522f9cac78c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:57:08.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-861" for this suite.
Mar  1 09:57:14.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:57:14.426: INFO: namespace emptydir-861 deletion completed in 6.169493835s

• [SLOW TEST:12.701 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:57:14.427: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  1 09:57:22.815: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:22.819: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:24.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:24.827: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:26.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:26.825: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:28.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:28.826: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:30.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:30.825: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:32.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:32.825: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:34.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:34.825: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:36.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:36.826: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:38.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:38.825: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:40.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:40.826: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:42.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:42.825: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 09:57:44.820: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 09:57:44.830: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:57:44.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3587" for this suite.
Mar  1 09:58:08.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:58:09.076: INFO: namespace container-lifecycle-hook-3587 deletion completed in 24.225856936s

• [SLOW TEST:54.649 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:58:09.076: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1454
I0301 09:58:09.641400      17 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1454, replica count: 1
I0301 09:58:10.692104      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 09:58:11.692403      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 09:58:12.692666      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 09:58:13.692952      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 09:58:14.693240      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 09:58:14.945: INFO: Created: latency-svc-29pzs
Mar  1 09:58:14.977: INFO: Got endpoints: latency-svc-29pzs [183.919329ms]
Mar  1 09:58:15.152: INFO: Created: latency-svc-gsfx6
Mar  1 09:58:15.263: INFO: Got endpoints: latency-svc-gsfx6 [285.787006ms]
Mar  1 09:58:15.264: INFO: Created: latency-svc-949kx
Mar  1 09:58:15.445: INFO: Got endpoints: latency-svc-949kx [466.788559ms]
Mar  1 09:58:15.461: INFO: Created: latency-svc-lvq6s
Mar  1 09:58:15.557: INFO: Got endpoints: latency-svc-lvq6s [578.562159ms]
Mar  1 09:58:15.605: INFO: Created: latency-svc-fmhhp
Mar  1 09:58:15.628: INFO: Got endpoints: latency-svc-fmhhp [649.910141ms]
Mar  1 09:58:15.781: INFO: Created: latency-svc-jnbgk
Mar  1 09:58:15.807: INFO: Created: latency-svc-5rwg6
Mar  1 09:58:15.808: INFO: Got endpoints: latency-svc-jnbgk [829.126391ms]
Mar  1 09:58:15.914: INFO: Got endpoints: latency-svc-5rwg6 [935.173575ms]
Mar  1 09:58:15.936: INFO: Created: latency-svc-bp4s9
Mar  1 09:58:16.099: INFO: Got endpoints: latency-svc-bp4s9 [1.120563553s]
Mar  1 09:58:16.100: INFO: Created: latency-svc-nwpcx
Mar  1 09:58:16.147: INFO: Got endpoints: latency-svc-nwpcx [1.168103008s]
Mar  1 09:58:16.198: INFO: Created: latency-svc-nn6w8
Mar  1 09:58:16.223: INFO: Got endpoints: latency-svc-nn6w8 [1.245000543s]
Mar  1 09:58:16.299: INFO: Created: latency-svc-dplzb
Mar  1 09:58:16.401: INFO: Got endpoints: latency-svc-dplzb [1.422271699s]
Mar  1 09:58:16.405: INFO: Created: latency-svc-84r5k
Mar  1 09:58:16.506: INFO: Got endpoints: latency-svc-84r5k [1.527650499s]
Mar  1 09:58:16.507: INFO: Created: latency-svc-gcqw8
Mar  1 09:58:16.556: INFO: Got endpoints: latency-svc-gcqw8 [1.577313419s]
Mar  1 09:58:16.649: INFO: Created: latency-svc-mcfqq
Mar  1 09:58:16.720: INFO: Got endpoints: latency-svc-mcfqq [1.741003503s]
Mar  1 09:58:16.766: INFO: Created: latency-svc-sphjh
Mar  1 09:58:16.820: INFO: Got endpoints: latency-svc-sphjh [1.841700857s]
Mar  1 09:58:16.908: INFO: Created: latency-svc-v424r
Mar  1 09:58:16.921: INFO: Got endpoints: latency-svc-v424r [1.942138751s]
Mar  1 09:58:16.981: INFO: Created: latency-svc-z22gq
Mar  1 09:58:17.057: INFO: Got endpoints: latency-svc-z22gq [1.793391503s]
Mar  1 09:58:17.072: INFO: Created: latency-svc-ws4m4
Mar  1 09:58:17.116: INFO: Got endpoints: latency-svc-ws4m4 [1.67127972s]
Mar  1 09:58:17.138: INFO: Created: latency-svc-99w28
Mar  1 09:58:17.257: INFO: Got endpoints: latency-svc-99w28 [1.699758395s]
Mar  1 09:58:17.258: INFO: Created: latency-svc-g99s7
Mar  1 09:58:17.299: INFO: Got endpoints: latency-svc-g99s7 [1.670785567s]
Mar  1 09:58:17.384: INFO: Created: latency-svc-n4b2d
Mar  1 09:58:17.441: INFO: Got endpoints: latency-svc-n4b2d [1.633828692s]
Mar  1 09:58:17.503: INFO: Created: latency-svc-tpdxf
Mar  1 09:58:17.527: INFO: Got endpoints: latency-svc-tpdxf [1.613701857s]
Mar  1 09:58:17.584: INFO: Created: latency-svc-5jvxv
Mar  1 09:58:17.672: INFO: Got endpoints: latency-svc-5jvxv [1.572824287s]
Mar  1 09:58:17.687: INFO: Created: latency-svc-p5vst
Mar  1 09:58:17.808: INFO: Got endpoints: latency-svc-p5vst [1.661279927s]
Mar  1 09:58:17.827: INFO: Created: latency-svc-jt4tt
Mar  1 09:58:17.851: INFO: Got endpoints: latency-svc-jt4tt [1.628330503s]
Mar  1 09:58:17.907: INFO: Created: latency-svc-ngmtz
Mar  1 09:58:17.995: INFO: Got endpoints: latency-svc-ngmtz [1.593990925s]
Mar  1 09:58:17.998: INFO: Created: latency-svc-jl87h
Mar  1 09:58:18.040: INFO: Got endpoints: latency-svc-jl87h [1.533627322s]
Mar  1 09:58:18.140: INFO: Created: latency-svc-gkmm9
Mar  1 09:58:18.188: INFO: Got endpoints: latency-svc-gkmm9 [1.63186953s]
Mar  1 09:58:18.262: INFO: Created: latency-svc-qkl7t
Mar  1 09:58:18.323: INFO: Got endpoints: latency-svc-qkl7t [1.603450241s]
Mar  1 09:58:18.324: INFO: Created: latency-svc-kc8fx
Mar  1 09:58:18.403: INFO: Got endpoints: latency-svc-kc8fx [1.582427272s]
Mar  1 09:58:18.427: INFO: Created: latency-svc-n8dph
Mar  1 09:58:18.467: INFO: Got endpoints: latency-svc-n8dph [1.546125428s]
Mar  1 09:58:18.515: INFO: Created: latency-svc-g2gl9
Mar  1 09:58:18.581: INFO: Got endpoints: latency-svc-g2gl9 [1.524038312s]
Mar  1 09:58:18.765: INFO: Created: latency-svc-fpn57
Mar  1 09:58:18.814: INFO: Got endpoints: latency-svc-fpn57 [1.697601062s]
Mar  1 09:58:18.815: INFO: Created: latency-svc-hr27v
Mar  1 09:58:18.863: INFO: Got endpoints: latency-svc-hr27v [1.605424148s]
Mar  1 09:58:18.958: INFO: Created: latency-svc-9tl4r
Mar  1 09:58:19.092: INFO: Got endpoints: latency-svc-9tl4r [1.792731144s]
Mar  1 09:58:19.092: INFO: Created: latency-svc-4m2bx
Mar  1 09:58:19.109: INFO: Got endpoints: latency-svc-4m2bx [1.667056247s]
Mar  1 09:58:19.178: INFO: Created: latency-svc-5kkh9
Mar  1 09:58:19.231: INFO: Got endpoints: latency-svc-5kkh9 [1.703205251s]
Mar  1 09:58:19.253: INFO: Created: latency-svc-dbkpf
Mar  1 09:58:19.302: INFO: Got endpoints: latency-svc-dbkpf [1.630296816s]
Mar  1 09:58:19.377: INFO: Created: latency-svc-mmmbn
Mar  1 09:58:19.421: INFO: Got endpoints: latency-svc-mmmbn [190.560085ms]
Mar  1 09:58:19.533: INFO: Created: latency-svc-tqj6g
Mar  1 09:58:19.546: INFO: Created: latency-svc-rs7vn
Mar  1 09:58:19.622: INFO: Got endpoints: latency-svc-tqj6g [1.814039952s]
Mar  1 09:58:19.629: INFO: Got endpoints: latency-svc-rs7vn [1.777093127s]
Mar  1 09:58:19.672: INFO: Created: latency-svc-dgmfw
Mar  1 09:58:19.721: INFO: Got endpoints: latency-svc-dgmfw [1.72634452s]
Mar  1 09:58:19.762: INFO: Created: latency-svc-m8djk
Mar  1 09:58:19.832: INFO: Got endpoints: latency-svc-m8djk [1.791635247s]
Mar  1 09:58:19.867: INFO: Created: latency-svc-wdcgj
Mar  1 09:58:19.964: INFO: Got endpoints: latency-svc-wdcgj [1.776023481s]
Mar  1 09:58:19.980: INFO: Created: latency-svc-95crp
Mar  1 09:58:20.019: INFO: Got endpoints: latency-svc-95crp [1.695309499s]
Mar  1 09:58:20.143: INFO: Created: latency-svc-j999k
Mar  1 09:58:20.247: INFO: Got endpoints: latency-svc-j999k [1.843750896s]
Mar  1 09:58:20.262: INFO: Created: latency-svc-zprpq
Mar  1 09:58:20.291: INFO: Got endpoints: latency-svc-zprpq [1.823766324s]
Mar  1 09:58:20.392: INFO: Created: latency-svc-bk584
Mar  1 09:58:20.437: INFO: Got endpoints: latency-svc-bk584 [1.855718766s]
Mar  1 09:58:20.479: INFO: Created: latency-svc-c8zzx
Mar  1 09:58:20.517: INFO: Got endpoints: latency-svc-c8zzx [1.702958772s]
Mar  1 09:58:20.695: INFO: Created: latency-svc-4fs8m
Mar  1 09:58:20.737: INFO: Got endpoints: latency-svc-4fs8m [1.873745259s]
Mar  1 09:58:20.843: INFO: Created: latency-svc-m6vth
Mar  1 09:58:20.887: INFO: Got endpoints: latency-svc-m6vth [1.794249059s]
Mar  1 09:58:20.998: INFO: Created: latency-svc-2z6zk
Mar  1 09:58:21.062: INFO: Got endpoints: latency-svc-2z6zk [1.953260097s]
Mar  1 09:58:21.071: INFO: Created: latency-svc-scmqg
Mar  1 09:58:21.158: INFO: Got endpoints: latency-svc-scmqg [1.855825522s]
Mar  1 09:58:21.218: INFO: Created: latency-svc-t4cps
Mar  1 09:58:21.239: INFO: Created: latency-svc-l4mn7
Mar  1 09:58:21.325: INFO: Got endpoints: latency-svc-l4mn7 [1.702473739s]
Mar  1 09:58:21.325: INFO: Got endpoints: latency-svc-t4cps [1.904037766s]
Mar  1 09:58:21.376: INFO: Created: latency-svc-pkcn9
Mar  1 09:58:21.392: INFO: Got endpoints: latency-svc-pkcn9 [1.762918589s]
Mar  1 09:58:21.502: INFO: Created: latency-svc-74gbs
Mar  1 09:58:21.529: INFO: Got endpoints: latency-svc-74gbs [1.807839615s]
Mar  1 09:58:21.609: INFO: Created: latency-svc-gjl5l
Mar  1 09:58:21.760: INFO: Got endpoints: latency-svc-gjl5l [1.927699295s]
Mar  1 09:58:21.778: INFO: Created: latency-svc-hb9rd
Mar  1 09:58:21.892: INFO: Got endpoints: latency-svc-hb9rd [1.927361675s]
Mar  1 09:58:21.919: INFO: Created: latency-svc-crnmx
Mar  1 09:58:21.977: INFO: Got endpoints: latency-svc-crnmx [1.957977166s]
Mar  1 09:58:21.985: INFO: Created: latency-svc-4j6cs
Mar  1 09:58:22.076: INFO: Got endpoints: latency-svc-4j6cs [1.829358768s]
Mar  1 09:58:22.086: INFO: Created: latency-svc-qkcnl
Mar  1 09:58:22.183: INFO: Got endpoints: latency-svc-qkcnl [1.891710898s]
Mar  1 09:58:22.275: INFO: Created: latency-svc-fjp5s
Mar  1 09:58:22.335: INFO: Got endpoints: latency-svc-fjp5s [1.897713347s]
Mar  1 09:58:22.395: INFO: Created: latency-svc-s99rm
Mar  1 09:58:22.503: INFO: Got endpoints: latency-svc-s99rm [1.986604561s]
Mar  1 09:58:22.529: INFO: Created: latency-svc-4tj9j
Mar  1 09:58:22.608: INFO: Got endpoints: latency-svc-4tj9j [1.870895719s]
Mar  1 09:58:22.684: INFO: Created: latency-svc-4jxpm
Mar  1 09:58:22.748: INFO: Got endpoints: latency-svc-4jxpm [1.861067389s]
Mar  1 09:58:22.749: INFO: Created: latency-svc-tfkqx
Mar  1 09:58:22.792: INFO: Got endpoints: latency-svc-tfkqx [1.72939033s]
Mar  1 09:58:22.889: INFO: Created: latency-svc-5ls7r
Mar  1 09:58:22.984: INFO: Got endpoints: latency-svc-5ls7r [1.825766961s]
Mar  1 09:58:22.984: INFO: Created: latency-svc-pf65h
Mar  1 09:58:23.025: INFO: Got endpoints: latency-svc-pf65h [1.699392172s]
Mar  1 09:58:23.082: INFO: Created: latency-svc-6vt8r
Mar  1 09:58:23.159: INFO: Got endpoints: latency-svc-6vt8r [1.833702624s]
Mar  1 09:58:23.178: INFO: Created: latency-svc-6gxhx
Mar  1 09:58:23.236: INFO: Got endpoints: latency-svc-6gxhx [1.84477807s]
Mar  1 09:58:23.360: INFO: Created: latency-svc-ts8l2
Mar  1 09:58:23.426: INFO: Created: latency-svc-w54g2
Mar  1 09:58:23.426: INFO: Got endpoints: latency-svc-ts8l2 [1.896265694s]
Mar  1 09:58:23.488: INFO: Got endpoints: latency-svc-w54g2 [1.728644827s]
Mar  1 09:58:23.494: INFO: Created: latency-svc-csbtz
Mar  1 09:58:23.572: INFO: Got endpoints: latency-svc-csbtz [1.680782073s]
Mar  1 09:58:23.595: INFO: Created: latency-svc-md2f8
Mar  1 09:58:23.611: INFO: Got endpoints: latency-svc-md2f8 [1.633568652s]
Mar  1 09:58:23.746: INFO: Created: latency-svc-p6rb5
Mar  1 09:58:23.765: INFO: Got endpoints: latency-svc-p6rb5 [1.688975726s]
Mar  1 09:58:23.964: INFO: Created: latency-svc-7kpcs
Mar  1 09:58:24.058: INFO: Got endpoints: latency-svc-7kpcs [1.875219742s]
Mar  1 09:58:24.192: INFO: Created: latency-svc-562fx
Mar  1 09:58:24.206: INFO: Got endpoints: latency-svc-562fx [1.871105771s]
Mar  1 09:58:24.279: INFO: Created: latency-svc-44qhc
Mar  1 09:58:24.378: INFO: Got endpoints: latency-svc-44qhc [1.874738682s]
Mar  1 09:58:24.391: INFO: Created: latency-svc-kr54n
Mar  1 09:58:24.483: INFO: Got endpoints: latency-svc-kr54n [1.875441381s]
Mar  1 09:58:24.551: INFO: Created: latency-svc-jxcgq
Mar  1 09:58:24.608: INFO: Got endpoints: latency-svc-jxcgq [1.860090023s]
Mar  1 09:58:24.615: INFO: Created: latency-svc-8q9gq
Mar  1 09:58:24.643: INFO: Got endpoints: latency-svc-8q9gq [1.851417489s]
Mar  1 09:58:24.701: INFO: Created: latency-svc-ldhfn
Mar  1 09:58:24.793: INFO: Got endpoints: latency-svc-ldhfn [1.808877422s]
Mar  1 09:58:24.810: INFO: Created: latency-svc-dl4qg
Mar  1 09:58:24.878: INFO: Got endpoints: latency-svc-dl4qg [1.852656262s]
Mar  1 09:58:25.003: INFO: Created: latency-svc-snxr9
Mar  1 09:58:25.020: INFO: Got endpoints: latency-svc-snxr9 [1.861158372s]
Mar  1 09:58:25.112: INFO: Created: latency-svc-q92jx
Mar  1 09:58:25.129: INFO: Got endpoints: latency-svc-q92jx [1.89257222s]
Mar  1 09:58:25.192: INFO: Created: latency-svc-s4vmp
Mar  1 09:58:25.253: INFO: Got endpoints: latency-svc-s4vmp [1.827338218s]
Mar  1 09:58:25.347: INFO: Created: latency-svc-xrc74
Mar  1 09:58:25.436: INFO: Got endpoints: latency-svc-xrc74 [1.947358643s]
Mar  1 09:58:25.456: INFO: Created: latency-svc-xl7nl
Mar  1 09:58:25.702: INFO: Got endpoints: latency-svc-xl7nl [2.1297295s]
Mar  1 09:58:25.740: INFO: Created: latency-svc-jlbvj
Mar  1 09:58:25.785: INFO: Got endpoints: latency-svc-jlbvj [2.17437857s]
Mar  1 09:58:25.844: INFO: Created: latency-svc-kmfs4
Mar  1 09:58:25.908: INFO: Got endpoints: latency-svc-kmfs4 [2.142445819s]
Mar  1 09:58:25.909: INFO: Created: latency-svc-n4v96
Mar  1 09:58:25.925: INFO: Got endpoints: latency-svc-n4v96 [1.866106172s]
Mar  1 09:58:26.032: INFO: Created: latency-svc-dp62m
Mar  1 09:58:26.134: INFO: Got endpoints: latency-svc-dp62m [1.927827206s]
Mar  1 09:58:26.150: INFO: Created: latency-svc-76r2d
Mar  1 09:58:26.171: INFO: Got endpoints: latency-svc-76r2d [1.792962546s]
Mar  1 09:58:26.278: INFO: Created: latency-svc-n9t5q
Mar  1 09:58:26.401: INFO: Got endpoints: latency-svc-n9t5q [1.917235219s]
Mar  1 09:58:26.413: INFO: Created: latency-svc-8vnlb
Mar  1 09:58:26.452: INFO: Got endpoints: latency-svc-8vnlb [1.84364926s]
Mar  1 09:58:26.551: INFO: Created: latency-svc-rrqgm
Mar  1 09:58:26.637: INFO: Got endpoints: latency-svc-rrqgm [1.993416775s]
Mar  1 09:58:26.692: INFO: Created: latency-svc-7c6fw
Mar  1 09:58:26.744: INFO: Got endpoints: latency-svc-7c6fw [1.951083998s]
Mar  1 09:58:26.851: INFO: Created: latency-svc-9t8nk
Mar  1 09:58:26.904: INFO: Got endpoints: latency-svc-9t8nk [2.025785743s]
Mar  1 09:58:26.978: INFO: Created: latency-svc-7w5mf
Mar  1 09:58:27.062: INFO: Got endpoints: latency-svc-7w5mf [2.042222675s]
Mar  1 09:58:27.077: INFO: Created: latency-svc-2gdgq
Mar  1 09:58:27.136: INFO: Got endpoints: latency-svc-2gdgq [2.00679305s]
Mar  1 09:58:27.187: INFO: Created: latency-svc-plvz6
Mar  1 09:58:27.277: INFO: Got endpoints: latency-svc-plvz6 [2.023421262s]
Mar  1 09:58:27.292: INFO: Created: latency-svc-jsxsl
Mar  1 09:58:27.344: INFO: Got endpoints: latency-svc-jsxsl [1.907700323s]
Mar  1 09:58:27.429: INFO: Created: latency-svc-r4wf2
Mar  1 09:58:27.479: INFO: Got endpoints: latency-svc-r4wf2 [1.776223078s]
Mar  1 09:58:27.488: INFO: Created: latency-svc-fbbl2
Mar  1 09:58:27.563: INFO: Got endpoints: latency-svc-fbbl2 [1.778314821s]
Mar  1 09:58:27.578: INFO: Created: latency-svc-wnf5c
Mar  1 09:58:27.636: INFO: Got endpoints: latency-svc-wnf5c [1.727847074s]
Mar  1 09:58:27.693: INFO: Created: latency-svc-smq2b
Mar  1 09:58:27.744: INFO: Got endpoints: latency-svc-smq2b [1.819412998s]
Mar  1 09:58:27.778: INFO: Created: latency-svc-bz8tr
Mar  1 09:58:27.918: INFO: Got endpoints: latency-svc-bz8tr [1.783795433s]
Mar  1 09:58:27.938: INFO: Created: latency-svc-2tbzk
Mar  1 09:58:27.994: INFO: Got endpoints: latency-svc-2tbzk [1.822911161s]
Mar  1 09:58:28.057: INFO: Created: latency-svc-4sqxk
Mar  1 09:58:28.103: INFO: Got endpoints: latency-svc-4sqxk [1.702567985s]
Mar  1 09:58:28.120: INFO: Created: latency-svc-5jsjc
Mar  1 09:58:28.190: INFO: Got endpoints: latency-svc-5jsjc [1.737867733s]
Mar  1 09:58:28.253: INFO: Created: latency-svc-bbkj4
Mar  1 09:58:28.287: INFO: Got endpoints: latency-svc-bbkj4 [1.650416022s]
Mar  1 09:58:28.341: INFO: Created: latency-svc-sstlt
Mar  1 09:58:28.398: INFO: Got endpoints: latency-svc-sstlt [1.653626868s]
Mar  1 09:58:28.401: INFO: Created: latency-svc-tjthf
Mar  1 09:58:28.472: INFO: Got endpoints: latency-svc-tjthf [1.56831973s]
Mar  1 09:58:28.492: INFO: Created: latency-svc-ssh9w
Mar  1 09:58:28.548: INFO: Got endpoints: latency-svc-ssh9w [1.485447414s]
Mar  1 09:58:28.593: INFO: Created: latency-svc-2c6cq
Mar  1 09:58:28.688: INFO: Created: latency-svc-2jgrj
Mar  1 09:58:28.689: INFO: Got endpoints: latency-svc-2c6cq [1.552483017s]
Mar  1 09:58:28.785: INFO: Got endpoints: latency-svc-2jgrj [1.50845028s]
Mar  1 09:58:28.796: INFO: Created: latency-svc-ndn2p
Mar  1 09:58:28.884: INFO: Got endpoints: latency-svc-ndn2p [1.540316345s]
Mar  1 09:58:28.914: INFO: Created: latency-svc-t5dx5
Mar  1 09:58:28.946: INFO: Got endpoints: latency-svc-t5dx5 [1.467436766s]
Mar  1 09:58:29.067: INFO: Created: latency-svc-7gms4
Mar  1 09:58:29.085: INFO: Got endpoints: latency-svc-7gms4 [1.5213274s]
Mar  1 09:58:29.191: INFO: Created: latency-svc-smptw
Mar  1 09:58:29.237: INFO: Got endpoints: latency-svc-smptw [1.601274921s]
Mar  1 09:58:29.238: INFO: Created: latency-svc-w2nkj
Mar  1 09:58:29.253: INFO: Got endpoints: latency-svc-w2nkj [1.509011523s]
Mar  1 09:58:29.452: INFO: Created: latency-svc-hjjv4
Mar  1 09:58:29.471: INFO: Created: latency-svc-knw5b
Mar  1 09:58:29.529: INFO: Got endpoints: latency-svc-hjjv4 [1.610647501s]
Mar  1 09:58:29.543: INFO: Got endpoints: latency-svc-knw5b [1.548572095s]
Mar  1 09:58:29.577: INFO: Created: latency-svc-9cjpz
Mar  1 09:58:29.637: INFO: Got endpoints: latency-svc-9cjpz [1.533584169s]
Mar  1 09:58:29.656: INFO: Created: latency-svc-8hj7f
Mar  1 09:58:29.731: INFO: Got endpoints: latency-svc-8hj7f [1.540916959s]
Mar  1 09:58:29.736: INFO: Created: latency-svc-5zpkl
Mar  1 09:58:29.770: INFO: Got endpoints: latency-svc-5zpkl [1.482662218s]
Mar  1 09:58:29.875: INFO: Created: latency-svc-fz9gq
Mar  1 09:58:29.902: INFO: Got endpoints: latency-svc-fz9gq [1.503828537s]
Mar  1 09:58:29.955: INFO: Created: latency-svc-hng98
Mar  1 09:58:30.045: INFO: Got endpoints: latency-svc-hng98 [1.573370806s]
Mar  1 09:58:30.055: INFO: Created: latency-svc-pz6x2
Mar  1 09:58:30.095: INFO: Got endpoints: latency-svc-pz6x2 [1.547143418s]
Mar  1 09:58:30.152: INFO: Created: latency-svc-wpgbz
Mar  1 09:58:30.187: INFO: Got endpoints: latency-svc-wpgbz [1.498668917s]
Mar  1 09:58:30.213: INFO: Created: latency-svc-6crth
Mar  1 09:58:30.304: INFO: Got endpoints: latency-svc-6crth [1.518582773s]
Mar  1 09:58:30.319: INFO: Created: latency-svc-gt2lg
Mar  1 09:58:30.398: INFO: Got endpoints: latency-svc-gt2lg [1.51369436s]
Mar  1 09:58:30.480: INFO: Created: latency-svc-j2fpd
Mar  1 09:58:30.529: INFO: Got endpoints: latency-svc-j2fpd [1.582914516s]
Mar  1 09:58:30.569: INFO: Created: latency-svc-nv6dn
Mar  1 09:58:30.630: INFO: Got endpoints: latency-svc-nv6dn [1.544871778s]
Mar  1 09:58:30.727: INFO: Created: latency-svc-85hcx
Mar  1 09:58:30.772: INFO: Got endpoints: latency-svc-85hcx [1.534415465s]
Mar  1 09:58:30.772: INFO: Created: latency-svc-f976k
Mar  1 09:58:30.823: INFO: Got endpoints: latency-svc-f976k [1.570220396s]
Mar  1 09:58:30.878: INFO: Created: latency-svc-mwsd9
Mar  1 09:58:30.950: INFO: Got endpoints: latency-svc-mwsd9 [1.421410778s]
Mar  1 09:58:30.976: INFO: Created: latency-svc-7mrts
Mar  1 09:58:31.080: INFO: Got endpoints: latency-svc-7mrts [1.536489455s]
Mar  1 09:58:31.096: INFO: Created: latency-svc-rhlgl
Mar  1 09:58:31.159: INFO: Got endpoints: latency-svc-rhlgl [1.522402296s]
Mar  1 09:58:31.197: INFO: Created: latency-svc-m6n6b
Mar  1 09:58:31.302: INFO: Got endpoints: latency-svc-m6n6b [1.57129134s]
Mar  1 09:58:31.317: INFO: Created: latency-svc-58jz5
Mar  1 09:58:31.369: INFO: Got endpoints: latency-svc-58jz5 [1.598881735s]
Mar  1 09:58:31.396: INFO: Created: latency-svc-nvjqg
Mar  1 09:58:31.577: INFO: Got endpoints: latency-svc-nvjqg [1.675123014s]
Mar  1 09:58:31.577: INFO: Created: latency-svc-cl4xg
Mar  1 09:58:31.661: INFO: Got endpoints: latency-svc-cl4xg [1.615591757s]
Mar  1 09:58:31.667: INFO: Created: latency-svc-rrw5d
Mar  1 09:58:31.780: INFO: Got endpoints: latency-svc-rrw5d [1.685017933s]
Mar  1 09:58:31.780: INFO: Created: latency-svc-49jsl
Mar  1 09:58:31.885: INFO: Got endpoints: latency-svc-49jsl [1.697306225s]
Mar  1 09:58:31.910: INFO: Created: latency-svc-gcr6c
Mar  1 09:58:31.980: INFO: Got endpoints: latency-svc-gcr6c [1.675818767s]
Mar  1 09:58:32.052: INFO: Created: latency-svc-68shg
Mar  1 09:58:32.106: INFO: Created: latency-svc-rmgpl
Mar  1 09:58:32.123: INFO: Got endpoints: latency-svc-68shg [1.72502785s]
Mar  1 09:58:32.133: INFO: Got endpoints: latency-svc-rmgpl [1.603865135s]
Mar  1 09:58:32.142: INFO: Created: latency-svc-kklqz
Mar  1 09:58:32.202: INFO: Got endpoints: latency-svc-kklqz [1.571744983s]
Mar  1 09:58:32.264: INFO: Created: latency-svc-9ldsb
Mar  1 09:58:32.353: INFO: Got endpoints: latency-svc-9ldsb [1.581231203s]
Mar  1 09:58:32.353: INFO: Created: latency-svc-sgq8d
Mar  1 09:58:32.427: INFO: Got endpoints: latency-svc-sgq8d [1.603669964s]
Mar  1 09:58:32.431: INFO: Created: latency-svc-l8c4l
Mar  1 09:58:32.513: INFO: Got endpoints: latency-svc-l8c4l [1.563221627s]
Mar  1 09:58:32.572: INFO: Created: latency-svc-6nwvp
Mar  1 09:58:32.594: INFO: Got endpoints: latency-svc-6nwvp [1.514142794s]
Mar  1 09:58:32.675: INFO: Created: latency-svc-p7tph
Mar  1 09:58:32.704: INFO: Got endpoints: latency-svc-p7tph [1.543998888s]
Mar  1 09:58:32.722: INFO: Created: latency-svc-bq6dq
Mar  1 09:58:32.827: INFO: Got endpoints: latency-svc-bq6dq [1.524807049s]
Mar  1 09:58:32.856: INFO: Created: latency-svc-tb46w
Mar  1 09:58:32.908: INFO: Got endpoints: latency-svc-tb46w [1.539263995s]
Mar  1 09:58:33.137: INFO: Created: latency-svc-zddtr
Mar  1 09:58:33.165: INFO: Got endpoints: latency-svc-zddtr [1.588521109s]
Mar  1 09:58:33.166: INFO: Created: latency-svc-s8dt2
Mar  1 09:58:33.221: INFO: Got endpoints: latency-svc-s8dt2 [1.560191763s]
Mar  1 09:58:33.281: INFO: Created: latency-svc-lx2nt
Mar  1 09:58:33.363: INFO: Created: latency-svc-fzx2t
Mar  1 09:58:33.433: INFO: Got endpoints: latency-svc-lx2nt [1.652905822s]
Mar  1 09:58:33.586: INFO: Got endpoints: latency-svc-fzx2t [1.700730442s]
Mar  1 09:58:33.606: INFO: Created: latency-svc-lgmzg
Mar  1 09:58:33.719: INFO: Got endpoints: latency-svc-lgmzg [1.73875585s]
Mar  1 09:58:33.721: INFO: Created: latency-svc-8d7p4
Mar  1 09:58:33.735: INFO: Got endpoints: latency-svc-8d7p4 [1.612256584s]
Mar  1 09:58:33.795: INFO: Created: latency-svc-7g6bj
Mar  1 09:58:33.883: INFO: Got endpoints: latency-svc-7g6bj [1.749607475s]
Mar  1 09:58:34.026: INFO: Created: latency-svc-xg4l8
Mar  1 09:58:34.092: INFO: Got endpoints: latency-svc-xg4l8 [1.889973434s]
Mar  1 09:58:34.100: INFO: Created: latency-svc-grqhw
Mar  1 09:58:34.112: INFO: Got endpoints: latency-svc-grqhw [1.759123505s]
Mar  1 09:58:34.192: INFO: Created: latency-svc-qgqht
Mar  1 09:58:34.311: INFO: Got endpoints: latency-svc-qgqht [1.883272001s]
Mar  1 09:58:34.312: INFO: Created: latency-svc-c9h77
Mar  1 09:58:34.355: INFO: Got endpoints: latency-svc-c9h77 [1.841512634s]
Mar  1 09:58:34.406: INFO: Created: latency-svc-p256r
Mar  1 09:58:34.450: INFO: Got endpoints: latency-svc-p256r [1.856062976s]
Mar  1 09:58:34.530: INFO: Created: latency-svc-75rfk
Mar  1 09:58:34.550: INFO: Created: latency-svc-vhjt5
Mar  1 09:58:34.647: INFO: Got endpoints: latency-svc-75rfk [1.943858971s]
Mar  1 09:58:34.648: INFO: Got endpoints: latency-svc-vhjt5 [1.821192155s]
Mar  1 09:58:34.666: INFO: Created: latency-svc-rwklp
Mar  1 09:58:34.692: INFO: Got endpoints: latency-svc-rwklp [1.783293784s]
Mar  1 09:58:34.818: INFO: Created: latency-svc-qw9s5
Mar  1 09:58:34.864: INFO: Got endpoints: latency-svc-qw9s5 [1.698339245s]
Mar  1 09:58:34.897: INFO: Created: latency-svc-ctf2x
Mar  1 09:58:34.955: INFO: Got endpoints: latency-svc-ctf2x [1.73375746s]
Mar  1 09:58:35.050: INFO: Created: latency-svc-drk4f
Mar  1 09:58:35.120: INFO: Created: latency-svc-sr5wh
Mar  1 09:58:35.121: INFO: Got endpoints: latency-svc-drk4f [1.687389176s]
Mar  1 09:58:35.137: INFO: Got endpoints: latency-svc-sr5wh [1.551263394s]
Mar  1 09:58:35.227: INFO: Created: latency-svc-q7xb2
Mar  1 09:58:35.265: INFO: Got endpoints: latency-svc-q7xb2 [1.545992481s]
Mar  1 09:58:35.380: INFO: Created: latency-svc-dpzsr
Mar  1 09:58:35.441: INFO: Got endpoints: latency-svc-dpzsr [1.705440365s]
Mar  1 09:58:35.511: INFO: Created: latency-svc-pxvst
Mar  1 09:58:35.564: INFO: Created: latency-svc-5v5jb
Mar  1 09:58:35.576: INFO: Got endpoints: latency-svc-pxvst [1.692787539s]
Mar  1 09:58:35.589: INFO: Got endpoints: latency-svc-5v5jb [1.497187554s]
Mar  1 09:58:35.598: INFO: Created: latency-svc-qwn74
Mar  1 09:58:35.653: INFO: Got endpoints: latency-svc-qwn74 [1.540098165s]
Mar  1 09:58:35.688: INFO: Created: latency-svc-n6k2v
Mar  1 09:58:35.787: INFO: Got endpoints: latency-svc-n6k2v [1.476062615s]
Mar  1 09:58:35.857: INFO: Created: latency-svc-p44fl
Mar  1 09:58:35.934: INFO: Got endpoints: latency-svc-p44fl [1.578674286s]
Mar  1 09:58:35.961: INFO: Created: latency-svc-t5r6f
Mar  1 09:58:36.015: INFO: Got endpoints: latency-svc-t5r6f [1.564880583s]
Mar  1 09:58:36.092: INFO: Created: latency-svc-z98qn
Mar  1 09:58:36.156: INFO: Got endpoints: latency-svc-z98qn [1.508902733s]
Mar  1 09:58:36.264: INFO: Created: latency-svc-gvjzn
Mar  1 09:58:36.295: INFO: Got endpoints: latency-svc-gvjzn [1.646797349s]
Mar  1 09:58:36.296: INFO: Created: latency-svc-5mhk9
Mar  1 09:58:36.395: INFO: Got endpoints: latency-svc-5mhk9 [1.703078982s]
Mar  1 09:58:36.409: INFO: Created: latency-svc-ftcnc
Mar  1 09:58:36.481: INFO: Got endpoints: latency-svc-ftcnc [1.616882626s]
Mar  1 09:58:36.545: INFO: Created: latency-svc-xgqgz
Mar  1 09:58:36.596: INFO: Got endpoints: latency-svc-xgqgz [1.640952373s]
Mar  1 09:58:36.670: INFO: Created: latency-svc-s259n
Mar  1 09:58:36.747: INFO: Got endpoints: latency-svc-s259n [1.626130939s]
Mar  1 09:58:36.767: INFO: Created: latency-svc-2f6rb
Mar  1 09:58:36.849: INFO: Got endpoints: latency-svc-2f6rb [1.711579421s]
Mar  1 09:58:36.853: INFO: Created: latency-svc-h65nx
Mar  1 09:58:36.979: INFO: Created: latency-svc-rtrvk
Mar  1 09:58:36.979: INFO: Got endpoints: latency-svc-h65nx [1.714232605s]
Mar  1 09:58:37.111: INFO: Got endpoints: latency-svc-rtrvk [1.670515391s]
Mar  1 09:58:37.113: INFO: Created: latency-svc-fp75w
Mar  1 09:58:37.252: INFO: Got endpoints: latency-svc-fp75w [1.675886581s]
Mar  1 09:58:37.252: INFO: Created: latency-svc-skpn5
Mar  1 09:58:37.281: INFO: Got endpoints: latency-svc-skpn5 [1.691643682s]
Mar  1 09:58:37.385: INFO: Created: latency-svc-vn5nt
Mar  1 09:58:37.405: INFO: Got endpoints: latency-svc-vn5nt [1.752468226s]
Mar  1 09:58:37.457: INFO: Created: latency-svc-bmx28
Mar  1 09:58:37.526: INFO: Got endpoints: latency-svc-bmx28 [1.739238676s]
Mar  1 09:58:37.542: INFO: Created: latency-svc-xgqtr
Mar  1 09:58:37.604: INFO: Got endpoints: latency-svc-xgqtr [1.67037147s]
Mar  1 09:58:37.622: INFO: Created: latency-svc-986td
Mar  1 09:58:37.739: INFO: Got endpoints: latency-svc-986td [1.724358817s]
Mar  1 09:58:37.755: INFO: Created: latency-svc-f9zrw
Mar  1 09:58:37.828: INFO: Got endpoints: latency-svc-f9zrw [1.671857301s]
Mar  1 09:58:37.872: INFO: Created: latency-svc-57bc5
Mar  1 09:58:37.895: INFO: Got endpoints: latency-svc-57bc5 [1.600157391s]
Mar  1 09:58:37.895: INFO: Latencies: [190.560085ms 285.787006ms 466.788559ms 578.562159ms 649.910141ms 829.126391ms 935.173575ms 1.120563553s 1.168103008s 1.245000543s 1.421410778s 1.422271699s 1.467436766s 1.476062615s 1.482662218s 1.485447414s 1.497187554s 1.498668917s 1.503828537s 1.50845028s 1.508902733s 1.509011523s 1.51369436s 1.514142794s 1.518582773s 1.5213274s 1.522402296s 1.524038312s 1.524807049s 1.527650499s 1.533584169s 1.533627322s 1.534415465s 1.536489455s 1.539263995s 1.540098165s 1.540316345s 1.540916959s 1.543998888s 1.544871778s 1.545992481s 1.546125428s 1.547143418s 1.548572095s 1.551263394s 1.552483017s 1.560191763s 1.563221627s 1.564880583s 1.56831973s 1.570220396s 1.57129134s 1.571744983s 1.572824287s 1.573370806s 1.577313419s 1.578674286s 1.581231203s 1.582427272s 1.582914516s 1.588521109s 1.593990925s 1.598881735s 1.600157391s 1.601274921s 1.603450241s 1.603669964s 1.603865135s 1.605424148s 1.610647501s 1.612256584s 1.613701857s 1.615591757s 1.616882626s 1.626130939s 1.628330503s 1.630296816s 1.63186953s 1.633568652s 1.633828692s 1.640952373s 1.646797349s 1.650416022s 1.652905822s 1.653626868s 1.661279927s 1.667056247s 1.67037147s 1.670515391s 1.670785567s 1.67127972s 1.671857301s 1.675123014s 1.675818767s 1.675886581s 1.680782073s 1.685017933s 1.687389176s 1.688975726s 1.691643682s 1.692787539s 1.695309499s 1.697306225s 1.697601062s 1.698339245s 1.699392172s 1.699758395s 1.700730442s 1.702473739s 1.702567985s 1.702958772s 1.703078982s 1.703205251s 1.705440365s 1.711579421s 1.714232605s 1.724358817s 1.72502785s 1.72634452s 1.727847074s 1.728644827s 1.72939033s 1.73375746s 1.737867733s 1.73875585s 1.739238676s 1.741003503s 1.749607475s 1.752468226s 1.759123505s 1.762918589s 1.776023481s 1.776223078s 1.777093127s 1.778314821s 1.783293784s 1.783795433s 1.791635247s 1.792731144s 1.792962546s 1.793391503s 1.794249059s 1.807839615s 1.808877422s 1.814039952s 1.819412998s 1.821192155s 1.822911161s 1.823766324s 1.825766961s 1.827338218s 1.829358768s 1.833702624s 1.841512634s 1.841700857s 1.84364926s 1.843750896s 1.84477807s 1.851417489s 1.852656262s 1.855718766s 1.855825522s 1.856062976s 1.860090023s 1.861067389s 1.861158372s 1.866106172s 1.870895719s 1.871105771s 1.873745259s 1.874738682s 1.875219742s 1.875441381s 1.883272001s 1.889973434s 1.891710898s 1.89257222s 1.896265694s 1.897713347s 1.904037766s 1.907700323s 1.917235219s 1.927361675s 1.927699295s 1.927827206s 1.942138751s 1.943858971s 1.947358643s 1.951083998s 1.953260097s 1.957977166s 1.986604561s 1.993416775s 2.00679305s 2.023421262s 2.025785743s 2.042222675s 2.1297295s 2.142445819s 2.17437857s]
Mar  1 09:58:37.896: INFO: 50 %ile: 1.692787539s
Mar  1 09:58:37.896: INFO: 90 %ile: 1.907700323s
Mar  1 09:58:37.896: INFO: 99 %ile: 2.142445819s
Mar  1 09:58:37.896: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:58:37.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1454" for this suite.
Mar  1 09:59:17.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:59:18.449: INFO: namespace svc-latency-1454 deletion completed in 40.546014477s

• [SLOW TEST:69.372 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:59:18.450: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 09:59:19.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5" in namespace "downward-api-7989" to be "success or failure"
Mar  1 09:59:19.057: INFO: Pod "downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007174ms
Mar  1 09:59:21.176: INFO: Pod "downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.153082088s
Mar  1 09:59:23.299: INFO: Pod "downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275982722s
Mar  1 09:59:25.371: INFO: Pod "downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.348409675s
STEP: Saw pod success
Mar  1 09:59:25.371: INFO: Pod "downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5" satisfied condition "success or failure"
Mar  1 09:59:25.400: INFO: Trying to get logs from node worker02 pod downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5 container client-container: <nil>
STEP: delete the pod
Mar  1 09:59:25.546: INFO: Waiting for pod downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5 to disappear
Mar  1 09:59:25.550: INFO: Pod downwardapi-volume-e674ca4d-c51a-4ae0-8668-a3d1782992d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:59:25.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7989" for this suite.
Mar  1 09:59:31.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:59:31.900: INFO: namespace downward-api-7989 deletion completed in 6.333168413s

• [SLOW TEST:13.450 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:59:31.901: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8883
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  1 09:59:32.313: INFO: Waiting up to 5m0s for pod "pod-f237a854-ad46-472c-a824-ae1fdc44e4b0" in namespace "emptydir-8883" to be "success or failure"
Mar  1 09:59:32.350: INFO: Pod "pod-f237a854-ad46-472c-a824-ae1fdc44e4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.947051ms
Mar  1 09:59:34.356: INFO: Pod "pod-f237a854-ad46-472c-a824-ae1fdc44e4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042692905s
Mar  1 09:59:36.362: INFO: Pod "pod-f237a854-ad46-472c-a824-ae1fdc44e4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049065922s
Mar  1 09:59:38.602: INFO: Pod "pod-f237a854-ad46-472c-a824-ae1fdc44e4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.288919035s
STEP: Saw pod success
Mar  1 09:59:38.602: INFO: Pod "pod-f237a854-ad46-472c-a824-ae1fdc44e4b0" satisfied condition "success or failure"
Mar  1 09:59:38.655: INFO: Trying to get logs from node worker02 pod pod-f237a854-ad46-472c-a824-ae1fdc44e4b0 container test-container: <nil>
STEP: delete the pod
Mar  1 09:59:38.877: INFO: Waiting for pod pod-f237a854-ad46-472c-a824-ae1fdc44e4b0 to disappear
Mar  1 09:59:38.884: INFO: Pod pod-f237a854-ad46-472c-a824-ae1fdc44e4b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:59:38.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8883" for this suite.
Mar  1 09:59:44.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 09:59:45.168: INFO: namespace emptydir-8883 deletion completed in 6.275927726s

• [SLOW TEST:13.268 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 09:59:45.169: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  1 09:59:49.521: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-4aa807b7-3a1b-449f-84f6-2883110a6d6f,GenerateName:,Namespace:events-3862,SelfLink:/api/v1/namespaces/events-3862/pods/send-events-4aa807b7-3a1b-449f-84f6-2883110a6d6f,UID:df91f84f-d0f6-4f9c-8fd3-4b8f054ab89d,ResourceVersion:199960,Generation:0,CreationTimestamp:2020-03-01 09:59:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 437399768,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ms5gr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ms5gr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p 172.20.8.7/library/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-ms5gr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af3590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af35b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:59:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:59:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:59:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 09:59:45 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.84,StartTime:2020-03-01 09:59:45 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-03-01 09:59:48 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/serve-hostname:1.1 docker-pullable://172.20.8.7/library/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 docker://216ef4cf2a2b4648d1e1255e1e195fa34adff397f8d817c1031507946d9665e9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Mar  1 09:59:51.535: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  1 09:59:53.543: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 09:59:53.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3862" for this suite.
Mar  1 10:00:35.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:00:36.006: INFO: namespace events-3862 deletion completed in 42.33434587s

• [SLOW TEST:50.837 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:00:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6550.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6550.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6550.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6550.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6550.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6550.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 10:00:42.470: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-6550/dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d: the server could not find the requested resource (get pods dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d)
Mar  1 10:00:42.478: INFO: Unable to read jessie_udp@PodARecord from pod dns-6550/dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d: the server could not find the requested resource (get pods dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d)
Mar  1 10:00:42.484: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6550/dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d: the server could not find the requested resource (get pods dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d)
Mar  1 10:00:42.484: INFO: Lookups using dns-6550/dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar  1 10:00:47.538: INFO: DNS probes using dns-6550/dns-test-517f8eaa-f9ed-40b3-bb29-77309b54791d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:00:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6550" for this suite.
Mar  1 10:00:53.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:00:53.826: INFO: namespace dns-6550 deletion completed in 6.200722003s

• [SLOW TEST:17.819 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:00:53.826: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l6tjg in namespace proxy-5082
I0301 10:00:54.524186      17 runners.go:180] Created replication controller with name: proxy-service-l6tjg, namespace: proxy-5082, replica count: 1
I0301 10:00:55.575123      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 10:00:56.575437      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 10:00:57.575775      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 10:00:58.576179      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 10:00:59.576555      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 10:01:00.576957      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 10:01:01.577360      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 10:01:02.577835      17 runners.go:180] proxy-service-l6tjg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 10:01:02.583: INFO: setup took 8.279778315s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  1 10:01:02.592: INFO: (0) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.525754ms)
Mar  1 10:01:02.597: INFO: (0) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 13.607038ms)
Mar  1 10:01:02.597: INFO: (0) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 13.427149ms)
Mar  1 10:01:02.608: INFO: (0) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 24.587715ms)
Mar  1 10:01:02.608: INFO: (0) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 24.602323ms)
Mar  1 10:01:02.608: INFO: (0) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 24.565631ms)
Mar  1 10:01:02.609: INFO: (0) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 24.859649ms)
Mar  1 10:01:02.609: INFO: (0) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 25.087129ms)
Mar  1 10:01:02.609: INFO: (0) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 25.228363ms)
Mar  1 10:01:02.609: INFO: (0) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 25.337282ms)
Mar  1 10:01:02.610: INFO: (0) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 27.305587ms)
Mar  1 10:01:02.612: INFO: (0) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 28.405793ms)
Mar  1 10:01:02.612: INFO: (0) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 28.522229ms)
Mar  1 10:01:02.613: INFO: (0) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 28.603265ms)
Mar  1 10:01:02.619: INFO: (0) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 34.745982ms)
Mar  1 10:01:02.619: INFO: (0) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 34.858201ms)
Mar  1 10:01:02.625: INFO: (1) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 6.698507ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 11.80189ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 12.161761ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.69894ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 12.289382ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 11.547296ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.441046ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.915943ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 11.838656ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 12.020952ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 11.82293ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 11.602741ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.882419ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 12.443779ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.498273ms)
Mar  1 10:01:02.632: INFO: (1) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 12.03727ms)
Mar  1 10:01:02.642: INFO: (2) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.995661ms)
Mar  1 10:01:02.642: INFO: (2) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 7.617621ms)
Mar  1 10:01:02.642: INFO: (2) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 7.810312ms)
Mar  1 10:01:02.643: INFO: (2) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.407331ms)
Mar  1 10:01:02.643: INFO: (2) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 8.833397ms)
Mar  1 10:01:02.643: INFO: (2) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.336108ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 11.792896ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 10.491138ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 10.239002ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 11.32012ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 10.470706ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 11.818086ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 11.590143ms)
Mar  1 10:01:02.645: INFO: (2) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.237323ms)
Mar  1 10:01:02.646: INFO: (2) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 11.883296ms)
Mar  1 10:01:02.646: INFO: (2) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 11.570819ms)
Mar  1 10:01:02.653: INFO: (3) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 7.442966ms)
Mar  1 10:01:02.656: INFO: (3) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 9.617813ms)
Mar  1 10:01:02.657: INFO: (3) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 10.324999ms)
Mar  1 10:01:02.658: INFO: (3) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 11.008943ms)
Mar  1 10:01:02.658: INFO: (3) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 10.695799ms)
Mar  1 10:01:02.658: INFO: (3) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 10.494175ms)
Mar  1 10:01:02.658: INFO: (3) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 11.544972ms)
Mar  1 10:01:02.658: INFO: (3) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 11.373949ms)
Mar  1 10:01:02.659: INFO: (3) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.122433ms)
Mar  1 10:01:02.659: INFO: (3) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 11.461358ms)
Mar  1 10:01:02.659: INFO: (3) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.967072ms)
Mar  1 10:01:02.659: INFO: (3) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.787799ms)
Mar  1 10:01:02.662: INFO: (3) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 14.624856ms)
Mar  1 10:01:02.662: INFO: (3) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 14.895732ms)
Mar  1 10:01:02.662: INFO: (3) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 15.218856ms)
Mar  1 10:01:02.663: INFO: (3) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 15.930449ms)
Mar  1 10:01:02.670: INFO: (4) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 6.722432ms)
Mar  1 10:01:02.673: INFO: (4) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 7.579725ms)
Mar  1 10:01:02.673: INFO: (4) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.705762ms)
Mar  1 10:01:02.674: INFO: (4) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.665308ms)
Mar  1 10:01:02.674: INFO: (4) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 9.102853ms)
Mar  1 10:01:02.674: INFO: (4) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 9.964273ms)
Mar  1 10:01:02.674: INFO: (4) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 10.442548ms)
Mar  1 10:01:02.675: INFO: (4) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.693766ms)
Mar  1 10:01:02.675: INFO: (4) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 10.201104ms)
Mar  1 10:01:02.675: INFO: (4) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 10.64811ms)
Mar  1 10:01:02.675: INFO: (4) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 10.388132ms)
Mar  1 10:01:02.676: INFO: (4) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 11.658176ms)
Mar  1 10:01:02.676: INFO: (4) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 11.335309ms)
Mar  1 10:01:02.676: INFO: (4) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 11.487738ms)
Mar  1 10:01:02.678: INFO: (4) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 12.813515ms)
Mar  1 10:01:02.678: INFO: (4) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 13.370841ms)
Mar  1 10:01:02.685: INFO: (5) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 6.632442ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.853513ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 12.111183ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 11.891309ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 11.988837ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 12.285076ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 12.181236ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 12.129879ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 11.951926ms)
Mar  1 10:01:02.691: INFO: (5) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 12.185959ms)
Mar  1 10:01:02.692: INFO: (5) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 12.921286ms)
Mar  1 10:01:02.692: INFO: (5) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 13.149946ms)
Mar  1 10:01:02.693: INFO: (5) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 14.585478ms)
Mar  1 10:01:02.694: INFO: (5) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 15.41581ms)
Mar  1 10:01:02.695: INFO: (5) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 16.040878ms)
Mar  1 10:01:02.695: INFO: (5) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 16.266376ms)
Mar  1 10:01:02.702: INFO: (6) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 6.650331ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 9.01341ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 9.14624ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.171019ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 9.507643ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 9.247488ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.791603ms)
Mar  1 10:01:02.705: INFO: (6) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.901189ms)
Mar  1 10:01:02.706: INFO: (6) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 10.564623ms)
Mar  1 10:01:02.706: INFO: (6) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 10.422622ms)
Mar  1 10:01:02.706: INFO: (6) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 9.787957ms)
Mar  1 10:01:02.706: INFO: (6) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 10.58595ms)
Mar  1 10:01:02.708: INFO: (6) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 12.610639ms)
Mar  1 10:01:02.708: INFO: (6) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 12.773848ms)
Mar  1 10:01:02.708: INFO: (6) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.674158ms)
Mar  1 10:01:02.709: INFO: (6) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 13.025112ms)
Mar  1 10:01:02.715: INFO: (7) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 6.20235ms)
Mar  1 10:01:02.717: INFO: (7) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 7.139922ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 8.769956ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 8.169899ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.32669ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.228699ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 8.87959ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.612549ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 9.212816ms)
Mar  1 10:01:02.718: INFO: (7) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.869321ms)
Mar  1 10:01:02.719: INFO: (7) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 9.013576ms)
Mar  1 10:01:02.721: INFO: (7) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 12.025973ms)
Mar  1 10:01:02.725: INFO: (7) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 15.549659ms)
Mar  1 10:01:02.726: INFO: (7) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 16.570676ms)
Mar  1 10:01:02.726: INFO: (7) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 16.878442ms)
Mar  1 10:01:02.726: INFO: (7) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 17.301956ms)
Mar  1 10:01:02.734: INFO: (8) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 7.870425ms)
Mar  1 10:01:02.735: INFO: (8) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 8.650015ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 9.201777ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 9.060898ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.909616ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.040603ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 8.923168ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.307038ms)
Mar  1 10:01:02.736: INFO: (8) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 9.428322ms)
Mar  1 10:01:02.737: INFO: (8) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 10.676199ms)
Mar  1 10:01:02.737: INFO: (8) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 10.839232ms)
Mar  1 10:01:02.738: INFO: (8) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 10.554385ms)
Mar  1 10:01:02.738: INFO: (8) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.102499ms)
Mar  1 10:01:02.739: INFO: (8) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 12.18235ms)
Mar  1 10:01:02.739: INFO: (8) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.468933ms)
Mar  1 10:01:02.740: INFO: (8) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.946553ms)
Mar  1 10:01:02.747: INFO: (9) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 6.97975ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.225242ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 10.035307ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.192919ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 9.076982ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 9.633724ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.501886ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 10.113617ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 9.908713ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 10.098568ms)
Mar  1 10:01:02.750: INFO: (9) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 9.506922ms)
Mar  1 10:01:02.751: INFO: (9) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.461222ms)
Mar  1 10:01:02.751: INFO: (9) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 10.82274ms)
Mar  1 10:01:02.752: INFO: (9) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 11.577348ms)
Mar  1 10:01:02.752: INFO: (9) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 12.13378ms)
Mar  1 10:01:02.752: INFO: (9) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.051193ms)
Mar  1 10:01:02.759: INFO: (10) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 6.686121ms)
Mar  1 10:01:02.760: INFO: (10) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 6.537434ms)
Mar  1 10:01:02.760: INFO: (10) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 8.054246ms)
Mar  1 10:01:02.760: INFO: (10) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 7.318531ms)
Mar  1 10:01:02.762: INFO: (10) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 9.289341ms)
Mar  1 10:01:02.762: INFO: (10) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 8.909425ms)
Mar  1 10:01:02.762: INFO: (10) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 9.054528ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 12.672445ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.865952ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 12.434291ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 12.309846ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.437756ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.867118ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 12.467834ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 13.131939ms)
Mar  1 10:01:02.766: INFO: (10) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 12.565227ms)
Mar  1 10:01:02.772: INFO: (11) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 6.290415ms)
Mar  1 10:01:02.775: INFO: (11) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 8.300495ms)
Mar  1 10:01:02.777: INFO: (11) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 9.907084ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 11.150435ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 11.693498ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.139021ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 11.495625ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.020101ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 11.84074ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 11.870026ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.705086ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 11.820774ms)
Mar  1 10:01:02.778: INFO: (11) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 11.671334ms)
Mar  1 10:01:02.779: INFO: (11) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 11.936058ms)
Mar  1 10:01:02.779: INFO: (11) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 11.630361ms)
Mar  1 10:01:02.779: INFO: (11) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.069406ms)
Mar  1 10:01:02.787: INFO: (12) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 8.175727ms)
Mar  1 10:01:02.789: INFO: (12) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.17447ms)
Mar  1 10:01:02.789: INFO: (12) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.090382ms)
Mar  1 10:01:02.789: INFO: (12) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 9.531286ms)
Mar  1 10:01:02.790: INFO: (12) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 10.118636ms)
Mar  1 10:01:02.790: INFO: (12) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 10.645136ms)
Mar  1 10:01:02.790: INFO: (12) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 10.083016ms)
Mar  1 10:01:02.790: INFO: (12) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 10.197526ms)
Mar  1 10:01:02.790: INFO: (12) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 10.642106ms)
Mar  1 10:01:02.790: INFO: (12) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 10.742429ms)
Mar  1 10:01:02.791: INFO: (12) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 11.773573ms)
Mar  1 10:01:02.791: INFO: (12) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 11.564116ms)
Mar  1 10:01:02.791: INFO: (12) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.214589ms)
Mar  1 10:01:02.791: INFO: (12) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 11.836659ms)
Mar  1 10:01:02.792: INFO: (12) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 11.952926ms)
Mar  1 10:01:02.793: INFO: (12) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 13.308309ms)
Mar  1 10:01:02.800: INFO: (13) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 7.016233ms)
Mar  1 10:01:02.803: INFO: (13) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 9.508976ms)
Mar  1 10:01:02.803: INFO: (13) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 9.930901ms)
Mar  1 10:01:02.807: INFO: (13) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 13.080266ms)
Mar  1 10:01:02.807: INFO: (13) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 13.223524ms)
Mar  1 10:01:02.807: INFO: (13) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 14.054588ms)
Mar  1 10:01:02.807: INFO: (13) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 13.274012ms)
Mar  1 10:01:02.807: INFO: (13) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 13.798918ms)
Mar  1 10:01:02.807: INFO: (13) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 14.45952ms)
Mar  1 10:01:02.808: INFO: (13) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 13.781601ms)
Mar  1 10:01:02.809: INFO: (13) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 14.991321ms)
Mar  1 10:01:02.809: INFO: (13) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 14.86977ms)
Mar  1 10:01:02.809: INFO: (13) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 14.978179ms)
Mar  1 10:01:02.809: INFO: (13) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 15.202269ms)
Mar  1 10:01:02.809: INFO: (13) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 15.408962ms)
Mar  1 10:01:02.809: INFO: (13) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 15.338544ms)
Mar  1 10:01:02.817: INFO: (14) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 7.42089ms)
Mar  1 10:01:02.820: INFO: (14) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 10.032673ms)
Mar  1 10:01:02.821: INFO: (14) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 10.473226ms)
Mar  1 10:01:02.821: INFO: (14) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 10.657363ms)
Mar  1 10:01:02.821: INFO: (14) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 10.565166ms)
Mar  1 10:01:02.821: INFO: (14) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 10.506639ms)
Mar  1 10:01:02.821: INFO: (14) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.073363ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 12.000035ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 11.535457ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 12.227646ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 13.172433ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 11.948525ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.262868ms)
Mar  1 10:01:02.822: INFO: (14) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.587117ms)
Mar  1 10:01:02.823: INFO: (14) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 12.723074ms)
Mar  1 10:01:02.823: INFO: (14) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 13.201187ms)
Mar  1 10:01:02.829: INFO: (15) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 6.349268ms)
Mar  1 10:01:02.832: INFO: (15) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.857355ms)
Mar  1 10:01:02.832: INFO: (15) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.274674ms)
Mar  1 10:01:02.833: INFO: (15) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 9.368136ms)
Mar  1 10:01:02.833: INFO: (15) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.598097ms)
Mar  1 10:01:02.833: INFO: (15) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 8.421333ms)
Mar  1 10:01:02.833: INFO: (15) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 8.970783ms)
Mar  1 10:01:02.833: INFO: (15) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 10.012725ms)
Mar  1 10:01:02.834: INFO: (15) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 10.30399ms)
Mar  1 10:01:02.834: INFO: (15) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 9.949663ms)
Mar  1 10:01:02.834: INFO: (15) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 10.005962ms)
Mar  1 10:01:02.836: INFO: (15) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 12.294784ms)
Mar  1 10:01:02.837: INFO: (15) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 12.675908ms)
Mar  1 10:01:02.837: INFO: (15) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 13.216674ms)
Mar  1 10:01:02.837: INFO: (15) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.697365ms)
Mar  1 10:01:02.837: INFO: (15) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.622536ms)
Mar  1 10:01:02.845: INFO: (16) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 7.828939ms)
Mar  1 10:01:02.847: INFO: (16) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.521646ms)
Mar  1 10:01:02.847: INFO: (16) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 9.223001ms)
Mar  1 10:01:02.847: INFO: (16) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 9.186748ms)
Mar  1 10:01:02.847: INFO: (16) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 9.113803ms)
Mar  1 10:01:02.847: INFO: (16) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 9.630979ms)
Mar  1 10:01:02.847: INFO: (16) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 9.464066ms)
Mar  1 10:01:02.848: INFO: (16) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 10.431058ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 10.809148ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 11.02419ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 11.032734ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 11.090838ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 10.938918ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 11.562946ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 11.094172ms)
Mar  1 10:01:02.849: INFO: (16) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 11.333124ms)
Mar  1 10:01:02.854: INFO: (17) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 4.708834ms)
Mar  1 10:01:02.857: INFO: (17) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 7.176169ms)
Mar  1 10:01:02.857: INFO: (17) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 7.187606ms)
Mar  1 10:01:02.858: INFO: (17) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 7.003929ms)
Mar  1 10:01:02.858: INFO: (17) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 7.026866ms)
Mar  1 10:01:02.858: INFO: (17) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 7.8025ms)
Mar  1 10:01:02.859: INFO: (17) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 8.477913ms)
Mar  1 10:01:02.859: INFO: (17) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 8.62412ms)
Mar  1 10:01:02.860: INFO: (17) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.662774ms)
Mar  1 10:01:02.860: INFO: (17) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 10.60061ms)
Mar  1 10:01:02.861: INFO: (17) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 9.801622ms)
Mar  1 10:01:02.861: INFO: (17) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 9.923351ms)
Mar  1 10:01:02.861: INFO: (17) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 10.582834ms)
Mar  1 10:01:02.862: INFO: (17) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 10.905207ms)
Mar  1 10:01:02.862: INFO: (17) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 11.003266ms)
Mar  1 10:01:02.862: INFO: (17) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 11.323053ms)
Mar  1 10:01:02.868: INFO: (18) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 5.71237ms)
Mar  1 10:01:02.870: INFO: (18) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 7.0574ms)
Mar  1 10:01:02.870: INFO: (18) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 7.387793ms)
Mar  1 10:01:02.872: INFO: (18) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 9.243641ms)
Mar  1 10:01:02.874: INFO: (18) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 11.663812ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 11.322572ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 11.664804ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 12.193913ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.483355ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 11.893287ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 11.617727ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 11.767126ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 11.807932ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 11.987544ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 12.298729ms)
Mar  1 10:01:02.875: INFO: (18) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 12.645795ms)
Mar  1 10:01:02.881: INFO: (19) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 5.383063ms)
Mar  1 10:01:02.882: INFO: (19) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv/proxy/rewriteme">test</a> (200; 6.049343ms)
Mar  1 10:01:02.882: INFO: (19) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:462/proxy/: tls qux (200; 6.920385ms)
Mar  1 10:01:02.883: INFO: (19) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:443/proxy/tlsrewritem... (200; 7.193793ms)
Mar  1 10:01:02.883: INFO: (19) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:162/proxy/: bar (200; 7.24066ms)
Mar  1 10:01:02.884: INFO: (19) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">... (200; 8.1181ms)
Mar  1 10:01:02.884: INFO: (19) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/: <a href="/api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:1080/proxy/rewriteme">test<... (200; 8.014563ms)
Mar  1 10:01:02.884: INFO: (19) /api/v1/namespaces/proxy-5082/pods/proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.464229ms)
Mar  1 10:01:02.885: INFO: (19) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname1/proxy/: foo (200; 8.977526ms)
Mar  1 10:01:02.885: INFO: (19) /api/v1/namespaces/proxy-5082/pods/http:proxy-service-l6tjg-k5pbv:160/proxy/: foo (200; 8.691713ms)
Mar  1 10:01:02.885: INFO: (19) /api/v1/namespaces/proxy-5082/pods/https:proxy-service-l6tjg-k5pbv:460/proxy/: tls baz (200; 8.635156ms)
Mar  1 10:01:02.885: INFO: (19) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname1/proxy/: foo (200; 9.805529ms)
Mar  1 10:01:02.886: INFO: (19) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname2/proxy/: tls qux (200; 9.561452ms)
Mar  1 10:01:02.886: INFO: (19) /api/v1/namespaces/proxy-5082/services/proxy-service-l6tjg:portname2/proxy/: bar (200; 10.047449ms)
Mar  1 10:01:02.922: INFO: (19) /api/v1/namespaces/proxy-5082/services/http:proxy-service-l6tjg:portname2/proxy/: bar (200; 45.409864ms)
Mar  1 10:01:02.922: INFO: (19) /api/v1/namespaces/proxy-5082/services/https:proxy-service-l6tjg:tlsportname1/proxy/: tls baz (200; 45.91275ms)
STEP: deleting ReplicationController proxy-service-l6tjg in namespace proxy-5082, will wait for the garbage collector to delete the pods
Mar  1 10:01:03.011: INFO: Deleting ReplicationController proxy-service-l6tjg took: 34.867613ms
Mar  1 10:01:03.512: INFO: Terminating ReplicationController proxy-service-l6tjg pods took: 500.641435ms
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:01:14.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5082" for this suite.
Mar  1 10:01:20.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:01:20.841: INFO: namespace proxy-5082 deletion completed in 6.220082829s

• [SLOW TEST:27.015 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:01:20.842: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-6b293754-c558-4257-9343-a083ac0130ed in namespace container-probe-7179
Mar  1 10:01:27.538: INFO: Started pod test-webserver-6b293754-c558-4257-9343-a083ac0130ed in namespace container-probe-7179
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 10:01:27.544: INFO: Initial restart count of pod test-webserver-6b293754-c558-4257-9343-a083ac0130ed is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:05:29.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7179" for this suite.
Mar  1 10:05:35.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:05:35.430: INFO: namespace container-probe-7179 deletion completed in 6.381393916s

• [SLOW TEST:254.588 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:05:35.431: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 10:05:35.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-rc --image=172.20.8.7/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4222'
Mar  1 10:05:41.353: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 10:05:41.353: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Mar  1 10:05:41.401: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-6jqbx]
Mar  1 10:05:41.402: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-6jqbx" in namespace "kubectl-4222" to be "running and ready"
Mar  1 10:05:41.416: INFO: Pod "e2e-test-nginx-rc-6jqbx": Phase="Pending", Reason="", readiness=false. Elapsed: 14.889249ms
Mar  1 10:05:43.423: INFO: Pod "e2e-test-nginx-rc-6jqbx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020941892s
Mar  1 10:05:45.430: INFO: Pod "e2e-test-nginx-rc-6jqbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.028200016s
Mar  1 10:05:45.430: INFO: Pod "e2e-test-nginx-rc-6jqbx" satisfied condition "running and ready"
Mar  1 10:05:45.430: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-6jqbx]
Mar  1 10:05:45.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs rc/e2e-test-nginx-rc --namespace=kubectl-4222'
Mar  1 10:05:45.668: INFO: stderr: ""
Mar  1 10:05:45.668: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Mar  1 10:05:45.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete rc e2e-test-nginx-rc --namespace=kubectl-4222'
Mar  1 10:05:45.948: INFO: stderr: ""
Mar  1 10:05:45.948: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:05:45.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4222" for this suite.
Mar  1 10:06:09.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:06:10.128: INFO: namespace kubectl-4222 deletion completed in 24.171578263s

• [SLOW TEST:34.697 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:06:10.128: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  1 10:06:10.414: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:06:17.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5153" for this suite.
Mar  1 10:06:24.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:06:24.416: INFO: namespace init-container-5153 deletion completed in 6.414539704s

• [SLOW TEST:14.288 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:06:24.417: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  1 10:06:29.830: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:06:30.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8037" for this suite.
Mar  1 10:06:54.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:06:55.052: INFO: namespace replicaset-8037 deletion completed in 24.178946209s

• [SLOW TEST:30.636 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:06:55.053: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 10:06:55.437: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f" in namespace "downward-api-2122" to be "success or failure"
Mar  1 10:06:55.442: INFO: Pod "downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.833763ms
Mar  1 10:06:57.461: INFO: Pod "downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024064106s
Mar  1 10:06:59.468: INFO: Pod "downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03112855s
Mar  1 10:07:01.476: INFO: Pod "downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039327317s
STEP: Saw pod success
Mar  1 10:07:01.476: INFO: Pod "downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f" satisfied condition "success or failure"
Mar  1 10:07:01.481: INFO: Trying to get logs from node worker02 pod downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f container client-container: <nil>
STEP: delete the pod
Mar  1 10:07:01.563: INFO: Waiting for pod downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f to disappear
Mar  1 10:07:01.569: INFO: Pod downwardapi-volume-2554c4bf-7498-46c3-900e-482af397e44f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:07:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2122" for this suite.
Mar  1 10:07:07.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:07:07.983: INFO: namespace downward-api-2122 deletion completed in 6.405822245s

• [SLOW TEST:12.929 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:07:07.983: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:07:08.264: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  1 10:07:08.300: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  1 10:07:13.308: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  1 10:07:13.308: INFO: Creating deployment "test-rolling-update-deployment"
Mar  1 10:07:13.363: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  1 10:07:13.404: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  1 10:07:15.419: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  1 10:07:15.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654033, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654033, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654033, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654033, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-5b5b44df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:17.432: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  1 10:07:17.619: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1687,SelfLink:/apis/apps/v1/namespaces/deployment-1687/deployments/test-rolling-update-deployment,UID:852c2d41-9e1b-4116-b31e-3f6dc1513721,ResourceVersion:201151,Generation:1,CreationTimestamp:2020-03-01 10:07:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-01 10:07:13 +0000 UTC 2020-03-01 10:07:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-01 10:07:16 +0000 UTC 2020-03-01 10:07:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-5b5b44df46" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 10:07:17.629: INFO: New ReplicaSet "test-rolling-update-deployment-5b5b44df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-5b5b44df46,GenerateName:,Namespace:deployment-1687,SelfLink:/apis/apps/v1/namespaces/deployment-1687/replicasets/test-rolling-update-deployment-5b5b44df46,UID:ab779aef-69e7-43bc-be75-dc62a9e1de1c,ResourceVersion:201139,Generation:1,CreationTimestamp:2020-03-01 10:07:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 852c2d41-9e1b-4116-b31e-3f6dc1513721 0xc003ec9017 0xc003ec9018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  1 10:07:17.629: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  1 10:07:17.629: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1687,SelfLink:/apis/apps/v1/namespaces/deployment-1687/replicasets/test-rolling-update-controller,UID:c6973bc4-b26e-4b50-89e7-27529472ba0a,ResourceVersion:201149,Generation:2,CreationTimestamp:2020-03-01 10:07:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 852c2d41-9e1b-4116-b31e-3f6dc1513721 0xc003ec8f47 0xc003ec8f48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:07:17.656: INFO: Pod "test-rolling-update-deployment-5b5b44df46-t5fsr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-5b5b44df46-t5fsr,GenerateName:test-rolling-update-deployment-5b5b44df46-,Namespace:deployment-1687,SelfLink:/api/v1/namespaces/deployment-1687/pods/test-rolling-update-deployment-5b5b44df46-t5fsr,UID:cbc3e618-c2b6-442b-83b9-0e037e59a1a8,ResourceVersion:201138,Generation:0,CreationTimestamp:2020-03-01 10:07:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 5b5b44df46,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-5b5b44df46 ab779aef-69e7-43bc-be75-dc62a9e1de1c 0xc003ec98f7 0xc003ec98f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-q7tzj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-q7tzj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-q7tzj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003ec9970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003ec9990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:07:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:07:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:07:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:07:13 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.94,StartTime:2020-03-01 10:07:13 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-01 10:07:16 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/redis:1.0 docker-pullable://172.20.8.7/library/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 docker://171b8a88323f00229c354e42296eff8a4e2727a7428d53ff86f19f335e7cf3fa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:07:17.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1687" for this suite.
Mar  1 10:07:25.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:07:25.839: INFO: namespace deployment-1687 deletion completed in 8.17506937s

• [SLOW TEST:17.856 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:07:25.840: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9565496c-37d2-4880-9d47-0ac7eb688089
STEP: Creating a pod to test consume secrets
Mar  1 10:07:26.231: INFO: Waiting up to 5m0s for pod "pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4" in namespace "secrets-1616" to be "success or failure"
Mar  1 10:07:26.250: INFO: Pod "pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.256995ms
Mar  1 10:07:28.257: INFO: Pod "pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025969283s
Mar  1 10:07:30.264: INFO: Pod "pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03339953s
Mar  1 10:07:32.272: INFO: Pod "pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041366497s
STEP: Saw pod success
Mar  1 10:07:32.272: INFO: Pod "pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4" satisfied condition "success or failure"
Mar  1 10:07:32.277: INFO: Trying to get logs from node worker02 pod pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4 container secret-env-test: <nil>
STEP: delete the pod
Mar  1 10:07:32.419: INFO: Waiting for pod pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4 to disappear
Mar  1 10:07:32.442: INFO: Pod pod-secrets-e823cdcd-715a-4404-aa61-83e61cd9c4e4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:07:32.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1616" for this suite.
Mar  1 10:07:40.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:07:40.690: INFO: namespace secrets-1616 deletion completed in 8.239553616s

• [SLOW TEST:14.850 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:07:40.693: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  1 10:07:41.022: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Mar  1 10:07:42.552: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  1 10:07:44.849: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:46.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:48.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:50.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:53.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:54.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:56.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:07:58.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:00.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:02.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:04.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:06.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:08.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:10.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654063, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654062, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55b4958655\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:08:14.484: INFO: Waited 1.579590669s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:08:15.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1080" for this suite.
Mar  1 10:08:23.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:08:24.061: INFO: namespace aggregator-1080 deletion completed in 8.269752452s

• [SLOW TEST:43.369 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:08:24.061: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  1 10:08:31.386: INFO: Successfully updated pod "annotationupdate50b5543b-b528-4b9a-82c4-f3cd0a1cc7da"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:08:33.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4996" for this suite.
Mar  1 10:08:47.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:08:47.626: INFO: namespace projected-4996 deletion completed in 14.171020385s

• [SLOW TEST:23.565 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:08:47.627: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-1a0dfd03-8f44-42ac-bbde-13f24674b5a5
STEP: Creating a pod to test consume secrets
Mar  1 10:08:48.102: INFO: Waiting up to 5m0s for pod "pod-secrets-38903117-9f1e-4856-930d-16d10b194ace" in namespace "secrets-5206" to be "success or failure"
Mar  1 10:08:48.106: INFO: Pod "pod-secrets-38903117-9f1e-4856-930d-16d10b194ace": Phase="Pending", Reason="", readiness=false. Elapsed: 3.992393ms
Mar  1 10:08:50.114: INFO: Pod "pod-secrets-38903117-9f1e-4856-930d-16d10b194ace": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011220826s
Mar  1 10:08:52.119: INFO: Pod "pod-secrets-38903117-9f1e-4856-930d-16d10b194ace": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01654903s
Mar  1 10:08:54.127: INFO: Pod "pod-secrets-38903117-9f1e-4856-930d-16d10b194ace": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024318024s
STEP: Saw pod success
Mar  1 10:08:54.127: INFO: Pod "pod-secrets-38903117-9f1e-4856-930d-16d10b194ace" satisfied condition "success or failure"
Mar  1 10:08:54.132: INFO: Trying to get logs from node worker02 pod pod-secrets-38903117-9f1e-4856-930d-16d10b194ace container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 10:08:54.206: INFO: Waiting for pod pod-secrets-38903117-9f1e-4856-930d-16d10b194ace to disappear
Mar  1 10:08:54.210: INFO: Pod pod-secrets-38903117-9f1e-4856-930d-16d10b194ace no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:08:54.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5206" for this suite.
Mar  1 10:09:00.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:09:00.472: INFO: namespace secrets-5206 deletion completed in 6.254944167s

• [SLOW TEST:12.845 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:09:00.473: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6007
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6007 to expose endpoints map[]
Mar  1 10:09:00.970: INFO: Get endpoints failed (3.679836ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar  1 10:09:01.975: INFO: successfully validated that service multi-endpoint-test in namespace services-6007 exposes endpoints map[] (1.008779959s elapsed)
STEP: Creating pod pod1 in namespace services-6007
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6007 to expose endpoints map[pod1:[100]]
Mar  1 10:09:06.059: INFO: successfully validated that service multi-endpoint-test in namespace services-6007 exposes endpoints map[pod1:[100]] (4.062144707s elapsed)
STEP: Creating pod pod2 in namespace services-6007
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6007 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  1 10:09:10.337: INFO: successfully validated that service multi-endpoint-test in namespace services-6007 exposes endpoints map[pod1:[100] pod2:[101]] (4.178152148s elapsed)
STEP: Deleting pod pod1 in namespace services-6007
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6007 to expose endpoints map[pod2:[101]]
Mar  1 10:09:11.476: INFO: successfully validated that service multi-endpoint-test in namespace services-6007 exposes endpoints map[pod2:[101]] (1.106612794s elapsed)
STEP: Deleting pod pod2 in namespace services-6007
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6007 to expose endpoints map[]
Mar  1 10:09:11.566: INFO: successfully validated that service multi-endpoint-test in namespace services-6007 exposes endpoints map[] (27.853309ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:09:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6007" for this suite.
Mar  1 10:09:35.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:09:36.254: INFO: namespace services-6007 deletion completed in 24.379463514s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:35.781 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:09:36.255: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a5e369c1-59c6-4f76-b1fc-f1243cca28e7
STEP: Creating a pod to test consume configMaps
Mar  1 10:09:36.730: INFO: Waiting up to 5m0s for pod "pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37" in namespace "configmap-3778" to be "success or failure"
Mar  1 10:09:36.735: INFO: Pod "pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407063ms
Mar  1 10:09:38.740: INFO: Pod "pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009648697s
Mar  1 10:09:40.762: INFO: Pod "pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031346253s
Mar  1 10:09:42.769: INFO: Pod "pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03825805s
STEP: Saw pod success
Mar  1 10:09:42.769: INFO: Pod "pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37" satisfied condition "success or failure"
Mar  1 10:09:42.773: INFO: Trying to get logs from node worker02 pod pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 10:09:42.883: INFO: Waiting for pod pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37 to disappear
Mar  1 10:09:42.889: INFO: Pod pod-configmaps-eac646a9-bd2b-40aa-b622-f3e201694d37 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:09:42.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3778" for this suite.
Mar  1 10:09:48.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:09:49.068: INFO: namespace configmap-3778 deletion completed in 6.172936765s

• [SLOW TEST:12.813 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:09:49.068: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-38609bb7-60aa-4f75-80c5-e4bee9af1a34 in namespace container-probe-3160
Mar  1 10:09:53.407: INFO: Started pod busybox-38609bb7-60aa-4f75-80c5-e4bee9af1a34 in namespace container-probe-3160
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 10:09:53.411: INFO: Initial restart count of pod busybox-38609bb7-60aa-4f75-80c5-e4bee9af1a34 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:13:54.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3160" for this suite.
Mar  1 10:14:00.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:14:00.936: INFO: namespace container-probe-3160 deletion completed in 6.19610719s

• [SLOW TEST:251.867 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:14:00.936: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  1 10:14:01.274: INFO: Waiting up to 5m0s for pod "pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0" in namespace "emptydir-5358" to be "success or failure"
Mar  1 10:14:01.279: INFO: Pod "pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.911423ms
Mar  1 10:14:03.285: INFO: Pod "pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011274002s
Mar  1 10:14:05.291: INFO: Pod "pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017351763s
Mar  1 10:14:07.299: INFO: Pod "pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024427228s
STEP: Saw pod success
Mar  1 10:14:07.299: INFO: Pod "pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0" satisfied condition "success or failure"
Mar  1 10:14:07.303: INFO: Trying to get logs from node worker02 pod pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0 container test-container: <nil>
STEP: delete the pod
Mar  1 10:14:07.391: INFO: Waiting for pod pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0 to disappear
Mar  1 10:14:07.395: INFO: Pod pod-7f5cc75b-f66e-4dd3-9182-4bd7bc972bb0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:14:07.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5358" for this suite.
Mar  1 10:14:13.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:14:13.597: INFO: namespace emptydir-5358 deletion completed in 6.194822196s

• [SLOW TEST:12.661 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:14:13.597: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-32
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-n8zp
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 10:14:14.022: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-n8zp" in namespace "subpath-32" to be "success or failure"
Mar  1 10:14:14.027: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.054107ms
Mar  1 10:14:16.039: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016614092s
Mar  1 10:14:18.060: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 4.038408005s
Mar  1 10:14:20.066: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 6.044076433s
Mar  1 10:14:22.086: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 8.064490366s
Mar  1 10:14:24.092: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 10.070107384s
Mar  1 10:14:26.097: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 12.075201842s
Mar  1 10:14:28.114: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 14.091926661s
Mar  1 10:14:30.119: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 16.097065242s
Mar  1 10:14:32.133: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 18.111416641s
Mar  1 10:14:34.139: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 20.116645486s
Mar  1 10:14:36.143: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Running", Reason="", readiness=true. Elapsed: 22.121260674s
Mar  1 10:14:38.149: INFO: Pod "pod-subpath-test-configmap-n8zp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.127394585s
STEP: Saw pod success
Mar  1 10:14:38.149: INFO: Pod "pod-subpath-test-configmap-n8zp" satisfied condition "success or failure"
Mar  1 10:14:38.154: INFO: Trying to get logs from node worker02 pod pod-subpath-test-configmap-n8zp container test-container-subpath-configmap-n8zp: <nil>
STEP: delete the pod
Mar  1 10:14:38.299: INFO: Waiting for pod pod-subpath-test-configmap-n8zp to disappear
Mar  1 10:14:38.302: INFO: Pod pod-subpath-test-configmap-n8zp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-n8zp
Mar  1 10:14:38.303: INFO: Deleting pod "pod-subpath-test-configmap-n8zp" in namespace "subpath-32"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:14:38.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-32" for this suite.
Mar  1 10:14:44.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:14:44.614: INFO: namespace subpath-32 deletion completed in 6.301566003s

• [SLOW TEST:31.017 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:14:44.615: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4133
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 10:14:44.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-deployment --image=172.20.8.7/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-4133'
Mar  1 10:14:45.235: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 10:14:45.235: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Mar  1 10:14:49.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4133'
Mar  1 10:14:49.604: INFO: stderr: ""
Mar  1 10:14:49.604: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:14:49.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4133" for this suite.
Mar  1 10:15:13.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:15:13.801: INFO: namespace kubectl-4133 deletion completed in 24.18806069s

• [SLOW TEST:29.186 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:15:13.802: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Mar  1 10:15:14.215: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-761" to be "success or failure"
Mar  1 10:15:14.219: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57875ms
Mar  1 10:15:16.228: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012875105s
Mar  1 10:15:18.234: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01889291s
Mar  1 10:15:20.239: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024542145s
STEP: Saw pod success
Mar  1 10:15:20.239: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar  1 10:15:20.243: INFO: Trying to get logs from node worker02 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  1 10:15:20.307: INFO: Waiting for pod pod-host-path-test to disappear
Mar  1 10:15:20.310: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:15:20.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-761" for this suite.
Mar  1 10:15:26.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:15:26.644: INFO: namespace hostpath-761 deletion completed in 6.326214368s

• [SLOW TEST:12.842 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:15:26.644: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:15:30.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4607" for this suite.
Mar  1 10:16:19.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:16:19.159: INFO: namespace kubelet-test-4607 deletion completed in 48.177966852s

• [SLOW TEST:52.515 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:16:19.160: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8172.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8172.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8172.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8172.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8172.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.64.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.64.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.64.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.64.239_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8172.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8172.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8172.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8172.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8172.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8172.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.64.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.64.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.64.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.64.239_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 10:16:25.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.715: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.722: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.728: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.775: INFO: Unable to read jessie_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.781: INFO: Unable to read jessie_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.787: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.792: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:25.829: INFO: Lookups using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 failed for: [wheezy_udp@dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_udp@dns-test-service.dns-8172.svc.cluster.local jessie_tcp@dns-test-service.dns-8172.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local]

Mar  1 10:16:30.838: INFO: Unable to read wheezy_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.844: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.854: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.937: INFO: Unable to read jessie_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.943: INFO: Unable to read jessie_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.953: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:30.990: INFO: Lookups using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 failed for: [wheezy_udp@dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_udp@dns-test-service.dns-8172.svc.cluster.local jessie_tcp@dns-test-service.dns-8172.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local]

Mar  1 10:16:35.839: INFO: Unable to read wheezy_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.845: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.851: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.857: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.894: INFO: Unable to read jessie_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.905: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.911: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:35.946: INFO: Lookups using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 failed for: [wheezy_udp@dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_udp@dns-test-service.dns-8172.svc.cluster.local jessie_tcp@dns-test-service.dns-8172.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local]

Mar  1 10:16:40.837: INFO: Unable to read wheezy_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.877: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.888: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.926: INFO: Unable to read jessie_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.931: INFO: Unable to read jessie_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.937: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.941: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:40.973: INFO: Lookups using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 failed for: [wheezy_udp@dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_udp@dns-test-service.dns-8172.svc.cluster.local jessie_tcp@dns-test-service.dns-8172.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local]

Mar  1 10:16:45.836: INFO: Unable to read wheezy_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.845: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.882: INFO: Unable to read jessie_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.886: INFO: Unable to read jessie_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.890: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.895: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:45.923: INFO: Lookups using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 failed for: [wheezy_udp@dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_udp@dns-test-service.dns-8172.svc.cluster.local jessie_tcp@dns-test-service.dns-8172.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local]

Mar  1 10:16:50.836: INFO: Unable to read wheezy_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.845: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.907: INFO: Unable to read jessie_udp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.916: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.920: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local from pod dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04: the server could not find the requested resource (get pods dns-test-f2881e78-c469-4427-b714-b821a67eab04)
Mar  1 10:16:50.950: INFO: Lookups using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 failed for: [wheezy_udp@dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@dns-test-service.dns-8172.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_udp@dns-test-service.dns-8172.svc.cluster.local jessie_tcp@dns-test-service.dns-8172.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8172.svc.cluster.local]

Mar  1 10:16:55.939: INFO: DNS probes using dns-8172/dns-test-f2881e78-c469-4427-b714-b821a67eab04 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:16:56.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8172" for this suite.
Mar  1 10:17:04.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:17:05.021: INFO: namespace dns-8172 deletion completed in 8.24942377s

• [SLOW TEST:45.861 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:17:05.022: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:17:05.673: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  1 10:17:10.681: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  1 10:17:10.681: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  1 10:17:11.226: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7656,SelfLink:/apis/apps/v1/namespaces/deployment-7656/deployments/test-cleanup-deployment,UID:dee0af4e-c5e5-43f3-801c-d49eab32559d,ResourceVersion:202808,Generation:1,CreationTimestamp:2020-03-01 10:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 10:17:11.260: INFO: New ReplicaSet "test-cleanup-deployment-78799fcdcb" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-78799fcdcb,GenerateName:,Namespace:deployment-7656,SelfLink:/apis/apps/v1/namespaces/deployment-7656/replicasets/test-cleanup-deployment-78799fcdcb,UID:ede716f7-8eea-47dc-8e25-afac7f681105,ResourceVersion:202810,Generation:1,CreationTimestamp:2020-03-01 10:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 78799fcdcb,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment dee0af4e-c5e5-43f3-801c-d49eab32559d 0xc001f9fac7 0xc001f9fac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 78799fcdcb,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 78799fcdcb,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:17:11.260: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  1 10:17:11.261: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-7656,SelfLink:/apis/apps/v1/namespaces/deployment-7656/replicasets/test-cleanup-controller,UID:03a9ee7b-103f-44dc-8d34-15dad8d3d457,ResourceVersion:202809,Generation:1,CreationTimestamp:2020-03-01 10:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment dee0af4e-c5e5-43f3-801c-d49eab32559d 0xc001f9f7c7 0xc001f9f7c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  1 10:17:11.266: INFO: Pod "test-cleanup-controller-v47zq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-v47zq,GenerateName:test-cleanup-controller-,Namespace:deployment-7656,SelfLink:/api/v1/namespaces/deployment-7656/pods/test-cleanup-controller-v47zq,UID:a793aa42-f4f9-4846-adef-3ed499974f54,ResourceVersion:202804,Generation:0,CreationTimestamp:2020-03-01 10:17:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 03a9ee7b-103f-44dc-8d34-15dad8d3d457 0xc0003d5077 0xc0003d5078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qxgdd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qxgdd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qxgdd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0003d5190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0003d51d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:05 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.108,StartTime:2020-03-01 10:17:05 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:17:08 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://cb4610440598f944f6f922134f7b09f1c94b4872a5d7b2232f11363a5d57fe19}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:17:11.266: INFO: Pod "test-cleanup-deployment-78799fcdcb-ljrdd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-78799fcdcb-ljrdd,GenerateName:test-cleanup-deployment-78799fcdcb-,Namespace:deployment-7656,SelfLink:/api/v1/namespaces/deployment-7656/pods/test-cleanup-deployment-78799fcdcb-ljrdd,UID:e42ce116-b23a-4a1f-bd24-64e2cd205f51,ResourceVersion:202812,Generation:0,CreationTimestamp:2020-03-01 10:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 78799fcdcb,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-78799fcdcb ede716f7-8eea-47dc-8e25-afac7f681105 0xc00037f187 0xc00037f188}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qxgdd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qxgdd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qxgdd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00037f290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00037f2c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:17:11.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7656" for this suite.
Mar  1 10:17:19.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:17:19.590: INFO: namespace deployment-7656 deletion completed in 8.316823698s

• [SLOW TEST:14.569 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:17:19.591: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:17:20.176: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  1 10:17:25.184: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  1 10:17:25.184: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  1 10:17:27.189: INFO: Creating deployment "test-rollover-deployment"
Mar  1 10:17:27.209: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  1 10:17:29.219: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  1 10:17:29.229: INFO: Ensure that both replica sets have 1 created replica
Mar  1 10:17:29.236: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  1 10:17:29.277: INFO: Updating deployment test-rollover-deployment
Mar  1 10:17:29.277: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  1 10:17:31.456: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  1 10:17:31.463: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  1 10:17:31.471: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 10:17:31.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654649, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:17:33.573: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 10:17:33.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654653, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:17:35.481: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 10:17:35.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654653, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:17:37.481: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 10:17:37.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654653, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:17:39.482: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 10:17:39.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654653, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:17:41.492: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 10:17:41.492: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654653, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718654647, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65ff5b8f77\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:17:43.535: INFO: 
Mar  1 10:17:43.535: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  1 10:17:43.548: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-8023,SelfLink:/apis/apps/v1/namespaces/deployment-8023/deployments/test-rollover-deployment,UID:0cacce28-7ec3-4286-a68f-fc2962bf8512,ResourceVersion:202976,Generation:2,CreationTimestamp:2020-03-01 10:17:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-03-01 10:17:27 +0000 UTC 2020-03-01 10:17:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-03-01 10:17:43 +0000 UTC 2020-03-01 10:17:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-65ff5b8f77" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 10:17:43.553: INFO: New ReplicaSet "test-rollover-deployment-65ff5b8f77" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-65ff5b8f77,GenerateName:,Namespace:deployment-8023,SelfLink:/apis/apps/v1/namespaces/deployment-8023/replicasets/test-rollover-deployment-65ff5b8f77,UID:f9ea981c-617b-4ee1-a243-05a6f2e373d6,ResourceVersion:202964,Generation:2,CreationTimestamp:2020-03-01 10:17:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0cacce28-7ec3-4286-a68f-fc2962bf8512 0xc002df1b17 0xc002df1b18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar  1 10:17:43.553: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  1 10:17:43.554: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-8023,SelfLink:/apis/apps/v1/namespaces/deployment-8023/replicasets/test-rollover-controller,UID:f61c1c62-42e8-4ce7-8696-ce7c0fe213a5,ResourceVersion:202975,Generation:2,CreationTimestamp:2020-03-01 10:17:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0cacce28-7ec3-4286-a68f-fc2962bf8512 0xc002df1a47 0xc002df1a48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:17:43.554: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-8023,SelfLink:/apis/apps/v1/namespaces/deployment-8023/replicasets/test-rollover-deployment-9b8b997cf,UID:daa047fa-cc5f-4cf9-b32a-2c096be83604,ResourceVersion:202931,Generation:2,CreationTimestamp:2020-03-01 10:17:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0cacce28-7ec3-4286-a68f-fc2962bf8512 0xc002df1be0 0xc002df1be1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:17:43.559: INFO: Pod "test-rollover-deployment-65ff5b8f77-pb949" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-65ff5b8f77-pb949,GenerateName:test-rollover-deployment-65ff5b8f77-,Namespace:deployment-8023,SelfLink:/api/v1/namespaces/deployment-8023/pods/test-rollover-deployment-65ff5b8f77-pb949,UID:e6734f5b-d31b-4ee3-9583-b415623a1b21,ResourceVersion:202942,Generation:0,CreationTimestamp:2020-03-01 10:17:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 65ff5b8f77,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-65ff5b8f77 f9ea981c-617b-4ee1-a243-05a6f2e373d6 0xc002e4c7b7 0xc002e4c7b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-g5xdn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g5xdn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-g5xdn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:17:29 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.111,StartTime:2020-03-01 10:17:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-03-01 10:17:32 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/redis:1.0 docker-pullable://172.20.8.7/library/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 docker://79edf97789e1c35db78df230326cc47256a91f287972bbb217ffe22bca68abc7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:17:43.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8023" for this suite.
Mar  1 10:17:51.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:17:51.732: INFO: namespace deployment-8023 deletion completed in 8.166398075s

• [SLOW TEST:32.142 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:17:51.734: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3147
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6475
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:17:58.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7825" for this suite.
Mar  1 10:18:04.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:18:05.138: INFO: namespace namespaces-7825 deletion completed in 6.340756502s
STEP: Destroying namespace "nsdeletetest-3147" for this suite.
Mar  1 10:18:05.141: INFO: Namespace nsdeletetest-3147 was already deleted
STEP: Destroying namespace "nsdeletetest-6475" for this suite.
Mar  1 10:18:11.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:18:11.368: INFO: namespace nsdeletetest-6475 deletion completed in 6.226367252s

• [SLOW TEST:19.634 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:18:11.369: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-6fdfc122-72a8-49fd-8a1f-cba4e1c2d56f
STEP: Creating a pod to test consume secrets
Mar  1 10:18:11.989: INFO: Waiting up to 5m0s for pod "pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687" in namespace "secrets-3009" to be "success or failure"
Mar  1 10:18:12.040: INFO: Pod "pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687": Phase="Pending", Reason="", readiness=false. Elapsed: 50.80844ms
Mar  1 10:18:14.140: INFO: Pod "pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687": Phase="Pending", Reason="", readiness=false. Elapsed: 2.150623811s
Mar  1 10:18:16.146: INFO: Pod "pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687": Phase="Pending", Reason="", readiness=false. Elapsed: 4.157165292s
Mar  1 10:18:18.156: INFO: Pod "pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.166589811s
STEP: Saw pod success
Mar  1 10:18:18.156: INFO: Pod "pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687" satisfied condition "success or failure"
Mar  1 10:18:18.162: INFO: Trying to get logs from node worker02 pod pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687 container secret-volume-test: <nil>
STEP: delete the pod
Mar  1 10:18:18.224: INFO: Waiting for pod pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687 to disappear
Mar  1 10:18:18.231: INFO: Pod pod-secrets-f70f6f33-7094-4b9c-836f-9f47359de687 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:18:18.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3009" for this suite.
Mar  1 10:18:24.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:18:24.525: INFO: namespace secrets-3009 deletion completed in 6.285081069s

• [SLOW TEST:13.157 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:18:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  1 10:18:29.483: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e7d84e8b-4b49-424c-9e48-6b3fa3a09e2b"
Mar  1 10:18:29.483: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e7d84e8b-4b49-424c-9e48-6b3fa3a09e2b" in namespace "pods-158" to be "terminated due to deadline exceeded"
Mar  1 10:18:29.487: INFO: Pod "pod-update-activedeadlineseconds-e7d84e8b-4b49-424c-9e48-6b3fa3a09e2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.178604ms
Mar  1 10:18:31.504: INFO: Pod "pod-update-activedeadlineseconds-e7d84e8b-4b49-424c-9e48-6b3fa3a09e2b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.021446735s
Mar  1 10:18:31.504: INFO: Pod "pod-update-activedeadlineseconds-e7d84e8b-4b49-424c-9e48-6b3fa3a09e2b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:18:31.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-158" for this suite.
Mar  1 10:18:37.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:18:37.763: INFO: namespace pods-158 deletion completed in 6.248614526s

• [SLOW TEST:13.238 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:18:37.765: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar  1 10:18:38.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-3466'
Mar  1 10:18:44.633: INFO: stderr: ""
Mar  1 10:18:44.633: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  1 10:18:45.640: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:18:45.640: INFO: Found 0 / 1
Mar  1 10:18:46.640: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:18:46.640: INFO: Found 0 / 1
Mar  1 10:18:47.642: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:18:47.642: INFO: Found 1 / 1
Mar  1 10:18:47.642: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  1 10:18:47.655: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:18:47.655: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 10:18:47.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 patch pod redis-master-rz824 --namespace=kubectl-3466 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  1 10:18:48.008: INFO: stderr: ""
Mar  1 10:18:48.008: INFO: stdout: "pod/redis-master-rz824 patched\n"
STEP: checking annotations
Mar  1 10:18:48.014: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:18:48.014: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:18:48.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3466" for this suite.
Mar  1 10:19:12.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:19:12.196: INFO: namespace kubectl-3466 deletion completed in 24.175930094s

• [SLOW TEST:34.431 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:19:12.196: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  1 10:19:12.514: INFO: Waiting up to 5m0s for pod "pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2" in namespace "emptydir-8565" to be "success or failure"
Mar  1 10:19:12.518: INFO: Pod "pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396984ms
Mar  1 10:19:14.523: INFO: Pod "pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008735045s
Mar  1 10:19:16.530: INFO: Pod "pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01549952s
STEP: Saw pod success
Mar  1 10:19:16.530: INFO: Pod "pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2" satisfied condition "success or failure"
Mar  1 10:19:16.535: INFO: Trying to get logs from node worker02 pod pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2 container test-container: <nil>
STEP: delete the pod
Mar  1 10:19:16.645: INFO: Waiting for pod pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2 to disappear
Mar  1 10:19:16.649: INFO: Pod pod-a409cc4a-9176-42ca-afc5-e66ecc7487f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:19:16.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8565" for this suite.
Mar  1 10:19:22.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:19:22.808: INFO: namespace emptydir-8565 deletion completed in 6.153983441s

• [SLOW TEST:10.612 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:19:22.809: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8721
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  1 10:19:25.900: INFO: Pod name wrapped-volume-race-9c36d0f0-f63c-4123-a5b2-f897b38d3cfb: Found 0 pods out of 5
Mar  1 10:19:30.917: INFO: Pod name wrapped-volume-race-9c36d0f0-f63c-4123-a5b2-f897b38d3cfb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9c36d0f0-f63c-4123-a5b2-f897b38d3cfb in namespace emptydir-wrapper-8721, will wait for the garbage collector to delete the pods
Mar  1 10:19:45.133: INFO: Deleting ReplicationController wrapped-volume-race-9c36d0f0-f63c-4123-a5b2-f897b38d3cfb took: 44.879837ms
Mar  1 10:19:46.033: INFO: Terminating ReplicationController wrapped-volume-race-9c36d0f0-f63c-4123-a5b2-f897b38d3cfb pods took: 900.356505ms
STEP: Creating RC which spawns configmap-volume pods
Mar  1 10:20:21.478: INFO: Pod name wrapped-volume-race-8cb4a8c3-47ae-4dc8-b38f-276da36d8cb4: Found 0 pods out of 5
Mar  1 10:20:26.489: INFO: Pod name wrapped-volume-race-8cb4a8c3-47ae-4dc8-b38f-276da36d8cb4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8cb4a8c3-47ae-4dc8-b38f-276da36d8cb4 in namespace emptydir-wrapper-8721, will wait for the garbage collector to delete the pods
Mar  1 10:20:38.679: INFO: Deleting ReplicationController wrapped-volume-race-8cb4a8c3-47ae-4dc8-b38f-276da36d8cb4 took: 24.857645ms
Mar  1 10:20:39.379: INFO: Terminating ReplicationController wrapped-volume-race-8cb4a8c3-47ae-4dc8-b38f-276da36d8cb4 pods took: 700.510053ms
STEP: Creating RC which spawns configmap-volume pods
Mar  1 10:21:25.652: INFO: Pod name wrapped-volume-race-43799eec-10e6-4cc7-8b77-9496edd3b1d8: Found 0 pods out of 5
Mar  1 10:21:30.663: INFO: Pod name wrapped-volume-race-43799eec-10e6-4cc7-8b77-9496edd3b1d8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-43799eec-10e6-4cc7-8b77-9496edd3b1d8 in namespace emptydir-wrapper-8721, will wait for the garbage collector to delete the pods
Mar  1 10:21:44.802: INFO: Deleting ReplicationController wrapped-volume-race-43799eec-10e6-4cc7-8b77-9496edd3b1d8 took: 26.014018ms
Mar  1 10:21:45.403: INFO: Terminating ReplicationController wrapped-volume-race-43799eec-10e6-4cc7-8b77-9496edd3b1d8 pods took: 600.334483ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:22:33.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8721" for this suite.
Mar  1 10:22:47.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:22:48.459: INFO: namespace emptydir-wrapper-8721 deletion completed in 14.74119785s

• [SLOW TEST:205.650 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:22:48.460: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  1 10:23:06.901: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 10:23:06.906: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 10:23:08.906: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 10:23:08.914: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 10:23:10.906: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 10:23:10.911: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 10:23:12.906: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 10:23:12.913: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 10:23:14.906: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 10:23:14.913: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:23:14.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3905" for this suite.
Mar  1 10:23:39.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:23:39.309: INFO: namespace container-lifecycle-hook-3905 deletion completed in 24.373576295s

• [SLOW TEST:50.849 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:23:39.309: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:23:46.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8011" for this suite.
Mar  1 10:24:10.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:24:10.962: INFO: namespace replication-controller-8011 deletion completed in 24.264940381s

• [SLOW TEST:31.653 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:24:10.963: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4214/configmap-test-e385fdf8-469a-482a-acf6-03e0682c0b85
STEP: Creating a pod to test consume configMaps
Mar  1 10:24:11.396: INFO: Waiting up to 5m0s for pod "pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0" in namespace "configmap-4214" to be "success or failure"
Mar  1 10:24:11.402: INFO: Pod "pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195214ms
Mar  1 10:24:13.409: INFO: Pod "pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012731096s
Mar  1 10:24:15.416: INFO: Pod "pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019887354s
Mar  1 10:24:17.423: INFO: Pod "pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026281532s
STEP: Saw pod success
Mar  1 10:24:17.423: INFO: Pod "pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0" satisfied condition "success or failure"
Mar  1 10:24:17.427: INFO: Trying to get logs from node worker02 pod pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0 container env-test: <nil>
STEP: delete the pod
Mar  1 10:24:17.510: INFO: Waiting for pod pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0 to disappear
Mar  1 10:24:17.537: INFO: Pod pod-configmaps-58f28634-5b37-4fe5-9fd0-3f17d46163f0 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:24:17.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4214" for this suite.
Mar  1 10:24:23.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:24:23.754: INFO: namespace configmap-4214 deletion completed in 6.2090364s

• [SLOW TEST:12.791 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:24:23.755: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-3843da39-fae9-49b3-bb7a-175feac38ad3 in namespace container-probe-1744
Mar  1 10:24:30.266: INFO: Started pod liveness-3843da39-fae9-49b3-bb7a-175feac38ad3 in namespace container-probe-1744
STEP: checking the pod's current state and verifying that restartCount is present
Mar  1 10:24:30.272: INFO: Initial restart count of pod liveness-3843da39-fae9-49b3-bb7a-175feac38ad3 is 0
Mar  1 10:24:48.346: INFO: Restart count of pod container-probe-1744/liveness-3843da39-fae9-49b3-bb7a-175feac38ad3 is now 1 (18.074323678s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:24:48.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1744" for this suite.
Mar  1 10:24:54.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:24:54.652: INFO: namespace container-probe-1744 deletion completed in 6.249078764s

• [SLOW TEST:30.897 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:24:54.652: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  1 10:24:59.642: INFO: Successfully updated pod "annotationupdate959df663-149b-436a-881d-81257f20b3f7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:25:03.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6006" for this suite.
Mar  1 10:25:27.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:25:28.043: INFO: namespace downward-api-6006 deletion completed in 24.348775209s

• [SLOW TEST:33.391 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:25:28.043: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 10:25:28.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=172.20.8.7/library/nginx:1.14-alpine --namespace=kubectl-105'
Mar  1 10:25:28.630: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 10:25:28.630: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Mar  1 10:25:28.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete jobs e2e-test-nginx-job --namespace=kubectl-105'
Mar  1 10:25:28.996: INFO: stderr: ""
Mar  1 10:25:28.996: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:25:28.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-105" for this suite.
Mar  1 10:25:53.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:25:53.261: INFO: namespace kubectl-105 deletion completed in 24.251052206s

• [SLOW TEST:25.218 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:25:53.261: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:25:53.522: INFO: Creating deployment "nginx-deployment"
Mar  1 10:25:53.557: INFO: Waiting for observed generation 1
Mar  1 10:25:55.570: INFO: Waiting for all required pods to come up
Mar  1 10:25:55.610: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  1 10:26:01.631: INFO: Waiting for deployment "nginx-deployment" to complete
Mar  1 10:26:01.641: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar  1 10:26:01.776: INFO: Updating deployment nginx-deployment
Mar  1 10:26:01.776: INFO: Waiting for observed generation 2
Mar  1 10:26:03.818: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  1 10:26:03.823: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  1 10:26:03.827: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  1 10:26:03.839: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  1 10:26:03.839: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  1 10:26:03.843: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar  1 10:26:03.850: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar  1 10:26:03.850: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar  1 10:26:03.882: INFO: Updating deployment nginx-deployment
Mar  1 10:26:03.882: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar  1 10:26:03.911: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  1 10:26:03.926: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  1 10:26:06.032: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1881,SelfLink:/apis/apps/v1/namespaces/deployment-1881/deployments/nginx-deployment,UID:5ab55368-2f17-44e0-8c4a-a67b0a15b446,ResourceVersion:205388,Generation:3,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2020-03-01 10:26:03 +0000 UTC 2020-03-01 10:26:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-03-01 10:26:04 +0000 UTC 2020-03-01 10:25:53 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar  1 10:26:06.062: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-1881,SelfLink:/apis/apps/v1/namespaces/deployment-1881/replicasets/nginx-deployment-55fb7cb77f,UID:544d99e8-bb47-4df0-b723-3ae9e9775e51,ResourceVersion:205380,Generation:3,CreationTimestamp:2020-03-01 10:26:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5ab55368-2f17-44e0-8c4a-a67b0a15b446 0xc002855a97 0xc002855a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:26:06.062: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar  1 10:26:06.063: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9,GenerateName:,Namespace:deployment-1881,SelfLink:/apis/apps/v1/namespaces/deployment-1881/replicasets/nginx-deployment-785f69f5d9,UID:f4e91643-0777-44a4-aa8c-1c71744e1909,ResourceVersion:205378,Generation:3,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5ab55368-2f17-44e0-8c4a-a67b0a15b446 0xc002855b67 0xc002855b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar  1 10:26:06.075: INFO: Pod "nginx-deployment-55fb7cb77f-2nkdf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2nkdf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-2nkdf,UID:cd369771-6e19-420e-832c-5ed988fdcfea,ResourceVersion:205399,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc001f9f247 0xc001f9f248}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f9f3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f9f480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-01 10:26:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.075: INFO: Pod "nginx-deployment-55fb7cb77f-4clvx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4clvx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-4clvx,UID:25b0c09b-cd1a-4d64-837a-adc785780e7b,ResourceVersion:205308,Generation:0,CreationTimestamp:2020-03-01 10:26:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc001f9f780 0xc001f9f781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f9f8c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f9f9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-01 10:26:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.076: INFO: Pod "nginx-deployment-55fb7cb77f-5qzjw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5qzjw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-5qzjw,UID:0c5fe82b-19ce-4b93-891f-e5d1215d6a96,ResourceVersion:205363,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc001f9fc50 0xc001f9fc51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f9fce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f9fd00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.076: INFO: Pod "nginx-deployment-55fb7cb77f-679j9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-679j9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-679j9,UID:8a84a1c0-87a2-4dc7-8fd1-a736bf6fbe65,ResourceVersion:205318,Generation:0,CreationTimestamp:2020-03-01 10:26:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc001f9fd90 0xc001f9fd91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f9fe10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f9fe30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-01 10:26:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.076: INFO: Pod "nginx-deployment-55fb7cb77f-8llzt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8llzt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-8llzt,UID:6f3f6643-9310-4ab4-be72-6da1752d5ab1,ResourceVersion:205390,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc001f9ff00 0xc001f9ff01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f9ff80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f9ffa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-01 10:26:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.077: INFO: Pod "nginx-deployment-55fb7cb77f-9pxk4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9pxk4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-9pxk4,UID:9ec43fc4-1071-47b1-8e07-e9dd858abf35,ResourceVersion:205397,Generation:0,CreationTimestamp:2020-03-01 10:26:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac080 0xc0021ac081}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ac100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ac120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-01 10:26:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.077: INFO: Pod "nginx-deployment-55fb7cb77f-dtrpb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dtrpb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-dtrpb,UID:dd804eb5-ae11-4b34-8d4a-ea6e40c00bac,ResourceVersion:205364,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac210 0xc0021ac211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ac2a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ac370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.077: INFO: Pod "nginx-deployment-55fb7cb77f-f6v82" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-f6v82,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-f6v82,UID:77869c4e-5fcc-4af3-8707-41012dae1015,ResourceVersion:205367,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac3f0 0xc0021ac3f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ac470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ac490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.077: INFO: Pod "nginx-deployment-55fb7cb77f-j4kwk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j4kwk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-j4kwk,UID:b0da28c2-5df5-4071-8b66-fa22aaad1421,ResourceVersion:205376,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac510 0xc0021ac511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ac5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ac5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.078: INFO: Pod "nginx-deployment-55fb7cb77f-lb8x4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lb8x4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-lb8x4,UID:0a662f98-d5fc-4058-8a6a-1aff0c8b479d,ResourceVersion:205312,Generation:0,CreationTimestamp:2020-03-01 10:26:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac650 0xc0021ac651}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ac6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ac700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-01 10:26:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.078: INFO: Pod "nginx-deployment-55fb7cb77f-pdkj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pdkj2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-pdkj2,UID:6345bd84-8c21-4464-bee2-ae963be254e5,ResourceVersion:205317,Generation:0,CreationTimestamp:2020-03-01 10:26:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac7e0 0xc0021ac7e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ac860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ac880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-01 10:26:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.078: INFO: Pod "nginx-deployment-55fb7cb77f-tkl6d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tkl6d,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-tkl6d,UID:e141ea82-2bad-4c77-9f8b-eb7dbcbd61c3,ResourceVersion:205400,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ac960 0xc0021ac961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021aca50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021aca80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-01 10:26:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.079: INFO: Pod "nginx-deployment-55fb7cb77f-x4d4j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-x4d4j,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-55fb7cb77f-x4d4j,UID:a4307e3c-afdf-4d29-8516-6f56d77dbf93,ResourceVersion:205315,Generation:0,CreationTimestamp:2020-03-01 10:26:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 544d99e8-bb47-4df0-b723-3ae9e9775e51 0xc0021ad200 0xc0021ad201}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ad350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ad370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:02 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-01 10:26:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.079: INFO: Pod "nginx-deployment-785f69f5d9-52m5g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-52m5g,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-52m5g,UID:ba3fe103-a2fd-4cbf-94bd-f53abf1899f4,ResourceVersion:205370,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc0021ad440 0xc0021ad441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ad4c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ad4e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.079: INFO: Pod "nginx-deployment-785f69f5d9-649fq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-649fq,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-649fq,UID:3c73ae9f-e84c-4fd7-9014-09294e974827,ResourceVersion:205249,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c050 0xc002e4c051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c0c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c0f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.125,StartTime:2020-03-01 10:25:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:58 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://1a33a1b613f87e7b937787bc39245e3ae92dc7bb05247adf09d061cb996519c2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.080: INFO: Pod "nginx-deployment-785f69f5d9-87w8m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-87w8m,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-87w8m,UID:45accd46-cda5-47a4-8f1d-dfa199a892fa,ResourceVersion:205372,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c1c0 0xc002e4c1c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.080: INFO: Pod "nginx-deployment-785f69f5d9-bmdtg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-bmdtg,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-bmdtg,UID:fd5090fa-fd46-49f9-8636-755f7a31cb98,ResourceVersion:205360,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c2e0 0xc002e4c2e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.080: INFO: Pod "nginx-deployment-785f69f5d9-bx4p8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-bx4p8,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-bx4p8,UID:464476d9-b440-4b0f-9cec-36e7c60a5872,ResourceVersion:205359,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c400 0xc002e4c401}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.080: INFO: Pod "nginx-deployment-785f69f5d9-dgvjw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-dgvjw,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-dgvjw,UID:2673f0d8-4754-4f0c-8d5d-bcaab1ff81c2,ResourceVersion:205218,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c510 0xc002e4c511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c5a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.172,StartTime:2020-03-01 10:25:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:57 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://ce1ef48e75cb3445aa850d69307c0a4638139a0fed898ba705a7b7bd7feafad1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.081: INFO: Pod "nginx-deployment-785f69f5d9-dxdkc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-dxdkc,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-dxdkc,UID:1eff12d9-656e-4596-a652-a19771758989,ResourceVersion:205343,Generation:0,CreationTimestamp:2020-03-01 10:26:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c670 0xc002e4c671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c6f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.081: INFO: Pod "nginx-deployment-785f69f5d9-dxt62" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-dxt62,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-dxt62,UID:9c55e64c-9a16-4f81-8fe5-5fe5e3b21e89,ResourceVersion:205235,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c790 0xc002e4c791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.169,StartTime:2020-03-01 10:25:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:56 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://85c63c717bdab5c88e6a5070e547cd30c4e04340e96ba520d405d17fcc17f8f5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.081: INFO: Pod "nginx-deployment-785f69f5d9-k54pz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-k54pz,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-k54pz,UID:0336d70a-5ee3-4dc3-b3d3-33c20e56f466,ResourceVersion:205358,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4c8f0 0xc002e4c8f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4c960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4c980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.082: INFO: Pod "nginx-deployment-785f69f5d9-m9l9n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-m9l9n,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-m9l9n,UID:68bc5929-12fc-49e2-81fe-1f033e67c76f,ResourceVersion:205352,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4ca10 0xc002e4ca11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4ca80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4caa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.082: INFO: Pod "nginx-deployment-785f69f5d9-mkl4g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-mkl4g,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-mkl4g,UID:fb252eb6-9564-45ae-a463-a5af326dfd6f,ResourceVersion:205401,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4cb20 0xc002e4cb21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4cbb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4cbd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:,StartTime:2020-03-01 10:26:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 172.20.8.7/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.082: INFO: Pod "nginx-deployment-785f69f5d9-nd6zk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-nd6zk,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-nd6zk,UID:344cf659-efeb-458e-89f4-00b06af4210d,ResourceVersion:205371,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4cc90 0xc002e4cc91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4cd10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4cd30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.083: INFO: Pod "nginx-deployment-785f69f5d9-p5tdr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-p5tdr,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-p5tdr,UID:01d0430d-3f1a-457b-ad26-7ff3d2108d09,ResourceVersion:205213,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4cdb0 0xc002e4cdb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4ce30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4ce50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.171,StartTime:2020-03-01 10:25:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:57 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://00c9feefa2f2069b21fe8d3fd24039852cd056c3c48b201f073684e0f4c6c34e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.083: INFO: Pod "nginx-deployment-785f69f5d9-psz6t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-psz6t,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-psz6t,UID:6b036e03-4e2a-44cf-aaed-a46ebf7d5152,ResourceVersion:205375,Generation:0,CreationTimestamp:2020-03-01 10:26:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4cf30 0xc002e4cf31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4cfa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4cfc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.083: INFO: Pod "nginx-deployment-785f69f5d9-q74t7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-q74t7,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-q74t7,UID:15ce6336-a30d-4633-9b4f-222f45db7973,ResourceVersion:205338,Generation:0,CreationTimestamp:2020-03-01 10:26:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4d040 0xc002e4d041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4d0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4d0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.084: INFO: Pod "nginx-deployment-785f69f5d9-rkbr4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-rkbr4,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-rkbr4,UID:64a21d05-bb39-4725-a410-197b317c4430,ResourceVersion:205391,Generation:0,CreationTimestamp:2020-03-01 10:26:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4d150 0xc002e4d151}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4d1d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4d1f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:26:03 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:,StartTime:2020-03-01 10:26:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 172.20.8.7/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.084: INFO: Pod "nginx-deployment-785f69f5d9-s27cv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-s27cv,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-s27cv,UID:de888902-a2d6-45a3-a742-6ee23bba421d,ResourceVersion:205222,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4d2b0 0xc002e4d2b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4d330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4d350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.170,StartTime:2020-03-01 10:25:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:57 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://4a5ecbd7123420a68c052e40ef8009053716e6c0641f0a9b1bbe0b1691d08c7a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.084: INFO: Pod "nginx-deployment-785f69f5d9-t7z7r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-t7z7r,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-t7z7r,UID:c3ddda91-9e9f-47d8-8f1b-d974776633ef,ResourceVersion:205231,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4d420 0xc002e4d421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4d4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4d4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.5,PodIP:10.244.3.173,StartTime:2020-03-01 10:25:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:57 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://66fdea1b009f41c6e41de59bdec593ced26ae9fc2ee08efa5720c85851662911}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.085: INFO: Pod "nginx-deployment-785f69f5d9-wbrtg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-wbrtg,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-wbrtg,UID:f9675700-76ed-47eb-a998-5f1140b69486,ResourceVersion:205244,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4d590 0xc002e4d591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4d600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4d620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.123,StartTime:2020-03-01 10:25:54 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:58 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://89964d5558916444c95b1bb5e7ec4d153385a5541be8b4c963cb9bc4cc7a0dcc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar  1 10:26:06.085: INFO: Pod "nginx-deployment-785f69f5d9-z7qfb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-785f69f5d9-z7qfb,GenerateName:nginx-deployment-785f69f5d9-,Namespace:deployment-1881,SelfLink:/api/v1/namespaces/deployment-1881/pods/nginx-deployment-785f69f5d9-z7qfb,UID:6b39908d-d440-4579-842d-f19bba0d8568,ResourceVersion:205253,Generation:0,CreationTimestamp:2020-03-01 10:25:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 785f69f5d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-785f69f5d9 f4e91643-0777-44a4-aa8c-1c71744e1909 0xc002e4d6f0 0xc002e4d6f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d6x5m {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d6x5m,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d6x5m true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e4d760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e4d780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:25:53 +0000 UTC  }],Message:,Reason:,HostIP:172.20.8.6,PodIP:10.244.4.124,StartTime:2020-03-01 10:25:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-03-01 10:25:58 +0000 UTC,} nil} {nil nil nil} true 0 172.20.8.7/library/nginx:1.14-alpine docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://8e6ba34800e7eab94501c9e34fe9c5db3dafd2d5bd537c75be626f7a6a6f6558}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:26:06.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1881" for this suite.
Mar  1 10:26:18.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:26:18.414: INFO: namespace deployment-1881 deletion completed in 12.321476554s

• [SLOW TEST:25.153 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:26:18.415: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Mar  1 10:26:18.783: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:26:30.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3857" for this suite.
Mar  1 10:26:54.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:26:55.136: INFO: namespace init-container-3857 deletion completed in 24.192648343s

• [SLOW TEST:36.721 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:26:55.136: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Mar  1 10:26:55.474: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar  1 10:26:55.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8554'
Mar  1 10:26:57.128: INFO: stderr: ""
Mar  1 10:26:57.128: INFO: stdout: "service/redis-slave created\n"
Mar  1 10:26:57.128: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar  1 10:26:57.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8554'
Mar  1 10:26:57.948: INFO: stderr: ""
Mar  1 10:26:57.948: INFO: stdout: "service/redis-master created\n"
Mar  1 10:26:57.948: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  1 10:26:57.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8554'
Mar  1 10:26:58.692: INFO: stderr: ""
Mar  1 10:26:58.692: INFO: stdout: "service/frontend created\n"
Mar  1 10:26:58.693: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: 172.20.8.7/library/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar  1 10:26:58.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8554'
Mar  1 10:26:59.552: INFO: stderr: ""
Mar  1 10:26:59.552: INFO: stdout: "deployment.apps/frontend created\n"
Mar  1 10:26:59.552: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: 172.20.8.7/library/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  1 10:26:59.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8554'
Mar  1 10:27:00.406: INFO: stderr: ""
Mar  1 10:27:00.406: INFO: stdout: "deployment.apps/redis-master created\n"
Mar  1 10:27:00.406: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: 172.20.8.7/library/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar  1 10:27:00.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-8554'
Mar  1 10:27:01.350: INFO: stderr: ""
Mar  1 10:27:01.350: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar  1 10:27:01.350: INFO: Waiting for all frontend pods to be Running.
Mar  1 10:27:06.425: INFO: Waiting for frontend to serve content.
Mar  1 10:27:06.597: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Mar  1 10:27:13.028: INFO: Trying to add a new entry to the guestbook.
Mar  1 10:27:13.155: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  1 10:27:13.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8554'
Mar  1 10:27:13.803: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:27:13.803: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 10:27:13.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8554'
Mar  1 10:27:14.128: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:27:14.128: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 10:27:14.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8554'
Mar  1 10:27:14.502: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:27:14.502: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 10:27:14.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8554'
Mar  1 10:27:14.741: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:27:14.741: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 10:27:14.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8554'
Mar  1 10:27:14.986: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:27:14.986: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  1 10:27:14.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-8554'
Mar  1 10:27:15.265: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:27:15.265: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:27:15.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8554" for this suite.
Mar  1 10:27:55.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:27:55.704: INFO: namespace kubectl-8554 deletion completed in 40.346061292s

• [SLOW TEST:60.568 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:27:55.705: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  1 10:27:59.332: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0301 10:27:59.332701      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  1 10:27:59.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-816" for this suite.
Mar  1 10:28:07.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:28:07.560: INFO: namespace gc-816 deletion completed in 8.18378975s

• [SLOW TEST:11.855 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:28:07.560: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  1 10:28:08.032: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9082,SelfLink:/api/v1/namespaces/watch-9082/configmaps/e2e-watch-test-label-changed,UID:36a8d6c9-2652-4e29-aa42-64b939bdf21c,ResourceVersion:206171,Generation:0,CreationTimestamp:2020-03-01 10:28:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 10:28:08.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9082,SelfLink:/api/v1/namespaces/watch-9082/configmaps/e2e-watch-test-label-changed,UID:36a8d6c9-2652-4e29-aa42-64b939bdf21c,ResourceVersion:206172,Generation:0,CreationTimestamp:2020-03-01 10:28:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar  1 10:28:08.054: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9082,SelfLink:/api/v1/namespaces/watch-9082/configmaps/e2e-watch-test-label-changed,UID:36a8d6c9-2652-4e29-aa42-64b939bdf21c,ResourceVersion:206173,Generation:0,CreationTimestamp:2020-03-01 10:28:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  1 10:28:18.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9082,SelfLink:/api/v1/namespaces/watch-9082/configmaps/e2e-watch-test-label-changed,UID:36a8d6c9-2652-4e29-aa42-64b939bdf21c,ResourceVersion:206193,Generation:0,CreationTimestamp:2020-03-01 10:28:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 10:28:18.224: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9082,SelfLink:/api/v1/namespaces/watch-9082/configmaps/e2e-watch-test-label-changed,UID:36a8d6c9-2652-4e29-aa42-64b939bdf21c,ResourceVersion:206194,Generation:0,CreationTimestamp:2020-03-01 10:28:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar  1 10:28:18.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9082,SelfLink:/api/v1/namespaces/watch-9082/configmaps/e2e-watch-test-label-changed,UID:36a8d6c9-2652-4e29-aa42-64b939bdf21c,ResourceVersion:206195,Generation:0,CreationTimestamp:2020-03-01 10:28:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:28:18.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9082" for this suite.
Mar  1 10:28:24.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:28:24.418: INFO: namespace watch-9082 deletion completed in 6.178278015s

• [SLOW TEST:16.858 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:28:24.419: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar  1 10:28:30.852: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-062681313 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar  1 10:28:46.250: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:28:46.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2119" for this suite.
Mar  1 10:28:52.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:28:52.553: INFO: namespace pods-2119 deletion completed in 6.290990475s

• [SLOW TEST:28.134 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:28:52.554: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  1 10:28:52.950: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:52.951: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:52.951: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:52.984: INFO: Number of nodes with available pods: 0
Mar  1 10:28:52.984: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:28:53.992: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:53.993: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:53.993: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:53.998: INFO: Number of nodes with available pods: 0
Mar  1 10:28:53.998: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:28:54.993: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:54.993: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:54.993: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:54.999: INFO: Number of nodes with available pods: 0
Mar  1 10:28:54.999: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:28:55.992: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:55.992: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:55.992: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:55.996: INFO: Number of nodes with available pods: 1
Mar  1 10:28:55.996: INFO: Node worker02 is running more than one daemon pod
Mar  1 10:28:56.992: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:56.992: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:56.992: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:56.997: INFO: Number of nodes with available pods: 2
Mar  1 10:28:56.997: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  1 10:28:57.042: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:57.042: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:57.043: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:57.047: INFO: Number of nodes with available pods: 1
Mar  1 10:28:57.048: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:28:58.057: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:58.057: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:58.057: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:58.063: INFO: Number of nodes with available pods: 1
Mar  1 10:28:58.063: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:28:59.056: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:59.056: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:59.056: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:28:59.061: INFO: Number of nodes with available pods: 1
Mar  1 10:28:59.061: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:29:00.055: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:00.055: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:00.056: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:00.061: INFO: Number of nodes with available pods: 1
Mar  1 10:29:00.061: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:29:01.057: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:01.057: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:01.057: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:01.064: INFO: Number of nodes with available pods: 1
Mar  1 10:29:01.064: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:29:02.055: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:02.055: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:02.055: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:02.060: INFO: Number of nodes with available pods: 1
Mar  1 10:29:02.060: INFO: Node worker01 is running more than one daemon pod
Mar  1 10:29:03.058: INFO: DaemonSet pods can't tolerate node centos02 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:03.058: INFO: DaemonSet pods can't tolerate node centos03 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:03.059: INFO: DaemonSet pods can't tolerate node centos04 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 10:29:03.064: INFO: Number of nodes with available pods: 2
Mar  1 10:29:03.064: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7664, will wait for the garbage collector to delete the pods
Mar  1 10:29:03.167: INFO: Deleting DaemonSet.extensions daemon-set took: 40.993471ms
Mar  1 10:29:03.667: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.417274ms
Mar  1 10:29:14.978: INFO: Number of nodes with available pods: 0
Mar  1 10:29:14.978: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 10:29:14.983: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7664/daemonsets","resourceVersion":"206395"},"items":null}

Mar  1 10:29:14.988: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7664/pods","resourceVersion":"206395"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:29:15.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7664" for this suite.
Mar  1 10:29:23.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:29:23.277: INFO: namespace daemonsets-7664 deletion completed in 8.264715958s

• [SLOW TEST:30.723 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:29:23.278: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:29:23.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1164" for this suite.
Mar  1 10:29:47.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:29:47.841: INFO: namespace pods-1164 deletion completed in 24.204983384s

• [SLOW TEST:24.563 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:29:47.841: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0301 10:29:54.268058      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  1 10:29:54.353: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:29:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3936" for this suite.
Mar  1 10:30:02.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:30:02.686: INFO: namespace gc-3936 deletion completed in 8.309417358s

• [SLOW TEST:14.845 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:30:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Mar  1 10:30:02.957: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-062681313 proxy --unix-socket=/tmp/kubectl-proxy-unix114504384/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:30:03.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9695" for this suite.
Mar  1 10:30:09.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:30:09.521: INFO: namespace kubectl-9695 deletion completed in 6.21011812s

• [SLOW TEST:6.834 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:30:09.521: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 10:30:14.940: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:30:15.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7014" for this suite.
Mar  1 10:30:21.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:30:21.195: INFO: namespace container-runtime-7014 deletion completed in 6.176354538s

• [SLOW TEST:11.674 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:30:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  1 10:30:26.647: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:30:26.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5170" for this suite.
Mar  1 10:30:32.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:30:33.041: INFO: namespace container-runtime-5170 deletion completed in 6.31943967s

• [SLOW TEST:11.845 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:30:33.041: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 10:30:33.397: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6" in namespace "projected-583" to be "success or failure"
Mar  1 10:30:33.404: INFO: Pod "downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.378756ms
Mar  1 10:30:35.410: INFO: Pod "downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012801403s
Mar  1 10:30:37.613: INFO: Pod "downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215634865s
Mar  1 10:30:39.619: INFO: Pod "downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.221707645s
STEP: Saw pod success
Mar  1 10:30:39.619: INFO: Pod "downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6" satisfied condition "success or failure"
Mar  1 10:30:39.624: INFO: Trying to get logs from node worker02 pod downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6 container client-container: <nil>
STEP: delete the pod
Mar  1 10:30:39.697: INFO: Waiting for pod downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6 to disappear
Mar  1 10:30:39.702: INFO: Pod downwardapi-volume-fb3fff23-e9fb-46e1-95ca-20896c2455c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:30:39.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-583" for this suite.
Mar  1 10:30:45.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:30:45.931: INFO: namespace projected-583 deletion completed in 6.221802276s

• [SLOW TEST:12.890 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:30:45.931: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  1 10:30:46.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3849,SelfLink:/api/v1/namespaces/watch-3849/configmaps/e2e-watch-test-watch-closed,UID:18f64137-120d-48a7-ae22-4d1df15b1155,ResourceVersion:206898,Generation:0,CreationTimestamp:2020-03-01 10:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar  1 10:30:46.431: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3849,SelfLink:/api/v1/namespaces/watch-3849/configmaps/e2e-watch-test-watch-closed,UID:18f64137-120d-48a7-ae22-4d1df15b1155,ResourceVersion:206899,Generation:0,CreationTimestamp:2020-03-01 10:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  1 10:30:46.638: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3849,SelfLink:/api/v1/namespaces/watch-3849/configmaps/e2e-watch-test-watch-closed,UID:18f64137-120d-48a7-ae22-4d1df15b1155,ResourceVersion:206900,Generation:0,CreationTimestamp:2020-03-01 10:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 10:30:46.638: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3849,SelfLink:/api/v1/namespaces/watch-3849/configmaps/e2e-watch-test-watch-closed,UID:18f64137-120d-48a7-ae22-4d1df15b1155,ResourceVersion:206903,Generation:0,CreationTimestamp:2020-03-01 10:30:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:30:46.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3849" for this suite.
Mar  1 10:30:52.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:30:52.885: INFO: namespace watch-3849 deletion completed in 6.233478431s

• [SLOW TEST:6.953 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:30:52.885: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f9328a2a-d910-4053-ad40-f4c9ff498ca9
STEP: Creating a pod to test consume configMaps
Mar  1 10:30:53.265: INFO: Waiting up to 5m0s for pod "pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178" in namespace "configmap-2663" to be "success or failure"
Mar  1 10:30:53.300: INFO: Pod "pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178": Phase="Pending", Reason="", readiness=false. Elapsed: 34.865552ms
Mar  1 10:30:55.308: INFO: Pod "pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042829793s
Mar  1 10:30:57.314: INFO: Pod "pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049187176s
STEP: Saw pod success
Mar  1 10:30:57.314: INFO: Pod "pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178" satisfied condition "success or failure"
Mar  1 10:30:57.318: INFO: Trying to get logs from node worker02 pod pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 10:30:57.399: INFO: Waiting for pod pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178 to disappear
Mar  1 10:30:57.403: INFO: Pod pod-configmaps-44874f21-6edc-4424-9e38-8fa4bccee178 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:30:57.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2663" for this suite.
Mar  1 10:31:03.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:31:03.589: INFO: namespace configmap-2663 deletion completed in 6.176383954s

• [SLOW TEST:10.704 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:31:03.589: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3451
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  1 10:31:08.542: INFO: Successfully updated pod "labelsupdatee528ecc6-99f0-4ed8-922d-132a5f5ba335"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:31:12.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3451" for this suite.
Mar  1 10:31:36.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:31:36.835: INFO: namespace projected-3451 deletion completed in 24.239944625s

• [SLOW TEST:33.246 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:31:36.836: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:31:37.100: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  1 10:31:39.387: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:31:40.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8295" for this suite.
Mar  1 10:31:48.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:31:48.600: INFO: namespace replication-controller-8295 deletion completed in 8.191706612s

• [SLOW TEST:11.764 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:31:48.601: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  1 10:31:48.891: INFO: Waiting up to 5m0s for pod "pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3" in namespace "emptydir-7836" to be "success or failure"
Mar  1 10:31:48.895: INFO: Pod "pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.871056ms
Mar  1 10:31:50.918: INFO: Pod "pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026250586s
Mar  1 10:31:52.938: INFO: Pod "pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046834742s
STEP: Saw pod success
Mar  1 10:31:52.938: INFO: Pod "pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3" satisfied condition "success or failure"
Mar  1 10:31:52.943: INFO: Trying to get logs from node worker02 pod pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3 container test-container: <nil>
STEP: delete the pod
Mar  1 10:31:53.015: INFO: Waiting for pod pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3 to disappear
Mar  1 10:31:53.020: INFO: Pod pod-1ece9ccd-6d5c-4485-b5d2-94529f0148e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:31:53.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7836" for this suite.
Mar  1 10:31:59.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:31:59.191: INFO: namespace emptydir-7836 deletion completed in 6.164682929s

• [SLOW TEST:10.590 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:31:59.191: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Mar  1 10:31:59.451: INFO: namespace kubectl-1671
Mar  1 10:31:59.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-1671'
Mar  1 10:32:04.623: INFO: stderr: ""
Mar  1 10:32:04.623: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar  1 10:32:05.630: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:32:05.630: INFO: Found 0 / 1
Mar  1 10:32:06.630: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:32:06.630: INFO: Found 0 / 1
Mar  1 10:32:07.628: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:32:07.629: INFO: Found 0 / 1
Mar  1 10:32:08.630: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:32:08.630: INFO: Found 1 / 1
Mar  1 10:32:08.630: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 10:32:08.634: INFO: Selector matched 1 pods for map[app:redis]
Mar  1 10:32:08.634: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 10:32:08.634: INFO: wait on redis-master startup in kubectl-1671 
Mar  1 10:32:08.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 logs redis-master-tlrtj redis-master --namespace=kubectl-1671'
Mar  1 10:32:08.872: INFO: stderr: ""
Mar  1 10:32:08.872: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 01 Mar 10:32:07.527 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 01 Mar 10:32:07.528 # Server started, Redis version 3.2.12\n1:M 01 Mar 10:32:07.528 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 01 Mar 10:32:07.528 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Mar  1 10:32:08.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1671'
Mar  1 10:32:09.239: INFO: stderr: ""
Mar  1 10:32:09.239: INFO: stdout: "service/rm2 exposed\n"
Mar  1 10:32:09.245: INFO: Service rm2 in namespace kubectl-1671 found.
STEP: exposing service
Mar  1 10:32:11.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1671'
Mar  1 10:32:11.646: INFO: stderr: ""
Mar  1 10:32:11.646: INFO: stdout: "service/rm3 exposed\n"
Mar  1 10:32:11.669: INFO: Service rm3 in namespace kubectl-1671 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:32:13.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1671" for this suite.
Mar  1 10:32:37.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:32:37.943: INFO: namespace kubectl-1671 deletion completed in 24.186809469s

• [SLOW TEST:38.752 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:32:37.943: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-nwrw
STEP: Creating a pod to test atomic-volume-subpath
Mar  1 10:32:38.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nwrw" in namespace "subpath-1096" to be "success or failure"
Mar  1 10:32:38.293: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153129ms
Mar  1 10:32:40.299: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009304323s
Mar  1 10:32:42.306: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015842237s
Mar  1 10:32:44.311: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 6.021700124s
Mar  1 10:32:46.317: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 8.027446694s
Mar  1 10:32:48.323: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 10.033143745s
Mar  1 10:32:50.328: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 12.038485732s
Mar  1 10:32:52.334: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 14.044584346s
Mar  1 10:32:54.341: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 16.051276873s
Mar  1 10:32:56.348: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 18.05783858s
Mar  1 10:32:58.353: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 20.063711537s
Mar  1 10:33:00.360: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 22.069885427s
Mar  1 10:33:02.366: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Running", Reason="", readiness=true. Elapsed: 24.076557874s
Mar  1 10:33:04.372: INFO: Pod "pod-subpath-test-downwardapi-nwrw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.082292612s
STEP: Saw pod success
Mar  1 10:33:04.372: INFO: Pod "pod-subpath-test-downwardapi-nwrw" satisfied condition "success or failure"
Mar  1 10:33:04.376: INFO: Trying to get logs from node worker02 pod pod-subpath-test-downwardapi-nwrw container test-container-subpath-downwardapi-nwrw: <nil>
STEP: delete the pod
Mar  1 10:33:04.442: INFO: Waiting for pod pod-subpath-test-downwardapi-nwrw to disappear
Mar  1 10:33:04.446: INFO: Pod pod-subpath-test-downwardapi-nwrw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nwrw
Mar  1 10:33:04.446: INFO: Deleting pod "pod-subpath-test-downwardapi-nwrw" in namespace "subpath-1096"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:33:04.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1096" for this suite.
Mar  1 10:33:10.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:33:10.825: INFO: namespace subpath-1096 deletion completed in 6.368157754s

• [SLOW TEST:32.882 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:33:10.826: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:33:19.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8837" for this suite.
Mar  1 10:33:25.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:33:25.463: INFO: namespace kubelet-test-8837 deletion completed in 6.277123623s

• [SLOW TEST:14.637 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:33:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:33:25.822: INFO: (0) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 21.779552ms)
Mar  1 10:33:25.827: INFO: (1) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.421933ms)
Mar  1 10:33:25.832: INFO: (2) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.84506ms)
Mar  1 10:33:25.837: INFO: (3) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.1375ms)
Mar  1 10:33:25.842: INFO: (4) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.400044ms)
Mar  1 10:33:25.848: INFO: (5) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.698773ms)
Mar  1 10:33:25.854: INFO: (6) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.363643ms)
Mar  1 10:33:25.858: INFO: (7) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.006707ms)
Mar  1 10:33:25.862: INFO: (8) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.063703ms)
Mar  1 10:33:25.867: INFO: (9) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.447736ms)
Mar  1 10:33:25.871: INFO: (10) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.44887ms)
Mar  1 10:33:25.875: INFO: (11) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.38522ms)
Mar  1 10:33:25.880: INFO: (12) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.364519ms)
Mar  1 10:33:25.885: INFO: (13) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.779603ms)
Mar  1 10:33:25.954: INFO: (14) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 68.723086ms)
Mar  1 10:33:25.981: INFO: (15) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 27.633969ms)
Mar  1 10:33:25.987: INFO: (16) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.86074ms)
Mar  1 10:33:25.993: INFO: (17) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.976466ms)
Mar  1 10:33:25.999: INFO: (18) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.334286ms)
Mar  1 10:33:26.004: INFO: (19) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.286336ms)
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:33:26.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-374" for this suite.
Mar  1 10:33:32.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:33:32.255: INFO: namespace proxy-374 deletion completed in 6.243795228s

• [SLOW TEST:6.792 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:33:32.255: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-0bbfb612-ef33-413d-a526-4bf331baf777
STEP: Creating a pod to test consume secrets
Mar  1 10:33:32.641: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78" in namespace "projected-3673" to be "success or failure"
Mar  1 10:33:32.658: INFO: Pod "pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.364959ms
Mar  1 10:33:34.664: INFO: Pod "pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022574889s
Mar  1 10:33:36.673: INFO: Pod "pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031604303s
Mar  1 10:33:38.681: INFO: Pod "pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03943089s
STEP: Saw pod success
Mar  1 10:33:38.681: INFO: Pod "pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78" satisfied condition "success or failure"
Mar  1 10:33:38.688: INFO: Trying to get logs from node worker02 pod pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  1 10:33:38.782: INFO: Waiting for pod pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78 to disappear
Mar  1 10:33:38.788: INFO: Pod pod-projected-secrets-af847f05-4c55-4b4f-bb75-f71c1ae2ff78 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:33:38.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3673" for this suite.
Mar  1 10:33:46.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:33:47.006: INFO: namespace projected-3673 deletion completed in 8.187565445s

• [SLOW TEST:14.751 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:33:47.007: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:33:53.480: INFO: Waiting up to 5m0s for pod "client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da" in namespace "pods-6471" to be "success or failure"
Mar  1 10:33:53.487: INFO: Pod "client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.262373ms
Mar  1 10:33:55.492: INFO: Pod "client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011918031s
Mar  1 10:33:57.502: INFO: Pod "client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021052204s
Mar  1 10:33:59.507: INFO: Pod "client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026909261s
STEP: Saw pod success
Mar  1 10:33:59.508: INFO: Pod "client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da" satisfied condition "success or failure"
Mar  1 10:33:59.512: INFO: Trying to get logs from node worker02 pod client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da container env3cont: <nil>
STEP: delete the pod
Mar  1 10:33:59.597: INFO: Waiting for pod client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da to disappear
Mar  1 10:33:59.601: INFO: Pod client-envvars-cd080104-d45e-406b-a646-8f94d3ebc7da no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:33:59.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6471" for this suite.
Mar  1 10:34:45.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:34:45.796: INFO: namespace pods-6471 deletion completed in 46.186590657s

• [SLOW TEST:58.790 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:34:45.797: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:34:50.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-407" for this suite.
Mar  1 10:34:56.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:34:56.708: INFO: namespace emptydir-wrapper-407 deletion completed in 6.289653748s

• [SLOW TEST:10.912 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:34:56.709: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 10:34:56.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-pod --generator=run-pod/v1 --image=172.20.8.7/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1875'
Mar  1 10:34:57.198: INFO: stderr: ""
Mar  1 10:34:57.198: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Mar  1 10:35:02.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pod e2e-test-nginx-pod --namespace=kubectl-1875 -o json'
Mar  1 10:35:02.445: INFO: stderr: ""
Mar  1 10:35:02.445: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-01T10:34:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1875\",\n        \"resourceVersion\": \"207766\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1875/pods/e2e-test-nginx-pod\",\n        \"uid\": \"06e68fac-1895-4e9b-8aa9-ea1c38647da6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"172.20.8.7/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-r9xhc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker02\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-r9xhc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-r9xhc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T10:34:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T10:35:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T10:35:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-01T10:34:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ea34b896368a2169f89749770ca12e249437abcf9b7e257e15c6df97c27034a9\",\n                \"image\": \"172.20.8.7/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://172.20.8.7/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-01T10:35:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.20.8.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.173\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-01T10:34:57Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  1 10:35:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 replace -f - --namespace=kubectl-1875'
Mar  1 10:35:03.068: INFO: stderr: ""
Mar  1 10:35:03.069: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image 172.20.8.7/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Mar  1 10:35:03.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete pods e2e-test-nginx-pod --namespace=kubectl-1875'
Mar  1 10:35:06.764: INFO: stderr: ""
Mar  1 10:35:06.765: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:35:06.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1875" for this suite.
Mar  1 10:35:12.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:35:13.046: INFO: namespace kubectl-1875 deletion completed in 6.27391467s

• [SLOW TEST:16.337 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:35:13.046: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6357
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6357
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6357
Mar  1 10:35:13.414: INFO: Found 0 stateful pods, waiting for 1
Mar  1 10:35:23.422: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  1 10:35:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 10:35:24.146: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 10:35:24.146: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 10:35:24.146: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 10:35:24.152: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 10:35:34.169: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 10:35:34.169: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 10:35:34.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999902s
Mar  1 10:35:35.227: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.976233857s
Mar  1 10:35:36.233: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.970855747s
Mar  1 10:35:37.239: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.965250691s
Mar  1 10:35:38.246: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.959390201s
Mar  1 10:35:39.272: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.952462047s
Mar  1 10:35:40.278: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.926256202s
Mar  1 10:35:41.284: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.920189845s
Mar  1 10:35:42.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.913672045s
Mar  1 10:35:43.319: INFO: Verifying statefulset ss doesn't scale past 1 for another 886.572236ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6357
Mar  1 10:35:44.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:35:44.767: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  1 10:35:44.767: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 10:35:44.767: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 10:35:44.786: INFO: Found 1 stateful pods, waiting for 3
Mar  1 10:35:54.793: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 10:35:54.793: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 10:35:54.793: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  1 10:35:54.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 10:35:55.268: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 10:35:55.268: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 10:35:55.268: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 10:35:55.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 10:35:55.753: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 10:35:55.753: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 10:35:55.753: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 10:35:55.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar  1 10:35:56.272: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar  1 10:35:56.272: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar  1 10:35:56.272: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar  1 10:35:56.272: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 10:35:56.281: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  1 10:36:06.293: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 10:36:06.293: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 10:36:06.293: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 10:36:06.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999931s
Mar  1 10:36:07.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99494659s
Mar  1 10:36:08.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98848818s
Mar  1 10:36:09.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982045807s
Mar  1 10:36:10.342: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9721528s
Mar  1 10:36:11.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9651549s
Mar  1 10:36:12.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957139841s
Mar  1 10:36:13.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950112787s
Mar  1 10:36:14.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.909527913s
Mar  1 10:36:15.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.144405ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6357
Mar  1 10:36:16.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:16.979: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  1 10:36:16.979: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 10:36:16.979: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 10:36:16.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:17.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar  1 10:36:17.503: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar  1 10:36:17.503: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar  1 10:36:17.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:18.359: INFO: rc: 1
Mar  1 10:36:18.443: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server: 
 [] <nil> 0xc003c91ec0 exit status 1 <nil> <nil> true [0xc0009e3cd8 0xc0009e3d90 0xc0009e3e10] [0xc0009e3cd8 0xc0009e3d90 0xc0009e3e10] [0xc0009e3d58 0xc0009e3df0] [0xba70a0 0xba70a0] 0xc0039cb9e0 <nil>}:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Mar  1 10:36:28.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:28.633: INFO: rc: 1
Mar  1 10:36:28.633: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d69aa0 exit status 1 <nil> <nil> true [0xc0000d7288 0xc0000d74a8 0xc0000d7720] [0xc0000d7288 0xc0000d74a8 0xc0000d7720] [0xc0000d7430 0xc0000d74e8] [0xba70a0 0xba70a0] 0xc002ffd500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:36:38.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:38.826: INFO: rc: 1
Mar  1 10:36:38.826: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b2c2a0 exit status 1 <nil> <nil> true [0xc0009e3e28 0xc0009e3f40 0xc0009e3fc0] [0xc0009e3e28 0xc0009e3f40 0xc0009e3fc0] [0xc0009e3f20 0xc0009e3fa8] [0xba70a0 0xba70a0] 0xc0039cbec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:36:48.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:49.017: INFO: rc: 1
Mar  1 10:36:49.018: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f8b500 exit status 1 <nil> <nil> true [0xc0000113d0 0xc000011460 0xc0000114c0] [0xc0000113d0 0xc000011460 0xc0000114c0] [0xc000011410 0xc000011498] [0xba70a0 0xba70a0] 0xc000a07ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:36:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:36:59.205: INFO: rc: 1
Mar  1 10:36:59.205: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b2c630 exit status 1 <nil> <nil> true [0xc0009e3ff8 0xc00063b3f8 0xc00063b4e0] [0xc0009e3ff8 0xc00063b3f8 0xc00063b4e0] [0xc00063b388 0xc00063b4b8] [0xba70a0 0xba70a0] 0xc001e143c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:37:09.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:37:09.393: INFO: rc: 1
Mar  1 10:37:09.394: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b2c9f0 exit status 1 <nil> <nil> true [0xc00063b6a8 0xc00063b8a0 0xc00063b9a8] [0xc00063b6a8 0xc00063b8a0 0xc00063b9a8] [0xc00063b890 0xc00063b920] [0xba70a0 0xba70a0] 0xc001e14780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:37:19.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:37:19.593: INFO: rc: 1
Mar  1 10:37:19.593: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b2cd80 exit status 1 <nil> <nil> true [0xc00063ba40 0xc0024a8008 0xc0024a80e0] [0xc00063ba40 0xc0024a8008 0xc0024a80e0] [0xc00063bb90 0xc0024a8060] [0xba70a0 0xba70a0] 0xc001e14b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:37:29.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:37:29.820: INFO: rc: 1
Mar  1 10:37:29.820: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003b2d140 exit status 1 <nil> <nil> true [0xc0024a8100 0xc0024a8148 0xc0024a81d0] [0xc0024a8100 0xc0024a8148 0xc0024a81d0] [0xc0024a8130 0xc0024a8160] [0xba70a0 0xba70a0] 0xc001e14fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:37:39.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:37:40.015: INFO: rc: 1
Mar  1 10:37:40.015: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d69ec0 exit status 1 <nil> <nil> true [0xc0000d7790 0xc0000d7a08 0xc0000d7a58] [0xc0000d7790 0xc0000d7a08 0xc0000d7a58] [0xc0000d7930 0xc0000d7a38] [0xba70a0 0xba70a0] 0xc002b02060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:37:50.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:37:50.214: INFO: rc: 1
Mar  1 10:37:50.214: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f8b9b0 exit status 1 <nil> <nil> true [0xc0000114f8 0xc000011580 0xc000011608] [0xc0000114f8 0xc000011580 0xc000011608] [0xc000011550 0xc0000115c0] [0xba70a0 0xba70a0] 0xc001fb84e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:38:00.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:38:00.406: INFO: rc: 1
Mar  1 10:38:00.406: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0024e82d0 exit status 1 <nil> <nil> true [0xc0000d7a88 0xc0000d7b20 0xc0000d7ba0] [0xc0000d7a88 0xc0000d7b20 0xc0000d7ba0] [0xc0000d7ab8 0xc0000d7b88] [0xba70a0 0xba70a0] 0xc002b02420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:38:10.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:38:10.595: INFO: rc: 1
Mar  1 10:38:10.595: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d68360 exit status 1 <nil> <nil> true [0xc00063b388 0xc00063b4b8 0xc00063b728] [0xc00063b388 0xc00063b4b8 0xc00063b728] [0xc00063b430 0xc00063b6a8] [0xba70a0 0xba70a0] 0xc0039ca2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:38:20.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:38:20.976: INFO: rc: 1
Mar  1 10:38:20.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d68990 exit status 1 <nil> <nil> true [0xc00063b890 0xc00063b920 0xc00063bae8] [0xc00063b890 0xc00063b920 0xc00063bae8] [0xc00063b8b8 0xc00063ba40] [0xba70a0 0xba70a0] 0xc0039caba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:38:30.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:38:31.163: INFO: rc: 1
Mar  1 10:38:31.163: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c90390 exit status 1 <nil> <nil> true [0xc0009e20a0 0xc0009e23c0 0xc0009e2888] [0xc0009e20a0 0xc0009e23c0 0xc0009e2888] [0xc0009e2370 0xc0009e2868] [0xba70a0 0xba70a0] 0xc002ffc300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:38:41.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:38:41.358: INFO: rc: 1
Mar  1 10:38:41.359: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ec8360 exit status 1 <nil> <nil> true [0xc0000d64b0 0xc0000d6780 0xc0000d6d00] [0xc0000d64b0 0xc0000d6780 0xc0000d6d00] [0xc0000d6718 0xc0000d6848] [0xba70a0 0xba70a0] 0xc000a06900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:38:51.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:38:51.550: INFO: rc: 1
Mar  1 10:38:51.551: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c90750 exit status 1 <nil> <nil> true [0xc0009e28b0 0xc0009e2fa0 0xc0009e3420] [0xc0009e28b0 0xc0009e2fa0 0xc0009e3420] [0xc0009e2db8 0xc0009e3388] [0xba70a0 0xba70a0] 0xc002ffc840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:39:01.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:39:01.740: INFO: rc: 1
Mar  1 10:39:01.740: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c90ae0 exit status 1 <nil> <nil> true [0xc0009e3458 0xc0009e3608 0xc0009e36d0] [0xc0009e3458 0xc0009e3608 0xc0009e36d0] [0xc0009e35b8 0xc0009e3690] [0xba70a0 0xba70a0] 0xc002ffcd80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:39:11.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:39:11.933: INFO: rc: 1
Mar  1 10:39:11.934: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c2390 exit status 1 <nil> <nil> true [0xc0024a8008 0xc0024a80e0 0xc0024a8130] [0xc0024a8008 0xc0024a80e0 0xc0024a8130] [0xc0024a8060 0xc0024a8110] [0xba70a0 0xba70a0] 0xc002b02360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:39:21.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:39:22.128: INFO: rc: 1
Mar  1 10:39:22.129: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d68db0 exit status 1 <nil> <nil> true [0xc00063bb90 0xc000010248 0xc000010500] [0xc00063bb90 0xc000010248 0xc000010500] [0xc000010238 0xc0000104e0] [0xba70a0 0xba70a0] 0xc0039cb500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:39:32.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:39:32.316: INFO: rc: 1
Mar  1 10:39:32.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d69320 exit status 1 <nil> <nil> true [0xc000010518 0xc000010ca0 0xc000010e00] [0xc000010518 0xc000010ca0 0xc000010e00] [0xc000010690 0xc000010dd0] [0xba70a0 0xba70a0] 0xc0039cbb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:39:42.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:39:42.559: INFO: rc: 1
Mar  1 10:39:42.560: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d696b0 exit status 1 <nil> <nil> true [0xc000010f28 0xc000010fe8 0xc000011038] [0xc000010f28 0xc000010fe8 0xc000011038] [0xc000010fc8 0xc000011018] [0xba70a0 0xba70a0] 0xc0039cbf20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:39:52.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:39:52.761: INFO: rc: 1
Mar  1 10:39:52.761: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d69a70 exit status 1 <nil> <nil> true [0xc000011060 0xc000011180 0xc000011218] [0xc000011060 0xc000011180 0xc000011218] [0xc000011148 0xc000011208] [0xba70a0 0xba70a0] 0xc001e14420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:40:02.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:40:02.956: INFO: rc: 1
Mar  1 10:40:02.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ec8780 exit status 1 <nil> <nil> true [0xc0000d6d18 0xc0000d6d98 0xc0000d6e40] [0xc0000d6d18 0xc0000d6d98 0xc0000d6e40] [0xc0000d6d60 0xc0000d6df0] [0xba70a0 0xba70a0] 0xc000a06fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:40:12.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:40:13.192: INFO: rc: 1
Mar  1 10:40:13.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c903c0 exit status 1 <nil> <nil> true [0xc00063b388 0xc00063b4b8 0xc00063b728] [0xc00063b388 0xc00063b4b8 0xc00063b728] [0xc00063b430 0xc00063b6a8] [0xba70a0 0xba70a0] 0xc0039ca2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:40:23.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:40:23.389: INFO: rc: 1
Mar  1 10:40:23.389: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002ec8390 exit status 1 <nil> <nil> true [0xc0009e20a0 0xc0009e23c0 0xc0009e2888] [0xc0009e20a0 0xc0009e23c0 0xc0009e2888] [0xc0009e2370 0xc0009e2868] [0xba70a0 0xba70a0] 0xc002ffc300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:40:33.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:40:33.626: INFO: rc: 1
Mar  1 10:40:33.626: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003d68390 exit status 1 <nil> <nil> true [0xc0024a8008 0xc0024a80e0 0xc0024a8130] [0xc0024a8008 0xc0024a80e0 0xc0024a8130] [0xc0024a8060 0xc0024a8110] [0xba70a0 0xba70a0] 0xc002b02360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:40:43.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:40:43.820: INFO: rc: 1
Mar  1 10:40:43.820: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c907b0 exit status 1 <nil> <nil> true [0xc00063b890 0xc00063b920 0xc00063bae8] [0xc00063b890 0xc00063b920 0xc00063bae8] [0xc00063b8b8 0xc00063ba40] [0xba70a0 0xba70a0] 0xc0039caba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:40:53.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:40:54.017: INFO: rc: 1
Mar  1 10:40:54.017: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0022c23f0 exit status 1 <nil> <nil> true [0xc0000d64b0 0xc0000d6780 0xc0000d6d00] [0xc0000d64b0 0xc0000d6780 0xc0000d6d00] [0xc0000d6718 0xc0000d6848] [0xba70a0 0xba70a0] 0xc000a06900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:41:04.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:41:04.200: INFO: rc: 1
Mar  1 10:41:04.200: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c90b70 exit status 1 <nil> <nil> true [0xc00063bb90 0xc000010248 0xc000010500] [0xc00063bb90 0xc000010248 0xc000010500] [0xc000010238 0xc0000104e0] [0xba70a0 0xba70a0] 0xc0039cb500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:41:14.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:41:14.388: INFO: rc: 1
Mar  1 10:41:14.388: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003c91410 exit status 1 <nil> <nil> true [0xc000010518 0xc000010ca0 0xc000010e00] [0xc000010518 0xc000010ca0 0xc000010e00] [0xc000010690 0xc000010dd0] [0xba70a0 0xba70a0] 0xc0039cbb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 10:41:24.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 exec --namespace=statefulset-6357 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar  1 10:41:24.611: INFO: rc: 1
Mar  1 10:41:24.611: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Mar  1 10:41:24.611: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Mar  1 10:41:24.628: INFO: Deleting all statefulset in ns statefulset-6357
Mar  1 10:41:24.632: INFO: Scaling statefulset ss to 0
Mar  1 10:41:24.646: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 10:41:24.649: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:41:24.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6357" for this suite.
Mar  1 10:41:32.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:41:33.104: INFO: namespace statefulset-6357 deletion completed in 8.396055079s

• [SLOW TEST:380.058 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:41:33.104: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:41:33.382: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:41:37.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2717" for this suite.
Mar  1 10:42:18.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:42:18.314: INFO: namespace pods-2717 deletion completed in 40.31741842s

• [SLOW TEST:45.209 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:42:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1991
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9437
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:42:48.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8740" for this suite.
Mar  1 10:42:54.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:42:55.195: INFO: namespace namespaces-8740 deletion completed in 6.552270823s
STEP: Destroying namespace "nsdeletetest-1991" for this suite.
Mar  1 10:42:55.198: INFO: Namespace nsdeletetest-1991 was already deleted
STEP: Destroying namespace "nsdeletetest-9437" for this suite.
Mar  1 10:43:01.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:43:01.433: INFO: namespace nsdeletetest-9437 deletion completed in 6.234488362s

• [SLOW TEST:43.119 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:43:01.434: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-5a901f13-6f7d-41b9-8f38-26668e457936
STEP: Creating a pod to test consume configMaps
Mar  1 10:43:01.768: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a" in namespace "projected-4853" to be "success or failure"
Mar  1 10:43:01.774: INFO: Pod "pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.263999ms
Mar  1 10:43:03.781: INFO: Pod "pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012824376s
Mar  1 10:43:05.788: INFO: Pod "pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02000937s
Mar  1 10:43:07.798: INFO: Pod "pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02948551s
STEP: Saw pod success
Mar  1 10:43:07.798: INFO: Pod "pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a" satisfied condition "success or failure"
Mar  1 10:43:07.804: INFO: Trying to get logs from node worker02 pod pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 10:43:07.959: INFO: Waiting for pod pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a to disappear
Mar  1 10:43:07.964: INFO: Pod pod-projected-configmaps-1e0b3c24-8ffb-4f63-8583-24fbdccd027a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:43:07.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4853" for this suite.
Mar  1 10:43:14.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:43:14.244: INFO: namespace projected-4853 deletion completed in 6.226672045s

• [SLOW TEST:12.810 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:43:14.245: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-20e1eb80-9951-49a2-90da-7f42aecb2b76
STEP: Creating a pod to test consume configMaps
Mar  1 10:43:14.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721" in namespace "configmap-9255" to be "success or failure"
Mar  1 10:43:14.569: INFO: Pod "pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721": Phase="Pending", Reason="", readiness=false. Elapsed: 6.270043ms
Mar  1 10:43:16.576: INFO: Pod "pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013058293s
Mar  1 10:43:18.583: INFO: Pod "pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020004974s
Mar  1 10:43:20.589: INFO: Pod "pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026559047s
STEP: Saw pod success
Mar  1 10:43:20.590: INFO: Pod "pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721" satisfied condition "success or failure"
Mar  1 10:43:20.594: INFO: Trying to get logs from node worker02 pod pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 10:43:20.746: INFO: Waiting for pod pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721 to disappear
Mar  1 10:43:20.751: INFO: Pod pod-configmaps-4b1731d7-2715-4ddd-902c-a6d20c1fb721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:43:20.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9255" for this suite.
Mar  1 10:43:28.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:43:29.084: INFO: namespace configmap-9255 deletion completed in 8.323946397s

• [SLOW TEST:14.840 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:43:29.084: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Mar  1 10:43:36.069: INFO: Successfully updated pod "labelsupdate2bf2f58f-62cd-4ca2-8ebc-74c4c4ba0b02"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:43:38.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4594" for this suite.
Mar  1 10:44:02.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:44:02.571: INFO: namespace downward-api-4594 deletion completed in 24.465928772s

• [SLOW TEST:33.487 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:44:02.571: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-24ca9ef1-82a3-4b15-bd4f-e8f5b1758fe7
STEP: Creating a pod to test consume configMaps
Mar  1 10:44:02.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d" in namespace "configmap-1920" to be "success or failure"
Mar  1 10:44:02.932: INFO: Pod "pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.318743ms
Mar  1 10:44:04.946: INFO: Pod "pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031164736s
Mar  1 10:44:06.952: INFO: Pod "pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03758551s
Mar  1 10:44:08.961: INFO: Pod "pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046342723s
STEP: Saw pod success
Mar  1 10:44:08.961: INFO: Pod "pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d" satisfied condition "success or failure"
Mar  1 10:44:08.966: INFO: Trying to get logs from node worker02 pod pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 10:44:09.333: INFO: Waiting for pod pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d to disappear
Mar  1 10:44:09.340: INFO: Pod pod-configmaps-b7c6b0f9-e5a6-4714-893a-6e27b800561d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:44:09.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1920" for this suite.
Mar  1 10:44:15.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:44:15.723: INFO: namespace configmap-1920 deletion completed in 6.373043597s

• [SLOW TEST:13.152 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:44:15.724: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 10:44:16.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc" in namespace "downward-api-9159" to be "success or failure"
Mar  1 10:44:16.041: INFO: Pod "downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.193329ms
Mar  1 10:44:18.048: INFO: Pod "downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0125448s
Mar  1 10:44:20.056: INFO: Pod "downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019959123s
STEP: Saw pod success
Mar  1 10:44:20.056: INFO: Pod "downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc" satisfied condition "success or failure"
Mar  1 10:44:20.061: INFO: Trying to get logs from node worker02 pod downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc container client-container: <nil>
STEP: delete the pod
Mar  1 10:44:20.142: INFO: Waiting for pod downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc to disappear
Mar  1 10:44:20.147: INFO: Pod downwardapi-volume-ee9c96a2-cd8e-4ac2-9cdc-d3874349e2dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:44:20.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9159" for this suite.
Mar  1 10:44:26.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:44:26.494: INFO: namespace downward-api-9159 deletion completed in 6.337290893s

• [SLOW TEST:10.770 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:44:26.494: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  1 10:44:26.918: INFO: Waiting up to 5m0s for pod "pod-a693575c-34b8-4b0f-a455-ef402428d8b5" in namespace "emptydir-3297" to be "success or failure"
Mar  1 10:44:26.996: INFO: Pod "pod-a693575c-34b8-4b0f-a455-ef402428d8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 77.957315ms
Mar  1 10:44:29.003: INFO: Pod "pod-a693575c-34b8-4b0f-a455-ef402428d8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084828732s
Mar  1 10:44:31.011: INFO: Pod "pod-a693575c-34b8-4b0f-a455-ef402428d8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092306082s
Mar  1 10:44:33.018: INFO: Pod "pod-a693575c-34b8-4b0f-a455-ef402428d8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.099176499s
STEP: Saw pod success
Mar  1 10:44:33.018: INFO: Pod "pod-a693575c-34b8-4b0f-a455-ef402428d8b5" satisfied condition "success or failure"
Mar  1 10:44:33.023: INFO: Trying to get logs from node worker02 pod pod-a693575c-34b8-4b0f-a455-ef402428d8b5 container test-container: <nil>
STEP: delete the pod
Mar  1 10:44:33.074: INFO: Waiting for pod pod-a693575c-34b8-4b0f-a455-ef402428d8b5 to disappear
Mar  1 10:44:33.079: INFO: Pod pod-a693575c-34b8-4b0f-a455-ef402428d8b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:44:33.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3297" for this suite.
Mar  1 10:44:39.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:44:39.344: INFO: namespace emptydir-3297 deletion completed in 6.25598105s

• [SLOW TEST:12.850 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:44:39.344: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 10:44:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=172.20.8.7/library/nginx:1.14-alpine --namespace=kubectl-6793'
Mar  1 10:44:45.251: INFO: stderr: ""
Mar  1 10:44:45.251: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Mar  1 10:44:45.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete pods e2e-test-nginx-pod --namespace=kubectl-6793'
Mar  1 10:44:49.407: INFO: stderr: ""
Mar  1 10:44:49.407: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:44:49.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6793" for this suite.
Mar  1 10:44:55.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:44:55.650: INFO: namespace kubectl-6793 deletion completed in 6.234185665s

• [SLOW TEST:16.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:44:55.650: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 10:44:56.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85" in namespace "projected-9456" to be "success or failure"
Mar  1 10:44:56.025: INFO: Pod "downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.517653ms
Mar  1 10:44:58.032: INFO: Pod "downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01257382s
Mar  1 10:45:00.067: INFO: Pod "downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047835838s
Mar  1 10:45:02.073: INFO: Pod "downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053956416s
STEP: Saw pod success
Mar  1 10:45:02.073: INFO: Pod "downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85" satisfied condition "success or failure"
Mar  1 10:45:02.077: INFO: Trying to get logs from node worker02 pod downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85 container client-container: <nil>
STEP: delete the pod
Mar  1 10:45:02.171: INFO: Waiting for pod downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85 to disappear
Mar  1 10:45:02.176: INFO: Pod downwardapi-volume-3edc9183-3c4c-43a0-abd2-8aa11f529c85 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:45:02.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9456" for this suite.
Mar  1 10:45:08.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:45:08.532: INFO: namespace projected-9456 deletion completed in 6.338126722s

• [SLOW TEST:12.882 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:45:08.532: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  1 10:45:09.168: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-749,SelfLink:/api/v1/namespaces/watch-749/configmaps/e2e-watch-test-resource-version,UID:bc02ceba-175a-482c-9d2b-b3a35b70825b,ResourceVersion:209418,Generation:0,CreationTimestamp:2020-03-01 10:45:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar  1 10:45:09.168: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-749,SelfLink:/api/v1/namespaces/watch-749/configmaps/e2e-watch-test-resource-version,UID:bc02ceba-175a-482c-9d2b-b3a35b70825b,ResourceVersion:209419,Generation:0,CreationTimestamp:2020-03-01 10:45:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:45:09.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-749" for this suite.
Mar  1 10:45:15.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:45:15.416: INFO: namespace watch-749 deletion completed in 6.237493028s

• [SLOW TEST:6.884 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:45:15.417: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  1 10:45:20.511: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5255 pod-service-account-ef8aedde-2ac9-413f-9932-fb768fee48cc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  1 10:45:20.972: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5255 pod-service-account-ef8aedde-2ac9-413f-9932-fb768fee48cc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  1 10:45:21.416: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5255 pod-service-account-ef8aedde-2ac9-413f-9932-fb768fee48cc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:45:21.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5255" for this suite.
Mar  1 10:45:28.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:45:28.308: INFO: namespace svcaccounts-5255 deletion completed in 6.368704067s

• [SLOW TEST:12.892 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:45:28.310: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Mar  1 10:45:28.605: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-062681313 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:45:28.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-334" for this suite.
Mar  1 10:45:34.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:45:35.016: INFO: namespace kubectl-334 deletion completed in 6.233993775s

• [SLOW TEST:6.707 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:45:35.016: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Mar  1 10:45:35.373: INFO: Waiting up to 5m0s for pod "client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa" in namespace "containers-2852" to be "success or failure"
Mar  1 10:45:35.378: INFO: Pod "client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.50536ms
Mar  1 10:45:37.385: INFO: Pod "client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012024257s
Mar  1 10:45:39.391: INFO: Pod "client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018211248s
Mar  1 10:45:41.398: INFO: Pod "client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025193968s
STEP: Saw pod success
Mar  1 10:45:41.398: INFO: Pod "client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa" satisfied condition "success or failure"
Mar  1 10:45:41.402: INFO: Trying to get logs from node worker02 pod client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa container test-container: <nil>
STEP: delete the pod
Mar  1 10:45:41.448: INFO: Waiting for pod client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa to disappear
Mar  1 10:45:41.452: INFO: Pod client-containers-1207829d-9ca7-49b7-9d87-4b7c0b758efa no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:45:41.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2852" for this suite.
Mar  1 10:45:47.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:45:47.702: INFO: namespace containers-2852 deletion completed in 6.243127402s

• [SLOW TEST:12.685 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:45:47.702: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 10:45:48.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b" in namespace "projected-2481" to be "success or failure"
Mar  1 10:45:48.013: INFO: Pod "downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49327ms
Mar  1 10:45:50.019: INFO: Pod "downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011175607s
Mar  1 10:45:52.027: INFO: Pod "downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019593014s
Mar  1 10:45:54.035: INFO: Pod "downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027832881s
STEP: Saw pod success
Mar  1 10:45:54.035: INFO: Pod "downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b" satisfied condition "success or failure"
Mar  1 10:45:54.042: INFO: Trying to get logs from node worker02 pod downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b container client-container: <nil>
STEP: delete the pod
Mar  1 10:45:54.128: INFO: Waiting for pod downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b to disappear
Mar  1 10:45:54.134: INFO: Pod downwardapi-volume-13eff32f-bef9-4369-834d-51ee406eb12b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:45:54.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2481" for this suite.
Mar  1 10:46:00.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:46:00.469: INFO: namespace projected-2481 deletion completed in 6.32565821s

• [SLOW TEST:12.766 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:46:00.470: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:46:36.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7818" for this suite.
Mar  1 10:46:44.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:46:44.870: INFO: namespace container-runtime-7818 deletion completed in 8.22339508s

• [SLOW TEST:44.400 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:46:44.870: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Mar  1 10:46:45.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3" in namespace "downward-api-4701" to be "success or failure"
Mar  1 10:46:45.579: INFO: Pod "downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.113459ms
Mar  1 10:46:47.605: INFO: Pod "downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030663945s
Mar  1 10:46:49.612: INFO: Pod "downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038068242s
Mar  1 10:46:51.642: INFO: Pod "downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068126281s
STEP: Saw pod success
Mar  1 10:46:51.642: INFO: Pod "downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3" satisfied condition "success or failure"
Mar  1 10:46:51.647: INFO: Trying to get logs from node worker02 pod downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3 container client-container: <nil>
STEP: delete the pod
Mar  1 10:46:51.737: INFO: Waiting for pod downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3 to disappear
Mar  1 10:46:51.775: INFO: Pod downwardapi-volume-edd94c26-0789-4341-badf-bd33b768deb3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:46:51.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4701" for this suite.
Mar  1 10:46:57.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:46:58.063: INFO: namespace downward-api-4701 deletion completed in 6.274158919s

• [SLOW TEST:13.192 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:46:58.063: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-27249bbe-4b9c-4c1a-9980-eb025d4e8a6a
STEP: Creating a pod to test consume configMaps
Mar  1 10:46:58.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb" in namespace "configmap-3320" to be "success or failure"
Mar  1 10:46:58.523: INFO: Pod "pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383846ms
Mar  1 10:47:00.542: INFO: Pod "pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024313523s
Mar  1 10:47:02.549: INFO: Pod "pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031132113s
Mar  1 10:47:04.556: INFO: Pod "pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038126387s
STEP: Saw pod success
Mar  1 10:47:04.556: INFO: Pod "pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb" satisfied condition "success or failure"
Mar  1 10:47:04.562: INFO: Trying to get logs from node worker02 pod pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb container configmap-volume-test: <nil>
STEP: delete the pod
Mar  1 10:47:04.665: INFO: Waiting for pod pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb to disappear
Mar  1 10:47:04.670: INFO: Pod pod-configmaps-8fcbf750-a0c5-4b35-a89e-e0829c5a0bbb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:47:04.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3320" for this suite.
Mar  1 10:47:10.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:47:10.914: INFO: namespace configmap-3320 deletion completed in 6.236003472s

• [SLOW TEST:12.851 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:47:10.915: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4632.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4632.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 10:47:19.375: INFO: DNS probes using dns-test-0b8cfac8-eb4b-4baf-b309-ba0b713fadf9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4632.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4632.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 10:47:25.740: INFO: File wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:25.747: INFO: File jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:25.747: INFO: Lookups using dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b failed for: [wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local]

Mar  1 10:47:30.755: INFO: File wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:30.760: INFO: File jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:30.760: INFO: Lookups using dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b failed for: [wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local]

Mar  1 10:47:35.754: INFO: File wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:35.761: INFO: File jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:35.761: INFO: Lookups using dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b failed for: [wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local]

Mar  1 10:47:40.755: INFO: File wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:40.763: INFO: File jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:40.763: INFO: Lookups using dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b failed for: [wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local]

Mar  1 10:47:45.773: INFO: File jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local from pod  dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 10:47:45.773: INFO: Lookups using dns-4632/dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b failed for: [jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local]

Mar  1 10:47:50.760: INFO: DNS probes using dns-test-bdd49fac-91b5-4eab-81b4-fd838c97561b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4632.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4632.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4632.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4632.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  1 10:47:57.210: INFO: DNS probes using dns-test-abd4b891-f767-40bc-868e-5c1c9c5f9fde succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:47:57.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4632" for this suite.
Mar  1 10:48:05.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:48:05.711: INFO: namespace dns-4632 deletion completed in 8.261818108s

• [SLOW TEST:54.796 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:48:05.712: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c
Mar  1 10:48:06.038: INFO: Pod name my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c: Found 0 pods out of 1
Mar  1 10:48:11.045: INFO: Pod name my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c: Found 1 pods out of 1
Mar  1 10:48:11.045: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c" are running
Mar  1 10:48:11.050: INFO: Pod "my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c-shd8r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 10:48:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 10:48:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 10:48:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-01 10:48:06 +0000 UTC Reason: Message:}])
Mar  1 10:48:11.050: INFO: Trying to dial the pod
Mar  1 10:48:16.113: INFO: Controller my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c: Got expected result from replica 1 [my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c-shd8r]: "my-hostname-basic-008bc61a-3887-4c4f-acc1-723fc0e1cf9c-shd8r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:48:16.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2590" for this suite.
Mar  1 10:48:22.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:48:22.693: INFO: namespace replication-controller-2590 deletion completed in 6.572392355s

• [SLOW TEST:16.981 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:48:22.693: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Mar  1 10:48:23.035: INFO: Waiting up to 5m0s for pod "downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1" in namespace "downward-api-743" to be "success or failure"
Mar  1 10:48:23.097: INFO: Pod "downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 61.804996ms
Mar  1 10:48:25.104: INFO: Pod "downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06934825s
Mar  1 10:48:27.122: INFO: Pod "downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087509183s
Mar  1 10:48:29.128: INFO: Pod "downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093032517s
STEP: Saw pod success
Mar  1 10:48:29.128: INFO: Pod "downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1" satisfied condition "success or failure"
Mar  1 10:48:29.132: INFO: Trying to get logs from node worker02 pod downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1 container dapi-container: <nil>
STEP: delete the pod
Mar  1 10:48:29.223: INFO: Waiting for pod downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1 to disappear
Mar  1 10:48:29.226: INFO: Pod downward-api-10875491-b6d3-4107-9c7a-df0096dcbdb1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:48:29.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-743" for this suite.
Mar  1 10:48:35.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:48:35.554: INFO: namespace downward-api-743 deletion completed in 6.320169116s

• [SLOW TEST:12.861 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:48:35.554: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8721
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8721 to expose endpoints map[]
Mar  1 10:48:36.044: INFO: Get endpoints failed (4.53042ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  1 10:48:37.049: INFO: successfully validated that service endpoint-test2 in namespace services-8721 exposes endpoints map[] (1.009695416s elapsed)
STEP: Creating pod pod1 in namespace services-8721
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8721 to expose endpoints map[pod1:[80]]
Mar  1 10:48:41.277: INFO: successfully validated that service endpoint-test2 in namespace services-8721 exposes endpoints map[pod1:[80]] (4.176592308s elapsed)
STEP: Creating pod pod2 in namespace services-8721
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8721 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  1 10:48:44.444: INFO: successfully validated that service endpoint-test2 in namespace services-8721 exposes endpoints map[pod1:[80] pod2:[80]] (3.093856622s elapsed)
STEP: Deleting pod pod1 in namespace services-8721
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8721 to expose endpoints map[pod2:[80]]
Mar  1 10:48:45.550: INFO: successfully validated that service endpoint-test2 in namespace services-8721 exposes endpoints map[pod2:[80]] (1.070574317s elapsed)
STEP: Deleting pod pod2 in namespace services-8721
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8721 to expose endpoints map[]
Mar  1 10:48:46.581: INFO: successfully validated that service endpoint-test2 in namespace services-8721 exposes endpoints map[] (1.009725129s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:48:46.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8721" for this suite.
Mar  1 10:49:10.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:49:11.109: INFO: namespace services-8721 deletion completed in 24.231886736s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:35.554 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:49:11.109: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:49:11.613: INFO: Creating deployment "test-recreate-deployment"
Mar  1 10:49:11.655: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  1 10:49:11.673: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar  1 10:49:13.684: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  1 10:49:13.688: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68487ddfcd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:49:15.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718656551, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68487ddfcd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 10:49:17.700: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  1 10:49:17.737: INFO: Updating deployment test-recreate-deployment
Mar  1 10:49:17.737: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Mar  1 10:49:18.315: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9749,SelfLink:/apis/apps/v1/namespaces/deployment-9749/deployments/test-recreate-deployment,UID:34eda0a9-de27-4411-9298-8d93e3e5d0fb,ResourceVersion:210384,Generation:2,CreationTimestamp:2020-03-01 10:49:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-03-01 10:49:18 +0000 UTC 2020-03-01 10:49:18 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-03-01 10:49:18 +0000 UTC 2020-03-01 10:49:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-677f89d4b5" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 10:49:18.320: INFO: New ReplicaSet "test-recreate-deployment-677f89d4b5" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-677f89d4b5,GenerateName:,Namespace:deployment-9749,SelfLink:/apis/apps/v1/namespaces/deployment-9749/replicasets/test-recreate-deployment-677f89d4b5,UID:4221afc8-7e46-405d-abfd-cafb7db4a816,ResourceVersion:210382,Generation:1,CreationTimestamp:2020-03-01 10:49:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 34eda0a9-de27-4411-9298-8d93e3e5d0fb 0xc0017b4957 0xc0017b4958}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:49:18.320: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  1 10:49:18.320: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-68487ddfcd,GenerateName:,Namespace:deployment-9749,SelfLink:/apis/apps/v1/namespaces/deployment-9749/replicasets/test-recreate-deployment-68487ddfcd,UID:3295c26b-2cdf-4157-830e-5e40e8ecda54,ResourceVersion:210371,Generation:2,CreationTimestamp:2020-03-01 10:49:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 68487ddfcd,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 34eda0a9-de27-4411-9298-8d93e3e5d0fb 0xc0017b4a27 0xc0017b4a28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68487ddfcd,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 68487ddfcd,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis 172.20.8.7/library/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar  1 10:49:18.326: INFO: Pod "test-recreate-deployment-677f89d4b5-k8pv7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-677f89d4b5-k8pv7,GenerateName:test-recreate-deployment-677f89d4b5-,Namespace:deployment-9749,SelfLink:/api/v1/namespaces/deployment-9749/pods/test-recreate-deployment-677f89d4b5-k8pv7,UID:7fada761-bf72-4ed7-8de7-0d7741af99b9,ResourceVersion:210378,Generation:0,CreationTimestamp:2020-03-01 10:49:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 677f89d4b5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-677f89d4b5 4221afc8-7e46-405d-abfd-cafb7db4a816 0xc0017b5667 0xc0017b5668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vd94z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vd94z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx 172.20.8.7/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vd94z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017b5730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017b5750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-01 10:49:18 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:49:18.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9749" for this suite.
Mar  1 10:49:26.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:49:26.687: INFO: namespace deployment-9749 deletion completed in 8.353874218s

• [SLOW TEST:15.577 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:49:26.687: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6059
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-39a21660-f4b0-4b55-8c91-549f6f8de297
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-39a21660-f4b0-4b55-8c91-549f6f8de297
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:50:46.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6059" for this suite.
Mar  1 10:51:10.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:51:10.320: INFO: namespace projected-6059 deletion completed in 24.196624574s

• [SLOW TEST:103.632 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:51:10.320: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Mar  1 10:51:10.985: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:51:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7489" for this suite.
Mar  1 10:51:57.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:51:57.339: INFO: namespace pods-7489 deletion completed in 40.219427689s

• [SLOW TEST:47.019 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:51:57.340: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Mar  1 10:51:57.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 api-versions'
Mar  1 10:51:58.163: INFO: stderr: ""
Mar  1 10:51:58.163: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncontour.heptio.com/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nprojectcontour.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:51:58.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-601" for this suite.
Mar  1 10:52:04.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:52:04.364: INFO: namespace kubectl-601 deletion completed in 6.191322317s

• [SLOW TEST:7.024 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:52:04.364: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Mar  1 10:52:04.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 create -f - --namespace=kubectl-7814'
Mar  1 10:52:05.302: INFO: stderr: ""
Mar  1 10:52:05.302: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  1 10:52:05.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7814'
Mar  1 10:52:05.518: INFO: stderr: ""
Mar  1 10:52:05.518: INFO: stdout: "update-demo-nautilus-gkbtg update-demo-nautilus-vvjf6 "
Mar  1 10:52:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-gkbtg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7814'
Mar  1 10:52:05.782: INFO: stderr: ""
Mar  1 10:52:05.782: INFO: stdout: ""
Mar  1 10:52:05.782: INFO: update-demo-nautilus-gkbtg is created but not running
Mar  1 10:52:10.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7814'
Mar  1 10:52:10.982: INFO: stderr: ""
Mar  1 10:52:10.982: INFO: stdout: "update-demo-nautilus-gkbtg update-demo-nautilus-vvjf6 "
Mar  1 10:52:10.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-gkbtg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7814'
Mar  1 10:52:11.172: INFO: stderr: ""
Mar  1 10:52:11.172: INFO: stdout: "true"
Mar  1 10:52:11.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-gkbtg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7814'
Mar  1 10:52:11.393: INFO: stderr: ""
Mar  1 10:52:11.394: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 10:52:11.394: INFO: validating pod update-demo-nautilus-gkbtg
Mar  1 10:52:11.404: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 10:52:11.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 10:52:11.405: INFO: update-demo-nautilus-gkbtg is verified up and running
Mar  1 10:52:11.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vvjf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7814'
Mar  1 10:52:11.582: INFO: stderr: ""
Mar  1 10:52:11.582: INFO: stdout: "true"
Mar  1 10:52:11.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods update-demo-nautilus-vvjf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7814'
Mar  1 10:52:11.759: INFO: stderr: ""
Mar  1 10:52:11.760: INFO: stdout: "172.20.8.7/library/nautilus:1.0"
Mar  1 10:52:11.760: INFO: validating pod update-demo-nautilus-vvjf6
Mar  1 10:52:11.769: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 10:52:11.769: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 10:52:11.770: INFO: update-demo-nautilus-vvjf6 is verified up and running
STEP: using delete to clean up resources
Mar  1 10:52:11.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete --grace-period=0 --force -f - --namespace=kubectl-7814'
Mar  1 10:52:12.026: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 10:52:12.026: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 10:52:12.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7814'
Mar  1 10:52:12.328: INFO: stderr: "No resources found.\n"
Mar  1 10:52:12.328: INFO: stdout: ""
Mar  1 10:52:12.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -l name=update-demo --namespace=kubectl-7814 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 10:52:12.522: INFO: stderr: ""
Mar  1 10:52:12.522: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:52:12.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7814" for this suite.
Mar  1 10:52:36.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:52:36.792: INFO: namespace kubectl-7814 deletion completed in 24.232337636s

• [SLOW TEST:32.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:52:36.793: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5585, will wait for the garbage collector to delete the pods
Mar  1 10:52:43.223: INFO: Deleting Job.batch foo took: 57.388546ms
Mar  1 10:52:43.723: INFO: Terminating Job.batch foo pods took: 500.462907ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:53:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5585" for this suite.
Mar  1 10:53:27.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:53:27.701: INFO: namespace job-5585 deletion completed in 8.357871765s

• [SLOW TEST:50.908 seconds]
[sig-apps] Job
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:53:27.701: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image 172.20.8.7/library/nginx:1.14-alpine
Mar  1 10:53:28.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 run e2e-test-nginx-rc --image=172.20.8.7/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4093'
Mar  1 10:53:28.275: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar  1 10:53:28.275: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Mar  1 10:53:28.282: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar  1 10:53:28.425: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar  1 10:53:28.463: INFO: scanned /root for discovery docs: <nil>
Mar  1 10:53:28.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 rolling-update e2e-test-nginx-rc --update-period=1s --image=172.20.8.7/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4093'
Mar  1 10:53:46.810: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar  1 10:53:46.810: INFO: stdout: "Created e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377\nScaling up e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar  1 10:53:46.810: INFO: stdout: "Created e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377\nScaling up e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar  1 10:53:46.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4093'
Mar  1 10:53:47.138: INFO: stderr: ""
Mar  1 10:53:47.138: INFO: stdout: "e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377-smdc8 "
Mar  1 10:53:47.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377-smdc8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4093'
Mar  1 10:53:47.325: INFO: stderr: ""
Mar  1 10:53:47.325: INFO: stdout: "true"
Mar  1 10:53:47.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 get pods e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377-smdc8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4093'
Mar  1 10:53:47.506: INFO: stderr: ""
Mar  1 10:53:47.506: INFO: stdout: "172.20.8.7/library/nginx:1.14-alpine"
Mar  1 10:53:47.506: INFO: e2e-test-nginx-rc-4871d5ba053eff56ad849aa504e97377-smdc8 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Mar  1 10:53:47.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-062681313 delete rc e2e-test-nginx-rc --namespace=kubectl-4093'
Mar  1 10:53:47.724: INFO: stderr: ""
Mar  1 10:53:47.724: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:53:47.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4093" for this suite.
Mar  1 10:54:11.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:54:11.981: INFO: namespace kubectl-4093 deletion completed in 24.248056345s

• [SLOW TEST:44.280 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:54:11.981: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4965
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-5b3e9285-14c0-4b7a-9ada-388d32d2b38a
STEP: Creating configMap with name cm-test-opt-upd-48653da5-ae5b-41e0-9109-ac846ed95205
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5b3e9285-14c0-4b7a-9ada-388d32d2b38a
STEP: Updating configmap cm-test-opt-upd-48653da5-ae5b-41e0-9109-ac846ed95205
STEP: Creating configMap with name cm-test-opt-create-c6488944-7dc3-4438-a02e-c6e71ff9685c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:55:36.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4965" for this suite.
Mar  1 10:56:00.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:56:00.587: INFO: namespace configmap-4965 deletion completed in 24.350809555s

• [SLOW TEST:108.606 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Mar  1 10:56:00.587: INFO: >>> kubeConfig: /tmp/kubeconfig-062681313
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Mar  1 10:56:01.047: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 10:56:01.097: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 10:56:01.101: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Mar  1 10:56:01.116: INFO: metrics-server-5848cb774b-bw57f from kube-system started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container metrics-server ready: true, restart count 0
Mar  1 10:56:01.117: INFO: kube-flannel-ds-amd64-dsrkh from kube-system started at 2020-02-29 08:55:44 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  1 10:56:01.117: INFO: sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-dlz68 from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  1 10:56:01.117: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 10:56:01.117: INFO: contour-certgen-6w5mk from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container contour ready: false, restart count 0
Mar  1 10:56:01.117: INFO: kuard-55c79684d4-pfs2p from default started at 2020-02-29 08:57:18 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container kuard ready: true, restart count 0
Mar  1 10:56:01.117: INFO: contour-6f89bf4d5f-szsh8 from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container contour ready: false, restart count 0
Mar  1 10:56:01.117: INFO: kuard-55c79684d4-bf6hv from default started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container kuard ready: true, restart count 0
Mar  1 10:56:01.117: INFO: contour-6f89bf4d5f-cbp6b from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container contour ready: false, restart count 0
Mar  1 10:56:01.117: INFO: kube-proxy-s67df from kube-system started at 2020-02-29 08:55:43 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 10:56:01.117: INFO: envoy-fgn4v from projectcontour started at 2020-02-29 08:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container envoy ready: false, restart count 0
Mar  1 10:56:01.117: INFO: kuard-55c79684d4-mtkkk from default started at 2020-02-29 08:57:18 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.117: INFO: 	Container kuard ready: true, restart count 0
Mar  1 10:56:01.117: INFO: 
Logging pods the kubelet thinks is on node worker02 before test
Mar  1 10:56:01.128: INFO: envoy-zlkbx from projectcontour started at 2020-02-29 08:57:26 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.128: INFO: 	Container envoy ready: false, restart count 0
Mar  1 10:56:01.128: INFO: kube-flannel-ds-amd64-5brn5 from kube-system started at 2020-02-29 08:55:44 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.128: INFO: 	Container kube-flannel ready: true, restart count 0
Mar  1 10:56:01.128: INFO: sonobuoy from sonobuoy started at 2020-03-01 09:01:02 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  1 10:56:01.128: INFO: sonobuoy-e2e-job-9a6dafb81c774c83 from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 10:56:01.128: INFO: 	Container e2e ready: true, restart count 0
Mar  1 10:56:01.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  1 10:56:01.129: INFO: sonobuoy-systemd-logs-daemon-set-bac3b14cb07747e5-zbtnp from sonobuoy started at 2020-03-01 09:01:06 +0000 UTC (2 container statuses recorded)
Mar  1 10:56:01.129: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Mar  1 10:56:01.129: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  1 10:56:01.129: INFO: kube-proxy-wxn7v from kube-system started at 2020-02-29 08:55:43 +0000 UTC (1 container statuses recorded)
Mar  1 10:56:01.129: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f828d4456a2fcf], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Mar  1 10:56:02.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4557" for this suite.
Mar  1 10:56:08.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar  1 10:56:08.406: INFO: namespace sched-pred-4557 deletion completed in 6.208142923s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.819 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSMar  1 10:56:08.406: INFO: Running AfterSuite actions on all nodes
Mar  1 10:56:08.407: INFO: Running AfterSuite actions on node 1
Mar  1 10:56:08.407: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 6877.729 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h54m44.66767391s
Test Suite Passed
