I0906 19:45:24.299950      19 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-256622946
I0906 19:45:24.300406      19 e2e.go:241] Starting e2e run "735a8c57-7bd0-485a-b511-afad8d5fc6c9" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1567799122 - Will randomize all specs
Will run 215 of 4413 specs

Sep  6 19:45:24.542: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 19:45:24.548: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  6 19:45:24.576: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  6 19:45:24.611: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  6 19:45:24.611: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  6 19:45:24.611: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  6 19:45:24.621: INFO: e2e test version: v1.15.3
Sep  6 19:45:24.621: INFO: kube-apiserver version: v1.15.3
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:45:24.622: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
Sep  6 19:45:24.654: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep  6 19:45:24.667: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 19:45:24.786: INFO: Waiting up to 5m0s for pod "downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2" in namespace "downward-api-9307" to be "success or failure"
Sep  6 19:45:24.788: INFO: Pod "downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.265302ms
Sep  6 19:45:26.792: INFO: Pod "downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006330653s
Sep  6 19:45:28.796: INFO: Pod "downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010067265s
STEP: Saw pod success
Sep  6 19:45:28.796: INFO: Pod "downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2" satisfied condition "success or failure"
Sep  6 19:45:28.799: INFO: Trying to get logs from node appserv10 pod downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2 container dapi-container: <nil>
STEP: delete the pod
Sep  6 19:45:28.831: INFO: Waiting for pod downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2 to disappear
Sep  6 19:45:28.834: INFO: Pod downward-api-ffe21fd3-0534-4000-ab4b-18bab5d969c2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:45:28.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9307" for this suite.
Sep  6 19:45:34.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:45:34.935: INFO: namespace downward-api-9307 deletion completed in 6.097376238s

• [SLOW TEST:10.313 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:45:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-8637/secret-test-4c8ccdf4-dc23-4241-9de4-d629a55d1f58
STEP: Creating a pod to test consume secrets
Sep  6 19:45:35.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2" in namespace "secrets-8637" to be "success or failure"
Sep  6 19:45:35.080: INFO: Pod "pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040657ms
Sep  6 19:45:37.084: INFO: Pod "pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006025823s
Sep  6 19:45:39.087: INFO: Pod "pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009419335s
STEP: Saw pod success
Sep  6 19:45:39.087: INFO: Pod "pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2" satisfied condition "success or failure"
Sep  6 19:45:39.090: INFO: Trying to get logs from node appserv9 pod pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2 container env-test: <nil>
STEP: delete the pod
Sep  6 19:45:39.120: INFO: Waiting for pod pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2 to disappear
Sep  6 19:45:39.123: INFO: Pod pod-configmaps-c3751cb2-101d-429c-9378-3c71d36f3aa2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:45:39.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8637" for this suite.
Sep  6 19:45:45.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:45:45.236: INFO: namespace secrets-8637 deletion completed in 6.109200772s

• [SLOW TEST:10.301 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:45:45.237: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 19:45:45.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4298'
Sep  6 19:45:45.664: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 19:45:45.664: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Sep  6 19:45:45.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete jobs e2e-test-nginx-job --namespace=kubectl-4298'
Sep  6 19:45:45.789: INFO: stderr: ""
Sep  6 19:45:45.789: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:45:45.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4298" for this suite.
Sep  6 19:45:51.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:45:51.902: INFO: namespace kubectl-4298 deletion completed in 6.109413315s

• [SLOW TEST:6.665 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:45:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 19:45:52.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc" in namespace "projected-9151" to be "success or failure"
Sep  6 19:45:52.050: INFO: Pod "downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521406ms
Sep  6 19:45:54.054: INFO: Pod "downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006821883s
STEP: Saw pod success
Sep  6 19:45:54.054: INFO: Pod "downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc" satisfied condition "success or failure"
Sep  6 19:45:54.057: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc container client-container: <nil>
STEP: delete the pod
Sep  6 19:45:54.077: INFO: Waiting for pod downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc to disappear
Sep  6 19:45:54.079: INFO: Pod downwardapi-volume-0c872cff-43d4-465a-8e0a-28fac36199cc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:45:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9151" for this suite.
Sep  6 19:46:00.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:46:00.189: INFO: namespace projected-9151 deletion completed in 6.105357134s

• [SLOW TEST:8.286 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:46:00.189: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 19:46:00.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d" in namespace "projected-3753" to be "success or failure"
Sep  6 19:46:00.335: INFO: Pod "downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.605799ms
Sep  6 19:46:02.339: INFO: Pod "downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00639743s
STEP: Saw pod success
Sep  6 19:46:02.339: INFO: Pod "downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d" satisfied condition "success or failure"
Sep  6 19:46:02.342: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d container client-container: <nil>
STEP: delete the pod
Sep  6 19:46:02.361: INFO: Waiting for pod downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d to disappear
Sep  6 19:46:02.364: INFO: Pod downwardapi-volume-eaf1a49b-b757-4c4a-bcbb-bb024743708d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:46:02.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3753" for this suite.
Sep  6 19:46:08.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:46:08.471: INFO: namespace projected-3753 deletion completed in 6.103532958s

• [SLOW TEST:8.282 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:46:08.472: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4025
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1718
STEP: Creating secret with name secret-test-4aed5ab8-9445-4e44-973f-327afb44a822
STEP: Creating a pod to test consume secrets
Sep  6 19:46:08.755: INFO: Waiting up to 5m0s for pod "pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a" in namespace "secrets-4025" to be "success or failure"
Sep  6 19:46:08.758: INFO: Pod "pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.914449ms
Sep  6 19:46:10.762: INFO: Pod "pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006483924s
STEP: Saw pod success
Sep  6 19:46:10.762: INFO: Pod "pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a" satisfied condition "success or failure"
Sep  6 19:46:10.765: INFO: Trying to get logs from node appserv9 pod pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 19:46:10.784: INFO: Waiting for pod pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a to disappear
Sep  6 19:46:10.786: INFO: Pod pod-secrets-f1efe7f9-8810-4e82-ba69-098713ba8c1a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:46:10.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4025" for this suite.
Sep  6 19:46:16.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:46:16.896: INFO: namespace secrets-4025 deletion completed in 6.105621324s
STEP: Destroying namespace "secret-namespace-1718" for this suite.
Sep  6 19:46:22.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:46:23.005: INFO: namespace secret-namespace-1718 deletion completed in 6.109493996s

• [SLOW TEST:14.533 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:46:23.006: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 19:46:23.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626" in namespace "downward-api-7337" to be "success or failure"
Sep  6 19:46:23.153: INFO: Pod "downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626": Phase="Pending", Reason="", readiness=false. Elapsed: 3.427256ms
Sep  6 19:46:25.157: INFO: Pod "downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007145188s
STEP: Saw pod success
Sep  6 19:46:25.157: INFO: Pod "downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626" satisfied condition "success or failure"
Sep  6 19:46:25.160: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626 container client-container: <nil>
STEP: delete the pod
Sep  6 19:46:25.179: INFO: Waiting for pod downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626 to disappear
Sep  6 19:46:25.181: INFO: Pod downwardapi-volume-4bfc2768-682d-4338-99bb-eb47dfc32626 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:46:25.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7337" for this suite.
Sep  6 19:46:31.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:46:31.288: INFO: namespace downward-api-7337 deletion completed in 6.102592458s

• [SLOW TEST:8.282 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:46:31.288: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:46:33.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-948" for this suite.
Sep  6 19:47:23.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:47:23.558: INFO: namespace kubelet-test-948 deletion completed in 50.105398027s

• [SLOW TEST:52.270 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:47:23.558: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 19:47:23.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524" in namespace "downward-api-1792" to be "success or failure"
Sep  6 19:47:23.704: INFO: Pod "downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924043ms
Sep  6 19:47:25.707: INFO: Pod "downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006371175s
Sep  6 19:47:27.711: INFO: Pod "downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010117268s
STEP: Saw pod success
Sep  6 19:47:27.711: INFO: Pod "downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524" satisfied condition "success or failure"
Sep  6 19:47:27.714: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524 container client-container: <nil>
STEP: delete the pod
Sep  6 19:47:27.734: INFO: Waiting for pod downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524 to disappear
Sep  6 19:47:27.737: INFO: Pod downwardapi-volume-472b2e48-fff6-4c8a-95bc-c6d380ff7524 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:47:27.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1792" for this suite.
Sep  6 19:47:33.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:47:33.852: INFO: namespace downward-api-1792 deletion completed in 6.11074591s

• [SLOW TEST:10.294 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:47:33.852: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1ba6a3e1-b08c-40f2-9677-525210c537e3
STEP: Creating a pod to test consume secrets
Sep  6 19:47:34.000: INFO: Waiting up to 5m0s for pod "pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d" in namespace "secrets-3620" to be "success or failure"
Sep  6 19:47:34.003: INFO: Pod "pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518377ms
Sep  6 19:47:36.006: INFO: Pod "pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005613098s
Sep  6 19:47:38.009: INFO: Pod "pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009082333s
STEP: Saw pod success
Sep  6 19:47:38.009: INFO: Pod "pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d" satisfied condition "success or failure"
Sep  6 19:47:38.012: INFO: Trying to get logs from node appserv9 pod pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 19:47:38.032: INFO: Waiting for pod pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d to disappear
Sep  6 19:47:38.035: INFO: Pod pod-secrets-f44bef10-e9b3-4652-a623-840bb238c93d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:47:38.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3620" for this suite.
Sep  6 19:47:44.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:47:44.154: INFO: namespace secrets-3620 deletion completed in 6.114129756s

• [SLOW TEST:10.302 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:47:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-78
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 19:47:44.298: INFO: Waiting up to 5m0s for pod "pod-0994b66d-da09-4f22-8d04-a4bfbd12300f" in namespace "emptydir-78" to be "success or failure"
Sep  6 19:47:44.300: INFO: Pod "pod-0994b66d-da09-4f22-8d04-a4bfbd12300f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.353829ms
Sep  6 19:47:46.303: INFO: Pod "pod-0994b66d-da09-4f22-8d04-a4bfbd12300f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005697123s
STEP: Saw pod success
Sep  6 19:47:46.303: INFO: Pod "pod-0994b66d-da09-4f22-8d04-a4bfbd12300f" satisfied condition "success or failure"
Sep  6 19:47:46.306: INFO: Trying to get logs from node appserv10 pod pod-0994b66d-da09-4f22-8d04-a4bfbd12300f container test-container: <nil>
STEP: delete the pod
Sep  6 19:47:46.324: INFO: Waiting for pod pod-0994b66d-da09-4f22-8d04-a4bfbd12300f to disappear
Sep  6 19:47:46.326: INFO: Pod pod-0994b66d-da09-4f22-8d04-a4bfbd12300f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:47:46.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-78" for this suite.
Sep  6 19:47:52.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:47:52.435: INFO: namespace emptydir-78 deletion completed in 6.104958395s

• [SLOW TEST:8.281 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:47:52.435: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-33a06168-a48f-4263-9c5b-779be39747a0
STEP: Creating a pod to test consume configMaps
Sep  6 19:47:52.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e" in namespace "configmap-2355" to be "success or failure"
Sep  6 19:47:52.584: INFO: Pod "pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.491433ms
Sep  6 19:47:54.587: INFO: Pod "pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005601246s
STEP: Saw pod success
Sep  6 19:47:54.587: INFO: Pod "pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e" satisfied condition "success or failure"
Sep  6 19:47:54.590: INFO: Trying to get logs from node appserv9 pod pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 19:47:54.605: INFO: Waiting for pod pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e to disappear
Sep  6 19:47:54.607: INFO: Pod pod-configmaps-594960fe-4eef-42a6-a08e-2f146ae02d8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:47:54.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2355" for this suite.
Sep  6 19:48:00.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:48:00.702: INFO: namespace configmap-2355 deletion completed in 6.091113041s

• [SLOW TEST:8.267 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:48:00.702: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ee35a5f6-cef1-42d3-80cd-30a2af433f7d
STEP: Creating a pod to test consume secrets
Sep  6 19:48:00.870: INFO: Waiting up to 5m0s for pod "pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc" in namespace "secrets-6121" to be "success or failure"
Sep  6 19:48:00.873: INFO: Pod "pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863422ms
Sep  6 19:48:02.877: INFO: Pod "pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006531507s
STEP: Saw pod success
Sep  6 19:48:02.877: INFO: Pod "pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc" satisfied condition "success or failure"
Sep  6 19:48:02.880: INFO: Trying to get logs from node appserv10 pod pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc container secret-env-test: <nil>
STEP: delete the pod
Sep  6 19:48:02.899: INFO: Waiting for pod pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc to disappear
Sep  6 19:48:02.903: INFO: Pod pod-secrets-8a76f67e-16cb-42fd-923d-059ab09baebc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:48:02.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6121" for this suite.
Sep  6 19:48:08.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:48:09.020: INFO: namespace secrets-6121 deletion completed in 6.113132056s

• [SLOW TEST:8.319 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:48:09.021: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 19:48:09.165: INFO: Waiting up to 5m0s for pod "pod-17920aa3-1421-4e1d-890f-b492248d7f9e" in namespace "emptydir-2333" to be "success or failure"
Sep  6 19:48:09.167: INFO: Pod "pod-17920aa3-1421-4e1d-890f-b492248d7f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920496ms
Sep  6 19:48:11.171: INFO: Pod "pod-17920aa3-1421-4e1d-890f-b492248d7f9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006587982s
STEP: Saw pod success
Sep  6 19:48:11.171: INFO: Pod "pod-17920aa3-1421-4e1d-890f-b492248d7f9e" satisfied condition "success or failure"
Sep  6 19:48:11.174: INFO: Trying to get logs from node appserv9 pod pod-17920aa3-1421-4e1d-890f-b492248d7f9e container test-container: <nil>
STEP: delete the pod
Sep  6 19:48:11.194: INFO: Waiting for pod pod-17920aa3-1421-4e1d-890f-b492248d7f9e to disappear
Sep  6 19:48:11.197: INFO: Pod pod-17920aa3-1421-4e1d-890f-b492248d7f9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:48:11.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2333" for this suite.
Sep  6 19:48:17.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:48:17.307: INFO: namespace emptydir-2333 deletion completed in 6.106200076s

• [SLOW TEST:8.287 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:48:17.308: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 19:48:17.444: INFO: Creating deployment "test-recreate-deployment"
Sep  6 19:48:17.448: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  6 19:48:17.455: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  6 19:48:19.462: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  6 19:48:19.465: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  6 19:48:19.472: INFO: Updating deployment test-recreate-deployment
Sep  6 19:48:19.472: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 19:48:19.520: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-307,SelfLink:/apis/apps/v1/namespaces/deployment-307/deployments/test-recreate-deployment,UID:f3dd3105-d6f5-4edd-8828-7e439ea63d62,ResourceVersion:2355,Generation:2,CreationTimestamp:2019-09-06 19:48:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-06 19:48:19 +0000 UTC 2019-09-06 19:48:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-06 19:48:19 +0000 UTC 2019-09-06 19:48:17 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 19:48:19.523: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-307,SelfLink:/apis/apps/v1/namespaces/deployment-307/replicasets/test-recreate-deployment-5c8c9cc69d,UID:223ae434-7956-4ae6-b15d-1c28c8876093,ResourceVersion:2354,Generation:1,CreationTimestamp:2019-09-06 19:48:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f3dd3105-d6f5-4edd-8828-7e439ea63d62 0xc003667ca7 0xc003667ca8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 19:48:19.524: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  6 19:48:19.524: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-307,SelfLink:/apis/apps/v1/namespaces/deployment-307/replicasets/test-recreate-deployment-6df85df6b9,UID:97877ff9-57c7-4148-85ac-3fca2001e606,ResourceVersion:2345,Generation:2,CreationTimestamp:2019-09-06 19:48:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f3dd3105-d6f5-4edd-8828-7e439ea63d62 0xc003834047 0xc003834048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 19:48:19.527: INFO: Pod "test-recreate-deployment-5c8c9cc69d-4jd78" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-4jd78,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-307,SelfLink:/api/v1/namespaces/deployment-307/pods/test-recreate-deployment-5c8c9cc69d-4jd78,UID:5bfbac0f-1270-42c4-8e4c-d021b0fecdf1,ResourceVersion:2350,Generation:0,CreationTimestamp:2019-09-06 19:48:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 223ae434-7956-4ae6-b15d-1c28c8876093 0xc0038c9dc7 0xc0038c9dc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-2wknc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-2wknc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-2wknc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038c9e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038c9e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:48:19.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-307" for this suite.
Sep  6 19:48:25.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:48:25.627: INFO: namespace deployment-307 deletion completed in 6.097012101s

• [SLOW TEST:8.320 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:48:25.628: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Sep  6 19:48:25.767: INFO: Waiting up to 5m0s for pod "var-expansion-70b6132c-b22c-455e-93e9-86046450b559" in namespace "var-expansion-4748" to be "success or failure"
Sep  6 19:48:25.770: INFO: Pod "var-expansion-70b6132c-b22c-455e-93e9-86046450b559": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130446ms
Sep  6 19:48:27.774: INFO: Pod "var-expansion-70b6132c-b22c-455e-93e9-86046450b559": Phase="Running", Reason="", readiness=true. Elapsed: 2.00675084s
Sep  6 19:48:29.778: INFO: Pod "var-expansion-70b6132c-b22c-455e-93e9-86046450b559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010576056s
STEP: Saw pod success
Sep  6 19:48:29.778: INFO: Pod "var-expansion-70b6132c-b22c-455e-93e9-86046450b559" satisfied condition "success or failure"
Sep  6 19:48:29.781: INFO: Trying to get logs from node appserv10 pod var-expansion-70b6132c-b22c-455e-93e9-86046450b559 container dapi-container: <nil>
STEP: delete the pod
Sep  6 19:48:29.800: INFO: Waiting for pod var-expansion-70b6132c-b22c-455e-93e9-86046450b559 to disappear
Sep  6 19:48:29.802: INFO: Pod var-expansion-70b6132c-b22c-455e-93e9-86046450b559 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:48:29.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4748" for this suite.
Sep  6 19:48:35.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:48:35.904: INFO: namespace var-expansion-4748 deletion completed in 6.097801594s

• [SLOW TEST:10.276 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:48:35.904: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Sep  6 19:48:36.043: INFO: Waiting up to 5m0s for pod "client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62" in namespace "containers-8525" to be "success or failure"
Sep  6 19:48:36.045: INFO: Pod "client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.468968ms
Sep  6 19:48:38.049: INFO: Pod "client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005834724s
STEP: Saw pod success
Sep  6 19:48:38.049: INFO: Pod "client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62" satisfied condition "success or failure"
Sep  6 19:48:38.052: INFO: Trying to get logs from node appserv9 pod client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62 container test-container: <nil>
STEP: delete the pod
Sep  6 19:48:38.070: INFO: Waiting for pod client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62 to disappear
Sep  6 19:48:38.073: INFO: Pod client-containers-909bfacc-5324-405b-8142-6fc7d4c49e62 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:48:38.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8525" for this suite.
Sep  6 19:48:44.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:48:44.184: INFO: namespace containers-8525 deletion completed in 6.10724455s

• [SLOW TEST:8.280 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:48:44.185: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 19:48:44.343: INFO: Create a RollingUpdate DaemonSet
Sep  6 19:48:44.347: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 19:48:44.353: INFO: Number of nodes with available pods: 0
Sep  6 19:48:44.353: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:48:45.360: INFO: Number of nodes with available pods: 0
Sep  6 19:48:45.361: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:48:46.361: INFO: Number of nodes with available pods: 3
Sep  6 19:48:46.361: INFO: Number of running nodes: 3, number of available pods: 3
Sep  6 19:48:46.361: INFO: Update the DaemonSet to trigger a rollout
Sep  6 19:48:46.368: INFO: Updating DaemonSet daemon-set
Sep  6 19:49:02.383: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 19:49:02.389: INFO: Updating DaemonSet daemon-set
Sep  6 19:49:02.389: INFO: Make sure DaemonSet rollback is complete
Sep  6 19:49:02.392: INFO: Wrong image for pod: daemon-set-cggn2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 19:49:02.392: INFO: Pod daemon-set-cggn2 is not available
Sep  6 19:49:03.400: INFO: Wrong image for pod: daemon-set-cggn2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 19:49:03.400: INFO: Pod daemon-set-cggn2 is not available
Sep  6 19:49:04.399: INFO: Wrong image for pod: daemon-set-cggn2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 19:49:04.399: INFO: Pod daemon-set-cggn2 is not available
Sep  6 19:49:05.399: INFO: Wrong image for pod: daemon-set-cggn2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 19:49:05.400: INFO: Pod daemon-set-cggn2 is not available
Sep  6 19:49:06.400: INFO: Wrong image for pod: daemon-set-cggn2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 19:49:06.400: INFO: Pod daemon-set-cggn2 is not available
Sep  6 19:49:07.400: INFO: Pod daemon-set-wwkqp is not available
Sep  6 19:49:08.400: INFO: Pod daemon-set-wwkqp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4190, will wait for the garbage collector to delete the pods
Sep  6 19:49:08.470: INFO: Deleting DaemonSet.extensions daemon-set took: 6.433514ms
Sep  6 19:49:08.970: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.242933ms
Sep  6 19:50:42.274: INFO: Number of nodes with available pods: 0
Sep  6 19:50:42.275: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 19:50:42.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4190/daemonsets","resourceVersion":"2834"},"items":null}

Sep  6 19:50:42.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4190/pods","resourceVersion":"2834"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:50:42.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4190" for this suite.
Sep  6 19:50:48.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:50:48.406: INFO: namespace daemonsets-4190 deletion completed in 6.106583273s

• [SLOW TEST:124.221 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:50:48.406: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6302
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep  6 19:50:48.553: INFO: Found 0 stateful pods, waiting for 3
Sep  6 19:50:58.559: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 19:50:58.559: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 19:50:58.559: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 19:50:58.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6302 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 19:50:58.825: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 19:50:58.825: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 19:50:58.825: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 19:51:08.856: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  6 19:51:18.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6302 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 19:51:19.118: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 19:51:19.118: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 19:51:19.118: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 19:51:29.138: INFO: Waiting for StatefulSet statefulset-6302/ss2 to complete update
Sep  6 19:51:29.139: INFO: Waiting for Pod statefulset-6302/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 19:51:29.139: INFO: Waiting for Pod statefulset-6302/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 19:51:39.147: INFO: Waiting for StatefulSet statefulset-6302/ss2 to complete update
Sep  6 19:51:39.147: INFO: Waiting for Pod statefulset-6302/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 19:51:39.147: INFO: Waiting for Pod statefulset-6302/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 19:51:49.146: INFO: Waiting for StatefulSet statefulset-6302/ss2 to complete update
Sep  6 19:51:49.146: INFO: Waiting for Pod statefulset-6302/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 19:51:59.147: INFO: Waiting for StatefulSet statefulset-6302/ss2 to complete update
STEP: Rolling back to a previous revision
Sep  6 19:52:09.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6302 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 19:52:09.396: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 19:52:09.396: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 19:52:09.396: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 19:52:19.428: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  6 19:52:29.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6302 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 19:52:29.681: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 19:52:29.681: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 19:52:29.681: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 19:52:59.702: INFO: Waiting for StatefulSet statefulset-6302/ss2 to complete update
Sep  6 19:52:59.702: INFO: Waiting for Pod statefulset-6302/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 19:53:09.710: INFO: Waiting for StatefulSet statefulset-6302/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 19:53:19.712: INFO: Deleting all statefulset in ns statefulset-6302
Sep  6 19:53:19.715: INFO: Scaling statefulset ss2 to 0
Sep  6 19:53:39.730: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 19:53:39.734: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:53:39.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6302" for this suite.
Sep  6 19:53:45.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:53:45.856: INFO: namespace statefulset-6302 deletion completed in 6.10528105s

• [SLOW TEST:177.450 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:53:45.856: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1485
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-173237b6-885c-45cd-8db7-3f35119b600b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-173237b6-885c-45cd-8db7-3f35119b600b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:53:50.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1485" for this suite.
Sep  6 19:54:12.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:54:12.160: INFO: namespace configmap-1485 deletion completed in 22.10860118s

• [SLOW TEST:26.304 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:54:12.161: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 19:54:12.296: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:54:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7738" for this suite.
Sep  6 19:55:00.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:55:00.433: INFO: namespace pods-7738 deletion completed in 46.098580407s

• [SLOW TEST:48.272 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:55:00.434: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-38a92d46-6078-416c-8148-a7b8a8e978c1 in namespace container-probe-3628
Sep  6 19:55:02.579: INFO: Started pod busybox-38a92d46-6078-416c-8148-a7b8a8e978c1 in namespace container-probe-3628
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 19:55:02.582: INFO: Initial restart count of pod busybox-38a92d46-6078-416c-8148-a7b8a8e978c1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:59:03.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3628" for this suite.
Sep  6 19:59:09.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:59:09.149: INFO: namespace container-probe-3628 deletion completed in 6.110339991s

• [SLOW TEST:248.716 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:59:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-69f83a9f-c74b-4260-8f44-a0c59c25ce7f
STEP: Creating a pod to test consume configMaps
Sep  6 19:59:09.298: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6" in namespace "configmap-2108" to be "success or failure"
Sep  6 19:59:09.300: INFO: Pod "pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664286ms
Sep  6 19:59:11.306: INFO: Pod "pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008082804s
Sep  6 19:59:13.309: INFO: Pod "pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011598524s
STEP: Saw pod success
Sep  6 19:59:13.309: INFO: Pod "pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6" satisfied condition "success or failure"
Sep  6 19:59:13.312: INFO: Trying to get logs from node appserv10 pod pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 19:59:13.331: INFO: Waiting for pod pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6 to disappear
Sep  6 19:59:13.334: INFO: Pod pod-configmaps-aa6772a7-ccb5-4a2e-957c-d27110d3ccb6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:59:13.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2108" for this suite.
Sep  6 19:59:19.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:59:19.442: INFO: namespace configmap-2108 deletion completed in 6.104319894s

• [SLOW TEST:10.292 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:59:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5924
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 19:59:19.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38" in namespace "projected-5924" to be "success or failure"
Sep  6 19:59:19.589: INFO: Pod "downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07381ms
Sep  6 19:59:21.592: INFO: Pod "downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006356673s
STEP: Saw pod success
Sep  6 19:59:21.592: INFO: Pod "downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38" satisfied condition "success or failure"
Sep  6 19:59:21.595: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38 container client-container: <nil>
STEP: delete the pod
Sep  6 19:59:21.613: INFO: Waiting for pod downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38 to disappear
Sep  6 19:59:21.616: INFO: Pod downwardapi-volume-54a32619-e2e4-44af-81db-27e64a571a38 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:59:21.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5924" for this suite.
Sep  6 19:59:27.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:59:27.730: INFO: namespace projected-5924 deletion completed in 6.109674115s

• [SLOW TEST:8.287 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:59:27.730: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 19:59:30.891: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:59:30.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-809" for this suite.
Sep  6 19:59:36.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:59:37.021: INFO: namespace container-runtime-809 deletion completed in 6.104087891s

• [SLOW TEST:9.291 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:59:37.021: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-7616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Sep  6 19:59:37.161: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7616" to be "success or failure"
Sep  6 19:59:37.164: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794922ms
Sep  6 19:59:39.167: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006465045s
STEP: Saw pod success
Sep  6 19:59:39.167: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep  6 19:59:39.170: INFO: Trying to get logs from node appserv9 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  6 19:59:39.188: INFO: Waiting for pod pod-host-path-test to disappear
Sep  6 19:59:39.190: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:59:39.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7616" for this suite.
Sep  6 19:59:45.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:59:45.296: INFO: namespace hostpath-7616 deletion completed in 6.101544984s

• [SLOW TEST:8.274 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:59:45.296: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 19:59:45.436: INFO: Waiting up to 5m0s for pod "pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c" in namespace "emptydir-3224" to be "success or failure"
Sep  6 19:59:45.439: INFO: Pod "pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.433025ms
Sep  6 19:59:47.442: INFO: Pod "pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006023758s
STEP: Saw pod success
Sep  6 19:59:47.442: INFO: Pod "pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c" satisfied condition "success or failure"
Sep  6 19:59:47.445: INFO: Trying to get logs from node appserv10 pod pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c container test-container: <nil>
STEP: delete the pod
Sep  6 19:59:47.465: INFO: Waiting for pod pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c to disappear
Sep  6 19:59:47.467: INFO: Pod pod-32b47e45-a8e7-4a63-9cda-ed70fc54037c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 19:59:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3224" for this suite.
Sep  6 19:59:53.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 19:59:53.578: INFO: namespace emptydir-3224 deletion completed in 6.106605909s

• [SLOW TEST:8.282 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 19:59:53.578: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 19:59:53.734: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  6 19:59:53.741: INFO: Number of nodes with available pods: 0
Sep  6 19:59:53.741: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  6 19:59:53.755: INFO: Number of nodes with available pods: 0
Sep  6 19:59:53.755: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:59:54.759: INFO: Number of nodes with available pods: 0
Sep  6 19:59:54.759: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:59:55.759: INFO: Number of nodes with available pods: 0
Sep  6 19:59:55.759: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:59:56.759: INFO: Number of nodes with available pods: 1
Sep  6 19:59:56.759: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  6 19:59:56.773: INFO: Number of nodes with available pods: 1
Sep  6 19:59:56.773: INFO: Number of running nodes: 0, number of available pods: 1
Sep  6 19:59:57.777: INFO: Number of nodes with available pods: 0
Sep  6 19:59:57.777: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  6 19:59:57.790: INFO: Number of nodes with available pods: 0
Sep  6 19:59:57.790: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:59:58.795: INFO: Number of nodes with available pods: 0
Sep  6 19:59:58.795: INFO: Node appserv10 is running more than one daemon pod
Sep  6 19:59:59.795: INFO: Number of nodes with available pods: 0
Sep  6 19:59:59.795: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:00:00.794: INFO: Number of nodes with available pods: 0
Sep  6 20:00:00.794: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:00:01.794: INFO: Number of nodes with available pods: 0
Sep  6 20:00:01.794: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:00:02.794: INFO: Number of nodes with available pods: 0
Sep  6 20:00:02.794: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:00:03.794: INFO: Number of nodes with available pods: 0
Sep  6 20:00:03.794: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:00:04.793: INFO: Number of nodes with available pods: 1
Sep  6 20:00:04.793: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9658, will wait for the garbage collector to delete the pods
Sep  6 20:00:04.858: INFO: Deleting DaemonSet.extensions daemon-set took: 6.783462ms
Sep  6 20:00:05.359: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.278372ms
Sep  6 20:00:18.162: INFO: Number of nodes with available pods: 0
Sep  6 20:00:18.162: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 20:00:18.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9658/daemonsets","resourceVersion":"4678"},"items":null}

Sep  6 20:00:18.167: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9658/pods","resourceVersion":"4678"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:00:18.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9658" for this suite.
Sep  6 20:00:24.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:00:24.296: INFO: namespace daemonsets-9658 deletion completed in 6.105804291s

• [SLOW TEST:30.718 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:00:24.296: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0906 20:00:25.464044      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 20:00:25.464: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:00:25.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6055" for this suite.
Sep  6 20:00:31.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:00:31.573: INFO: namespace gc-6055 deletion completed in 6.106038283s

• [SLOW TEST:7.277 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:00:31.573: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:00:31.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770" in namespace "downward-api-7340" to be "success or failure"
Sep  6 20:00:31.718: INFO: Pod "downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.783839ms
Sep  6 20:00:33.722: INFO: Pod "downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006549874s
Sep  6 20:00:35.726: INFO: Pod "downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009981319s
STEP: Saw pod success
Sep  6 20:00:35.726: INFO: Pod "downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770" satisfied condition "success or failure"
Sep  6 20:00:35.728: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770 container client-container: <nil>
STEP: delete the pod
Sep  6 20:00:35.750: INFO: Waiting for pod downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770 to disappear
Sep  6 20:00:35.754: INFO: Pod downwardapi-volume-dda1599a-6f80-4228-9b26-71a3cd596770 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:00:35.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7340" for this suite.
Sep  6 20:00:41.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:00:41.863: INFO: namespace downward-api-7340 deletion completed in 6.103287571s

• [SLOW TEST:10.290 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:00:41.863: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-15e59834-9450-40b8-84b0-038e1e9a80df
STEP: Creating a pod to test consume configMaps
Sep  6 20:00:42.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c" in namespace "configmap-8662" to be "success or failure"
Sep  6 20:00:42.011: INFO: Pod "pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.801421ms
Sep  6 20:00:44.015: INFO: Pod "pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006868965s
STEP: Saw pod success
Sep  6 20:00:44.015: INFO: Pod "pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c" satisfied condition "success or failure"
Sep  6 20:00:44.018: INFO: Trying to get logs from node appserv11 pod pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:00:44.048: INFO: Waiting for pod pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c to disappear
Sep  6 20:00:44.050: INFO: Pod pod-configmaps-ea040060-a75c-4915-9520-9977cda3427c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:00:44.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8662" for this suite.
Sep  6 20:00:50.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:00:50.167: INFO: namespace configmap-8662 deletion completed in 6.112125254s

• [SLOW TEST:8.304 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:00:50.167: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:00:53.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7635" for this suite.
Sep  6 20:01:15.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:01:15.436: INFO: namespace replication-controller-7635 deletion completed in 22.100779674s

• [SLOW TEST:25.269 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:01:15.436: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2059
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep  6 20:01:17.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec pod-sharedvolume-9646d8fc-9d34-4915-afe5-17a3e515684b -c busybox-main-container --namespace=emptydir-2059 -- cat /usr/share/volumeshare/shareddata.txt'
Sep  6 20:01:17.893: INFO: stderr: ""
Sep  6 20:01:17.893: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:01:17.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2059" for this suite.
Sep  6 20:01:23.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:01:23.997: INFO: namespace emptydir-2059 deletion completed in 6.099407143s

• [SLOW TEST:8.561 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:01:23.998: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9292
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-7c4af544-c51c-4601-81c3-71cf9871a509
STEP: Creating a pod to test consume configMaps
Sep  6 20:01:24.143: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed" in namespace "projected-9292" to be "success or failure"
Sep  6 20:01:24.146: INFO: Pod "pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.662809ms
Sep  6 20:01:26.149: INFO: Pod "pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005866814s
STEP: Saw pod success
Sep  6 20:01:26.149: INFO: Pod "pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed" satisfied condition "success or failure"
Sep  6 20:01:26.152: INFO: Trying to get logs from node appserv10 pod pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:01:26.169: INFO: Waiting for pod pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed to disappear
Sep  6 20:01:26.172: INFO: Pod pod-projected-configmaps-55a39831-9423-4855-8453-acee3b01c4ed no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:01:26.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9292" for this suite.
Sep  6 20:01:32.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:01:32.283: INFO: namespace projected-9292 deletion completed in 6.10732984s

• [SLOW TEST:8.285 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:01:32.283: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-e222d47d-b15c-4361-a1ed-b1ab75223026 in namespace container-probe-1825
Sep  6 20:01:34.431: INFO: Started pod busybox-e222d47d-b15c-4361-a1ed-b1ab75223026 in namespace container-probe-1825
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 20:01:34.433: INFO: Initial restart count of pod busybox-e222d47d-b15c-4361-a1ed-b1ab75223026 is 0
Sep  6 20:02:20.516: INFO: Restart count of pod container-probe-1825/busybox-e222d47d-b15c-4361-a1ed-b1ab75223026 is now 1 (46.08290372s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:02:20.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1825" for this suite.
Sep  6 20:02:26.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:02:26.633: INFO: namespace container-probe-1825 deletion completed in 6.102752689s

• [SLOW TEST:54.349 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:02:26.633: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 20:02:26.773: INFO: Waiting up to 5m0s for pod "pod-c2f70209-2f43-4eab-acab-5216b3ec288c" in namespace "emptydir-3875" to be "success or failure"
Sep  6 20:02:26.776: INFO: Pod "pod-c2f70209-2f43-4eab-acab-5216b3ec288c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648453ms
Sep  6 20:02:28.780: INFO: Pod "pod-c2f70209-2f43-4eab-acab-5216b3ec288c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006421227s
Sep  6 20:02:30.783: INFO: Pod "pod-c2f70209-2f43-4eab-acab-5216b3ec288c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009923005s
STEP: Saw pod success
Sep  6 20:02:30.783: INFO: Pod "pod-c2f70209-2f43-4eab-acab-5216b3ec288c" satisfied condition "success or failure"
Sep  6 20:02:30.786: INFO: Trying to get logs from node appserv9 pod pod-c2f70209-2f43-4eab-acab-5216b3ec288c container test-container: <nil>
STEP: delete the pod
Sep  6 20:02:30.819: INFO: Waiting for pod pod-c2f70209-2f43-4eab-acab-5216b3ec288c to disappear
Sep  6 20:02:30.821: INFO: Pod pod-c2f70209-2f43-4eab-acab-5216b3ec288c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:02:30.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3875" for this suite.
Sep  6 20:02:36.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:02:36.933: INFO: namespace emptydir-3875 deletion completed in 6.108287457s

• [SLOW TEST:10.300 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:02:36.933: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3811
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 20:02:37.069: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 20:02:59.579: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.11:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3811 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:02:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:02:59.707: INFO: Found all expected endpoints: [netserver-0]
Sep  6 20:02:59.710: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.12:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3811 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:02:59.710: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:02:59.812: INFO: Found all expected endpoints: [netserver-1]
Sep  6 20:02:59.815: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.141.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3811 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:02:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:02:59.923: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:02:59.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3811" for this suite.
Sep  6 20:03:21.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:03:22.031: INFO: namespace pod-network-test-3811 deletion completed in 22.104266055s

• [SLOW TEST:44.657 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:03:22.031: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3568
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  6 20:03:22.173: INFO: Waiting up to 5m0s for pod "pod-234d754a-b0a4-425b-81bb-1def722f12b9" in namespace "emptydir-3568" to be "success or failure"
Sep  6 20:03:22.176: INFO: Pod "pod-234d754a-b0a4-425b-81bb-1def722f12b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309952ms
Sep  6 20:03:24.180: INFO: Pod "pod-234d754a-b0a4-425b-81bb-1def722f12b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006245507s
STEP: Saw pod success
Sep  6 20:03:24.180: INFO: Pod "pod-234d754a-b0a4-425b-81bb-1def722f12b9" satisfied condition "success or failure"
Sep  6 20:03:24.183: INFO: Trying to get logs from node appserv10 pod pod-234d754a-b0a4-425b-81bb-1def722f12b9 container test-container: <nil>
STEP: delete the pod
Sep  6 20:03:24.203: INFO: Waiting for pod pod-234d754a-b0a4-425b-81bb-1def722f12b9 to disappear
Sep  6 20:03:24.205: INFO: Pod pod-234d754a-b0a4-425b-81bb-1def722f12b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:03:24.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3568" for this suite.
Sep  6 20:03:30.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:03:30.317: INFO: namespace emptydir-3568 deletion completed in 6.107934906s

• [SLOW TEST:8.286 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:03:30.318: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0906 20:03:36.481383      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 20:03:36.481: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:03:36.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1802" for this suite.
Sep  6 20:03:42.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:03:42.586: INFO: namespace gc-1802 deletion completed in 6.102173573s

• [SLOW TEST:12.268 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:03:42.587: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 20:03:42.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3211'
Sep  6 20:03:42.857: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 20:03:42.857: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Sep  6 20:03:42.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3211'
Sep  6 20:03:42.971: INFO: stderr: ""
Sep  6 20:03:42.971: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:03:42.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3211" for this suite.
Sep  6 20:04:04.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:04:05.069: INFO: namespace kubectl-3211 deletion completed in 22.094545868s

• [SLOW TEST:22.483 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:04:05.070: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 20:04:05.209: INFO: Waiting up to 5m0s for pod "pod-03659cea-b112-470a-a0bc-cd82f96a158a" in namespace "emptydir-386" to be "success or failure"
Sep  6 20:04:05.212: INFO: Pod "pod-03659cea-b112-470a-a0bc-cd82f96a158a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534843ms
Sep  6 20:04:07.216: INFO: Pod "pod-03659cea-b112-470a-a0bc-cd82f96a158a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006574696s
STEP: Saw pod success
Sep  6 20:04:07.216: INFO: Pod "pod-03659cea-b112-470a-a0bc-cd82f96a158a" satisfied condition "success or failure"
Sep  6 20:04:07.219: INFO: Trying to get logs from node appserv10 pod pod-03659cea-b112-470a-a0bc-cd82f96a158a container test-container: <nil>
STEP: delete the pod
Sep  6 20:04:07.239: INFO: Waiting for pod pod-03659cea-b112-470a-a0bc-cd82f96a158a to disappear
Sep  6 20:04:07.241: INFO: Pod pod-03659cea-b112-470a-a0bc-cd82f96a158a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:04:07.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-386" for this suite.
Sep  6 20:04:13.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:04:13.358: INFO: namespace emptydir-386 deletion completed in 6.103622134s

• [SLOW TEST:8.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:04:13.360: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 20:04:13.503: INFO: Waiting up to 5m0s for pod "downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d" in namespace "downward-api-1606" to be "success or failure"
Sep  6 20:04:13.506: INFO: Pod "downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911833ms
Sep  6 20:04:15.510: INFO: Pod "downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006594293s
STEP: Saw pod success
Sep  6 20:04:15.510: INFO: Pod "downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d" satisfied condition "success or failure"
Sep  6 20:04:15.512: INFO: Trying to get logs from node appserv9 pod downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d container dapi-container: <nil>
STEP: delete the pod
Sep  6 20:04:15.532: INFO: Waiting for pod downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d to disappear
Sep  6 20:04:15.535: INFO: Pod downward-api-cfb0dc8d-0813-455f-9ebe-7737e9164c9d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:04:15.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1606" for this suite.
Sep  6 20:04:21.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:04:21.652: INFO: namespace downward-api-1606 deletion completed in 6.114322974s

• [SLOW TEST:8.293 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:04:21.653: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6cc23dd2-2df7-4521-945a-90522c08b94f
STEP: Creating a pod to test consume configMaps
Sep  6 20:04:21.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a" in namespace "configmap-9264" to be "success or failure"
Sep  6 20:04:21.798: INFO: Pod "pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56917ms
Sep  6 20:04:23.802: INFO: Pod "pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006098838s
STEP: Saw pod success
Sep  6 20:04:23.802: INFO: Pod "pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a" satisfied condition "success or failure"
Sep  6 20:04:23.805: INFO: Trying to get logs from node appserv10 pod pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:04:23.823: INFO: Waiting for pod pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a to disappear
Sep  6 20:04:23.826: INFO: Pod pod-configmaps-0884bfa1-5993-4c60-8479-4a4a6e41655a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:04:23.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9264" for this suite.
Sep  6 20:04:29.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:04:29.937: INFO: namespace configmap-9264 deletion completed in 6.106221396s

• [SLOW TEST:8.284 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:04:29.937: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 20:04:34.122: INFO: DNS probes using dns-7252/dns-test-4c802e87-1b69-4e85-8663-04459836a405 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:04:34.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7252" for this suite.
Sep  6 20:04:40.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:04:40.246: INFO: namespace dns-7252 deletion completed in 6.109807048s

• [SLOW TEST:10.309 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:04:40.246: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  6 20:04:44.411: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:44.411: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:44.527: INFO: Exec stderr: ""
Sep  6 20:04:44.528: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:44.528: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:44.629: INFO: Exec stderr: ""
Sep  6 20:04:44.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:44.630: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:44.735: INFO: Exec stderr: ""
Sep  6 20:04:44.735: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:44.735: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:44.825: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  6 20:04:44.825: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:44.825: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:44.914: INFO: Exec stderr: ""
Sep  6 20:04:44.914: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:44.914: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:45.017: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  6 20:04:45.017: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:45.017: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:45.115: INFO: Exec stderr: ""
Sep  6 20:04:45.115: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:45.115: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:45.203: INFO: Exec stderr: ""
Sep  6 20:04:45.203: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:45.203: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:45.288: INFO: Exec stderr: ""
Sep  6 20:04:45.288: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4681 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:04:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:04:45.393: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:04:45.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4681" for this suite.
Sep  6 20:05:31.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:05:31.500: INFO: namespace e2e-kubelet-etc-hosts-4681 deletion completed in 46.102978445s

• [SLOW TEST:51.254 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:05:31.500: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6749
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f019debd-5aaa-45e8-b4b5-ee6a11190356
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f019debd-5aaa-45e8-b4b5-ee6a11190356
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:05:35.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6749" for this suite.
Sep  6 20:05:57.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:05:57.804: INFO: namespace projected-6749 deletion completed in 22.11086619s

• [SLOW TEST:26.303 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:05:57.804: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1684
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 20:05:57.940: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 20:06:16.020: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.11 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1684 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:06:16.020: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:06:17.122: INFO: Found all expected endpoints: [netserver-0]
Sep  6 20:06:17.125: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1684 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:06:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:06:18.229: INFO: Found all expected endpoints: [netserver-1]
Sep  6 20:06:18.232: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.141.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1684 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:06:18.232: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:06:19.343: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:06:19.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1684" for this suite.
Sep  6 20:06:41.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:06:41.458: INFO: namespace pod-network-test-1684 deletion completed in 22.110141185s

• [SLOW TEST:43.654 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:06:41.458: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 20:06:41.594: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 20:07:23.650: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-95164ab4-0e16-4f23-9384-68298c0821a5", GenerateName:"", Namespace:"init-container-3400", SelfLink:"/api/v1/namespaces/init-container-3400/pods/pod-init-95164ab4-0e16-4f23-9384-68298c0821a5", UID:"cfd216ff-d88c-4801-927f-702be181542c", ResourceVersion:"6652", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703397201, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"594772878"}, Annotations:map[string]string{"kubernetes.io/psp":"collecting"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-j7vhw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002d73600), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j7vhw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j7vhw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j7vhw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e16648), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"appserv9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0032c01e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e166d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e166f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e166f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e166fc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397201, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397201, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397201, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397201, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.6.109", PodIP:"172.16.141.11", StartTime:(*v1.Time)(0xc0024ab400), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c1e9a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c1ea10)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://5130b5caee794245d9afb1ada2dc4470ebad641437579437bf16fad7f0bcbb16"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0024ab440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0024ab420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:07:23.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3400" for this suite.
Sep  6 20:07:45.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:07:45.751: INFO: namespace init-container-3400 deletion completed in 22.095290417s

• [SLOW TEST:64.293 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:07:45.751: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Sep  6 20:07:45.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-1957'
Sep  6 20:07:46.207: INFO: stderr: ""
Sep  6 20:07:46.207: INFO: stdout: "pod/pause created\n"
Sep  6 20:07:46.207: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 20:07:46.207: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1957" to be "running and ready"
Sep  6 20:07:46.209: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.562848ms
Sep  6 20:07:48.213: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005961302s
Sep  6 20:07:48.213: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 20:07:48.213: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  6 20:07:48.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 label pods pause testing-label=testing-label-value --namespace=kubectl-1957'
Sep  6 20:07:48.333: INFO: stderr: ""
Sep  6 20:07:48.333: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  6 20:07:48.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pod pause -L testing-label --namespace=kubectl-1957'
Sep  6 20:07:48.454: INFO: stderr: ""
Sep  6 20:07:48.454: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  6 20:07:48.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 label pods pause testing-label- --namespace=kubectl-1957'
Sep  6 20:07:48.569: INFO: stderr: ""
Sep  6 20:07:48.569: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  6 20:07:48.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pod pause -L testing-label --namespace=kubectl-1957'
Sep  6 20:07:48.684: INFO: stderr: ""
Sep  6 20:07:48.684: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Sep  6 20:07:48.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-1957'
Sep  6 20:07:48.785: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:07:48.785: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 20:07:48.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get rc,svc -l name=pause --no-headers --namespace=kubectl-1957'
Sep  6 20:07:48.880: INFO: stderr: "No resources found.\n"
Sep  6 20:07:48.880: INFO: stdout: ""
Sep  6 20:07:48.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -l name=pause --namespace=kubectl-1957 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 20:07:48.990: INFO: stderr: ""
Sep  6 20:07:48.990: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:07:48.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1957" for this suite.
Sep  6 20:07:55.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:07:55.090: INFO: namespace kubectl-1957 deletion completed in 6.096371372s

• [SLOW TEST:9.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:07:55.091: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-4xd7
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 20:07:55.247: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4xd7" in namespace "subpath-5141" to be "success or failure"
Sep  6 20:07:55.250: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362573ms
Sep  6 20:07:57.253: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006016738s
Sep  6 20:07:59.263: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 4.016132336s
Sep  6 20:08:01.267: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 6.019770843s
Sep  6 20:08:03.271: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 8.023341201s
Sep  6 20:08:05.275: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 10.02735289s
Sep  6 20:08:07.279: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 12.031357511s
Sep  6 20:08:09.282: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 14.034806286s
Sep  6 20:08:11.286: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 16.038353961s
Sep  6 20:08:13.290: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 18.042461169s
Sep  6 20:08:15.293: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Running", Reason="", readiness=true. Elapsed: 20.046142443s
Sep  6 20:08:17.297: INFO: Pod "pod-subpath-test-secret-4xd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04990879s
STEP: Saw pod success
Sep  6 20:08:17.297: INFO: Pod "pod-subpath-test-secret-4xd7" satisfied condition "success or failure"
Sep  6 20:08:17.300: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-secret-4xd7 container test-container-subpath-secret-4xd7: <nil>
STEP: delete the pod
Sep  6 20:08:17.321: INFO: Waiting for pod pod-subpath-test-secret-4xd7 to disappear
Sep  6 20:08:17.323: INFO: Pod pod-subpath-test-secret-4xd7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-4xd7
Sep  6 20:08:17.323: INFO: Deleting pod "pod-subpath-test-secret-4xd7" in namespace "subpath-5141"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:08:17.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5141" for this suite.
Sep  6 20:08:23.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:08:23.439: INFO: namespace subpath-5141 deletion completed in 6.109624423s

• [SLOW TEST:28.348 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:08:23.439: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:08:23.593: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 20:08:23.603: INFO: Number of nodes with available pods: 0
Sep  6 20:08:23.604: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:08:24.611: INFO: Number of nodes with available pods: 0
Sep  6 20:08:24.612: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:08:25.611: INFO: Number of nodes with available pods: 3
Sep  6 20:08:25.611: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  6 20:08:25.634: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:25.634: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:25.634: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:26.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:26.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:26.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:27.641: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:27.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:27.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:28.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:28.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:28.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:29.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:29.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:29.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:30.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:30.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:30.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:31.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:31.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:31.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:31.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:32.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:32.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:32.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:32.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:33.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:33.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:33.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:33.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:34.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:34.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:34.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:34.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:35.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:35.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:35.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:35.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:36.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:36.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:36.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:36.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:37.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:37.642: INFO: Wrong image for pod: daemon-set-zbd94. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:37.642: INFO: Pod daemon-set-zbd94 is not available
Sep  6 20:08:37.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:38.642: INFO: Pod daemon-set-b2jsr is not available
Sep  6 20:08:38.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:38.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:39.642: INFO: Pod daemon-set-b2jsr is not available
Sep  6 20:08:39.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:39.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:40.641: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:40.641: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:41.643: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:41.643: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:42.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:42.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:43.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:43.642: INFO: Pod daemon-set-kjrww is not available
Sep  6 20:08:43.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:44.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:44.642: INFO: Pod daemon-set-kjrww is not available
Sep  6 20:08:44.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:45.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:45.642: INFO: Pod daemon-set-kjrww is not available
Sep  6 20:08:45.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:46.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:46.642: INFO: Pod daemon-set-kjrww is not available
Sep  6 20:08:46.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:47.642: INFO: Wrong image for pod: daemon-set-kjrww. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:47.642: INFO: Pod daemon-set-kjrww is not available
Sep  6 20:08:47.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:48.642: INFO: Pod daemon-set-v2s75 is not available
Sep  6 20:08:48.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:49.642: INFO: Pod daemon-set-v2s75 is not available
Sep  6 20:08:49.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:50.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:51.641: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:52.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:53.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:54.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:54.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:08:55.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:55.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:08:56.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:56.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:08:57.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:57.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:08:58.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:58.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:08:59.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:08:59.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:09:00.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:09:00.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:09:01.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:09:01.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:09:02.642: INFO: Wrong image for pod: daemon-set-zh9q6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 20:09:02.642: INFO: Pod daemon-set-zh9q6 is not available
Sep  6 20:09:03.642: INFO: Pod daemon-set-xzjhd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  6 20:09:03.653: INFO: Number of nodes with available pods: 2
Sep  6 20:09:03.653: INFO: Node appserv9 is running more than one daemon pod
Sep  6 20:09:04.660: INFO: Number of nodes with available pods: 3
Sep  6 20:09:04.660: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8570, will wait for the garbage collector to delete the pods
Sep  6 20:09:04.735: INFO: Deleting DaemonSet.extensions daemon-set took: 6.345814ms
Sep  6 20:09:05.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.184892ms
Sep  6 20:09:18.639: INFO: Number of nodes with available pods: 0
Sep  6 20:09:18.640: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 20:09:18.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8570/daemonsets","resourceVersion":"7111"},"items":null}

Sep  6 20:09:18.645: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8570/pods","resourceVersion":"7111"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:09:18.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8570" for this suite.
Sep  6 20:09:24.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:09:24.768: INFO: namespace daemonsets-8570 deletion completed in 6.106067724s

• [SLOW TEST:61.329 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:09:24.768: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:09:24.907: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c" in namespace "projected-3681" to be "success or failure"
Sep  6 20:09:24.910: INFO: Pod "downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570223ms
Sep  6 20:09:26.913: INFO: Pod "downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006195124s
STEP: Saw pod success
Sep  6 20:09:26.913: INFO: Pod "downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c" satisfied condition "success or failure"
Sep  6 20:09:26.916: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c container client-container: <nil>
STEP: delete the pod
Sep  6 20:09:26.932: INFO: Waiting for pod downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c to disappear
Sep  6 20:09:26.936: INFO: Pod downwardapi-volume-dabbeed5-9fb6-4921-aec0-c71efbe8e23c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:09:26.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3681" for this suite.
Sep  6 20:09:32.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:09:33.046: INFO: namespace projected-3681 deletion completed in 6.107274824s

• [SLOW TEST:8.278 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:09:33.047: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:09:33.184: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:09:35.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9429" for this suite.
Sep  6 20:10:25.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:10:25.390: INFO: namespace pods-9429 deletion completed in 50.094641804s

• [SLOW TEST:52.343 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:10:25.391: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 20:10:28.059: INFO: Successfully updated pod "annotationupdate8d8aa150-6aa6-4be0-a23d-34e7956e9ce8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:10:30.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7920" for this suite.
Sep  6 20:10:52.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:10:52.184: INFO: namespace projected-7920 deletion completed in 22.101280081s

• [SLOW TEST:26.793 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:10:52.185: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Sep  6 20:10:52.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 --namespace=kubectl-4408 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep  6 20:10:54.269: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep  6 20:10:54.269: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:10:56.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4408" for this suite.
Sep  6 20:11:02.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:11:02.384: INFO: namespace kubectl-4408 deletion completed in 6.104764739s

• [SLOW TEST:10.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:11:02.384: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:11:02.525: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 20:11:07.529: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 20:11:07.529: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 20:11:09.533: INFO: Creating deployment "test-rollover-deployment"
Sep  6 20:11:09.541: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 20:11:11.548: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 20:11:11.553: INFO: Ensure that both replica sets have 1 created replica
Sep  6 20:11:11.558: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 20:11:11.565: INFO: Updating deployment test-rollover-deployment
Sep  6 20:11:11.565: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 20:11:13.571: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 20:11:13.578: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 20:11:13.584: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 20:11:13.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397471, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 20:11:15.592: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 20:11:15.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397475, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 20:11:17.592: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 20:11:17.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397475, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 20:11:19.592: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 20:11:19.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397475, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 20:11:21.591: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 20:11:21.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397475, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 20:11:23.592: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 20:11:23.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397475, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703397469, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 20:11:25.591: INFO: 
Sep  6 20:11:25.591: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 20:11:25.599: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4355,SelfLink:/apis/apps/v1/namespaces/deployment-4355/deployments/test-rollover-deployment,UID:1f47d5da-395a-4e7b-9f5d-b667c4fdcb0b,ResourceVersion:7627,Generation:2,CreationTimestamp:2019-09-06 20:11:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 20:11:09 +0000 UTC 2019-09-06 20:11:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 20:11:25 +0000 UTC 2019-09-06 20:11:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 20:11:25.603: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-4355,SelfLink:/apis/apps/v1/namespaces/deployment-4355/replicasets/test-rollover-deployment-854595fc44,UID:f4ff0780-db0d-4f66-ae75-9165ad207d2c,ResourceVersion:7615,Generation:2,CreationTimestamp:2019-09-06 20:11:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 1f47d5da-395a-4e7b-9f5d-b667c4fdcb0b 0xc00090d397 0xc00090d398}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 20:11:25.603: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 20:11:25.603: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4355,SelfLink:/apis/apps/v1/namespaces/deployment-4355/replicasets/test-rollover-controller,UID:d10068a7-c3f7-481d-8cb1-f6b7838a9bd9,ResourceVersion:7625,Generation:2,CreationTimestamp:2019-09-06 20:11:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 1f47d5da-395a-4e7b-9f5d-b667c4fdcb0b 0xc00090d2bf 0xc00090d2d0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 20:11:25.604: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-4355,SelfLink:/apis/apps/v1/namespaces/deployment-4355/replicasets/test-rollover-deployment-9b8b997cf,UID:8646d443-2f92-4290-9653-e7a279418e0e,ResourceVersion:7567,Generation:2,CreationTimestamp:2019-09-06 20:11:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 1f47d5da-395a-4e7b-9f5d-b667c4fdcb0b 0xc00090d4b0 0xc00090d4b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 20:11:25.607: INFO: Pod "test-rollover-deployment-854595fc44-2slp4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-2slp4,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-4355,SelfLink:/api/v1/namespaces/deployment-4355/pods/test-rollover-deployment-854595fc44-2slp4,UID:50370287-7d70-4fd2-9294-4f933fb0f74b,ResourceVersion:7592,Generation:0,CreationTimestamp:2019-09-06 20:11:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 f4ff0780-db0d-4f66-ae75-9165ad207d2c 0xc00385a9a7 0xc00385a9a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-x6c5f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-x6c5f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-x6c5f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00385aa20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00385aa40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:11:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:11:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:11:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:11:11 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.13,StartTime:2019-09-06 20:11:11 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 20:11:14 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://84cf5e0b2f7b58394f45ba74d7f50d2ae300b1958b8be97ba08b1a3cb793d67b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:11:25.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4355" for this suite.
Sep  6 20:11:31.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:11:31.703: INFO: namespace deployment-4355 deletion completed in 6.091900855s

• [SLOW TEST:29.319 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:11:31.704: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1734
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Sep  6 20:11:31.840: INFO: Waiting up to 5m0s for pod "client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68" in namespace "containers-1734" to be "success or failure"
Sep  6 20:11:31.843: INFO: Pod "client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.549973ms
Sep  6 20:11:33.847: INFO: Pod "client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006140484s
STEP: Saw pod success
Sep  6 20:11:33.847: INFO: Pod "client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68" satisfied condition "success or failure"
Sep  6 20:11:33.849: INFO: Trying to get logs from node appserv10 pod client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68 container test-container: <nil>
STEP: delete the pod
Sep  6 20:11:33.870: INFO: Waiting for pod client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68 to disappear
Sep  6 20:11:33.873: INFO: Pod client-containers-c4e2b0e0-23d7-4726-9f9b-6db97e188e68 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:11:33.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1734" for this suite.
Sep  6 20:11:39.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:11:39.980: INFO: namespace containers-1734 deletion completed in 6.103030153s

• [SLOW TEST:8.276 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:11:39.980: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8969
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6793
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:11:46.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2769" for this suite.
Sep  6 20:11:52.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:11:52.505: INFO: namespace namespaces-2769 deletion completed in 6.105322136s
STEP: Destroying namespace "nsdeletetest-8969" for this suite.
Sep  6 20:11:52.507: INFO: Namespace nsdeletetest-8969 was already deleted
STEP: Destroying namespace "nsdeletetest-6793" for this suite.
Sep  6 20:11:58.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:11:58.615: INFO: namespace nsdeletetest-6793 deletion completed in 6.10765371s

• [SLOW TEST:18.635 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:11:58.615: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 20:12:02.788: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:02.792: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:04.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:04.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:06.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:06.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:08.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:08.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:10.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:10.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:12.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:12.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:14.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:14.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:16.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:16.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:18.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:18.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:20.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:20.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:22.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:22.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:24.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:24.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:26.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:26.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:28.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:28.796: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:30.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:30.795: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 20:12:32.792: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 20:12:32.795: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:12:32.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4888" for this suite.
Sep  6 20:12:54.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:12:54.906: INFO: namespace container-lifecycle-hook-4888 deletion completed in 22.097264123s

• [SLOW TEST:56.291 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:12:54.907: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  6 20:12:55.037: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 20:12:55.044: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 20:12:55.047: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Sep  6 20:12:55.054: INFO: csi-external-snapshotter-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.054: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Sep  6 20:12:55.054: INFO: prometheus-v1-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.054: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:12:55.054: INFO: collectd-v0.8-md6fj from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (4 container statuses recorded)
Sep  6 20:12:55.054: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:12:55.054: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:12:55.054: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:12:55.054: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:12:55.054: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 19:45:19 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.054: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 20:12:55.054: INFO: csi-diamanti-driver-h8mbm from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.054: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:12:55.054: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:12:55.054: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-2wkwb from heptio-sonobuoy started at 2019-09-06 19:45:21 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.054: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:12:55.054: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 20:12:55.054: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Sep  6 20:12:55.064: INFO: alertmanager-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 20:12:55.064: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-plh6j from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 20:12:55.064: INFO: prometheus-v1-1 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:12:55.064: INFO: csi-external-provisioner-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Sep  6 20:12:55.064: INFO: sonobuoy-e2e-job-8e772d81d3704c93 from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container e2e ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:12:55.064: INFO: collectd-v0.8-xfwsc from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (4 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:12:55.064: INFO: tiller-deploy-5668df8bc4-6pgjb from kube-system started at 2019-09-06 19:41:52 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container tiller ready: true, restart count 0
Sep  6 20:12:55.064: INFO: snapshot-controller-66fb5f8fbd-xjhnl from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Sep  6 20:12:55.064: INFO: metrics-server-v1-5d46b6d959-48m72 from kube-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 20:12:55.064: INFO: csi-diamanti-driver-wztns from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:12:55.064: INFO: csi-external-attacher-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container csi-attacher ready: true, restart count 0
Sep  6 20:12:55.064: INFO: csi-external-resizer-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.064: INFO: 	Container csi-external-resizer ready: true, restart count 0
Sep  6 20:12:55.064: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Sep  6 20:12:55.071: INFO: provisioner-7b58589b9d-2zpz4 from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.071: INFO: 	Container provisioner ready: true, restart count 0
Sep  6 20:12:55.071: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-kngjc from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.071: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:12:55.071: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 20:12:55.071: INFO: csi-diamanti-driver-fc5gk from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (2 container statuses recorded)
Sep  6 20:12:55.071: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:12:55.071: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:12:55.071: INFO: collectd-v0.8-f5qhk from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (4 container statuses recorded)
Sep  6 20:12:55.071: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:12:55.071: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:12:55.071: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:12:55.071: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:12:55.071: INFO: helm-chart-687577f867-4r7c2 from kube-system started at 2019-09-06 19:41:53 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.071: INFO: 	Container helm-chart ready: true, restart count 0
Sep  6 20:12:55.071: INFO: prometheus-v1-2 from diamanti-system started at 2019-09-06 19:41:53 +0000 UTC (1 container statuses recorded)
Sep  6 20:12:55.071: INFO: 	Container prometheus ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c1f27f98182426], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:12:56.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8588" for this suite.
Sep  6 20:13:02.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:13:02.235: INFO: namespace sched-pred-8588 deletion completed in 6.102500413s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.329 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:13:02.235: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 20:13:02.378: INFO: Waiting up to 5m0s for pod "pod-91d85630-0b6e-40d6-9198-2f0c8b716072" in namespace "emptydir-6644" to be "success or failure"
Sep  6 20:13:02.380: INFO: Pod "pod-91d85630-0b6e-40d6-9198-2f0c8b716072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643719ms
Sep  6 20:13:04.384: INFO: Pod "pod-91d85630-0b6e-40d6-9198-2f0c8b716072": Phase="Running", Reason="", readiness=true. Elapsed: 2.005921891s
Sep  6 20:13:06.388: INFO: Pod "pod-91d85630-0b6e-40d6-9198-2f0c8b716072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009914629s
STEP: Saw pod success
Sep  6 20:13:06.388: INFO: Pod "pod-91d85630-0b6e-40d6-9198-2f0c8b716072" satisfied condition "success or failure"
Sep  6 20:13:06.390: INFO: Trying to get logs from node appserv10 pod pod-91d85630-0b6e-40d6-9198-2f0c8b716072 container test-container: <nil>
STEP: delete the pod
Sep  6 20:13:06.410: INFO: Waiting for pod pod-91d85630-0b6e-40d6-9198-2f0c8b716072 to disappear
Sep  6 20:13:06.414: INFO: Pod pod-91d85630-0b6e-40d6-9198-2f0c8b716072 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:13:06.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6644" for this suite.
Sep  6 20:13:12.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:13:12.523: INFO: namespace emptydir-6644 deletion completed in 6.105195927s

• [SLOW TEST:10.287 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:13:12.524: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep  6 20:13:12.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-5486'
Sep  6 20:13:12.929: INFO: stderr: ""
Sep  6 20:13:12.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 20:13:12.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5486'
Sep  6 20:13:13.048: INFO: stderr: ""
Sep  6 20:13:13.048: INFO: stdout: "update-demo-nautilus-4pd29 update-demo-nautilus-bwc9q "
Sep  6 20:13:13.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:13.156: INFO: stderr: ""
Sep  6 20:13:13.156: INFO: stdout: ""
Sep  6 20:13:13.156: INFO: update-demo-nautilus-4pd29 is created but not running
Sep  6 20:13:18.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5486'
Sep  6 20:13:18.277: INFO: stderr: ""
Sep  6 20:13:18.277: INFO: stdout: "update-demo-nautilus-4pd29 update-demo-nautilus-bwc9q "
Sep  6 20:13:18.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:18.387: INFO: stderr: ""
Sep  6 20:13:18.387: INFO: stdout: "true"
Sep  6 20:13:18.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:18.503: INFO: stderr: ""
Sep  6 20:13:18.503: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:13:18.503: INFO: validating pod update-demo-nautilus-4pd29
Sep  6 20:13:18.512: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:13:18.512: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:13:18.512: INFO: update-demo-nautilus-4pd29 is verified up and running
Sep  6 20:13:18.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-bwc9q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:18.631: INFO: stderr: ""
Sep  6 20:13:18.632: INFO: stdout: "true"
Sep  6 20:13:18.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-bwc9q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:18.746: INFO: stderr: ""
Sep  6 20:13:18.746: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:13:18.746: INFO: validating pod update-demo-nautilus-bwc9q
Sep  6 20:13:18.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:13:18.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:13:18.752: INFO: update-demo-nautilus-bwc9q is verified up and running
STEP: scaling down the replication controller
Sep  6 20:13:18.754: INFO: scanned /root for discovery docs: <nil>
Sep  6 20:13:18.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5486'
Sep  6 20:13:19.888: INFO: stderr: ""
Sep  6 20:13:19.888: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 20:13:19.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5486'
Sep  6 20:13:20.002: INFO: stderr: ""
Sep  6 20:13:20.002: INFO: stdout: "update-demo-nautilus-4pd29 update-demo-nautilus-bwc9q "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 20:13:25.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5486'
Sep  6 20:13:25.119: INFO: stderr: ""
Sep  6 20:13:25.119: INFO: stdout: "update-demo-nautilus-4pd29 "
Sep  6 20:13:25.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:25.228: INFO: stderr: ""
Sep  6 20:13:25.228: INFO: stdout: "true"
Sep  6 20:13:25.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:25.322: INFO: stderr: ""
Sep  6 20:13:25.322: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:13:25.322: INFO: validating pod update-demo-nautilus-4pd29
Sep  6 20:13:25.326: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:13:25.326: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:13:25.326: INFO: update-demo-nautilus-4pd29 is verified up and running
STEP: scaling up the replication controller
Sep  6 20:13:25.328: INFO: scanned /root for discovery docs: <nil>
Sep  6 20:13:25.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5486'
Sep  6 20:13:26.450: INFO: stderr: ""
Sep  6 20:13:26.450: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 20:13:26.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5486'
Sep  6 20:13:26.568: INFO: stderr: ""
Sep  6 20:13:26.568: INFO: stdout: "update-demo-nautilus-4pd29 update-demo-nautilus-kh7bm "
Sep  6 20:13:26.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:26.679: INFO: stderr: ""
Sep  6 20:13:26.679: INFO: stdout: "true"
Sep  6 20:13:26.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:26.775: INFO: stderr: ""
Sep  6 20:13:26.775: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:13:26.775: INFO: validating pod update-demo-nautilus-4pd29
Sep  6 20:13:26.778: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:13:26.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:13:26.778: INFO: update-demo-nautilus-4pd29 is verified up and running
Sep  6 20:13:26.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-kh7bm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:26.876: INFO: stderr: ""
Sep  6 20:13:26.876: INFO: stdout: ""
Sep  6 20:13:26.876: INFO: update-demo-nautilus-kh7bm is created but not running
Sep  6 20:13:31.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5486'
Sep  6 20:13:31.990: INFO: stderr: ""
Sep  6 20:13:31.990: INFO: stdout: "update-demo-nautilus-4pd29 update-demo-nautilus-kh7bm "
Sep  6 20:13:31.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:32.086: INFO: stderr: ""
Sep  6 20:13:32.086: INFO: stdout: "true"
Sep  6 20:13:32.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-4pd29 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:32.185: INFO: stderr: ""
Sep  6 20:13:32.185: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:13:32.185: INFO: validating pod update-demo-nautilus-4pd29
Sep  6 20:13:32.188: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:13:32.189: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:13:32.189: INFO: update-demo-nautilus-4pd29 is verified up and running
Sep  6 20:13:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-kh7bm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:32.292: INFO: stderr: ""
Sep  6 20:13:32.292: INFO: stdout: "true"
Sep  6 20:13:32.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-kh7bm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5486'
Sep  6 20:13:32.385: INFO: stderr: ""
Sep  6 20:13:32.385: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:13:32.385: INFO: validating pod update-demo-nautilus-kh7bm
Sep  6 20:13:32.390: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:13:32.390: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:13:32.391: INFO: update-demo-nautilus-kh7bm is verified up and running
STEP: using delete to clean up resources
Sep  6 20:13:32.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-5486'
Sep  6 20:13:32.500: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:13:32.501: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 20:13:32.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5486'
Sep  6 20:13:32.602: INFO: stderr: "No resources found.\n"
Sep  6 20:13:32.602: INFO: stdout: ""
Sep  6 20:13:32.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -l name=update-demo --namespace=kubectl-5486 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 20:13:32.693: INFO: stderr: ""
Sep  6 20:13:32.693: INFO: stdout: "update-demo-nautilus-4pd29\nupdate-demo-nautilus-kh7bm\n"
Sep  6 20:13:33.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5486'
Sep  6 20:13:33.319: INFO: stderr: "No resources found.\n"
Sep  6 20:13:33.319: INFO: stdout: ""
Sep  6 20:13:33.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -l name=update-demo --namespace=kubectl-5486 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 20:13:33.443: INFO: stderr: ""
Sep  6 20:13:33.443: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:13:33.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5486" for this suite.
Sep  6 20:13:55.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:13:55.554: INFO: namespace kubectl-5486 deletion completed in 22.10673175s

• [SLOW TEST:43.030 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:13:55.555: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:13:59.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7955" for this suite.
Sep  6 20:14:05.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:14:05.843: INFO: namespace emptydir-wrapper-7955 deletion completed in 6.103750299s

• [SLOW TEST:10.289 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:14:05.844: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6725
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6725
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6725
Sep  6 20:14:05.986: INFO: Found 0 stateful pods, waiting for 1
Sep  6 20:14:15.990: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  6 20:14:15.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:14:16.239: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:14:16.239: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:14:16.239: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:14:16.242: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 20:14:26.247: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:14:26.247: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:14:26.260: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:26.260: INFO: ss-0  appserv10  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:26.260: INFO: 
Sep  6 20:14:26.260: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  6 20:14:27.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996591967s
Sep  6 20:14:28.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983971003s
Sep  6 20:14:29.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97957613s
Sep  6 20:14:30.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975211716s
Sep  6 20:14:31.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96949045s
Sep  6 20:14:32.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964919525s
Sep  6 20:14:33.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960386685s
Sep  6 20:14:34.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95589542s
Sep  6 20:14:35.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.374848ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6725
Sep  6 20:14:36.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:14:36.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 20:14:36.567: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:14:36.567: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:14:36.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:14:36.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 20:14:36.775: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:14:36.775: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:14:36.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:14:36.995: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 20:14:36.995: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:14:36.995: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:14:36.998: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:14:36.998: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:14:36.998: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  6 20:14:37.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:14:37.224: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:14:37.224: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:14:37.224: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:14:37.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:14:37.450: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:14:37.450: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:14:37.450: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:14:37.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:14:37.676: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:14:37.677: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:14:37.677: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:14:37.677: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:14:37.680: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  6 20:14:47.688: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:14:47.688: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:14:47.688: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:14:47.698: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:47.699: INFO: ss-0  appserv10  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:47.699: INFO: ss-1  appserv9   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:47.699: INFO: ss-2  appserv11  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:47.699: INFO: 
Sep  6 20:14:47.699: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:48.703: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:48.703: INFO: ss-0  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:48.703: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:48.703: INFO: ss-2  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:48.703: INFO: 
Sep  6 20:14:48.703: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:49.708: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:49.708: INFO: ss-0  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:49.708: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:49.708: INFO: ss-2  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:49.708: INFO: 
Sep  6 20:14:49.708: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:50.712: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:50.712: INFO: ss-0  appserv10  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:50.712: INFO: ss-1  appserv9   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:50.712: INFO: ss-2  appserv11  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:50.712: INFO: 
Sep  6 20:14:50.712: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:51.717: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:51.717: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:51.717: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:51.717: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:51.717: INFO: 
Sep  6 20:14:51.717: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:52.721: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:52.721: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:52.721: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:52.721: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:52.722: INFO: 
Sep  6 20:14:52.722: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:53.726: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:53.726: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:53.726: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:53.726: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:53.726: INFO: 
Sep  6 20:14:53.726: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:54.731: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:54.731: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:54.731: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:54.731: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:54.731: INFO: 
Sep  6 20:14:54.731: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:55.735: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:55.735: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:55.735: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:55.735: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:55.735: INFO: 
Sep  6 20:14:55.735: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 20:14:56.739: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Sep  6 20:14:56.739: INFO: ss-0  appserv10  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:06 +0000 UTC  }]
Sep  6 20:14:56.740: INFO: ss-1  appserv9   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:56.740: INFO: ss-2  appserv11  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:14:26 +0000 UTC  }]
Sep  6 20:14:56.740: INFO: 
Sep  6 20:14:56.740: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6725
Sep  6 20:14:57.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:14:57.906: INFO: rc: 1
Sep  6 20:14:57.906: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002a7be00 exit status 1 <nil> <nil> true [0xc0005f7260 0xc0005f7a90 0xc003942000] [0xc0005f7260 0xc0005f7a90 0xc003942000] [0xc0005f7908 0xc0005f7ff0] [0x9d21f0 0x9d21f0] 0xc002c76b40 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Sep  6 20:15:07.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:15:08.039: INFO: rc: 1
Sep  6 20:15:08.039: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00213b920 exit status 1 <nil> <nil> true [0xc0003b2d08 0xc0003b2da0 0xc0003b2e48] [0xc0003b2d08 0xc0003b2da0 0xc0003b2e48] [0xc0003b2d90 0xc0003b2e28] [0x9d21f0 0x9d21f0] 0xc002bc54a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:15:18.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:15:18.156: INFO: rc: 1
Sep  6 20:15:18.156: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ed4480 exit status 1 <nil> <nil> true [0xc00054ef08 0xc00054f0f8 0xc00054f1b0] [0xc00054ef08 0xc00054f0f8 0xc00054f1b0] [0xc00054f020 0xc00054f170] [0x9d21f0 0x9d21f0] 0xc0035352c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:15:28.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:15:28.284: INFO: rc: 1
Sep  6 20:15:28.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cf01b0 exit status 1 <nil> <nil> true [0xc003942008 0xc003942020 0xc003942038] [0xc003942008 0xc003942020 0xc003942038] [0xc003942018 0xc003942030] [0x9d21f0 0x9d21f0] 0xc002c76f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:15:38.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:15:38.410: INFO: rc: 1
Sep  6 20:15:38.410: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cf0510 exit status 1 <nil> <nil> true [0xc003942040 0xc003942058 0xc003942070] [0xc003942040 0xc003942058 0xc003942070] [0xc003942050 0xc003942068] [0x9d21f0 0x9d21f0] 0xc002c77320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:15:48.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:15:48.534: INFO: rc: 1
Sep  6 20:15:48.534: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cf08a0 exit status 1 <nil> <nil> true [0xc003942078 0xc003942090 0xc0039420a8] [0xc003942078 0xc003942090 0xc0039420a8] [0xc003942088 0xc0039420a0] [0x9d21f0 0x9d21f0] 0xc002c77680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:15:58.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:15:58.661: INFO: rc: 1
Sep  6 20:15:58.661: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cf0c30 exit status 1 <nil> <nil> true [0xc0039420b0 0xc0039420c8 0xc0039420e0] [0xc0039420b0 0xc0039420c8 0xc0039420e0] [0xc0039420c0 0xc0039420d8] [0x9d21f0 0x9d21f0] 0xc002c779e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:16:08.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:16:08.788: INFO: rc: 1
Sep  6 20:16:08.788: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002cf0fc0 exit status 1 <nil> <nil> true [0xc0039420e8 0xc003942100 0xc003942118] [0xc0039420e8 0xc003942100 0xc003942118] [0xc0039420f8 0xc003942110] [0x9d21f0 0x9d21f0] 0xc002c77d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:16:18.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:16:18.902: INFO: rc: 1
Sep  6 20:16:18.902: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ed4840 exit status 1 <nil> <nil> true [0xc00054f1e0 0xc00054f280 0xc00054f2f0] [0xc00054f1e0 0xc00054f280 0xc00054f2f0] [0xc00054f258 0xc00054f2b8] [0x9d21f0 0x9d21f0] 0xc003535620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:16:28.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:16:29.025: INFO: rc: 1
Sep  6 20:16:29.025: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ed4bd0 exit status 1 <nil> <nil> true [0xc00054f320 0xc00054f3a0 0xc00054f458] [0xc00054f320 0xc00054f3a0 0xc00054f458] [0xc00054f390 0xc00054f408] [0x9d21f0 0x9d21f0] 0xc003535980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:16:39.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:16:39.149: INFO: rc: 1
Sep  6 20:16:39.149: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0031187b0 exit status 1 <nil> <nil> true [0xc003552000 0xc003552018 0xc003552030] [0xc003552000 0xc003552018 0xc003552030] [0xc003552010 0xc003552028] [0x9d21f0 0x9d21f0] 0xc000d5c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:16:49.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:16:49.276: INFO: rc: 1
Sep  6 20:16:49.276: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a7a330 exit status 1 <nil> <nil> true [0xc0005f7740 0xc0005f7c38 0xc0001064e8] [0xc0005f7740 0xc0005f7c38 0xc0001064e8] [0xc0005f7a90 0xc000106320] [0x9d21f0 0x9d21f0] 0xc002bc4360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:16:59.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:16:59.401: INFO: rc: 1
Sep  6 20:16:59.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d50390 exit status 1 <nil> <nil> true [0xc000010010 0xc0003b23c0 0xc0003b25f0] [0xc000010010 0xc0003b23c0 0xc0003b25f0] [0xc0003b22d8 0xc0003b2530] [0x9d21f0 0x9d21f0] 0xc000d5c2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:17:09.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:17:09.525: INFO: rc: 1
Sep  6 20:17:09.525: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d506f0 exit status 1 <nil> <nil> true [0xc0003b26b0 0xc0003b2730 0xc0003b28e8] [0xc0003b26b0 0xc0003b2730 0xc0003b28e8] [0xc0003b2710 0xc0003b28b0] [0x9d21f0 0x9d21f0] 0xc000d5c660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:17:19.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:17:19.653: INFO: rc: 1
Sep  6 20:17:19.653: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d50a80 exit status 1 <nil> <nil> true [0xc0003b2908 0xc0003b2998 0xc0003b2a90] [0xc0003b2908 0xc0003b2998 0xc0003b2a90] [0xc0003b2980 0xc0003b2a60] [0x9d21f0 0x9d21f0] 0xc000d5ca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:17:29.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:17:29.776: INFO: rc: 1
Sep  6 20:17:29.776: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235c330 exit status 1 <nil> <nil> true [0xc003552000 0xc003552018 0xc003552030] [0xc003552000 0xc003552018 0xc003552030] [0xc003552010 0xc003552028] [0x9d21f0 0x9d21f0] 0xc0035345a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:17:39.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:17:39.900: INFO: rc: 1
Sep  6 20:17:39.900: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002a7a720 exit status 1 <nil> <nil> true [0xc0001065c0 0xc000106f60 0xc000107200] [0xc0001065c0 0xc000106f60 0xc000107200] [0xc000106798 0xc000107148] [0x9d21f0 0x9d21f0] 0xc002bc4720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:17:49.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:17:50.024: INFO: rc: 1
Sep  6 20:17:50.025: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00213a360 exit status 1 <nil> <nil> true [0xc00054e070 0xc00054e3c0 0xc00054e550] [0xc00054e070 0xc00054e3c0 0xc00054e550] [0xc00054e378 0xc00054e480] [0x9d21f0 0x9d21f0] 0xc002c765a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:18:00.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:18:00.145: INFO: rc: 1
Sep  6 20:18:00.145: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235c6c0 exit status 1 <nil> <nil> true [0xc003552040 0xc003552080 0xc003552098] [0xc003552040 0xc003552080 0xc003552098] [0xc003552078 0xc003552090] [0x9d21f0 0x9d21f0] 0xc003534b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:18:10.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:18:10.262: INFO: rc: 1
Sep  6 20:18:10.262: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d50e10 exit status 1 <nil> <nil> true [0xc0003b2ab8 0xc0003b2bf0 0xc0003b2c28] [0xc0003b2ab8 0xc0003b2bf0 0xc0003b2c28] [0xc0003b2b88 0xc0003b2c18] [0x9d21f0 0x9d21f0] 0xc000d5cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:18:20.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:18:20.372: INFO: rc: 1
Sep  6 20:18:20.372: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235ca20 exit status 1 <nil> <nil> true [0xc0035520a0 0xc0035520e8 0xc003552128] [0xc0035520a0 0xc0035520e8 0xc003552128] [0xc0035520d0 0xc003552120] [0x9d21f0 0x9d21f0] 0xc003535080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:18:30.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:18:30.490: INFO: rc: 1
Sep  6 20:18:30.490: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d51200 exit status 1 <nil> <nil> true [0xc0003b2c78 0xc0003b2cd8 0xc0003b2d90] [0xc0003b2c78 0xc0003b2cd8 0xc0003b2d90] [0xc0003b2cb8 0xc0003b2d68] [0x9d21f0 0x9d21f0] 0xc000d5d2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:18:40.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:18:40.585: INFO: rc: 1
Sep  6 20:18:40.585: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d516e0 exit status 1 <nil> <nil> true [0xc0003b2da0 0xc0003b2e48 0xc0003b2e70] [0xc0003b2da0 0xc0003b2e48 0xc0003b2e70] [0xc0003b2e28 0xc0003b2e68] [0x9d21f0 0x9d21f0] 0xc000d5db00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:18:50.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:18:50.713: INFO: rc: 1
Sep  6 20:18:50.713: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235c360 exit status 1 <nil> <nil> true [0xc0005f7260 0xc0005f7a90 0xc003552000] [0xc0005f7260 0xc0005f7a90 0xc003552000] [0xc0005f7908 0xc0005f7ff0] [0x9d21f0 0x9d21f0] 0xc0035345a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:19:00.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:19:00.821: INFO: rc: 1
Sep  6 20:19:00.821: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235c720 exit status 1 <nil> <nil> true [0xc003552008 0xc003552020 0xc003552040] [0xc003552008 0xc003552020 0xc003552040] [0xc003552018 0xc003552030] [0x9d21f0 0x9d21f0] 0xc003534b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:19:10.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:19:10.939: INFO: rc: 1
Sep  6 20:19:10.939: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235cae0 exit status 1 <nil> <nil> true [0xc003552060 0xc003552088 0xc0035520a0] [0xc003552060 0xc003552088 0xc0035520a0] [0xc003552080 0xc003552098] [0x9d21f0 0x9d21f0] 0xc003535080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:19:20.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:19:21.053: INFO: rc: 1
Sep  6 20:19:21.053: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235ce70 exit status 1 <nil> <nil> true [0xc0035520b8 0xc003552108 0xc003552148] [0xc0035520b8 0xc003552108 0xc003552148] [0xc0035520e8 0xc003552128] [0x9d21f0 0x9d21f0] 0xc003535440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:19:31.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:19:31.165: INFO: rc: 1
Sep  6 20:19:31.165: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235d1d0 exit status 1 <nil> <nil> true [0xc003552160 0xc0035521a8 0xc0035521d8] [0xc003552160 0xc0035521a8 0xc0035521d8] [0xc003552190 0xc0035521d0] [0x9d21f0 0x9d21f0] 0xc0035357a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:19:41.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:19:41.279: INFO: rc: 1
Sep  6 20:19:41.280: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235d560 exit status 1 <nil> <nil> true [0xc0035521e0 0xc0035521f8 0xc003552210] [0xc0035521e0 0xc0035521f8 0xc003552210] [0xc0035521f0 0xc003552208] [0x9d21f0 0x9d21f0] 0xc003535b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:19:51.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:19:51.399: INFO: rc: 1
Sep  6 20:19:51.399: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00235d8c0 exit status 1 <nil> <nil> true [0xc003552228 0xc003552268 0xc003552280] [0xc003552228 0xc003552268 0xc003552280] [0xc003552260 0xc003552278] [0x9d21f0 0x9d21f0] 0xc003535e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  6 20:20:01.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-6725 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:20:01.515: INFO: rc: 1
Sep  6 20:20:01.515: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Sep  6 20:20:01.515: INFO: Scaling statefulset ss to 0
Sep  6 20:20:01.525: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 20:20:01.528: INFO: Deleting all statefulset in ns statefulset-6725
Sep  6 20:20:01.531: INFO: Scaling statefulset ss to 0
Sep  6 20:20:01.539: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:20:01.542: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:20:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6725" for this suite.
Sep  6 20:20:07.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:20:07.669: INFO: namespace statefulset-6725 deletion completed in 6.110189846s

• [SLOW TEST:361.825 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:20:07.669: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6835
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:20:07.806: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  6 20:20:09.835: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:20:10.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6835" for this suite.
Sep  6 20:20:16.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:20:16.982: INFO: namespace replication-controller-6835 deletion completed in 6.137193719s

• [SLOW TEST:9.313 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:20:16.983: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  6 20:20:17.115: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 20:20:17.122: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 20:20:17.125: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Sep  6 20:20:17.133: INFO: csi-external-snapshotter-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.133: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Sep  6 20:20:17.134: INFO: prometheus-v1-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.134: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:20:17.134: INFO: collectd-v0.8-md6fj from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (4 container statuses recorded)
Sep  6 20:20:17.134: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:20:17.134: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:20:17.134: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:20:17.134: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:20:17.134: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 19:45:19 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.134: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 20:20:17.134: INFO: csi-diamanti-driver-h8mbm from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.134: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:20:17.134: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:20:17.134: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-2wkwb from heptio-sonobuoy started at 2019-09-06 19:45:21 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.134: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:20:17.134: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 20:20:17.134: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Sep  6 20:20:17.143: INFO: prometheus-v1-1 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:20:17.143: INFO: csi-external-provisioner-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Sep  6 20:20:17.143: INFO: collectd-v0.8-xfwsc from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (4 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:20:17.143: INFO: tiller-deploy-5668df8bc4-6pgjb from kube-system started at 2019-09-06 19:41:52 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container tiller ready: true, restart count 0
Sep  6 20:20:17.143: INFO: sonobuoy-e2e-job-8e772d81d3704c93 from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container e2e ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:20:17.143: INFO: csi-diamanti-driver-wztns from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:20:17.143: INFO: csi-external-attacher-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container csi-attacher ready: true, restart count 0
Sep  6 20:20:17.143: INFO: csi-external-resizer-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container csi-external-resizer ready: true, restart count 0
Sep  6 20:20:17.143: INFO: snapshot-controller-66fb5f8fbd-xjhnl from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Sep  6 20:20:17.143: INFO: metrics-server-v1-5d46b6d959-48m72 from kube-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 20:20:17.143: INFO: alertmanager-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 20:20:17.143: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-plh6j from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 20:20:17.143: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Sep  6 20:20:17.151: INFO: csi-diamanti-driver-fc5gk from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.151: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:20:17.151: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:20:17.151: INFO: collectd-v0.8-f5qhk from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (4 container statuses recorded)
Sep  6 20:20:17.151: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:20:17.151: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:20:17.151: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:20:17.151: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:20:17.151: INFO: helm-chart-687577f867-4r7c2 from kube-system started at 2019-09-06 19:41:53 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.151: INFO: 	Container helm-chart ready: true, restart count 0
Sep  6 20:20:17.151: INFO: prometheus-v1-2 from diamanti-system started at 2019-09-06 19:41:53 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.151: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:20:17.151: INFO: provisioner-7b58589b9d-2zpz4 from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (1 container statuses recorded)
Sep  6 20:20:17.151: INFO: 	Container provisioner ready: true, restart count 0
Sep  6 20:20:17.151: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-kngjc from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:20:17.151: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:20:17.151: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5ef93fac-807d-4afa-be60-7825900e358d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5ef93fac-807d-4afa-be60-7825900e358d off the node appserv10
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5ef93fac-807d-4afa-be60-7825900e358d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:20:25.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7757" for this suite.
Sep  6 20:20:43.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:20:43.342: INFO: namespace sched-pred-7757 deletion completed in 18.125899342s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:26.359 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:20:43.343: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7183
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 20:20:43.483: INFO: Waiting up to 5m0s for pod "pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797" in namespace "emptydir-7183" to be "success or failure"
Sep  6 20:20:43.487: INFO: Pod "pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263031ms
Sep  6 20:20:45.490: INFO: Pod "pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006707278s
Sep  6 20:20:47.494: INFO: Pod "pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010828098s
STEP: Saw pod success
Sep  6 20:20:47.494: INFO: Pod "pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797" satisfied condition "success or failure"
Sep  6 20:20:47.497: INFO: Trying to get logs from node appserv9 pod pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797 container test-container: <nil>
STEP: delete the pod
Sep  6 20:20:47.517: INFO: Waiting for pod pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797 to disappear
Sep  6 20:20:47.520: INFO: Pod pod-b9fa8861-fcb0-4c2d-b93d-c5454ff56797 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:20:47.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7183" for this suite.
Sep  6 20:20:53.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:20:53.634: INFO: namespace emptydir-7183 deletion completed in 6.109701641s

• [SLOW TEST:10.291 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:20:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:21:53.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8534" for this suite.
Sep  6 20:22:15.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:22:15.895: INFO: namespace container-probe-8534 deletion completed in 22.107807337s

• [SLOW TEST:82.261 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:22:15.895: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-68fl6 in namespace proxy-2288
I0906 20:22:16.040024      19 runners.go:180] Created replication controller with name: proxy-service-68fl6, namespace: proxy-2288, replica count: 1
I0906 20:22:17.090447      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 20:22:18.090635      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 20:22:19.090850      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 20:22:20.091072      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 20:22:21.091275      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 20:22:22.091541      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 20:22:23.091735      19 runners.go:180] proxy-service-68fl6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 20:22:23.095: INFO: setup took 7.065580594s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 20:22:23.100: INFO: (0) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 5.671534ms)
Sep  6 20:22:23.100: INFO: (0) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 5.680427ms)
Sep  6 20:22:23.100: INFO: (0) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 5.640685ms)
Sep  6 20:22:23.100: INFO: (0) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 5.722247ms)
Sep  6 20:22:23.101: INFO: (0) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 5.711635ms)
Sep  6 20:22:23.101: INFO: (0) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 5.746107ms)
Sep  6 20:22:23.101: INFO: (0) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 6.012884ms)
Sep  6 20:22:23.101: INFO: (0) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 6.678732ms)
Sep  6 20:22:23.101: INFO: (0) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 6.84299ms)
Sep  6 20:22:23.101: INFO: (0) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 6.787639ms)
Sep  6 20:22:23.108: INFO: (0) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 13.495361ms)
Sep  6 20:22:23.110: INFO: (0) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 15.20206ms)
Sep  6 20:22:23.110: INFO: (0) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 15.555485ms)
Sep  6 20:22:23.111: INFO: (0) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 15.930116ms)
Sep  6 20:22:23.111: INFO: (0) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 16.32643ms)
Sep  6 20:22:23.116: INFO: (0) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 21.432213ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.518788ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.436578ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.622645ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.643585ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.857619ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.808478ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.767692ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.974778ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.917832ms)
Sep  6 20:22:23.121: INFO: (1) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.902741ms)
Sep  6 20:22:23.122: INFO: (1) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.615835ms)
Sep  6 20:22:23.122: INFO: (1) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 6.055707ms)
Sep  6 20:22:23.123: INFO: (1) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 6.166197ms)
Sep  6 20:22:23.123: INFO: (1) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 6.206885ms)
Sep  6 20:22:23.123: INFO: (1) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 6.260006ms)
Sep  6 20:22:23.123: INFO: (1) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 6.227688ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.033666ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.548751ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.476858ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.619788ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.539968ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.571566ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.577679ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.594669ms)
Sep  6 20:22:23.127: INFO: (2) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.66572ms)
Sep  6 20:22:23.128: INFO: (2) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.604112ms)
Sep  6 20:22:23.128: INFO: (2) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 4.906658ms)
Sep  6 20:22:23.129: INFO: (2) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.755262ms)
Sep  6 20:22:23.129: INFO: (2) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.746393ms)
Sep  6 20:22:23.129: INFO: (2) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.916923ms)
Sep  6 20:22:23.130: INFO: (2) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 7.398659ms)
Sep  6 20:22:23.130: INFO: (2) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 7.428527ms)
Sep  6 20:22:23.134: INFO: (3) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.860496ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.211844ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.376932ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.320036ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.238957ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.325569ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.405651ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.298558ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.378413ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.572083ms)
Sep  6 20:22:23.135: INFO: (3) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.81889ms)
Sep  6 20:22:23.136: INFO: (3) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.651403ms)
Sep  6 20:22:23.136: INFO: (3) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.538658ms)
Sep  6 20:22:23.136: INFO: (3) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.531709ms)
Sep  6 20:22:23.136: INFO: (3) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.773919ms)
Sep  6 20:22:23.136: INFO: (3) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.793024ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 9.425219ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 9.331985ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 9.471402ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 9.428772ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 9.372176ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 9.379559ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 9.392089ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 9.415212ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 9.535772ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 9.41469ms)
Sep  6 20:22:23.146: INFO: (4) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 9.815319ms)
Sep  6 20:22:23.147: INFO: (4) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 10.834145ms)
Sep  6 20:22:23.147: INFO: (4) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 10.755509ms)
Sep  6 20:22:23.147: INFO: (4) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 10.842146ms)
Sep  6 20:22:23.147: INFO: (4) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 10.855471ms)
Sep  6 20:22:23.147: INFO: (4) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 10.823394ms)
Sep  6 20:22:23.151: INFO: (5) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.994846ms)
Sep  6 20:22:23.151: INFO: (5) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.018925ms)
Sep  6 20:22:23.151: INFO: (5) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.051119ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.03684ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.113851ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.389843ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.431947ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.348783ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.372368ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.584839ms)
Sep  6 20:22:23.152: INFO: (5) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 4.752253ms)
Sep  6 20:22:23.153: INFO: (5) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.478846ms)
Sep  6 20:22:23.153: INFO: (5) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.377985ms)
Sep  6 20:22:23.153: INFO: (5) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.491451ms)
Sep  6 20:22:23.153: INFO: (5) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.44566ms)
Sep  6 20:22:23.153: INFO: (5) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.430331ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.428953ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.400925ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.436813ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.35865ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.328269ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.342613ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.416644ms)
Sep  6 20:22:23.157: INFO: (6) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.355316ms)
Sep  6 20:22:23.158: INFO: (6) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.438985ms)
Sep  6 20:22:23.158: INFO: (6) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.480699ms)
Sep  6 20:22:23.158: INFO: (6) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 4.774665ms)
Sep  6 20:22:23.158: INFO: (6) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.366906ms)
Sep  6 20:22:23.159: INFO: (6) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.470147ms)
Sep  6 20:22:23.159: INFO: (6) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.433245ms)
Sep  6 20:22:23.159: INFO: (6) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.532083ms)
Sep  6 20:22:23.159: INFO: (6) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.415646ms)
Sep  6 20:22:23.162: INFO: (7) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.036984ms)
Sep  6 20:22:23.162: INFO: (7) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.820071ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.84868ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.191837ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.16107ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.263434ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.193161ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.322605ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.324294ms)
Sep  6 20:22:23.163: INFO: (7) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.246591ms)
Sep  6 20:22:23.164: INFO: (7) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 4.972317ms)
Sep  6 20:22:23.165: INFO: (7) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 6.080613ms)
Sep  6 20:22:23.165: INFO: (7) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 6.067666ms)
Sep  6 20:22:23.165: INFO: (7) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 6.157881ms)
Sep  6 20:22:23.165: INFO: (7) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 6.197354ms)
Sep  6 20:22:23.165: INFO: (7) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 6.180242ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.805078ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.183969ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.18314ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.274031ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.196927ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.189642ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.309562ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.417507ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.380776ms)
Sep  6 20:22:23.169: INFO: (8) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 4.395674ms)
Sep  6 20:22:23.170: INFO: (8) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.794201ms)
Sep  6 20:22:23.171: INFO: (8) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.443669ms)
Sep  6 20:22:23.171: INFO: (8) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.587735ms)
Sep  6 20:22:23.171: INFO: (8) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.764268ms)
Sep  6 20:22:23.171: INFO: (8) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.67158ms)
Sep  6 20:22:23.171: INFO: (8) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.821445ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.945408ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.954844ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.08305ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.069074ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.102096ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.196746ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 4.245149ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 4.176292ms)
Sep  6 20:22:23.175: INFO: (9) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.315043ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 4.94145ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.876311ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.438003ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.469816ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.497419ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.615864ms)
Sep  6 20:22:23.176: INFO: (9) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.475778ms)
Sep  6 20:22:23.180: INFO: (10) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 2.91943ms)
Sep  6 20:22:23.180: INFO: (10) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.246141ms)
Sep  6 20:22:23.180: INFO: (10) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.63457ms)
Sep  6 20:22:23.180: INFO: (10) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.605115ms)
Sep  6 20:22:23.180: INFO: (10) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.689025ms)
Sep  6 20:22:23.181: INFO: (10) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.94508ms)
Sep  6 20:22:23.181: INFO: (10) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 4.05072ms)
Sep  6 20:22:23.181: INFO: (10) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.990094ms)
Sep  6 20:22:23.181: INFO: (10) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.981678ms)
Sep  6 20:22:23.181: INFO: (10) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.93724ms)
Sep  6 20:22:23.182: INFO: (10) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.08663ms)
Sep  6 20:22:23.183: INFO: (10) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.870592ms)
Sep  6 20:22:23.183: INFO: (10) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.928832ms)
Sep  6 20:22:23.183: INFO: (10) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.905435ms)
Sep  6 20:22:23.183: INFO: (10) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.911279ms)
Sep  6 20:22:23.183: INFO: (10) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 6.028486ms)
Sep  6 20:22:23.186: INFO: (11) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.368213ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.864868ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.811548ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.803298ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.794184ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.813168ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.935334ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.836179ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.972108ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.885135ms)
Sep  6 20:22:23.187: INFO: (11) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 4.231994ms)
Sep  6 20:22:23.188: INFO: (11) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.882359ms)
Sep  6 20:22:23.188: INFO: (11) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 4.949494ms)
Sep  6 20:22:23.188: INFO: (11) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.082721ms)
Sep  6 20:22:23.188: INFO: (11) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.086858ms)
Sep  6 20:22:23.188: INFO: (11) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.645805ms)
Sep  6 20:22:23.191: INFO: (12) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 2.989813ms)
Sep  6 20:22:23.191: INFO: (12) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 2.95618ms)
Sep  6 20:22:23.192: INFO: (12) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.596884ms)
Sep  6 20:22:23.192: INFO: (12) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.998469ms)
Sep  6 20:22:23.192: INFO: (12) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.951775ms)
Sep  6 20:22:23.192: INFO: (12) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.917875ms)
Sep  6 20:22:23.192: INFO: (12) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 4.050931ms)
Sep  6 20:22:23.192: INFO: (12) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.92517ms)
Sep  6 20:22:23.193: INFO: (12) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.092503ms)
Sep  6 20:22:23.193: INFO: (12) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.084147ms)
Sep  6 20:22:23.193: INFO: (12) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 4.766987ms)
Sep  6 20:22:23.194: INFO: (12) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.389451ms)
Sep  6 20:22:23.194: INFO: (12) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.583616ms)
Sep  6 20:22:23.194: INFO: (12) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.603721ms)
Sep  6 20:22:23.194: INFO: (12) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.646809ms)
Sep  6 20:22:23.194: INFO: (12) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.66617ms)
Sep  6 20:22:23.197: INFO: (13) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 2.786678ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.419876ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.483517ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.696129ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.750595ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.804447ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.745881ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.966341ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.857905ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.923142ms)
Sep  6 20:22:23.198: INFO: (13) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.261136ms)
Sep  6 20:22:23.199: INFO: (13) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.044645ms)
Sep  6 20:22:23.199: INFO: (13) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.148414ms)
Sep  6 20:22:23.199: INFO: (13) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.177413ms)
Sep  6 20:22:23.199: INFO: (13) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.184316ms)
Sep  6 20:22:23.200: INFO: (13) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.197934ms)
Sep  6 20:22:23.203: INFO: (14) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.834746ms)
Sep  6 20:22:23.203: INFO: (14) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.898782ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.883163ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.974249ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.971699ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.911363ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 4.007038ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.932922ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.944951ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.961155ms)
Sep  6 20:22:23.204: INFO: (14) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.236583ms)
Sep  6 20:22:23.205: INFO: (14) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 4.877539ms)
Sep  6 20:22:23.205: INFO: (14) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 4.916178ms)
Sep  6 20:22:23.205: INFO: (14) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.075946ms)
Sep  6 20:22:23.205: INFO: (14) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 4.881365ms)
Sep  6 20:22:23.205: INFO: (14) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.104364ms)
Sep  6 20:22:23.208: INFO: (15) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 2.813873ms)
Sep  6 20:22:23.208: INFO: (15) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.496125ms)
Sep  6 20:22:23.208: INFO: (15) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.477739ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.819996ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.82127ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.937287ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.925955ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.805651ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.920634ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.941085ms)
Sep  6 20:22:23.209: INFO: (15) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 4.627439ms)
Sep  6 20:22:23.210: INFO: (15) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.417506ms)
Sep  6 20:22:23.210: INFO: (15) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.359277ms)
Sep  6 20:22:23.210: INFO: (15) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 5.330277ms)
Sep  6 20:22:23.210: INFO: (15) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.371274ms)
Sep  6 20:22:23.210: INFO: (15) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.411128ms)
Sep  6 20:22:23.213: INFO: (16) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.097365ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.604654ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.529834ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.679561ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.587236ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.556709ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.761986ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.701852ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.753494ms)
Sep  6 20:22:23.214: INFO: (16) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.705912ms)
Sep  6 20:22:23.215: INFO: (16) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.450332ms)
Sep  6 20:22:23.216: INFO: (16) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 5.163111ms)
Sep  6 20:22:23.216: INFO: (16) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 5.520257ms)
Sep  6 20:22:23.216: INFO: (16) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 5.514198ms)
Sep  6 20:22:23.216: INFO: (16) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 5.550126ms)
Sep  6 20:22:23.216: INFO: (16) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.647513ms)
Sep  6 20:22:23.219: INFO: (17) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.485571ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.456428ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.87068ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.812077ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.857463ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.912292ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 4.001191ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.929671ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 4.01219ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.965784ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.241672ms)
Sep  6 20:22:23.220: INFO: (17) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 4.236707ms)
Sep  6 20:22:23.221: INFO: (17) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 4.762952ms)
Sep  6 20:22:23.221: INFO: (17) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 4.808263ms)
Sep  6 20:22:23.221: INFO: (17) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 4.785319ms)
Sep  6 20:22:23.222: INFO: (17) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 5.503328ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.211573ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.51583ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.55452ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.702412ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.583892ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.63841ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.644118ms)
Sep  6 20:22:23.225: INFO: (18) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.640834ms)
Sep  6 20:22:23.226: INFO: (18) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.839533ms)
Sep  6 20:22:23.226: INFO: (18) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.84687ms)
Sep  6 20:22:23.226: INFO: (18) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 4.572353ms)
Sep  6 20:22:23.226: INFO: (18) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 4.665456ms)
Sep  6 20:22:23.227: INFO: (18) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 4.866836ms)
Sep  6 20:22:23.226: INFO: (18) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 4.854048ms)
Sep  6 20:22:23.226: INFO: (18) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 4.676825ms)
Sep  6 20:22:23.227: INFO: (18) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 4.817136ms)
Sep  6 20:22:23.230: INFO: (19) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:443/proxy/tlsrewritem... (200; 3.586022ms)
Sep  6 20:22:23.230: INFO: (19) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.609314ms)
Sep  6 20:22:23.230: INFO: (19) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">... (200; 3.525958ms)
Sep  6 20:22:23.230: INFO: (19) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.623822ms)
Sep  6 20:22:23.230: INFO: (19) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:162/proxy/: bar (200; 3.696774ms)
Sep  6 20:22:23.230: INFO: (19) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname1/proxy/: foo (200; 3.892323ms)
Sep  6 20:22:23.231: INFO: (19) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s:1080/proxy/rewriteme">test<... (200; 3.781837ms)
Sep  6 20:22:23.231: INFO: (19) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:460/proxy/: tls baz (200; 3.784822ms)
Sep  6 20:22:23.231: INFO: (19) /api/v1/namespaces/proxy-2288/pods/http:proxy-service-68fl6-6wg4s:160/proxy/: foo (200; 3.845349ms)
Sep  6 20:22:23.231: INFO: (19) /api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/: <a href="/api/v1/namespaces/proxy-2288/pods/proxy-service-68fl6-6wg4s/proxy/rewriteme">test</a> (200; 3.907645ms)
Sep  6 20:22:23.231: INFO: (19) /api/v1/namespaces/proxy-2288/pods/https:proxy-service-68fl6-6wg4s:462/proxy/: tls qux (200; 3.920555ms)
Sep  6 20:22:23.236: INFO: (19) /api/v1/namespaces/proxy-2288/services/proxy-service-68fl6:portname2/proxy/: bar (200; 8.954687ms)
Sep  6 20:22:23.237: INFO: (19) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname1/proxy/: tls baz (200; 9.943851ms)
Sep  6 20:22:23.237: INFO: (19) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname2/proxy/: bar (200; 9.909056ms)
Sep  6 20:22:23.237: INFO: (19) /api/v1/namespaces/proxy-2288/services/https:proxy-service-68fl6:tlsportname2/proxy/: tls qux (200; 9.893939ms)
Sep  6 20:22:23.237: INFO: (19) /api/v1/namespaces/proxy-2288/services/http:proxy-service-68fl6:portname1/proxy/: foo (200; 10.001232ms)
STEP: deleting ReplicationController proxy-service-68fl6 in namespace proxy-2288, will wait for the garbage collector to delete the pods
Sep  6 20:22:23.296: INFO: Deleting ReplicationController proxy-service-68fl6 took: 6.197452ms
Sep  6 20:22:23.796: INFO: Terminating ReplicationController proxy-service-68fl6 pods took: 500.257057ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:22:32.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2288" for this suite.
Sep  6 20:22:38.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:22:38.810: INFO: namespace proxy-2288 deletion completed in 6.109415679s

• [SLOW TEST:22.914 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:22:38.810: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-ae469ee6-8e7d-4cbb-9d0c-b9a196aa3a37
STEP: Creating a pod to test consume secrets
Sep  6 20:22:38.955: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40" in namespace "projected-7964" to be "success or failure"
Sep  6 20:22:38.958: INFO: Pod "pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648296ms
Sep  6 20:22:40.962: INFO: Pod "pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005973914s
STEP: Saw pod success
Sep  6 20:22:40.962: INFO: Pod "pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40" satisfied condition "success or failure"
Sep  6 20:22:40.964: INFO: Trying to get logs from node appserv10 pod pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 20:22:40.982: INFO: Waiting for pod pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40 to disappear
Sep  6 20:22:40.985: INFO: Pod pod-projected-secrets-6c85d612-7ca8-487e-a95f-1b195f6b6d40 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:22:40.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7964" for this suite.
Sep  6 20:22:46.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:22:47.088: INFO: namespace projected-7964 deletion completed in 6.099731391s

• [SLOW TEST:8.279 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:22:47.089: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:22:47.235: INFO: (0) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 10.553369ms)
Sep  6 20:22:47.241: INFO: (1) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 5.920485ms)
Sep  6 20:22:47.245: INFO: (2) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.784689ms)
Sep  6 20:22:47.249: INFO: (3) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 4.24167ms)
Sep  6 20:22:47.253: INFO: (4) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.560788ms)
Sep  6 20:22:47.259: INFO: (5) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 6.716098ms)
Sep  6 20:22:47.263: INFO: (6) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.668045ms)
Sep  6 20:22:47.267: INFO: (7) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.557321ms)
Sep  6 20:22:47.270: INFO: (8) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.264001ms)
Sep  6 20:22:47.274: INFO: (9) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.610712ms)
Sep  6 20:22:47.278: INFO: (10) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.803824ms)
Sep  6 20:22:47.281: INFO: (11) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.725006ms)
Sep  6 20:22:47.285: INFO: (12) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.686111ms)
Sep  6 20:22:47.289: INFO: (13) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.803436ms)
Sep  6 20:22:47.293: INFO: (14) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.877299ms)
Sep  6 20:22:47.297: INFO: (15) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.959011ms)
Sep  6 20:22:47.301: INFO: (16) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.893646ms)
Sep  6 20:22:47.304: INFO: (17) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.705185ms)
Sep  6 20:22:47.308: INFO: (18) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.983798ms)
Sep  6 20:22:47.312: INFO: (19) /api/v1/nodes/appserv10:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.701231ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:22:47.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7449" for this suite.
Sep  6 20:22:53.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:22:53.429: INFO: namespace proxy-7449 deletion completed in 6.112476367s

• [SLOW TEST:6.340 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:22:53.429: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1387
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1387
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 20:22:53.565: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 20:23:15.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.11&port=8080&tries=1'] Namespace:pod-network-test-1387 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:23:15.638: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:23:15.769: INFO: Waiting for endpoints: map[]
Sep  6 20:23:15.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.12&port=8080&tries=1'] Namespace:pod-network-test-1387 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:23:15.771: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:23:15.886: INFO: Waiting for endpoints: map[]
Sep  6 20:23:15.889: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.14:8080/dial?request=hostName&protocol=http&host=172.16.141.13&port=8080&tries=1'] Namespace:pod-network-test-1387 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 20:23:15.889: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 20:23:16.180: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:23:16.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1387" for this suite.
Sep  6 20:23:38.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:23:38.296: INFO: namespace pod-network-test-1387 deletion completed in 22.111634511s

• [SLOW TEST:44.867 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:23:38.296: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  6 20:23:38.451: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5435,SelfLink:/api/v1/namespaces/watch-5435/configmaps/e2e-watch-test-label-changed,UID:41fd0713-3879-4a10-a085-ec373ee93e7f,ResourceVersion:9962,Generation:0,CreationTimestamp:2019-09-06 20:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 20:23:38.451: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5435,SelfLink:/api/v1/namespaces/watch-5435/configmaps/e2e-watch-test-label-changed,UID:41fd0713-3879-4a10-a085-ec373ee93e7f,ResourceVersion:9963,Generation:0,CreationTimestamp:2019-09-06 20:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 20:23:38.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5435,SelfLink:/api/v1/namespaces/watch-5435/configmaps/e2e-watch-test-label-changed,UID:41fd0713-3879-4a10-a085-ec373ee93e7f,ResourceVersion:9964,Generation:0,CreationTimestamp:2019-09-06 20:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  6 20:23:48.478: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5435,SelfLink:/api/v1/namespaces/watch-5435/configmaps/e2e-watch-test-label-changed,UID:41fd0713-3879-4a10-a085-ec373ee93e7f,ResourceVersion:9984,Generation:0,CreationTimestamp:2019-09-06 20:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 20:23:48.478: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5435,SelfLink:/api/v1/namespaces/watch-5435/configmaps/e2e-watch-test-label-changed,UID:41fd0713-3879-4a10-a085-ec373ee93e7f,ResourceVersion:9985,Generation:0,CreationTimestamp:2019-09-06 20:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep  6 20:23:48.479: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5435,SelfLink:/api/v1/namespaces/watch-5435/configmaps/e2e-watch-test-label-changed,UID:41fd0713-3879-4a10-a085-ec373ee93e7f,ResourceVersion:9986,Generation:0,CreationTimestamp:2019-09-06 20:23:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:23:48.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5435" for this suite.
Sep  6 20:23:54.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:23:54.588: INFO: namespace watch-5435 deletion completed in 6.105259587s

• [SLOW TEST:16.292 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:23:54.588: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep  6 20:23:56.742: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-256622946 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep  6 20:24:16.865: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:24:16.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7672" for this suite.
Sep  6 20:24:22.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:24:22.982: INFO: namespace pods-7672 deletion completed in 6.108960291s

• [SLOW TEST:28.394 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:24:22.982: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:24:23.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-1252'
Sep  6 20:24:23.407: INFO: stderr: ""
Sep  6 20:24:23.407: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep  6 20:24:23.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-1252'
Sep  6 20:24:23.642: INFO: stderr: ""
Sep  6 20:24:23.642: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 20:24:24.646: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:24:24.646: INFO: Found 0 / 1
Sep  6 20:24:25.646: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:24:25.646: INFO: Found 1 / 1
Sep  6 20:24:25.646: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 20:24:25.649: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:24:25.649: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 20:24:25.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 describe pod redis-master-h2vbl --namespace=kubectl-1252'
Sep  6 20:24:25.790: INFO: stderr: ""
Sep  6 20:24:25.790: INFO: stdout: "Name:           redis-master-h2vbl\nNamespace:      kubectl-1252\nPriority:       0\nNode:           appserv10/172.16.6.110\nStart Time:     Fri, 06 Sep 2019 20:24:23 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    kubernetes.io/psp: collecting\nStatus:         Running\nIP:             172.16.141.11\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://a24a05a6b6fce760f45095c29d1fb67b6f9e0c54bb6b68521839a92618141d4b\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 06 Sep 2019 20:24:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8k5qr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8k5qr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8k5qr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                Message\n  ----    ------     ----  ----                -------\n  Normal  Scheduled  2s    default-scheduler   Successfully assigned kubectl-1252/redis-master-h2vbl to appserv10\n  Normal  Pulled     1s    kubelet, appserv10  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, appserv10  Created container redis-master\n  Normal  Started    1s    kubelet, appserv10  Started container redis-master\n"
Sep  6 20:24:25.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 describe rc redis-master --namespace=kubectl-1252'
Sep  6 20:24:25.931: INFO: stderr: ""
Sep  6 20:24:25.931: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1252\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-h2vbl\n"
Sep  6 20:24:25.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 describe service redis-master --namespace=kubectl-1252'
Sep  6 20:24:26.068: INFO: stderr: ""
Sep  6 20:24:26.068: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1252\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.0.0.80\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.141.11:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  6 20:24:26.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 describe node appserv10'
Sep  6 20:24:26.213: INFO: stderr: ""
Sep  6 20:24:26.213: INFO: stdout: "Name:               appserv10\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=appserv10\n                    kubernetes.io/os=linux\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"dcx.csi.diamanti.com\":\"appserv10\"}\n                    node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Fri, 06 Sep 2019 19:41:38 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 06 Sep 2019 20:23:50 +0000   Fri, 06 Sep 2019 19:41:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 06 Sep 2019 20:23:50 +0000   Fri, 06 Sep 2019 19:41:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 06 Sep 2019 20:23:50 +0000   Fri, 06 Sep 2019 19:41:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 06 Sep 2019 20:23:50 +0000   Fri, 06 Sep 2019 19:41:48 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.16.6.110\n  Hostname:    appserv10\nCapacity:\n cpu:                32\n ephemeral-storage:  65504Mi\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65698416Ki\n pods:               110\nAllocatable:\n cpu:                32\n ephemeral-storage:  61817329972\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65596016Ki\n pods:               110\nSystem Info:\n Machine ID:                 7b92f3dcc670423e9daa144ee7f0a6e3\n System UUID:                8D3B0B1C-7BF9-1000-AE0C-54AB3A29191F\n Boot ID:                    8d22817c-1215-4f09-adb8-643cfdeccea3\n Kernel Version:             3.10.0-693.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  diamanti-system            collectd-v0.8-md6fj                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  diamanti-system            csi-diamanti-driver-h8mbm                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  diamanti-system            csi-external-snapshotter-0                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         42m\n  diamanti-system            prometheus-v1-0                                            0 (0%)        0 (0%)      1Gi (1%)         1Gi (1%)       42m\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-2wkwb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  kubectl-1252               redis-master-h2vbl                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             1Gi (1%)  1Gi (1%)\n  ephemeral-storage  0 (0%)    0 (0%)\nEvents:\n  Type    Reason                   Age   From                   Message\n  ----    ------                   ----  ----                   -------\n  Normal  Starting                 43m   kube-proxy, appserv10  Starting kube-proxy.\n  Normal  Starting                 42m   kubelet, appserv10     Starting kubelet.\n  Normal  NodeHasSufficientMemory  42m   kubelet, appserv10     Node appserv10 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    42m   kubelet, appserv10     Node appserv10 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     42m   kubelet, appserv10     Node appserv10 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  42m   kubelet, appserv10     Updated Node Allocatable limit across pods\n  Normal  NodeReady                42m   kubelet, appserv10     Node appserv10 status is now: NodeReady\n"
Sep  6 20:24:26.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 describe namespace kubectl-1252'
Sep  6 20:24:26.321: INFO: stderr: ""
Sep  6 20:24:26.321: INFO: stdout: "Name:         kubectl-1252\nLabels:       e2e-framework=kubectl\n              e2e-run=735a8c57-7bd0-485a-b511-afad8d5fc6c9\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:24:26.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1252" for this suite.
Sep  6 20:24:48.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:24:48.436: INFO: namespace kubectl-1252 deletion completed in 22.111261752s

• [SLOW TEST:25.454 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:24:48.437: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  6 20:24:50.592: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-66ba3fd8-c3d7-4354-b93f-bb50c7acb15b,GenerateName:,Namespace:events-3091,SelfLink:/api/v1/namespaces/events-3091/pods/send-events-66ba3fd8-c3d7-4354-b93f-bb50c7acb15b,UID:07f50286-a253-4af1-ac63-d29edfd6a4ca,ResourceVersion:10207,Generation:0,CreationTimestamp:2019-09-06 20:24:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 573283909,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-prgh4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-prgh4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-prgh4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003674de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003674e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:24:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:24:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:24:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:24:48 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.11,StartTime:2019-09-06 20:24:48 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-06 20:24:49 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://77dc64f8642e162e98fcc830e05ceb3a628c0b419aec45596fca5a32396d786c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep  6 20:24:52.597: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  6 20:24:54.600: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:24:54.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3091" for this suite.
Sep  6 20:25:34.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:25:34.715: INFO: namespace events-3091 deletion completed in 40.105205191s

• [SLOW TEST:46.279 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:25:34.716: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:25:34.857: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97" in namespace "downward-api-3064" to be "success or failure"
Sep  6 20:25:34.860: INFO: Pod "downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.694245ms
Sep  6 20:25:36.864: INFO: Pod "downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007112129s
STEP: Saw pod success
Sep  6 20:25:36.864: INFO: Pod "downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97" satisfied condition "success or failure"
Sep  6 20:25:36.867: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97 container client-container: <nil>
STEP: delete the pod
Sep  6 20:25:36.886: INFO: Waiting for pod downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97 to disappear
Sep  6 20:25:36.888: INFO: Pod downwardapi-volume-d631ec62-7fb1-43d9-93d8-bffbe26e6d97 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:25:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3064" for this suite.
Sep  6 20:25:42.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:25:42.992: INFO: namespace downward-api-3064 deletion completed in 6.099870797s

• [SLOW TEST:8.276 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:25:42.992: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-8a538373-38b8-4b88-81c8-5e412b126066 in namespace container-probe-6423
Sep  6 20:25:45.154: INFO: Started pod liveness-8a538373-38b8-4b88-81c8-5e412b126066 in namespace container-probe-6423
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 20:25:45.157: INFO: Initial restart count of pod liveness-8a538373-38b8-4b88-81c8-5e412b126066 is 0
Sep  6 20:26:05.203: INFO: Restart count of pod container-probe-6423/liveness-8a538373-38b8-4b88-81c8-5e412b126066 is now 1 (20.045960679s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:26:05.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6423" for this suite.
Sep  6 20:26:11.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:26:11.338: INFO: namespace container-probe-6423 deletion completed in 6.12116563s

• [SLOW TEST:28.346 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:26:11.338: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-14b33ecb-8b8a-4581-ab9a-ba99ad8ef7dc
STEP: Creating a pod to test consume secrets
Sep  6 20:26:11.483: INFO: Waiting up to 5m0s for pod "pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754" in namespace "secrets-8784" to be "success or failure"
Sep  6 20:26:11.486: INFO: Pod "pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682032ms
Sep  6 20:26:13.490: INFO: Pod "pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006233883s
STEP: Saw pod success
Sep  6 20:26:13.490: INFO: Pod "pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754" satisfied condition "success or failure"
Sep  6 20:26:13.493: INFO: Trying to get logs from node appserv10 pod pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 20:26:13.512: INFO: Waiting for pod pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754 to disappear
Sep  6 20:26:13.515: INFO: Pod pod-secrets-79e56aa8-753e-4b2f-b70c-c440addb6754 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:26:13.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8784" for this suite.
Sep  6 20:26:19.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:26:19.631: INFO: namespace secrets-8784 deletion completed in 6.110861682s

• [SLOW TEST:8.293 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:26:19.632: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 20:26:19.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-7613'
Sep  6 20:26:19.911: INFO: stderr: ""
Sep  6 20:26:19.911: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep  6 20:26:24.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pod e2e-test-nginx-pod --namespace=kubectl-7613 -o json'
Sep  6 20:26:25.069: INFO: stderr: ""
Sep  6 20:26:25.070: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"collecting\"\n        },\n        \"creationTimestamp\": \"2019-09-06T20:26:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-7613\",\n        \"resourceVersion\": \"10519\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7613/pods/e2e-test-nginx-pod\",\n        \"uid\": \"a09bc046-939d-4fb0-af53-c3ba29f34436\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-f2h5p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"appserv9\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-f2h5p\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-f2h5p\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T20:26:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T20:26:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T20:26:21Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T20:26:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c52ca09147b36b542375f9ba6029936a0a49387582e32b4a63427b2ce39a2f59\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-06T20:26:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.6.109\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.141.11\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-06T20:26:19Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  6 20:26:25.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 replace -f - --namespace=kubectl-7613'
Sep  6 20:26:25.312: INFO: stderr: ""
Sep  6 20:26:25.312: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Sep  6 20:26:25.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete pods e2e-test-nginx-pod --namespace=kubectl-7613'
Sep  6 20:26:29.091: INFO: stderr: ""
Sep  6 20:26:29.091: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:26:29.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7613" for this suite.
Sep  6 20:26:35.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:26:35.211: INFO: namespace kubectl-7613 deletion completed in 6.116483385s

• [SLOW TEST:15.579 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:26:35.212: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:26:35.364: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e" in namespace "projected-6467" to be "success or failure"
Sep  6 20:26:35.367: INFO: Pod "downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305459ms
Sep  6 20:26:37.371: INFO: Pod "downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006256041s
Sep  6 20:26:39.374: INFO: Pod "downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009977286s
STEP: Saw pod success
Sep  6 20:26:39.374: INFO: Pod "downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e" satisfied condition "success or failure"
Sep  6 20:26:39.377: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e container client-container: <nil>
STEP: delete the pod
Sep  6 20:26:39.396: INFO: Waiting for pod downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e to disappear
Sep  6 20:26:39.399: INFO: Pod downwardapi-volume-55246871-63bf-4311-bb0d-64531debc94e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:26:39.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6467" for this suite.
Sep  6 20:26:45.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:26:45.495: INFO: namespace projected-6467 deletion completed in 6.092472444s

• [SLOW TEST:10.283 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:26:45.495: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:26:45.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d" in namespace "projected-178" to be "success or failure"
Sep  6 20:26:45.635: INFO: Pod "downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584515ms
Sep  6 20:26:47.640: INFO: Pod "downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006961425s
STEP: Saw pod success
Sep  6 20:26:47.640: INFO: Pod "downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d" satisfied condition "success or failure"
Sep  6 20:26:47.643: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d container client-container: <nil>
STEP: delete the pod
Sep  6 20:26:47.664: INFO: Waiting for pod downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d to disappear
Sep  6 20:26:47.666: INFO: Pod downwardapi-volume-91366107-285d-46f7-92d8-49ff515cba9d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:26:47.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-178" for this suite.
Sep  6 20:26:53.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:26:53.779: INFO: namespace projected-178 deletion completed in 6.107749649s

• [SLOW TEST:8.284 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:26:53.779: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 20:26:56.938: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:26:56.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8002" for this suite.
Sep  6 20:27:02.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:27:03.064: INFO: namespace container-runtime-8002 deletion completed in 6.110533662s

• [SLOW TEST:9.285 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:27:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:27:03.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6" in namespace "downward-api-1858" to be "success or failure"
Sep  6 20:27:03.212: INFO: Pod "downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.828904ms
Sep  6 20:27:05.216: INFO: Pod "downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006390071s
STEP: Saw pod success
Sep  6 20:27:05.216: INFO: Pod "downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6" satisfied condition "success or failure"
Sep  6 20:27:05.219: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6 container client-container: <nil>
STEP: delete the pod
Sep  6 20:27:05.236: INFO: Waiting for pod downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6 to disappear
Sep  6 20:27:05.238: INFO: Pod downwardapi-volume-fc51cdad-64d1-4504-9202-fb10b6bb56b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:27:05.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1858" for this suite.
Sep  6 20:27:11.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:27:11.345: INFO: namespace downward-api-1858 deletion completed in 6.102759554s

• [SLOW TEST:8.281 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:27:11.345: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:27:13.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4018" for this suite.
Sep  6 20:27:55.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:27:55.617: INFO: namespace kubelet-test-4018 deletion completed in 42.104978797s

• [SLOW TEST:44.271 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:27:55.618: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-a3e1e677-3c54-4504-bfdf-3e8300c61083
STEP: Creating a pod to test consume secrets
Sep  6 20:27:55.762: INFO: Waiting up to 5m0s for pod "pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158" in namespace "secrets-9878" to be "success or failure"
Sep  6 20:27:55.764: INFO: Pod "pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.509702ms
Sep  6 20:27:57.768: INFO: Pod "pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006260976s
STEP: Saw pod success
Sep  6 20:27:57.768: INFO: Pod "pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158" satisfied condition "success or failure"
Sep  6 20:27:57.771: INFO: Trying to get logs from node appserv9 pod pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 20:27:57.788: INFO: Waiting for pod pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158 to disappear
Sep  6 20:27:57.790: INFO: Pod pod-secrets-9609d6dc-2683-49df-adad-d8e66be0c158 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:27:57.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9878" for this suite.
Sep  6 20:28:03.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:28:03.904: INFO: namespace secrets-9878 deletion completed in 6.109934467s

• [SLOW TEST:8.286 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:28:03.904: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0906 20:28:44.071385      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 20:28:44.071: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:28:44.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4875" for this suite.
Sep  6 20:28:50.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:28:50.189: INFO: namespace gc-4875 deletion completed in 6.113623886s

• [SLOW TEST:46.284 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:28:50.189: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-13778ece-8745-49b2-96e8-961e13401687
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:28:50.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2249" for this suite.
Sep  6 20:28:56.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:28:56.429: INFO: namespace configmap-2249 deletion completed in 6.098144816s

• [SLOW TEST:6.240 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:28:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:28:56.562: INFO: Creating deployment "nginx-deployment"
Sep  6 20:28:56.566: INFO: Waiting for observed generation 1
Sep  6 20:28:58.572: INFO: Waiting for all required pods to come up
Sep  6 20:28:58.578: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 20:29:00.585: INFO: Waiting for deployment "nginx-deployment" to complete
Sep  6 20:29:00.591: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep  6 20:29:00.597: INFO: Updating deployment nginx-deployment
Sep  6 20:29:00.597: INFO: Waiting for observed generation 2
Sep  6 20:29:02.605: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 20:29:02.607: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 20:29:02.611: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 20:29:02.620: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 20:29:02.620: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 20:29:02.622: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 20:29:02.627: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep  6 20:29:02.627: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep  6 20:29:02.634: INFO: Updating deployment nginx-deployment
Sep  6 20:29:02.634: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep  6 20:29:02.639: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 20:29:02.641: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 20:29:02.649: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-5983,SelfLink:/apis/apps/v1/namespaces/deployment-5983/deployments/nginx-deployment,UID:8906a913-88ff-4e89-95bb-ffe6c779478b,ResourceVersion:11554,Generation:3,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-06 20:29:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-09-06 20:29:02 +0000 UTC 2019-09-06 20:29:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 20:29:02.653: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-5983,SelfLink:/apis/apps/v1/namespaces/deployment-5983/replicasets/nginx-deployment-55fb7cb77f,UID:b5e32b2f-160f-46a0-985d-1d7d3754a85f,ResourceVersion:11552,Generation:3,CreationTimestamp:2019-09-06 20:29:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 8906a913-88ff-4e89-95bb-ffe6c779478b 0xc003333cb7 0xc003333cb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 20:29:02.653: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep  6 20:29:02.653: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-5983,SelfLink:/apis/apps/v1/namespaces/deployment-5983/replicasets/nginx-deployment-7b8c6f4498,UID:7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3,ResourceVersion:11550,Generation:3,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 8906a913-88ff-4e89-95bb-ffe6c779478b 0xc003333d87 0xc003333d88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep  6 20:29:02.660: INFO: Pod "nginx-deployment-55fb7cb77f-4lf2l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4lf2l,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-4lf2l,UID:79d2f5c9-d8d2-4ef6-876a-e92f570d1c44,ResourceVersion:11561,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121327 0xc002121328}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021213d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021213f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.660: INFO: Pod "nginx-deployment-55fb7cb77f-4tzdl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4tzdl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-4tzdl,UID:b7aebd74-d797-488c-876a-ac02c11c79e5,ResourceVersion:11569,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121537 0xc002121538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021215a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021215c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.660: INFO: Pod "nginx-deployment-55fb7cb77f-4v72w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4v72w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-4v72w,UID:ba615c27-c82e-4ed5-b792-d32135488e2e,ResourceVersion:11560,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121637 0xc002121638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021216a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021216d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.660: INFO: Pod "nginx-deployment-55fb7cb77f-9jt5m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9jt5m,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-9jt5m,UID:a8c75f79-8095-4dd3-9a6e-0de3bad2b4f9,ResourceVersion:11571,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121747 0xc002121748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021217b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021217d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.660: INFO: Pod "nginx-deployment-55fb7cb77f-d7fn9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-d7fn9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-d7fn9,UID:2482b0f1-8b2b-41bf-88df-bcb0103d8ab7,ResourceVersion:11532,Generation:0,CreationTimestamp:2019-09-06 20:29:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121847 0xc002121848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021218c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021218f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:,StartTime:2019-09-06 20:29:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.660: INFO: Pod "nginx-deployment-55fb7cb77f-fssxs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-fssxs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-fssxs,UID:e77fb8ce-3183-4cae-9472-d9544eaf1c84,ResourceVersion:11556,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc0021219d0 0xc0021219d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002121a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002121a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-55fb7cb77f-gxd9q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gxd9q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-gxd9q,UID:20adfb0a-6124-4773-b595-6900583989f3,ResourceVersion:11515,Generation:0,CreationTimestamp:2019-09-06 20:29:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121ad7 0xc002121ad8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002121b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002121b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:,StartTime:2019-09-06 20:29:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-55fb7cb77f-jgll8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jgll8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-jgll8,UID:bf2b2088-a700-475c-9de6-f7ce46e6d041,ResourceVersion:11570,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121c50 0xc002121c51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002121cc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002121ce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-55fb7cb77f-l88hh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l88hh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-l88hh,UID:49d6020a-7b58-4698-8423-6b0123e29106,ResourceVersion:11525,Generation:0,CreationTimestamp:2019-09-06 20:29:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121d57 0xc002121d58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002121dd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002121df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:,StartTime:2019-09-06 20:29:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-55fb7cb77f-lx96q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lx96q,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-lx96q,UID:213563f6-d4cf-4707-8bd9-a02cd1e8a768,ResourceVersion:11568,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121ee0 0xc002121ee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002121f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002121f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-55fb7cb77f-mvnjc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mvnjc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-mvnjc,UID:f0302249-e7aa-45af-bad7-ba288ab74715,ResourceVersion:11520,Generation:0,CreationTimestamp:2019-09-06 20:29:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002121ff7 0xc002121ff8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029240d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029240f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:,StartTime:2019-09-06 20:29:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-55fb7cb77f-ns8xq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ns8xq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-55fb7cb77f-ns8xq,UID:c4115d65-ecb0-4463-bcf1-6cf7ace24062,ResourceVersion:11537,Generation:0,CreationTimestamp:2019-09-06 20:29:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b5e32b2f-160f-46a0-985d-1d7d3754a85f 0xc002924310 0xc002924311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029243f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002924490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:29:00 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:,StartTime:2019-09-06 20:29:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.661: INFO: Pod "nginx-deployment-7b8c6f4498-2xcdp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2xcdp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-2xcdp,UID:d867cfaa-30ca-4633-94e6-337964c758e2,ResourceVersion:11553,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002924660 0xc002924661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002924760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002924780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.662: INFO: Pod "nginx-deployment-7b8c6f4498-5l5lv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5l5lv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-5l5lv,UID:7aa36312-920f-4b51-91d0-5714021403d5,ResourceVersion:11434,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002924877 0xc002924878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002924940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002924960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.17,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ed168eaebf24780eb5f8e19d018c992bce17159d45347d538987d281fa46a930}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.662: INFO: Pod "nginx-deployment-7b8c6f4498-847fg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-847fg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-847fg,UID:ddc98234-a982-4871-99f9-ee77ba2934a6,ResourceVersion:11419,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002924b87 0xc002924b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002924ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002924d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.16,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://699b6744dadf9d74d672b9578b7e350be0ec1414953c3f5f0df463af3e12bc13}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.662: INFO: Pod "nginx-deployment-7b8c6f4498-8r248" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8r248,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-8r248,UID:2bdb27e2-688c-4b69-9c88-e2b1138cfdd9,ResourceVersion:11558,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002924de7 0xc002924de8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002924e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002924e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.662: INFO: Pod "nginx-deployment-7b8c6f4498-9tlnw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9tlnw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-9tlnw,UID:d9323073-3faf-448d-8f79-79286020f968,ResourceVersion:11450,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002924ee7 0xc002924ee8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002924f60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002924f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.18,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c0102caa2429bb34dbf41dddd5a539399eaace3947915d60b9a25bed7ac12f04}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.663: INFO: Pod "nginx-deployment-7b8c6f4498-cx95d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cx95d,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-cx95d,UID:5e317fcb-800c-4dce-878c-0ae74268f352,ResourceVersion:11567,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002925067 0xc002925068}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029250d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029250f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.663: INFO: Pod "nginx-deployment-7b8c6f4498-dmkr6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dmkr6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-dmkr6,UID:5d0dc464-ec8e-46c9-a00b-5534a4ed2df0,ResourceVersion:11563,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002925167 0xc002925168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029251d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029251f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.663: INFO: Pod "nginx-deployment-7b8c6f4498-grgkq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-grgkq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-grgkq,UID:5f1fc759-1d74-42fb-bb31-ca621823cb21,ResourceVersion:11431,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002925267 0xc002925268}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029252e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002925300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.19,StartTime:2019-09-06 20:28:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3d27ae855f00da6c8c5ff735ab9a5f1d1b71d827d2a6d17d7308d5b91b20ca7c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.663: INFO: Pod "nginx-deployment-7b8c6f4498-jw5tl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jw5tl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-jw5tl,UID:4ed43d92-9df9-4dca-a6cc-7d35ef6c47ae,ResourceVersion:11557,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc0029253e7 0xc0029253e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002925450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002925570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.663: INFO: Pod "nginx-deployment-7b8c6f4498-lgrxc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lgrxc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-lgrxc,UID:219fa684-5ae3-49e8-83c2-6326383beb38,ResourceVersion:11453,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002925667 0xc002925668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002925730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002925750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.15,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://400568ac5d29e03778c71dd7be53c63ba91a08651e62660cd4af8626c8103b20}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.663: INFO: Pod "nginx-deployment-7b8c6f4498-mnsxf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mnsxf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-mnsxf,UID:a449347f-bddd-4594-a6b1-a4963dd8a6b1,ResourceVersion:11416,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc0029259a7 0xc0029259a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv11,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002925a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002925a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.111,PodIP:172.16.141.13,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0a7304b3203deb38bf43fa5ae8a4a608804b823a9830157c32bdad899a73d92c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.664: INFO: Pod "nginx-deployment-7b8c6f4498-tjkvf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tjkvf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-tjkvf,UID:e92d0dea-666d-4dc6-a250-516f6f4da8a7,ResourceVersion:11437,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002925c47 0xc002925c48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002925e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002925e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.12,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:57 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://db6f74a9120e2441f03e4ac419bf8b0f350cbf414e984a1ac662c313a3df5159}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.664: INFO: Pod "nginx-deployment-7b8c6f4498-vrgkb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vrgkb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-vrgkb,UID:5e7204b1-aa17-4ede-8690-fbde1941b371,ResourceVersion:11566,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc002925fb7 0xc002925fb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00250e030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00250e050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.664: INFO: Pod "nginx-deployment-7b8c6f4498-xxgnb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xxgnb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-xxgnb,UID:f8d17d7c-43be-49c8-af47-5284f9dc2bc4,ResourceVersion:11564,Generation:0,CreationTimestamp:2019-09-06 20:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc00250e0c7 0xc00250e0c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00250e130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00250e150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:29:02.664: INFO: Pod "nginx-deployment-7b8c6f4498-zrmqs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zrmqs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-5983,SelfLink:/api/v1/namespaces/deployment-5983/pods/nginx-deployment-7b8c6f4498-zrmqs,UID:48ec93ab-9a91-4970-ad14-18cb7db44439,ResourceVersion:11447,Generation:0,CreationTimestamp:2019-09-06 20:28:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 7ddb5c3f-c1d1-4f22-9a10-70e6bec4d0a3 0xc00250e237 0xc00250e238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jcdbd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jcdbd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jcdbd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00250e2b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00250e2d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:28:56 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.11,StartTime:2019-09-06 20:28:56 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:28:58 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9a30f47199f29602ac25ad9dee9c457e379d9a86df1ece6919ede1271a801e30}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:29:02.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5983" for this suite.
Sep  6 20:29:08.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:29:08.763: INFO: namespace deployment-5983 deletion completed in 6.090633337s

• [SLOW TEST:12.334 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:29:08.763: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:29:38.904: INFO: Container started at 2019-09-06 20:29:13 +0000 UTC, pod became ready at 2019-09-06 20:29:37 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:29:38.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8090" for this suite.
Sep  6 20:30:00.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:30:01.013: INFO: namespace container-probe-8090 deletion completed in 22.10513013s

• [SLOW TEST:52.250 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:30:01.013: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Sep  6 20:30:01.158: INFO: Waiting up to 5m0s for pod "client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1" in namespace "containers-1830" to be "success or failure"
Sep  6 20:30:01.161: INFO: Pod "client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.916627ms
Sep  6 20:30:03.166: INFO: Pod "client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007354286s
STEP: Saw pod success
Sep  6 20:30:03.166: INFO: Pod "client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1" satisfied condition "success or failure"
Sep  6 20:30:03.170: INFO: Trying to get logs from node appserv9 pod client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1 container test-container: <nil>
STEP: delete the pod
Sep  6 20:30:03.189: INFO: Waiting for pod client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1 to disappear
Sep  6 20:30:03.192: INFO: Pod client-containers-a8be2a98-31de-4f3b-a58d-0cac097445f1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:30:03.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1830" for this suite.
Sep  6 20:30:09.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:30:09.305: INFO: namespace containers-1830 deletion completed in 6.10928403s

• [SLOW TEST:8.292 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:30:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 20:30:11.971: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ed0823b6-3cd3-468f-bb8f-923c9870bb02"
Sep  6 20:30:11.971: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ed0823b6-3cd3-468f-bb8f-923c9870bb02" in namespace "pods-9282" to be "terminated due to deadline exceeded"
Sep  6 20:30:11.974: INFO: Pod "pod-update-activedeadlineseconds-ed0823b6-3cd3-468f-bb8f-923c9870bb02": Phase="Running", Reason="", readiness=true. Elapsed: 2.629017ms
Sep  6 20:30:13.978: INFO: Pod "pod-update-activedeadlineseconds-ed0823b6-3cd3-468f-bb8f-923c9870bb02": Phase="Running", Reason="", readiness=true. Elapsed: 2.006549829s
Sep  6 20:30:15.982: INFO: Pod "pod-update-activedeadlineseconds-ed0823b6-3cd3-468f-bb8f-923c9870bb02": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.010216961s
Sep  6 20:30:15.982: INFO: Pod "pod-update-activedeadlineseconds-ed0823b6-3cd3-468f-bb8f-923c9870bb02" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:30:15.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9282" for this suite.
Sep  6 20:30:21.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:30:22.089: INFO: namespace pods-9282 deletion completed in 6.10334018s

• [SLOW TEST:12.784 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:30:22.089: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:30:22.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803" in namespace "downward-api-192" to be "success or failure"
Sep  6 20:30:22.233: INFO: Pod "downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428464ms
Sep  6 20:30:24.237: INFO: Pod "downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006048706s
STEP: Saw pod success
Sep  6 20:30:24.237: INFO: Pod "downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803" satisfied condition "success or failure"
Sep  6 20:30:24.240: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803 container client-container: <nil>
STEP: delete the pod
Sep  6 20:30:24.259: INFO: Waiting for pod downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803 to disappear
Sep  6 20:30:24.262: INFO: Pod downwardapi-volume-a8e2e868-3aa4-4fd9-8056-6989af157803 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:30:24.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-192" for this suite.
Sep  6 20:30:30.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:30:30.359: INFO: namespace downward-api-192 deletion completed in 6.093120114s

• [SLOW TEST:8.269 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:30:30.359: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8611, will wait for the garbage collector to delete the pods
Sep  6 20:30:32.562: INFO: Deleting Job.batch foo took: 6.735468ms
Sep  6 20:30:33.062: INFO: Terminating Job.batch foo pods took: 500.256181ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:31:18.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8611" for this suite.
Sep  6 20:31:24.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:31:24.766: INFO: namespace job-8611 deletion completed in 6.096057214s

• [SLOW TEST:54.407 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:31:24.767: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9261.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9261.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9261.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9261.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9261.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9261.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 20:31:26.943: INFO: DNS probes using dns-9261/dns-test-14820caa-4873-418f-9be2-f4a75df46610 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:31:26.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9261" for this suite.
Sep  6 20:31:32.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:31:33.063: INFO: namespace dns-9261 deletion completed in 6.107636169s

• [SLOW TEST:8.296 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:31:33.063: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 20:31:35.734: INFO: Successfully updated pod "labelsupdate0de4af6f-e688-455f-9896-a05764209080"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:31:37.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3614" for this suite.
Sep  6 20:31:59.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:31:59.863: INFO: namespace downward-api-3614 deletion completed in 22.104657203s

• [SLOW TEST:26.800 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:31:59.863: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-93a7d3ce-2fc2-4f9f-8711-517f29d7565f
STEP: Creating a pod to test consume configMaps
Sep  6 20:32:00.010: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14" in namespace "projected-7255" to be "success or failure"
Sep  6 20:32:00.012: INFO: Pod "pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718115ms
Sep  6 20:32:02.016: INFO: Pod "pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006123751s
STEP: Saw pod success
Sep  6 20:32:02.016: INFO: Pod "pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14" satisfied condition "success or failure"
Sep  6 20:32:02.019: INFO: Trying to get logs from node appserv10 pod pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:32:02.039: INFO: Waiting for pod pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14 to disappear
Sep  6 20:32:02.041: INFO: Pod pod-projected-configmaps-550c32e0-27ca-44b4-9c81-ba663cdaed14 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:32:02.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7255" for this suite.
Sep  6 20:32:08.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:32:08.152: INFO: namespace projected-7255 deletion completed in 6.106990846s

• [SLOW TEST:8.288 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:32:08.152: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2468
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2468
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2468
Sep  6 20:32:08.299: INFO: Found 0 stateful pods, waiting for 1
Sep  6 20:32:18.305: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  6 20:32:18.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:32:18.558: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:32:18.558: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:32:18.558: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:32:18.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 20:32:28.567: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:32:28.567: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:32:28.581: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999953s
Sep  6 20:32:29.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99645924s
Sep  6 20:32:30.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992251486s
Sep  6 20:32:31.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988189859s
Sep  6 20:32:32.598: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984112641s
Sep  6 20:32:33.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980215968s
Sep  6 20:32:34.606: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97603984s
Sep  6 20:32:35.610: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97224624s
Sep  6 20:32:36.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968295833s
Sep  6 20:32:37.617: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.481295ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2468
Sep  6 20:32:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:32:38.858: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 20:32:38.858: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:32:38.858: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:32:38.862: INFO: Found 1 stateful pods, waiting for 3
Sep  6 20:32:48.866: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:32:48.867: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:32:48.867: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  6 20:32:48.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:32:49.111: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:32:49.111: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:32:49.111: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:32:49.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:32:49.350: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:32:49.350: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:32:49.350: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:32:49.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 20:32:49.566: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 20:32:49.566: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 20:32:49.566: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 20:32:49.566: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:32:49.569: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  6 20:32:59.577: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:32:59.578: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:32:59.578: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 20:32:59.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999472s
Sep  6 20:33:00.593: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996242394s
Sep  6 20:33:01.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99184929s
Sep  6 20:33:02.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98738754s
Sep  6 20:33:03.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982865107s
Sep  6 20:33:04.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978298408s
Sep  6 20:33:05.615: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.974033256s
Sep  6 20:33:06.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969856678s
Sep  6 20:33:07.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965792435s
Sep  6 20:33:08.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.164085ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2468
Sep  6 20:33:09.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:33:09.876: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 20:33:09.876: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:33:09.876: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:33:09.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:33:10.119: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 20:33:10.120: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:33:10.120: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:33:10.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 exec --namespace=statefulset-2468 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 20:33:10.334: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 20:33:10.334: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 20:33:10.334: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 20:33:10.334: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 20:33:40.349: INFO: Deleting all statefulset in ns statefulset-2468
Sep  6 20:33:40.352: INFO: Scaling statefulset ss to 0
Sep  6 20:33:40.361: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:33:40.363: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:33:40.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2468" for this suite.
Sep  6 20:33:46.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:33:46.481: INFO: namespace statefulset-2468 deletion completed in 6.103438118s

• [SLOW TEST:98.329 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:33:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:33:46.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41" in namespace "downward-api-7669" to be "success or failure"
Sep  6 20:33:46.623: INFO: Pod "downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.462672ms
Sep  6 20:33:48.626: INFO: Pod "downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006077044s
Sep  6 20:33:50.630: INFO: Pod "downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009600336s
STEP: Saw pod success
Sep  6 20:33:50.630: INFO: Pod "downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41" satisfied condition "success or failure"
Sep  6 20:33:50.633: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41 container client-container: <nil>
STEP: delete the pod
Sep  6 20:33:50.652: INFO: Waiting for pod downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41 to disappear
Sep  6 20:33:50.656: INFO: Pod downwardapi-volume-53417b6c-449e-4b84-89c6-d8d9f1508c41 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:33:50.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7669" for this suite.
Sep  6 20:33:56.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:33:56.764: INFO: namespace downward-api-7669 deletion completed in 6.103616405s

• [SLOW TEST:10.282 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:33:56.764: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-n46x
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 20:33:56.911: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-n46x" in namespace "subpath-300" to be "success or failure"
Sep  6 20:33:56.914: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.454328ms
Sep  6 20:33:58.918: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 2.00616106s
Sep  6 20:34:00.921: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 4.00979467s
Sep  6 20:34:02.925: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 6.013364573s
Sep  6 20:34:04.928: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 8.016578567s
Sep  6 20:34:06.931: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 10.01986634s
Sep  6 20:34:08.935: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 12.023528499s
Sep  6 20:34:10.938: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 14.026915635s
Sep  6 20:34:12.942: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 16.030567542s
Sep  6 20:34:14.945: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 18.034082827s
Sep  6 20:34:16.949: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Running", Reason="", readiness=true. Elapsed: 20.037381078s
Sep  6 20:34:18.952: INFO: Pod "pod-subpath-test-projected-n46x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040928381s
STEP: Saw pod success
Sep  6 20:34:18.952: INFO: Pod "pod-subpath-test-projected-n46x" satisfied condition "success or failure"
Sep  6 20:34:18.955: INFO: Trying to get logs from node appserv9 pod pod-subpath-test-projected-n46x container test-container-subpath-projected-n46x: <nil>
STEP: delete the pod
Sep  6 20:34:18.976: INFO: Waiting for pod pod-subpath-test-projected-n46x to disappear
Sep  6 20:34:18.978: INFO: Pod pod-subpath-test-projected-n46x no longer exists
STEP: Deleting pod pod-subpath-test-projected-n46x
Sep  6 20:34:18.978: INFO: Deleting pod "pod-subpath-test-projected-n46x" in namespace "subpath-300"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:34:18.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-300" for this suite.
Sep  6 20:34:24.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:34:25.083: INFO: namespace subpath-300 deletion completed in 6.098045345s

• [SLOW TEST:28.319 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:34:25.083: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep  6 20:34:25.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-1618'
Sep  6 20:34:25.491: INFO: stderr: ""
Sep  6 20:34:25.491: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 20:34:26.495: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:34:26.495: INFO: Found 0 / 1
Sep  6 20:34:27.495: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:34:27.495: INFO: Found 1 / 1
Sep  6 20:34:27.495: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  6 20:34:27.499: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:34:27.499: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 20:34:27.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 patch pod redis-master-7sfhb --namespace=kubectl-1618 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  6 20:34:27.622: INFO: stderr: ""
Sep  6 20:34:27.622: INFO: stdout: "pod/redis-master-7sfhb patched\n"
STEP: checking annotations
Sep  6 20:34:27.626: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:34:27.626: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:34:27.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1618" for this suite.
Sep  6 20:34:49.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:34:49.735: INFO: namespace kubectl-1618 deletion completed in 22.105714841s

• [SLOW TEST:24.652 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:34:49.736: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:34:49.872: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 20:34:49.879: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 20:34:54.882: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 20:34:54.882: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 20:34:54.886: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 20:34:54.892: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 20:34:56.899: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 20:34:56.901: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 20:34:56.911: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4511,SelfLink:/apis/apps/v1/namespaces/deployment-4511/deployments/test-rolling-update-deployment,UID:1fb2bc52-e080-43ab-a957-1967dcf641d4,ResourceVersion:13273,Generation:1,CreationTimestamp:2019-09-06 20:34:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 20:34:54 +0000 UTC 2019-09-06 20:34:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 20:34:56 +0000 UTC 2019-09-06 20:34:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 20:34:56.914: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4511,SelfLink:/apis/apps/v1/namespaces/deployment-4511/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:9dec5716-a40f-47e0-b169-a1192345282c,ResourceVersion:13261,Generation:1,CreationTimestamp:2019-09-06 20:34:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1fb2bc52-e080-43ab-a957-1967dcf641d4 0xc002c5b807 0xc002c5b808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 20:34:56.914: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 20:34:56.914: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4511,SelfLink:/apis/apps/v1/namespaces/deployment-4511/replicasets/test-rolling-update-controller,UID:87257a00-422e-41c0-9632-cee4d9ae4cbc,ResourceVersion:13271,Generation:2,CreationTimestamp:2019-09-06 20:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1fb2bc52-e080-43ab-a957-1967dcf641d4 0xc002c5b737 0xc002c5b738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 20:34:56.918: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-tmtxv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-tmtxv,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4511,SelfLink:/api/v1/namespaces/deployment-4511/pods/test-rolling-update-deployment-79f6b9d75c-tmtxv,UID:5906569a-5e7b-48bc-9c09-71e2ee02399b,ResourceVersion:13260,Generation:0,CreationTimestamp:2019-09-06 20:34:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 9dec5716-a40f-47e0-b169-a1192345282c 0xc0030f60f7 0xc0030f60f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rhjfb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rhjfb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rhjfb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv10,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030f6170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030f6190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:34:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:34:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:34:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:34:54 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.110,PodIP:172.16.141.12,StartTime:2019-09-06 20:34:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 20:34:56 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://13a5afb49224741cd220444fbbe7b468b18c2ad609ac3647f8f94b48bf46cc1a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:34:56.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4511" for this suite.
Sep  6 20:35:02.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:35:03.030: INFO: namespace deployment-4511 deletion completed in 6.109099918s

• [SLOW TEST:13.295 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:35:03.031: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 20:35:03.175: INFO: Waiting up to 5m0s for pod "pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8" in namespace "emptydir-7933" to be "success or failure"
Sep  6 20:35:03.177: INFO: Pod "pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604106ms
Sep  6 20:35:05.181: INFO: Pod "pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005816209s
STEP: Saw pod success
Sep  6 20:35:05.181: INFO: Pod "pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8" satisfied condition "success or failure"
Sep  6 20:35:05.183: INFO: Trying to get logs from node appserv9 pod pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8 container test-container: <nil>
STEP: delete the pod
Sep  6 20:35:05.200: INFO: Waiting for pod pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8 to disappear
Sep  6 20:35:05.202: INFO: Pod pod-2d73e4d3-8aa4-47d5-8958-09e1277465b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:35:05.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7933" for this suite.
Sep  6 20:35:11.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:35:11.324: INFO: namespace emptydir-7933 deletion completed in 6.118638619s

• [SLOW TEST:8.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:35:11.324: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:35:15.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8511" for this suite.
Sep  6 20:35:21.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:35:21.580: INFO: namespace kubelet-test-8511 deletion completed in 6.103193785s

• [SLOW TEST:10.255 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:35:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 20:35:21.739: INFO: Number of nodes with available pods: 0
Sep  6 20:35:21.739: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:35:22.746: INFO: Number of nodes with available pods: 0
Sep  6 20:35:22.746: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:35:23.747: INFO: Number of nodes with available pods: 3
Sep  6 20:35:23.747: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  6 20:35:23.765: INFO: Number of nodes with available pods: 2
Sep  6 20:35:23.765: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:35:24.773: INFO: Number of nodes with available pods: 2
Sep  6 20:35:24.773: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:35:25.774: INFO: Number of nodes with available pods: 2
Sep  6 20:35:25.774: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:35:26.773: INFO: Number of nodes with available pods: 2
Sep  6 20:35:26.773: INFO: Node appserv10 is running more than one daemon pod
Sep  6 20:35:27.773: INFO: Number of nodes with available pods: 3
Sep  6 20:35:27.773: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3388, will wait for the garbage collector to delete the pods
Sep  6 20:35:27.839: INFO: Deleting DaemonSet.extensions daemon-set took: 6.535308ms
Sep  6 20:35:27.939: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165881ms
Sep  6 20:35:42.742: INFO: Number of nodes with available pods: 0
Sep  6 20:35:42.742: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 20:35:42.745: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3388/daemonsets","resourceVersion":"13553"},"items":null}

Sep  6 20:35:42.747: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3388/pods","resourceVersion":"13553"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:35:42.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3388" for this suite.
Sep  6 20:35:48.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:35:48.874: INFO: namespace daemonsets-3388 deletion completed in 6.110057344s

• [SLOW TEST:27.294 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:35:48.875: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9494 to expose endpoints map[]
Sep  6 20:35:49.020: INFO: Get endpoints failed (2.631787ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep  6 20:35:50.024: INFO: successfully validated that service multi-endpoint-test in namespace services-9494 exposes endpoints map[] (1.005936281s elapsed)
STEP: Creating pod pod1 in namespace services-9494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9494 to expose endpoints map[pod1:[100]]
Sep  6 20:35:52.049: INFO: successfully validated that service multi-endpoint-test in namespace services-9494 exposes endpoints map[pod1:[100]] (2.017691777s elapsed)
STEP: Creating pod pod2 in namespace services-9494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9494 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 20:35:54.081: INFO: successfully validated that service multi-endpoint-test in namespace services-9494 exposes endpoints map[pod1:[100] pod2:[101]] (2.027038567s elapsed)
STEP: Deleting pod pod1 in namespace services-9494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9494 to expose endpoints map[pod2:[101]]
Sep  6 20:35:55.099: INFO: successfully validated that service multi-endpoint-test in namespace services-9494 exposes endpoints map[pod2:[101]] (1.012586222s elapsed)
STEP: Deleting pod pod2 in namespace services-9494
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9494 to expose endpoints map[]
Sep  6 20:35:56.110: INFO: successfully validated that service multi-endpoint-test in namespace services-9494 exposes endpoints map[] (1.006116464s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:35:56.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9494" for this suite.
Sep  6 20:36:18.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:36:18.234: INFO: namespace services-9494 deletion completed in 22.105378841s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.360 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:36:18.235: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 20:36:18.378: INFO: Waiting up to 5m0s for pod "downward-api-aa22eff1-9f29-48a6-895a-d37776858e55" in namespace "downward-api-7867" to be "success or failure"
Sep  6 20:36:18.381: INFO: Pod "downward-api-aa22eff1-9f29-48a6-895a-d37776858e55": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139987ms
Sep  6 20:36:20.384: INFO: Pod "downward-api-aa22eff1-9f29-48a6-895a-d37776858e55": Phase="Running", Reason="", readiness=true. Elapsed: 2.006783984s
Sep  6 20:36:22.388: INFO: Pod "downward-api-aa22eff1-9f29-48a6-895a-d37776858e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010099829s
STEP: Saw pod success
Sep  6 20:36:22.388: INFO: Pod "downward-api-aa22eff1-9f29-48a6-895a-d37776858e55" satisfied condition "success or failure"
Sep  6 20:36:22.391: INFO: Trying to get logs from node appserv9 pod downward-api-aa22eff1-9f29-48a6-895a-d37776858e55 container dapi-container: <nil>
STEP: delete the pod
Sep  6 20:36:22.410: INFO: Waiting for pod downward-api-aa22eff1-9f29-48a6-895a-d37776858e55 to disappear
Sep  6 20:36:22.413: INFO: Pod downward-api-aa22eff1-9f29-48a6-895a-d37776858e55 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:36:22.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7867" for this suite.
Sep  6 20:36:28.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:36:28.524: INFO: namespace downward-api-7867 deletion completed in 6.107280587s

• [SLOW TEST:10.290 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:36:28.524: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-cd5e871a-ba2f-4e51-b108-cd543a94e5b9
STEP: Creating a pod to test consume configMaps
Sep  6 20:36:28.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d" in namespace "configmap-8538" to be "success or failure"
Sep  6 20:36:28.674: INFO: Pod "pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67848ms
Sep  6 20:36:30.678: INFO: Pod "pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005829412s
STEP: Saw pod success
Sep  6 20:36:30.678: INFO: Pod "pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d" satisfied condition "success or failure"
Sep  6 20:36:30.680: INFO: Trying to get logs from node appserv10 pod pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:36:30.698: INFO: Waiting for pod pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d to disappear
Sep  6 20:36:30.701: INFO: Pod pod-configmaps-d659ce23-aadc-42e4-88e0-fd323ab4343d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:36:30.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8538" for this suite.
Sep  6 20:36:36.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:36:36.808: INFO: namespace configmap-8538 deletion completed in 6.102560212s

• [SLOW TEST:8.284 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:36:36.809: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Sep  6 20:36:36.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 api-versions'
Sep  6 20:36:37.056: INFO: stderr: ""
Sep  6 20:36:37.056: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvolumesnapshot.external-storage.k8s.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:36:37.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8319" for this suite.
Sep  6 20:36:43.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:36:43.171: INFO: namespace kubectl-8319 deletion completed in 6.110794799s

• [SLOW TEST:6.362 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:36:43.172: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-9ed2fc2a-9f0b-45b0-8b8c-5e597404c071 in namespace container-probe-3558
Sep  6 20:36:45.320: INFO: Started pod test-webserver-9ed2fc2a-9f0b-45b0-8b8c-5e597404c071 in namespace container-probe-3558
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 20:36:45.323: INFO: Initial restart count of pod test-webserver-9ed2fc2a-9f0b-45b0-8b8c-5e597404c071 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:40:45.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3558" for this suite.
Sep  6 20:40:51.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:40:51.878: INFO: namespace container-probe-3558 deletion completed in 6.102623196s

• [SLOW TEST:248.706 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:40:51.878: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Sep  6 20:40:52.020: INFO: Waiting up to 5m0s for pod "var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256" in namespace "var-expansion-9871" to be "success or failure"
Sep  6 20:40:52.022: INFO: Pod "var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698316ms
Sep  6 20:40:54.026: INFO: Pod "var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006044333s
STEP: Saw pod success
Sep  6 20:40:54.026: INFO: Pod "var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256" satisfied condition "success or failure"
Sep  6 20:40:54.029: INFO: Trying to get logs from node appserv10 pod var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256 container dapi-container: <nil>
STEP: delete the pod
Sep  6 20:40:54.050: INFO: Waiting for pod var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256 to disappear
Sep  6 20:40:54.053: INFO: Pod var-expansion-f556bd79-32b2-4c45-a944-4ce244e89256 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:40:54.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9871" for this suite.
Sep  6 20:41:00.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:41:00.164: INFO: namespace var-expansion-9871 deletion completed in 6.106683958s

• [SLOW TEST:8.286 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:41:00.164: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 in namespace container-probe-1452
Sep  6 20:41:04.316: INFO: Started pod liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 in namespace container-probe-1452
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 20:41:04.319: INFO: Initial restart count of pod liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 is 0
Sep  6 20:41:14.339: INFO: Restart count of pod container-probe-1452/liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 is now 1 (10.020321403s elapsed)
Sep  6 20:41:34.375: INFO: Restart count of pod container-probe-1452/liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 is now 2 (30.055468223s elapsed)
Sep  6 20:41:54.410: INFO: Restart count of pod container-probe-1452/liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 is now 3 (50.090920288s elapsed)
Sep  6 20:42:14.446: INFO: Restart count of pod container-probe-1452/liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 is now 4 (1m10.126680021s elapsed)
Sep  6 20:43:16.559: INFO: Restart count of pod container-probe-1452/liveness-1df38cd1-47a4-4f43-80e7-7ac2b84c53a3 is now 5 (2m12.239870749s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:43:16.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1452" for this suite.
Sep  6 20:43:22.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:43:22.672: INFO: namespace container-probe-1452 deletion completed in 6.100964396s

• [SLOW TEST:142.508 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:43:22.673: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3040
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-4582a5aa-6d74-4585-a206-5cd1d1aa34a8
STEP: Creating secret with name s-test-opt-upd-ce43c299-636f-496f-9b71-e7fb7fea3e3c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4582a5aa-6d74-4585-a206-5cd1d1aa34a8
STEP: Updating secret s-test-opt-upd-ce43c299-636f-496f-9b71-e7fb7fea3e3c
STEP: Creating secret with name s-test-opt-create-45b8ab3e-349d-41bf-b88c-2090b1890411
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:44:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3040" for this suite.
Sep  6 20:44:51.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:44:51.351: INFO: namespace secrets-3040 deletion completed in 22.117442644s

• [SLOW TEST:88.678 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:44:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6101
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:44:51.485: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:44:52.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6101" for this suite.
Sep  6 20:44:58.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:44:58.637: INFO: namespace custom-resource-definition-6101 deletion completed in 6.111811183s

• [SLOW TEST:7.286 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:44:58.638: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4a7441c8-476a-4f82-875d-6beafced9053
STEP: Creating a pod to test consume configMaps
Sep  6 20:44:58.787: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c" in namespace "projected-5680" to be "success or failure"
Sep  6 20:44:58.790: INFO: Pod "pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047343ms
Sep  6 20:45:00.794: INFO: Pod "pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006703964s
STEP: Saw pod success
Sep  6 20:45:00.794: INFO: Pod "pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c" satisfied condition "success or failure"
Sep  6 20:45:00.796: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:45:00.814: INFO: Waiting for pod pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c to disappear
Sep  6 20:45:00.817: INFO: Pod pod-projected-configmaps-a0fb7d62-0e1b-4371-a61a-d61894092e4c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:45:00.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5680" for this suite.
Sep  6 20:45:06.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:45:06.924: INFO: namespace projected-5680 deletion completed in 6.102882885s

• [SLOW TEST:8.286 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:45:06.925: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 20:45:07.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d" in namespace "downward-api-4822" to be "success or failure"
Sep  6 20:45:07.068: INFO: Pod "downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497286ms
Sep  6 20:45:09.072: INFO: Pod "downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005925099s
STEP: Saw pod success
Sep  6 20:45:09.072: INFO: Pod "downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d" satisfied condition "success or failure"
Sep  6 20:45:09.075: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d container client-container: <nil>
STEP: delete the pod
Sep  6 20:45:09.093: INFO: Waiting for pod downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d to disappear
Sep  6 20:45:09.096: INFO: Pod downwardapi-volume-8c4c6685-947d-4695-aa92-c56b82d35b8d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:45:09.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4822" for this suite.
Sep  6 20:45:15.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:45:15.241: INFO: namespace downward-api-4822 deletion completed in 6.140576256s

• [SLOW TEST:8.317 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:45:15.242: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:45:15.389: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  6 20:45:20.392: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 20:45:20.393: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 20:45:20.410: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-2985,SelfLink:/apis/apps/v1/namespaces/deployment-2985/deployments/test-cleanup-deployment,UID:e00fc885-8cc4-44bb-b9bb-f1363588be17,ResourceVersion:15084,Generation:1,CreationTimestamp:2019-09-06 20:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 20:45:20.413: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-2985,SelfLink:/apis/apps/v1/namespaces/deployment-2985/replicasets/test-cleanup-deployment-55bbcbc84c,UID:5d0140fe-b6c4-4f13-b1d7-a2a33b7ac9ae,ResourceVersion:15086,Generation:1,CreationTimestamp:2019-09-06 20:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment e00fc885-8cc4-44bb-b9bb-f1363588be17 0xc002300ce7 0xc002300ce8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 20:45:20.413: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  6 20:45:20.414: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-2985,SelfLink:/apis/apps/v1/namespaces/deployment-2985/replicasets/test-cleanup-controller,UID:1808ec12-f576-4267-8a8a-0351784f5efd,ResourceVersion:15085,Generation:1,CreationTimestamp:2019-09-06 20:45:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment e00fc885-8cc4-44bb-b9bb-f1363588be17 0xc002300be7 0xc002300be8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 20:45:20.417: INFO: Pod "test-cleanup-controller-dpv9h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-dpv9h,GenerateName:test-cleanup-controller-,Namespace:deployment-2985,SelfLink:/api/v1/namespaces/deployment-2985/pods/test-cleanup-controller-dpv9h,UID:c29fa864-9b90-4d78-b8e6-0bd25aa15b80,ResourceVersion:15076,Generation:0,CreationTimestamp:2019-09-06 20:45:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 1808ec12-f576-4267-8a8a-0351784f5efd 0xc003607e17 0xc003607e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qtpkj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qtpkj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qtpkj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:appserv9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003607ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003607ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:45:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:45:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:45:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 20:45:15 +0000 UTC  }],Message:,Reason:,HostIP:172.16.6.109,PodIP:172.16.141.11,StartTime:2019-09-06 20:45:15 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 20:45:16 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ee2020f3653aed2d3731304c3876731584a984d21598ed0b79e3c6734182ae14}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 20:45:20.418: INFO: Pod "test-cleanup-deployment-55bbcbc84c-hwgpq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-hwgpq,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-2985,SelfLink:/api/v1/namespaces/deployment-2985/pods/test-cleanup-deployment-55bbcbc84c-hwgpq,UID:b36ef43a-fdb5-4a4c-a84f-12906d7d9306,ResourceVersion:15088,Generation:0,CreationTimestamp:2019-09-06 20:45:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{kubernetes.io/psp: collecting,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 5d0140fe-b6c4-4f13-b1d7-a2a33b7ac9ae 0xc003607fb7 0xc003607fb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qtpkj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qtpkj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qtpkj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000c94020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000c94040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:45:20.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2985" for this suite.
Sep  6 20:45:26.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:45:26.521: INFO: namespace deployment-2985 deletion completed in 6.099240416s

• [SLOW TEST:11.279 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:45:26.521: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  6 20:45:26.654: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 20:45:26.661: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 20:45:26.664: INFO: 
Logging pods the kubelet thinks is on node appserv10 before test
Sep  6 20:45:26.671: INFO: csi-diamanti-driver-h8mbm from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.671: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:45:26.671: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:45:26.671: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-2wkwb from heptio-sonobuoy started at 2019-09-06 19:45:21 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.671: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 20:45:26.671: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 20:45:26.671: INFO: csi-external-snapshotter-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.671: INFO: 	Container csi-external-snapshotter ready: true, restart count 0
Sep  6 20:45:26.671: INFO: prometheus-v1-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.671: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:45:26.671: INFO: collectd-v0.8-md6fj from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (4 container statuses recorded)
Sep  6 20:45:26.671: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:45:26.671: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:45:26.671: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:45:26.671: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:45:26.671: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 19:45:19 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.671: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 20:45:26.671: INFO: 
Logging pods the kubelet thinks is on node appserv11 before test
Sep  6 20:45:26.680: INFO: alertmanager-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 20:45:26.680: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-plh6j from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 20:45:26.680: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 20:45:26.680: INFO: prometheus-v1-1 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:45:26.680: INFO: csi-external-provisioner-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container csi-external-provisioner ready: true, restart count 0
Sep  6 20:45:26.680: INFO: collectd-v0.8-xfwsc from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (4 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:45:26.680: INFO: tiller-deploy-5668df8bc4-6pgjb from kube-system started at 2019-09-06 19:41:52 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container tiller ready: true, restart count 0
Sep  6 20:45:26.680: INFO: sonobuoy-e2e-job-8e772d81d3704c93 from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container e2e ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 20:45:26.680: INFO: metrics-server-v1-5d46b6d959-48m72 from kube-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container metrics-server ready: true, restart count 0
Sep  6 20:45:26.680: INFO: csi-diamanti-driver-wztns from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:45:26.680: INFO: csi-external-attacher-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container csi-attacher ready: true, restart count 0
Sep  6 20:45:26.680: INFO: csi-external-resizer-0 from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container csi-external-resizer ready: true, restart count 0
Sep  6 20:45:26.680: INFO: snapshot-controller-66fb5f8fbd-xjhnl from diamanti-system started at 2019-09-06 19:41:48 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.680: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 	Container snapshot-provisioner ready: true, restart count 0
Sep  6 20:45:26.680: INFO: 
Logging pods the kubelet thinks is on node appserv9 before test
Sep  6 20:45:26.686: INFO: csi-diamanti-driver-fc5gk from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.686: INFO: 	Container diamanticsidriver ready: true, restart count 0
Sep  6 20:45:26.686: INFO: 	Container driver-registrar ready: true, restart count 0
Sep  6 20:45:26.686: INFO: collectd-v0.8-f5qhk from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (4 container statuses recorded)
Sep  6 20:45:26.686: INFO: 	Container cadvisor ready: true, restart count 0
Sep  6 20:45:26.686: INFO: 	Container collectd-es ready: true, restart count 0
Sep  6 20:45:26.686: INFO: 	Container collectd-exporter ready: true, restart count 0
Sep  6 20:45:26.686: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 20:45:26.686: INFO: helm-chart-687577f867-4r7c2 from kube-system started at 2019-09-06 19:41:53 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.686: INFO: 	Container helm-chart ready: true, restart count 0
Sep  6 20:45:26.686: INFO: prometheus-v1-2 from diamanti-system started at 2019-09-06 19:41:53 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.686: INFO: 	Container prometheus ready: true, restart count 0
Sep  6 20:45:26.686: INFO: provisioner-7b58589b9d-2zpz4 from diamanti-system started at 2019-09-06 19:41:52 +0000 UTC (1 container statuses recorded)
Sep  6 20:45:26.686: INFO: 	Container provisioner ready: true, restart count 0
Sep  6 20:45:26.686: INFO: sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-kngjc from heptio-sonobuoy started at 2019-09-06 19:45:20 +0000 UTC (2 container statuses recorded)
Sep  6 20:45:26.686: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Sep  6 20:45:26.686: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node appserv10
STEP: verifying the node has the label node appserv11
STEP: verifying the node has the label node appserv9
Sep  6 20:45:26.724: INFO: Pod alertmanager-0 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod collectd-v0.8-f5qhk requesting resource cpu=0m on Node appserv9
Sep  6 20:45:26.724: INFO: Pod collectd-v0.8-md6fj requesting resource cpu=0m on Node appserv10
Sep  6 20:45:26.724: INFO: Pod collectd-v0.8-xfwsc requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod csi-diamanti-driver-fc5gk requesting resource cpu=0m on Node appserv9
Sep  6 20:45:26.724: INFO: Pod csi-diamanti-driver-h8mbm requesting resource cpu=0m on Node appserv10
Sep  6 20:45:26.724: INFO: Pod csi-diamanti-driver-wztns requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod csi-external-attacher-0 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod csi-external-provisioner-0 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod csi-external-resizer-0 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod csi-external-snapshotter-0 requesting resource cpu=0m on Node appserv10
Sep  6 20:45:26.724: INFO: Pod prometheus-v1-0 requesting resource cpu=0m on Node appserv10
Sep  6 20:45:26.724: INFO: Pod prometheus-v1-1 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod prometheus-v1-2 requesting resource cpu=0m on Node appserv9
Sep  6 20:45:26.724: INFO: Pod provisioner-7b58589b9d-2zpz4 requesting resource cpu=0m on Node appserv9
Sep  6 20:45:26.724: INFO: Pod snapshot-controller-66fb5f8fbd-xjhnl requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod sonobuoy requesting resource cpu=0m on Node appserv10
Sep  6 20:45:26.724: INFO: Pod sonobuoy-e2e-job-8e772d81d3704c93 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-2wkwb requesting resource cpu=0m on Node appserv10
Sep  6 20:45:26.724: INFO: Pod sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-kngjc requesting resource cpu=0m on Node appserv9
Sep  6 20:45:26.724: INFO: Pod sonobuoy-systemd-logs-daemon-set-594d81d159ac4ea4-plh6j requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod helm-chart-687577f867-4r7c2 requesting resource cpu=0m on Node appserv9
Sep  6 20:45:26.724: INFO: Pod metrics-server-v1-5d46b6d959-48m72 requesting resource cpu=0m on Node appserv11
Sep  6 20:45:26.724: INFO: Pod tiller-deploy-5668df8bc4-6pgjb requesting resource cpu=250m on Node appserv11
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34.15c1f445ff0d0b2c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3887/filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34 to appserv10]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34.15c1f44638830a02], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34.15c1f4463af237b9], Reason = [Created], Message = [Created container filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34.15c1f446442fc921], Reason = [Started], Message = [Started container filler-pod-2986fbab-92c6-4f6a-b9b6-14c1c892eb34]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9.15c1f44601906cab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3887/filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9 to appserv11]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9.15c1f4463a369dfc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9.15c1f4463ca28b34], Reason = [Created], Message = [Created container filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9.15c1f44645bf9d95], Reason = [Started], Message = [Started container filler-pod-7cd12422-a402-4653-935a-130c4ecd2ca9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75.15c1f44603ec1f91], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3887/filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75 to appserv9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75.15c1f446542e1de9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75.15c1f4465661c0db], Reason = [Created], Message = [Created container filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75.15c1f4465f495e24], Reason = [Started], Message = [Started container filler-pod-c74b7ea6-3d3f-49b6-a12c-5f796f034e75]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c1f446ed1a6d02], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node appserv10
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node appserv11
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node appserv9
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:45:31.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3887" for this suite.
Sep  6 20:45:37.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:45:37.919: INFO: namespace sched-pred-3887 deletion completed in 6.112085203s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.398 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:45:37.919: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6791
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-1f9c89e7-b2c2-4994-87ca-94fb9e3fe2f5
STEP: Creating secret with name secret-projected-all-test-volume-033f12a1-9cd4-4663-a630-5a78e4fa52eb
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  6 20:45:38.071: INFO: Waiting up to 5m0s for pod "projected-volume-364f4b3a-e288-4156-bef7-27d50552e854" in namespace "projected-6791" to be "success or failure"
Sep  6 20:45:38.074: INFO: Pod "projected-volume-364f4b3a-e288-4156-bef7-27d50552e854": Phase="Pending", Reason="", readiness=false. Elapsed: 3.005885ms
Sep  6 20:45:40.078: INFO: Pod "projected-volume-364f4b3a-e288-4156-bef7-27d50552e854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006892484s
Sep  6 20:45:42.082: INFO: Pod "projected-volume-364f4b3a-e288-4156-bef7-27d50552e854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010194216s
STEP: Saw pod success
Sep  6 20:45:42.082: INFO: Pod "projected-volume-364f4b3a-e288-4156-bef7-27d50552e854" satisfied condition "success or failure"
Sep  6 20:45:42.085: INFO: Trying to get logs from node appserv9 pod projected-volume-364f4b3a-e288-4156-bef7-27d50552e854 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  6 20:45:42.103: INFO: Waiting for pod projected-volume-364f4b3a-e288-4156-bef7-27d50552e854 to disappear
Sep  6 20:45:42.105: INFO: Pod projected-volume-364f4b3a-e288-4156-bef7-27d50552e854 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:45:42.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6791" for this suite.
Sep  6 20:45:48.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:45:48.222: INFO: namespace projected-6791 deletion completed in 6.112553359s

• [SLOW TEST:10.303 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:45:48.223: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 20:45:52.905: INFO: Successfully updated pod "pod-update-2cb63837-090f-48c5-8ea2-55bd4cc8d4a9"
STEP: verifying the updated pod is in kubernetes
Sep  6 20:45:52.911: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:45:52.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5830" for this suite.
Sep  6 20:46:14.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:46:15.019: INFO: namespace pods-5830 deletion completed in 22.103590035s

• [SLOW TEST:26.797 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:46:15.019: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Sep  6 20:46:15.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-4742'
Sep  6 20:46:15.423: INFO: stderr: ""
Sep  6 20:46:15.423: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Sep  6 20:46:16.426: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:46:16.426: INFO: Found 0 / 1
Sep  6 20:46:17.427: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:46:17.427: INFO: Found 1 / 1
Sep  6 20:46:17.428: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 20:46:17.431: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 20:46:17.431: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep  6 20:46:17.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 logs redis-master-wfhgp redis-master --namespace=kubectl-4742'
Sep  6 20:46:17.576: INFO: stderr: ""
Sep  6 20:46:17.576: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 20:46:16.511 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 20:46:16.511 # Server started, Redis version 3.2.12\n1:M 06 Sep 20:46:16.511 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 20:46:16.511 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep  6 20:46:17.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 log redis-master-wfhgp redis-master --namespace=kubectl-4742 --tail=1'
Sep  6 20:46:17.713: INFO: stderr: ""
Sep  6 20:46:17.713: INFO: stdout: "1:M 06 Sep 20:46:16.511 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep  6 20:46:17.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 log redis-master-wfhgp redis-master --namespace=kubectl-4742 --limit-bytes=1'
Sep  6 20:46:17.840: INFO: stderr: ""
Sep  6 20:46:17.840: INFO: stdout: " "
STEP: exposing timestamps
Sep  6 20:46:17.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 log redis-master-wfhgp redis-master --namespace=kubectl-4742 --tail=1 --timestamps'
Sep  6 20:46:17.971: INFO: stderr: ""
Sep  6 20:46:17.971: INFO: stdout: "2019-09-06T20:46:16.511639245Z 1:M 06 Sep 20:46:16.511 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep  6 20:46:20.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 log redis-master-wfhgp redis-master --namespace=kubectl-4742 --since=1s'
Sep  6 20:46:20.603: INFO: stderr: ""
Sep  6 20:46:20.603: INFO: stdout: ""
Sep  6 20:46:20.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 log redis-master-wfhgp redis-master --namespace=kubectl-4742 --since=24h'
Sep  6 20:46:20.706: INFO: stderr: ""
Sep  6 20:46:20.706: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 20:46:16.511 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 20:46:16.511 # Server started, Redis version 3.2.12\n1:M 06 Sep 20:46:16.511 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 20:46:16.511 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Sep  6 20:46:20.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-4742'
Sep  6 20:46:20.783: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:46:20.784: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep  6 20:46:20.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4742'
Sep  6 20:46:20.863: INFO: stderr: "No resources found.\n"
Sep  6 20:46:20.863: INFO: stdout: ""
Sep  6 20:46:20.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -l name=nginx --namespace=kubectl-4742 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 20:46:20.937: INFO: stderr: ""
Sep  6 20:46:20.937: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:46:20.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4742" for this suite.
Sep  6 20:46:42.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:46:43.052: INFO: namespace kubectl-4742 deletion completed in 22.112107341s

• [SLOW TEST:28.033 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:46:43.053: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:46:45.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4576" for this suite.
Sep  6 20:47:31.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:47:31.346: INFO: namespace kubelet-test-4576 deletion completed in 46.12491311s

• [SLOW TEST:48.293 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:47:31.346: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-c1bd0114-792d-4033-8530-458f4642abcc
STEP: Creating a pod to test consume secrets
Sep  6 20:47:31.491: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47" in namespace "projected-4983" to be "success or failure"
Sep  6 20:47:31.493: INFO: Pod "pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695411ms
Sep  6 20:47:33.497: INFO: Pod "pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006402018s
STEP: Saw pod success
Sep  6 20:47:33.497: INFO: Pod "pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47" satisfied condition "success or failure"
Sep  6 20:47:33.500: INFO: Trying to get logs from node appserv9 pod pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 20:47:33.520: INFO: Waiting for pod pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47 to disappear
Sep  6 20:47:33.523: INFO: Pod pod-projected-secrets-d3e93d85-d2ff-4ec3-b553-73feea98ae47 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:47:33.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4983" for this suite.
Sep  6 20:47:39.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:47:39.636: INFO: namespace projected-4983 deletion completed in 6.109119177s

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:47:39.636: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  6 20:47:39.776: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:47:58.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5622" for this suite.
Sep  6 20:48:04.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:48:04.708: INFO: namespace pods-5622 deletion completed in 6.103750337s

• [SLOW TEST:25.072 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:48:04.709: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3896
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  6 20:48:04.851: INFO: Waiting up to 5m0s for pod "pod-b27dd54d-e74a-463d-8299-95cf01d705cf" in namespace "emptydir-3896" to be "success or failure"
Sep  6 20:48:04.854: INFO: Pod "pod-b27dd54d-e74a-463d-8299-95cf01d705cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787329ms
Sep  6 20:48:06.857: INFO: Pod "pod-b27dd54d-e74a-463d-8299-95cf01d705cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006081358s
STEP: Saw pod success
Sep  6 20:48:06.857: INFO: Pod "pod-b27dd54d-e74a-463d-8299-95cf01d705cf" satisfied condition "success or failure"
Sep  6 20:48:06.860: INFO: Trying to get logs from node appserv9 pod pod-b27dd54d-e74a-463d-8299-95cf01d705cf container test-container: <nil>
STEP: delete the pod
Sep  6 20:48:06.875: INFO: Waiting for pod pod-b27dd54d-e74a-463d-8299-95cf01d705cf to disappear
Sep  6 20:48:06.877: INFO: Pod pod-b27dd54d-e74a-463d-8299-95cf01d705cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:48:06.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3896" for this suite.
Sep  6 20:48:12.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:48:12.987: INFO: namespace emptydir-3896 deletion completed in 6.10611877s

• [SLOW TEST:8.278 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:48:12.987: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Sep  6 20:48:13.123: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep  6 20:48:13.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-9964'
Sep  6 20:48:13.365: INFO: stderr: ""
Sep  6 20:48:13.365: INFO: stdout: "service/redis-slave created\n"
Sep  6 20:48:13.365: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep  6 20:48:13.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-9964'
Sep  6 20:48:13.581: INFO: stderr: ""
Sep  6 20:48:13.581: INFO: stdout: "service/redis-master created\n"
Sep  6 20:48:13.581: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  6 20:48:13.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-9964'
Sep  6 20:48:13.808: INFO: stderr: ""
Sep  6 20:48:13.808: INFO: stdout: "service/frontend created\n"
Sep  6 20:48:13.808: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep  6 20:48:13.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-9964'
Sep  6 20:48:13.993: INFO: stderr: ""
Sep  6 20:48:13.993: INFO: stdout: "deployment.apps/frontend created\n"
Sep  6 20:48:13.993: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 20:48:13.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-9964'
Sep  6 20:48:14.186: INFO: stderr: ""
Sep  6 20:48:14.186: INFO: stdout: "deployment.apps/redis-master created\n"
Sep  6 20:48:14.186: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep  6 20:48:14.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-9964'
Sep  6 20:48:14.371: INFO: stderr: ""
Sep  6 20:48:14.371: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep  6 20:48:14.371: INFO: Waiting for all frontend pods to be Running.
Sep  6 20:48:19.422: INFO: Waiting for frontend to serve content.
Sep  6 20:48:19.453: INFO: Trying to add a new entry to the guestbook.
Sep  6 20:48:19.484: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  6 20:48:19.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-9964'
Sep  6 20:48:19.652: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:48:19.652: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 20:48:19.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-9964'
Sep  6 20:48:19.777: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:48:19.777: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 20:48:19.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-9964'
Sep  6 20:48:19.898: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:48:19.898: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 20:48:19.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-9964'
Sep  6 20:48:19.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:48:19.988: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 20:48:19.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-9964'
Sep  6 20:48:20.077: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:48:20.077: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 20:48:20.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-9964'
Sep  6 20:48:20.179: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 20:48:20.179: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:48:20.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9964" for this suite.
Sep  6 20:48:58.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:48:58.294: INFO: namespace kubectl-9964 deletion completed in 38.109805413s

• [SLOW TEST:45.307 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:48:58.294: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-2367a6a3-b13a-4df7-80d6-8651139fec0e
STEP: Creating a pod to test consume configMaps
Sep  6 20:48:58.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4" in namespace "projected-3371" to be "success or failure"
Sep  6 20:48:58.445: INFO: Pod "pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.760893ms
Sep  6 20:49:00.448: INFO: Pod "pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006156304s
STEP: Saw pod success
Sep  6 20:49:00.448: INFO: Pod "pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4" satisfied condition "success or failure"
Sep  6 20:49:00.451: INFO: Trying to get logs from node appserv10 pod pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:49:00.472: INFO: Waiting for pod pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4 to disappear
Sep  6 20:49:00.475: INFO: Pod pod-projected-configmaps-a148eebd-5c86-43a2-b310-c0b17e3478b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:49:00.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3371" for this suite.
Sep  6 20:49:06.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:49:06.579: INFO: namespace projected-3371 deletion completed in 6.099057698s

• [SLOW TEST:8.285 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:49:06.579: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5442/configmap-test-fe472c7e-caad-4fd0-89ef-449ddc28c77d
STEP: Creating a pod to test consume configMaps
Sep  6 20:49:06.721: INFO: Waiting up to 5m0s for pod "pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d" in namespace "configmap-5442" to be "success or failure"
Sep  6 20:49:06.724: INFO: Pod "pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.486904ms
Sep  6 20:49:08.727: INFO: Pod "pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00578597s
STEP: Saw pod success
Sep  6 20:49:08.727: INFO: Pod "pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d" satisfied condition "success or failure"
Sep  6 20:49:08.730: INFO: Trying to get logs from node appserv10 pod pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d container env-test: <nil>
STEP: delete the pod
Sep  6 20:49:08.751: INFO: Waiting for pod pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d to disappear
Sep  6 20:49:08.754: INFO: Pod pod-configmaps-1132b1ff-b61c-4680-a7c6-e9193b5cc51d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:49:08.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5442" for this suite.
Sep  6 20:49:14.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:49:14.860: INFO: namespace configmap-5442 deletion completed in 6.100483511s

• [SLOW TEST:8.281 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:49:14.860: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3791
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  6 20:49:15.015: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3791,SelfLink:/api/v1/namespaces/watch-3791/configmaps/e2e-watch-test-resource-version,UID:c95d93d2-97f4-4211-bc51-4ab6377bd1dd,ResourceVersion:16236,Generation:0,CreationTimestamp:2019-09-06 20:49:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 20:49:15.015: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3791,SelfLink:/api/v1/namespaces/watch-3791/configmaps/e2e-watch-test-resource-version,UID:c95d93d2-97f4-4211-bc51-4ab6377bd1dd,ResourceVersion:16237,Generation:0,CreationTimestamp:2019-09-06 20:49:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:49:15.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3791" for this suite.
Sep  6 20:49:21.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:49:21.123: INFO: namespace watch-3791 deletion completed in 6.104682228s

• [SLOW TEST:6.263 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:49:21.124: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Sep  6 20:49:21.266: INFO: Waiting up to 5m0s for pod "client-containers-85e05172-1be4-4d97-8360-5402980eaf66" in namespace "containers-3902" to be "success or failure"
Sep  6 20:49:21.268: INFO: Pod "client-containers-85e05172-1be4-4d97-8360-5402980eaf66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.693324ms
Sep  6 20:49:23.272: INFO: Pod "client-containers-85e05172-1be4-4d97-8360-5402980eaf66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006756696s
STEP: Saw pod success
Sep  6 20:49:23.272: INFO: Pod "client-containers-85e05172-1be4-4d97-8360-5402980eaf66" satisfied condition "success or failure"
Sep  6 20:49:23.276: INFO: Trying to get logs from node appserv10 pod client-containers-85e05172-1be4-4d97-8360-5402980eaf66 container test-container: <nil>
STEP: delete the pod
Sep  6 20:49:23.294: INFO: Waiting for pod client-containers-85e05172-1be4-4d97-8360-5402980eaf66 to disappear
Sep  6 20:49:23.297: INFO: Pod client-containers-85e05172-1be4-4d97-8360-5402980eaf66 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:49:23.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3902" for this suite.
Sep  6 20:49:29.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:49:29.410: INFO: namespace containers-3902 deletion completed in 6.108725727s

• [SLOW TEST:8.286 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:49:29.411: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9917/configmap-test-05fb8d8b-de2c-440e-acc6-ecd3de52dee6
STEP: Creating a pod to test consume configMaps
Sep  6 20:49:29.558: INFO: Waiting up to 5m0s for pod "pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440" in namespace "configmap-9917" to be "success or failure"
Sep  6 20:49:29.561: INFO: Pod "pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.787947ms
Sep  6 20:49:31.565: INFO: Pod "pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006331601s
STEP: Saw pod success
Sep  6 20:49:31.565: INFO: Pod "pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440" satisfied condition "success or failure"
Sep  6 20:49:31.568: INFO: Trying to get logs from node appserv10 pod pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440 container env-test: <nil>
STEP: delete the pod
Sep  6 20:49:31.586: INFO: Waiting for pod pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440 to disappear
Sep  6 20:49:31.588: INFO: Pod pod-configmaps-30f53c7c-33b0-40bf-af45-7faeed3e1440 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:49:31.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9917" for this suite.
Sep  6 20:49:37.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:49:37.700: INFO: namespace configmap-9917 deletion completed in 6.107688753s

• [SLOW TEST:8.289 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:49:37.700: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:49:59.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6754" for this suite.
Sep  6 20:50:05.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:50:05.103: INFO: namespace container-runtime-6754 deletion completed in 6.093191051s

• [SLOW TEST:27.403 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:50:05.104: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 20:50:05.240: INFO: Waiting up to 5m0s for pod "pod-d41a542f-ecc7-4d44-846b-238e54b94c37" in namespace "emptydir-5541" to be "success or failure"
Sep  6 20:50:05.244: INFO: Pod "pod-d41a542f-ecc7-4d44-846b-238e54b94c37": Phase="Pending", Reason="", readiness=false. Elapsed: 3.768088ms
Sep  6 20:50:07.249: INFO: Pod "pod-d41a542f-ecc7-4d44-846b-238e54b94c37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00865267s
STEP: Saw pod success
Sep  6 20:50:07.249: INFO: Pod "pod-d41a542f-ecc7-4d44-846b-238e54b94c37" satisfied condition "success or failure"
Sep  6 20:50:07.252: INFO: Trying to get logs from node appserv9 pod pod-d41a542f-ecc7-4d44-846b-238e54b94c37 container test-container: <nil>
STEP: delete the pod
Sep  6 20:50:07.271: INFO: Waiting for pod pod-d41a542f-ecc7-4d44-846b-238e54b94c37 to disappear
Sep  6 20:50:07.273: INFO: Pod pod-d41a542f-ecc7-4d44-846b-238e54b94c37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:50:07.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5541" for this suite.
Sep  6 20:50:13.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:50:13.385: INFO: namespace emptydir-5541 deletion completed in 6.107270782s

• [SLOW TEST:8.281 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:50:13.386: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5547
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 20:50:18.553: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:50:19.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5547" for this suite.
Sep  6 20:50:41.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:50:41.670: INFO: namespace replicaset-5547 deletion completed in 22.09817122s

• [SLOW TEST:28.285 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:50:41.671: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 20:50:44.341: INFO: Successfully updated pod "labelsupdate193caad0-8ada-4a2b-8ec9-986a5b3b4d1c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:50:48.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7427" for this suite.
Sep  6 20:51:10.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:51:10.481: INFO: namespace projected-7427 deletion completed in 22.10593254s

• [SLOW TEST:28.811 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:51:10.481: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:51:10.637: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"979ae017-a7cb-40b9-b65b-8686f69312d7", Controller:(*bool)(0xc0036679e6), BlockOwnerDeletion:(*bool)(0xc0036679e7)}}
Sep  6 20:51:10.641: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0fac452e-6aad-4160-b14c-22b5b40dbb40", Controller:(*bool)(0xc001e4bd66), BlockOwnerDeletion:(*bool)(0xc001e4bd67)}}
Sep  6 20:51:10.645: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d21e2459-af85-4121-8a3c-c07c73933346", Controller:(*bool)(0xc001f4a9a6), BlockOwnerDeletion:(*bool)(0xc001f4a9a7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:51:15.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9535" for this suite.
Sep  6 20:51:21.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:51:21.761: INFO: namespace gc-9535 deletion completed in 6.103693043s

• [SLOW TEST:11.279 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:51:21.761: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9c6214e5-1e46-4e6c-878e-8d7e2b2540b3
STEP: Creating a pod to test consume configMaps
Sep  6 20:51:21.906: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1" in namespace "configmap-9929" to be "success or failure"
Sep  6 20:51:21.909: INFO: Pod "pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774945ms
Sep  6 20:51:23.913: INFO: Pod "pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007217998s
Sep  6 20:51:25.917: INFO: Pod "pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011275441s
STEP: Saw pod success
Sep  6 20:51:25.917: INFO: Pod "pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1" satisfied condition "success or failure"
Sep  6 20:51:25.920: INFO: Trying to get logs from node appserv11 pod pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 20:51:25.940: INFO: Waiting for pod pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1 to disappear
Sep  6 20:51:25.943: INFO: Pod pod-configmaps-1d7e1a75-acc8-40fa-a458-1529780ca1e1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:51:25.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9929" for this suite.
Sep  6 20:51:31.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:51:32.053: INFO: namespace configmap-9929 deletion completed in 6.105936623s

• [SLOW TEST:10.292 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:51:32.053: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-a5f2eb7a-ebec-4586-b9a9-95f8690f2790
STEP: Creating a pod to test consume secrets
Sep  6 20:51:32.200: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812" in namespace "projected-3996" to be "success or failure"
Sep  6 20:51:32.203: INFO: Pod "pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671923ms
Sep  6 20:51:34.207: INFO: Pod "pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006711139s
STEP: Saw pod success
Sep  6 20:51:34.207: INFO: Pod "pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812" satisfied condition "success or failure"
Sep  6 20:51:34.210: INFO: Trying to get logs from node appserv9 pod pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 20:51:34.229: INFO: Waiting for pod pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812 to disappear
Sep  6 20:51:34.232: INFO: Pod pod-projected-secrets-4eede75c-d6f0-485e-90a0-2febaa897812 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:51:34.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3996" for this suite.
Sep  6 20:51:40.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:51:40.342: INFO: namespace projected-3996 deletion completed in 6.105126821s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:51:40.342: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Sep  6 20:51:40.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 cluster-info'
Sep  6 20:51:40.603: INFO: stderr: ""
Sep  6 20:51:40.603: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:51:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9720" for this suite.
Sep  6 20:51:46.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:51:46.710: INFO: namespace kubectl-9720 deletion completed in 6.102753118s

• [SLOW TEST:6.368 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:51:46.711: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8523
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-849ad78d-b677-4fd9-85d4-0f965126d07a
STEP: Creating configMap with name cm-test-opt-upd-2457f2d8-0219-43e9-a6cd-e276308c1abe
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-849ad78d-b677-4fd9-85d4-0f965126d07a
STEP: Updating configmap cm-test-opt-upd-2457f2d8-0219-43e9-a6cd-e276308c1abe
STEP: Creating configMap with name cm-test-opt-create-23869b2c-c456-4789-bdd2-0ee982e5e413
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:53:03.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8523" for this suite.
Sep  6 20:53:25.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:53:25.444: INFO: namespace configmap-8523 deletion completed in 22.105369927s

• [SLOW TEST:98.733 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:53:25.444: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-29968930-b640-449c-85b4-3ff8f7e98225
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:53:25.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8739" for this suite.
Sep  6 20:53:31.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:53:31.689: INFO: namespace secrets-8739 deletion completed in 6.099914576s

• [SLOW TEST:6.245 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:53:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Sep  6 20:53:31.824: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-256622946 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:53:31.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3341" for this suite.
Sep  6 20:53:37.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:53:38.043: INFO: namespace kubectl-3341 deletion completed in 6.111572778s

• [SLOW TEST:6.354 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:53:38.044: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Sep  6 20:53:38.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-8032'
Sep  6 20:53:38.409: INFO: stderr: ""
Sep  6 20:53:38.409: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 20:53:38.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8032'
Sep  6 20:53:38.512: INFO: stderr: ""
Sep  6 20:53:38.512: INFO: stdout: "update-demo-nautilus-jnrs7 update-demo-nautilus-n2fnm "
Sep  6 20:53:38.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-jnrs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:53:38.609: INFO: stderr: ""
Sep  6 20:53:38.609: INFO: stdout: ""
Sep  6 20:53:38.609: INFO: update-demo-nautilus-jnrs7 is created but not running
Sep  6 20:53:43.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8032'
Sep  6 20:53:43.735: INFO: stderr: ""
Sep  6 20:53:43.735: INFO: stdout: "update-demo-nautilus-jnrs7 update-demo-nautilus-n2fnm "
Sep  6 20:53:43.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-jnrs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:53:43.847: INFO: stderr: ""
Sep  6 20:53:43.847: INFO: stdout: "true"
Sep  6 20:53:43.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-jnrs7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:53:43.949: INFO: stderr: ""
Sep  6 20:53:43.949: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:53:43.949: INFO: validating pod update-demo-nautilus-jnrs7
Sep  6 20:53:43.955: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:53:43.955: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:53:43.955: INFO: update-demo-nautilus-jnrs7 is verified up and running
Sep  6 20:53:43.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-n2fnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:53:44.053: INFO: stderr: ""
Sep  6 20:53:44.053: INFO: stdout: "true"
Sep  6 20:53:44.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-n2fnm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:53:44.153: INFO: stderr: ""
Sep  6 20:53:44.154: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 20:53:44.154: INFO: validating pod update-demo-nautilus-n2fnm
Sep  6 20:53:44.159: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 20:53:44.159: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 20:53:44.159: INFO: update-demo-nautilus-n2fnm is verified up and running
STEP: rolling-update to new replication controller
Sep  6 20:53:44.161: INFO: scanned /root for discovery docs: <nil>
Sep  6 20:53:44.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8032'
Sep  6 20:54:06.558: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 20:54:06.558: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 20:54:06.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8032'
Sep  6 20:54:06.678: INFO: stderr: ""
Sep  6 20:54:06.678: INFO: stdout: "update-demo-kitten-kznvg update-demo-kitten-p86b9 "
Sep  6 20:54:06.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-kitten-kznvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:54:06.796: INFO: stderr: ""
Sep  6 20:54:06.796: INFO: stdout: "true"
Sep  6 20:54:06.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-kitten-kznvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:54:06.895: INFO: stderr: ""
Sep  6 20:54:06.895: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 20:54:06.895: INFO: validating pod update-demo-kitten-kznvg
Sep  6 20:54:06.901: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 20:54:06.901: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 20:54:06.901: INFO: update-demo-kitten-kznvg is verified up and running
Sep  6 20:54:06.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-kitten-p86b9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:54:07.009: INFO: stderr: ""
Sep  6 20:54:07.009: INFO: stdout: "true"
Sep  6 20:54:07.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-kitten-p86b9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8032'
Sep  6 20:54:07.121: INFO: stderr: ""
Sep  6 20:54:07.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 20:54:07.121: INFO: validating pod update-demo-kitten-p86b9
Sep  6 20:54:07.127: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 20:54:07.127: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 20:54:07.127: INFO: update-demo-kitten-p86b9 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:54:07.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8032" for this suite.
Sep  6 20:54:29.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:54:29.252: INFO: namespace kubectl-8032 deletion completed in 22.119869317s

• [SLOW TEST:51.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:54:29.253: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 20:54:29.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4765'
Sep  6 20:54:29.511: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 20:54:29.511: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Sep  6 20:54:29.518: INFO: scanned /root for discovery docs: <nil>
Sep  6 20:54:29.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4765'
Sep  6 20:54:45.313: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 20:54:45.313: INFO: stdout: "Created e2e-test-nginx-rc-56f280076c8a96548b0644582855e018\nScaling up e2e-test-nginx-rc-56f280076c8a96548b0644582855e018 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-56f280076c8a96548b0644582855e018 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-56f280076c8a96548b0644582855e018 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep  6 20:54:45.313: INFO: stdout: "Created e2e-test-nginx-rc-56f280076c8a96548b0644582855e018\nScaling up e2e-test-nginx-rc-56f280076c8a96548b0644582855e018 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-56f280076c8a96548b0644582855e018 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-56f280076c8a96548b0644582855e018 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep  6 20:54:45.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4765'
Sep  6 20:54:45.438: INFO: stderr: ""
Sep  6 20:54:45.438: INFO: stdout: "e2e-test-nginx-rc-56f280076c8a96548b0644582855e018-bpbjc e2e-test-nginx-rc-696t6 "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Sep  6 20:54:50.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4765'
Sep  6 20:54:50.550: INFO: stderr: ""
Sep  6 20:54:50.550: INFO: stdout: "e2e-test-nginx-rc-56f280076c8a96548b0644582855e018-bpbjc "
Sep  6 20:54:50.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods e2e-test-nginx-rc-56f280076c8a96548b0644582855e018-bpbjc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4765'
Sep  6 20:54:50.632: INFO: stderr: ""
Sep  6 20:54:50.632: INFO: stdout: "true"
Sep  6 20:54:50.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods e2e-test-nginx-rc-56f280076c8a96548b0644582855e018-bpbjc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4765'
Sep  6 20:54:50.742: INFO: stderr: ""
Sep  6 20:54:50.742: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep  6 20:54:50.742: INFO: e2e-test-nginx-rc-56f280076c8a96548b0644582855e018-bpbjc is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Sep  6 20:54:50.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete rc e2e-test-nginx-rc --namespace=kubectl-4765'
Sep  6 20:54:50.833: INFO: stderr: ""
Sep  6 20:54:50.833: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:54:50.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4765" for this suite.
Sep  6 20:55:12.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:55:12.947: INFO: namespace kubectl-4765 deletion completed in 22.109154739s

• [SLOW TEST:43.694 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:55:12.947: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-678
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-aaf482b5-8681-49e7-a42e-dc0c7299d5ca
STEP: Creating secret with name s-test-opt-upd-e42f9256-df7c-432b-bdf7-3715c09e07b6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-aaf482b5-8681-49e7-a42e-dc0c7299d5ca
STEP: Updating secret s-test-opt-upd-e42f9256-df7c-432b-bdf7-3715c09e07b6
STEP: Creating secret with name s-test-opt-create-ed6a872e-06af-4eea-a129-42493b01bf91
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:56:29.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-678" for this suite.
Sep  6 20:56:45.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:56:45.670: INFO: namespace projected-678 deletion completed in 16.10643378s

• [SLOW TEST:92.723 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:56:45.670: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 20:56:45.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-909'
Sep  6 20:56:45.994: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 20:56:45.995: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep  6 20:56:46.000: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-5m988]
Sep  6 20:56:46.000: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-5m988" in namespace "kubectl-909" to be "running and ready"
Sep  6 20:56:46.003: INFO: Pod "e2e-test-nginx-rc-5m988": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389628ms
Sep  6 20:56:48.007: INFO: Pod "e2e-test-nginx-rc-5m988": Phase="Running", Reason="", readiness=true. Elapsed: 2.006917825s
Sep  6 20:56:48.007: INFO: Pod "e2e-test-nginx-rc-5m988" satisfied condition "running and ready"
Sep  6 20:56:48.007: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-5m988]
Sep  6 20:56:48.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 logs rc/e2e-test-nginx-rc --namespace=kubectl-909'
Sep  6 20:56:48.156: INFO: stderr: ""
Sep  6 20:56:48.156: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Sep  6 20:56:48.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete rc e2e-test-nginx-rc --namespace=kubectl-909'
Sep  6 20:56:48.273: INFO: stderr: ""
Sep  6 20:56:48.273: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:56:48.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-909" for this suite.
Sep  6 20:57:10.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:57:10.385: INFO: namespace kubectl-909 deletion completed in 22.107250961s

• [SLOW TEST:24.715 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:57:10.385: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6321
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep  6 20:57:10.530: INFO: Found 0 stateful pods, waiting for 3
Sep  6 20:57:20.536: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:57:20.536: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:57:20.536: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 20:57:20.564: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  6 20:57:30.594: INFO: Updating stateful set ss2
Sep  6 20:57:30.600: INFO: Waiting for Pod statefulset-6321/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Sep  6 20:57:40.633: INFO: Found 2 stateful pods, waiting for 3
Sep  6 20:57:50.638: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:57:50.638: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 20:57:50.638: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  6 20:57:50.662: INFO: Updating stateful set ss2
Sep  6 20:57:50.668: INFO: Waiting for Pod statefulset-6321/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 20:58:00.693: INFO: Updating stateful set ss2
Sep  6 20:58:00.700: INFO: Waiting for StatefulSet statefulset-6321/ss2 to complete update
Sep  6 20:58:00.700: INFO: Waiting for Pod statefulset-6321/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 20:58:10.707: INFO: Waiting for StatefulSet statefulset-6321/ss2 to complete update
Sep  6 20:58:10.707: INFO: Waiting for Pod statefulset-6321/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 20:58:20.707: INFO: Deleting all statefulset in ns statefulset-6321
Sep  6 20:58:20.710: INFO: Scaling statefulset ss2 to 0
Sep  6 20:58:50.725: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 20:58:50.730: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:58:50.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6321" for this suite.
Sep  6 20:58:56.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:58:56.850: INFO: namespace statefulset-6321 deletion completed in 6.103607685s

• [SLOW TEST:106.465 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:58:56.850: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 20:58:59.013: INFO: Waiting up to 5m0s for pod "client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e" in namespace "pods-1414" to be "success or failure"
Sep  6 20:58:59.016: INFO: Pod "client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802949ms
Sep  6 20:59:01.019: INFO: Pod "client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00613009s
STEP: Saw pod success
Sep  6 20:59:01.019: INFO: Pod "client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e" satisfied condition "success or failure"
Sep  6 20:59:01.022: INFO: Trying to get logs from node appserv10 pod client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e container env3cont: <nil>
STEP: delete the pod
Sep  6 20:59:01.042: INFO: Waiting for pod client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e to disappear
Sep  6 20:59:01.045: INFO: Pod client-envvars-2aa7bb1a-5b1e-477d-9de7-1a5478a7ed4e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 20:59:01.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1414" for this suite.
Sep  6 20:59:47.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 20:59:47.165: INFO: namespace pods-1414 deletion completed in 46.115509315s

• [SLOW TEST:50.315 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 20:59:47.165: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1352
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7346
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:00:11.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2828" for this suite.
Sep  6 21:00:17.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:00:17.705: INFO: namespace namespaces-2828 deletion completed in 6.109420518s
STEP: Destroying namespace "nsdeletetest-1352" for this suite.
Sep  6 21:00:17.707: INFO: Namespace nsdeletetest-1352 was already deleted
STEP: Destroying namespace "nsdeletetest-7346" for this suite.
Sep  6 21:00:23.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:00:23.814: INFO: namespace nsdeletetest-7346 deletion completed in 6.106635893s

• [SLOW TEST:36.649 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:00:23.814: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3527
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3527
STEP: Creating statefulset with conflicting port in namespace statefulset-3527
STEP: Waiting until pod test-pod will start running in namespace statefulset-3527
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3527
Sep  6 21:00:27.980: INFO: Observed stateful pod in namespace: statefulset-3527, name: ss-0, uid: f30881d9-71fc-41e9-9b7e-a1bfe7a30e4c, status phase: Pending. Waiting for statefulset controller to delete.
Sep  6 21:00:28.174: INFO: Observed stateful pod in namespace: statefulset-3527, name: ss-0, uid: f30881d9-71fc-41e9-9b7e-a1bfe7a30e4c, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 21:00:28.184: INFO: Observed stateful pod in namespace: statefulset-3527, name: ss-0, uid: f30881d9-71fc-41e9-9b7e-a1bfe7a30e4c, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 21:00:28.188: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3527
STEP: Removing pod with conflicting port in namespace statefulset-3527
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3527 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 21:00:32.207: INFO: Deleting all statefulset in ns statefulset-3527
Sep  6 21:00:32.210: INFO: Scaling statefulset ss to 0
Sep  6 21:00:42.224: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 21:00:42.227: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:00:42.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3527" for this suite.
Sep  6 21:00:48.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:00:48.357: INFO: namespace statefulset-3527 deletion completed in 6.114251772s

• [SLOW TEST:24.543 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:00:48.358: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 21:00:48.501: INFO: Waiting up to 5m0s for pod "downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649" in namespace "downward-api-6037" to be "success or failure"
Sep  6 21:00:48.504: INFO: Pod "downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649": Phase="Pending", Reason="", readiness=false. Elapsed: 2.851684ms
Sep  6 21:00:50.508: INFO: Pod "downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00626574s
STEP: Saw pod success
Sep  6 21:00:50.508: INFO: Pod "downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649" satisfied condition "success or failure"
Sep  6 21:00:50.511: INFO: Trying to get logs from node appserv10 pod downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649 container dapi-container: <nil>
STEP: delete the pod
Sep  6 21:00:50.530: INFO: Waiting for pod downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649 to disappear
Sep  6 21:00:50.533: INFO: Pod downward-api-0952d98a-8779-4eb7-b5c1-5da1b823a649 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:00:50.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6037" for this suite.
Sep  6 21:00:56.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:00:56.634: INFO: namespace downward-api-6037 deletion completed in 6.0971476s

• [SLOW TEST:8.277 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:00:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-e72d9ca5-701d-48b6-82e0-f24ba42bd181
STEP: Creating a pod to test consume secrets
Sep  6 21:00:56.776: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522" in namespace "projected-7045" to be "success or failure"
Sep  6 21:00:56.778: INFO: Pod "pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146371ms
Sep  6 21:00:58.782: INFO: Pod "pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006142788s
STEP: Saw pod success
Sep  6 21:00:58.782: INFO: Pod "pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522" satisfied condition "success or failure"
Sep  6 21:00:58.785: INFO: Trying to get logs from node appserv9 pod pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 21:00:58.804: INFO: Waiting for pod pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522 to disappear
Sep  6 21:00:58.807: INFO: Pod pod-projected-secrets-dc78e1e9-62c4-4725-90e7-06af79caa522 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:00:58.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7045" for this suite.
Sep  6 21:01:04.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:01:04.911: INFO: namespace projected-7045 deletion completed in 6.099172684s

• [SLOW TEST:8.277 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:01:04.911: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 21:01:07.063: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:01:07.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-234" for this suite.
Sep  6 21:01:13.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:01:13.188: INFO: namespace container-runtime-234 deletion completed in 6.109174785s

• [SLOW TEST:8.277 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:01:13.188: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 21:01:15.863: INFO: Successfully updated pod "annotationupdate7ea72c91-d899-4b7d-a992-21386719c8e5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:01:17.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7006" for this suite.
Sep  6 21:01:39.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:01:39.998: INFO: namespace downward-api-7006 deletion completed in 22.111159884s

• [SLOW TEST:26.809 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:01:39.998: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 21:01:44.182: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 21:01:44.186: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 21:01:46.186: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 21:01:46.189: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 21:01:48.186: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 21:01:48.190: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 21:01:50.186: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 21:01:50.189: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 21:01:52.186: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 21:01:52.190: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 21:01:54.186: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 21:01:54.189: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:01:54.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5830" for this suite.
Sep  6 21:02:16.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:02:16.300: INFO: namespace container-lifecycle-hook-5830 deletion completed in 22.106250128s

• [SLOW TEST:36.302 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:02:16.300: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 21:02:20.479: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:20.483: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:22.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:22.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:24.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:24.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:26.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:26.486: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:28.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:28.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:30.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:30.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:32.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:32.486: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:34.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:34.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:36.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:36.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:38.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:38.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:40.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:40.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:42.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:42.487: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 21:02:44.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 21:02:44.486: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:02:44.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4478" for this suite.
Sep  6 21:03:06.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:03:06.592: INFO: namespace container-lifecycle-hook-4478 deletion completed in 22.101963599s

• [SLOW TEST:50.292 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:03:06.593: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep  6 21:03:37.262: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0906 21:03:37.262063      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 21:03:37.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7994" for this suite.
Sep  6 21:03:43.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:03:43.373: INFO: namespace gc-7994 deletion completed in 6.107389834s

• [SLOW TEST:36.780 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:03:43.373: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9785.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 148.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.148_udp@PTR;check="$$(dig +tcp +noall +answer +search 148.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.148_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9785.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9785.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9785.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9785.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9785.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 148.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.148_udp@PTR;check="$$(dig +tcp +noall +answer +search 148.0.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.0.148_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 21:03:47.613: INFO: DNS probes using dns-9785/dns-test-7d9339d0-9c57-43d2-b1f4-168a967aa25e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:03:47.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9785" for this suite.
Sep  6 21:03:53.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:03:53.764: INFO: namespace dns-9785 deletion completed in 6.10995409s

• [SLOW TEST:10.391 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:03:53.765: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9351
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:03:53.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9351" for this suite.
Sep  6 21:04:15.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:04:16.014: INFO: namespace pods-9351 deletion completed in 22.100505608s

• [SLOW TEST:22.250 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:04:16.015: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-b41e0de2-7e3b-45ff-ba06-5963193cec13
STEP: Creating a pod to test consume secrets
Sep  6 21:04:16.157: INFO: Waiting up to 5m0s for pod "pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5" in namespace "secrets-5044" to be "success or failure"
Sep  6 21:04:16.159: INFO: Pod "pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315876ms
Sep  6 21:04:18.163: INFO: Pod "pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005855471s
Sep  6 21:04:20.167: INFO: Pod "pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00969178s
STEP: Saw pod success
Sep  6 21:04:20.167: INFO: Pod "pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5" satisfied condition "success or failure"
Sep  6 21:04:20.170: INFO: Trying to get logs from node appserv10 pod pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 21:04:20.190: INFO: Waiting for pod pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5 to disappear
Sep  6 21:04:20.193: INFO: Pod pod-secrets-ed8ab371-fd9f-4327-8981-bc4835e2f5a5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:04:20.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5044" for this suite.
Sep  6 21:04:26.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:04:26.300: INFO: namespace secrets-5044 deletion completed in 6.103417342s

• [SLOW TEST:10.285 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:04:26.300: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep  6 21:04:26.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-961'
Sep  6 21:04:26.654: INFO: stderr: ""
Sep  6 21:04:26.655: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 21:04:26.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-961'
Sep  6 21:04:26.766: INFO: stderr: ""
Sep  6 21:04:26.766: INFO: stdout: "update-demo-nautilus-7txp4 update-demo-nautilus-t6hcm "
Sep  6 21:04:26.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-7txp4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-961'
Sep  6 21:04:26.877: INFO: stderr: ""
Sep  6 21:04:26.877: INFO: stdout: ""
Sep  6 21:04:26.877: INFO: update-demo-nautilus-7txp4 is created but not running
Sep  6 21:04:31.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-961'
Sep  6 21:04:31.987: INFO: stderr: ""
Sep  6 21:04:31.987: INFO: stdout: "update-demo-nautilus-7txp4 update-demo-nautilus-t6hcm "
Sep  6 21:04:31.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-7txp4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-961'
Sep  6 21:04:32.097: INFO: stderr: ""
Sep  6 21:04:32.097: INFO: stdout: "true"
Sep  6 21:04:32.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-7txp4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-961'
Sep  6 21:04:32.190: INFO: stderr: ""
Sep  6 21:04:32.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 21:04:32.190: INFO: validating pod update-demo-nautilus-7txp4
Sep  6 21:04:32.198: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 21:04:32.198: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 21:04:32.198: INFO: update-demo-nautilus-7txp4 is verified up and running
Sep  6 21:04:32.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-t6hcm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-961'
Sep  6 21:04:32.294: INFO: stderr: ""
Sep  6 21:04:32.294: INFO: stdout: "true"
Sep  6 21:04:32.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods update-demo-nautilus-t6hcm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-961'
Sep  6 21:04:32.385: INFO: stderr: ""
Sep  6 21:04:32.385: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 21:04:32.385: INFO: validating pod update-demo-nautilus-t6hcm
Sep  6 21:04:32.390: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 21:04:32.390: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 21:04:32.390: INFO: update-demo-nautilus-t6hcm is verified up and running
STEP: using delete to clean up resources
Sep  6 21:04:32.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete --grace-period=0 --force -f - --namespace=kubectl-961'
Sep  6 21:04:32.477: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 21:04:32.477: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 21:04:32.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-961'
Sep  6 21:04:32.577: INFO: stderr: "No resources found.\n"
Sep  6 21:04:32.577: INFO: stdout: ""
Sep  6 21:04:32.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -l name=update-demo --namespace=kubectl-961 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 21:04:32.681: INFO: stderr: ""
Sep  6 21:04:32.681: INFO: stdout: "update-demo-nautilus-7txp4\nupdate-demo-nautilus-t6hcm\n"
Sep  6 21:04:33.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-961'
Sep  6 21:04:33.301: INFO: stderr: "No resources found.\n"
Sep  6 21:04:33.301: INFO: stdout: ""
Sep  6 21:04:33.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 get pods -l name=update-demo --namespace=kubectl-961 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 21:04:33.407: INFO: stderr: ""
Sep  6 21:04:33.407: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:04:33.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-961" for this suite.
Sep  6 21:04:55.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:04:55.516: INFO: namespace kubectl-961 deletion completed in 22.1051767s

• [SLOW TEST:29.216 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:04:55.517: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  6 21:04:55.659: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19909,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 21:04:55.659: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19909,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  6 21:05:05.666: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19929,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 21:05:05.667: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19929,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  6 21:05:15.674: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19948,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 21:05:15.675: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19948,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  6 21:05:25.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19968,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 21:05:25.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-a,UID:ab7c6405-d785-4f61-81ae-8a1c19ce3192,ResourceVersion:19968,Generation:0,CreationTimestamp:2019-09-06 21:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  6 21:05:35.689: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-b,UID:ba3f6ce2-ece2-4b2e-964c-66fe30e0d28e,ResourceVersion:19986,Generation:0,CreationTimestamp:2019-09-06 21:05:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 21:05:35.689: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-b,UID:ba3f6ce2-ece2-4b2e-964c-66fe30e0d28e,ResourceVersion:19986,Generation:0,CreationTimestamp:2019-09-06 21:05:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  6 21:05:45.696: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-b,UID:ba3f6ce2-ece2-4b2e-964c-66fe30e0d28e,ResourceVersion:20005,Generation:0,CreationTimestamp:2019-09-06 21:05:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 21:05:45.696: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9659,SelfLink:/api/v1/namespaces/watch-9659/configmaps/e2e-watch-test-configmap-b,UID:ba3f6ce2-ece2-4b2e-964c-66fe30e0d28e,ResourceVersion:20005,Generation:0,CreationTimestamp:2019-09-06 21:05:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:05:55.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9659" for this suite.
Sep  6 21:06:01.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:06:01.805: INFO: namespace watch-9659 deletion completed in 6.104465698s

• [SLOW TEST:66.288 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:06:01.806: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep  6 21:06:01.941: INFO: namespace kubectl-5696
Sep  6 21:06:01.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 create -f - --namespace=kubectl-5696'
Sep  6 21:06:02.159: INFO: stderr: ""
Sep  6 21:06:02.159: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 21:06:03.163: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 21:06:03.164: INFO: Found 0 / 1
Sep  6 21:06:04.163: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 21:06:04.163: INFO: Found 1 / 1
Sep  6 21:06:04.163: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 21:06:04.166: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 21:06:04.166: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 21:06:04.166: INFO: wait on redis-master startup in kubectl-5696 
Sep  6 21:06:04.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 logs redis-master-7mngq redis-master --namespace=kubectl-5696'
Sep  6 21:06:04.306: INFO: stderr: ""
Sep  6 21:06:04.306: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 21:06:03.410 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 21:06:03.411 # Server started, Redis version 3.2.12\n1:M 06 Sep 21:06:03.411 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 21:06:03.411 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep  6 21:06:04.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5696'
Sep  6 21:06:04.447: INFO: stderr: ""
Sep  6 21:06:04.447: INFO: stdout: "service/rm2 exposed\n"
Sep  6 21:06:04.450: INFO: Service rm2 in namespace kubectl-5696 found.
STEP: exposing service
Sep  6 21:06:06.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5696'
Sep  6 21:06:06.593: INFO: stderr: ""
Sep  6 21:06:06.593: INFO: stdout: "service/rm3 exposed\n"
Sep  6 21:06:06.595: INFO: Service rm3 in namespace kubectl-5696 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:06:08.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5696" for this suite.
Sep  6 21:06:30.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:06:30.711: INFO: namespace kubectl-5696 deletion completed in 22.10550371s

• [SLOW TEST:28.906 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:06:30.713: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 21:06:30.850: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:06:34.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4399" for this suite.
Sep  6 21:06:40.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:06:40.354: INFO: namespace init-container-4399 deletion completed in 6.107852323s

• [SLOW TEST:9.641 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:06:40.354: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 21:06:40.506: INFO: Waiting up to 5m0s for pod "downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1" in namespace "downward-api-1486" to be "success or failure"
Sep  6 21:06:40.508: INFO: Pod "downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687305ms
Sep  6 21:06:42.512: INFO: Pod "downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006351918s
STEP: Saw pod success
Sep  6 21:06:42.512: INFO: Pod "downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1" satisfied condition "success or failure"
Sep  6 21:06:42.515: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1 container client-container: <nil>
STEP: delete the pod
Sep  6 21:06:42.533: INFO: Waiting for pod downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1 to disappear
Sep  6 21:06:42.536: INFO: Pod downwardapi-volume-569960e6-2489-4ffb-8ff8-a26ef47f77d1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:06:42.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1486" for this suite.
Sep  6 21:06:48.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:06:48.647: INFO: namespace downward-api-1486 deletion completed in 6.106694471s

• [SLOW TEST:8.293 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:06:48.647: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9725
I0906 21:06:48.785580      19 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9725, replica count: 1
I0906 21:06:49.836528      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 21:06:50.836808      19 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 21:06:50.944: INFO: Created: latency-svc-cwsdx
Sep  6 21:06:50.949: INFO: Got endpoints: latency-svc-cwsdx [11.99934ms]
Sep  6 21:06:50.955: INFO: Created: latency-svc-jbmvs
Sep  6 21:06:50.960: INFO: Created: latency-svc-prffq
Sep  6 21:06:50.960: INFO: Got endpoints: latency-svc-jbmvs [11.320512ms]
Sep  6 21:06:50.962: INFO: Created: latency-svc-zhqsg
Sep  6 21:06:50.963: INFO: Got endpoints: latency-svc-prffq [14.564966ms]
Sep  6 21:06:50.965: INFO: Got endpoints: latency-svc-zhqsg [16.383015ms]
Sep  6 21:06:50.966: INFO: Created: latency-svc-jj9mj
Sep  6 21:06:50.969: INFO: Created: latency-svc-qmcsp
Sep  6 21:06:50.969: INFO: Got endpoints: latency-svc-jj9mj [20.567878ms]
Sep  6 21:06:50.972: INFO: Got endpoints: latency-svc-qmcsp [23.316763ms]
Sep  6 21:06:50.972: INFO: Created: latency-svc-jxpf2
Sep  6 21:06:50.975: INFO: Got endpoints: latency-svc-jxpf2 [26.531834ms]
Sep  6 21:06:50.975: INFO: Created: latency-svc-rnvzr
Sep  6 21:06:50.978: INFO: Got endpoints: latency-svc-rnvzr [29.383406ms]
Sep  6 21:06:50.979: INFO: Created: latency-svc-8dtvs
Sep  6 21:06:50.981: INFO: Got endpoints: latency-svc-8dtvs [32.736783ms]
Sep  6 21:06:50.982: INFO: Created: latency-svc-92d2f
Sep  6 21:06:50.984: INFO: Got endpoints: latency-svc-92d2f [35.615561ms]
Sep  6 21:06:50.985: INFO: Created: latency-svc-b5b27
Sep  6 21:06:50.988: INFO: Got endpoints: latency-svc-b5b27 [39.018577ms]
Sep  6 21:06:50.988: INFO: Created: latency-svc-fd5mm
Sep  6 21:06:50.991: INFO: Got endpoints: latency-svc-fd5mm [42.024499ms]
Sep  6 21:06:50.991: INFO: Created: latency-svc-2xkt7
Sep  6 21:06:50.994: INFO: Got endpoints: latency-svc-2xkt7 [45.069971ms]
Sep  6 21:06:50.994: INFO: Created: latency-svc-v2q5t
Sep  6 21:06:50.997: INFO: Got endpoints: latency-svc-v2q5t [48.59285ms]
Sep  6 21:06:50.998: INFO: Created: latency-svc-mdglx
Sep  6 21:06:51.001: INFO: Created: latency-svc-7jx9k
Sep  6 21:06:51.001: INFO: Got endpoints: latency-svc-mdglx [51.889452ms]
Sep  6 21:06:51.003: INFO: Got endpoints: latency-svc-7jx9k [54.214443ms]
Sep  6 21:06:51.004: INFO: Created: latency-svc-vqs79
Sep  6 21:06:51.007: INFO: Got endpoints: latency-svc-vqs79 [46.886879ms]
Sep  6 21:06:51.007: INFO: Created: latency-svc-d76sd
Sep  6 21:06:51.010: INFO: Created: latency-svc-dvwt7
Sep  6 21:06:51.010: INFO: Got endpoints: latency-svc-d76sd [47.12177ms]
Sep  6 21:06:51.013: INFO: Got endpoints: latency-svc-dvwt7 [47.767401ms]
Sep  6 21:06:51.013: INFO: Created: latency-svc-lwbk5
Sep  6 21:06:51.016: INFO: Got endpoints: latency-svc-lwbk5 [46.731585ms]
Sep  6 21:06:51.016: INFO: Created: latency-svc-xnxjm
Sep  6 21:06:51.019: INFO: Got endpoints: latency-svc-xnxjm [46.711982ms]
Sep  6 21:06:51.019: INFO: Created: latency-svc-mgfnk
Sep  6 21:06:51.022: INFO: Got endpoints: latency-svc-mgfnk [46.365656ms]
Sep  6 21:06:51.022: INFO: Created: latency-svc-gv95j
Sep  6 21:06:51.025: INFO: Created: latency-svc-4gb5j
Sep  6 21:06:51.025: INFO: Got endpoints: latency-svc-gv95j [46.527057ms]
Sep  6 21:06:51.027: INFO: Got endpoints: latency-svc-4gb5j [45.798452ms]
Sep  6 21:06:51.028: INFO: Created: latency-svc-fcwbk
Sep  6 21:06:51.030: INFO: Got endpoints: latency-svc-fcwbk [45.566553ms]
Sep  6 21:06:51.030: INFO: Created: latency-svc-hs6pj
Sep  6 21:06:51.033: INFO: Got endpoints: latency-svc-hs6pj [45.369497ms]
Sep  6 21:06:51.033: INFO: Created: latency-svc-hk2r5
Sep  6 21:06:51.036: INFO: Got endpoints: latency-svc-hk2r5 [44.817664ms]
Sep  6 21:06:51.036: INFO: Created: latency-svc-7c7q2
Sep  6 21:06:51.039: INFO: Got endpoints: latency-svc-7c7q2 [44.862725ms]
Sep  6 21:06:51.039: INFO: Created: latency-svc-qmx2h
Sep  6 21:06:51.042: INFO: Got endpoints: latency-svc-qmx2h [44.306652ms]
Sep  6 21:06:51.042: INFO: Created: latency-svc-l72m7
Sep  6 21:06:51.044: INFO: Got endpoints: latency-svc-l72m7 [43.715229ms]
Sep  6 21:06:51.045: INFO: Created: latency-svc-mrbk6
Sep  6 21:06:51.047: INFO: Got endpoints: latency-svc-mrbk6 [43.938336ms]
Sep  6 21:06:51.048: INFO: Created: latency-svc-k7924
Sep  6 21:06:51.050: INFO: Got endpoints: latency-svc-k7924 [43.477169ms]
Sep  6 21:06:51.050: INFO: Created: latency-svc-xl25g
Sep  6 21:06:51.053: INFO: Created: latency-svc-4nr4f
Sep  6 21:06:51.056: INFO: Created: latency-svc-n84gp
Sep  6 21:06:51.058: INFO: Created: latency-svc-kfxk9
Sep  6 21:06:51.061: INFO: Created: latency-svc-vnh7x
Sep  6 21:06:51.063: INFO: Created: latency-svc-lnwz9
Sep  6 21:06:51.066: INFO: Created: latency-svc-8vqmz
Sep  6 21:06:51.068: INFO: Created: latency-svc-p6cd4
Sep  6 21:06:51.071: INFO: Created: latency-svc-j5vfb
Sep  6 21:06:51.073: INFO: Created: latency-svc-p7f77
Sep  6 21:06:51.076: INFO: Created: latency-svc-gsmfv
Sep  6 21:06:51.078: INFO: Created: latency-svc-cgfb4
Sep  6 21:06:51.081: INFO: Created: latency-svc-sfn95
Sep  6 21:06:51.084: INFO: Created: latency-svc-snb4d
Sep  6 21:06:51.086: INFO: Created: latency-svc-f6vx6
Sep  6 21:06:51.097: INFO: Got endpoints: latency-svc-xl25g [86.850016ms]
Sep  6 21:06:51.102: INFO: Created: latency-svc-zfmc2
Sep  6 21:06:51.148: INFO: Got endpoints: latency-svc-4nr4f [134.785546ms]
Sep  6 21:06:51.153: INFO: Created: latency-svc-4qv4w
Sep  6 21:06:51.197: INFO: Got endpoints: latency-svc-n84gp [181.313861ms]
Sep  6 21:06:51.203: INFO: Created: latency-svc-ftcc6
Sep  6 21:06:51.254: INFO: Got endpoints: latency-svc-kfxk9 [234.678542ms]
Sep  6 21:06:51.260: INFO: Created: latency-svc-hrc7j
Sep  6 21:06:51.298: INFO: Got endpoints: latency-svc-vnh7x [276.086392ms]
Sep  6 21:06:51.304: INFO: Created: latency-svc-gs2rh
Sep  6 21:06:51.348: INFO: Got endpoints: latency-svc-lnwz9 [322.814413ms]
Sep  6 21:06:51.354: INFO: Created: latency-svc-lhbjc
Sep  6 21:06:51.398: INFO: Got endpoints: latency-svc-8vqmz [370.387811ms]
Sep  6 21:06:51.404: INFO: Created: latency-svc-kd875
Sep  6 21:06:51.447: INFO: Got endpoints: latency-svc-p6cd4 [417.298852ms]
Sep  6 21:06:51.454: INFO: Created: latency-svc-9cmm7
Sep  6 21:06:51.498: INFO: Got endpoints: latency-svc-j5vfb [464.577521ms]
Sep  6 21:06:51.504: INFO: Created: latency-svc-l7msb
Sep  6 21:06:51.547: INFO: Got endpoints: latency-svc-p7f77 [511.642639ms]
Sep  6 21:06:51.554: INFO: Created: latency-svc-zbtj8
Sep  6 21:06:51.597: INFO: Got endpoints: latency-svc-gsmfv [558.555551ms]
Sep  6 21:06:51.604: INFO: Created: latency-svc-wwcnk
Sep  6 21:06:51.647: INFO: Got endpoints: latency-svc-cgfb4 [605.411716ms]
Sep  6 21:06:51.652: INFO: Created: latency-svc-9xks6
Sep  6 21:06:51.697: INFO: Got endpoints: latency-svc-sfn95 [652.473113ms]
Sep  6 21:06:51.702: INFO: Created: latency-svc-plfzc
Sep  6 21:06:51.747: INFO: Got endpoints: latency-svc-snb4d [700.237546ms]
Sep  6 21:06:51.753: INFO: Created: latency-svc-4scdc
Sep  6 21:06:51.797: INFO: Got endpoints: latency-svc-f6vx6 [746.919527ms]
Sep  6 21:06:51.803: INFO: Created: latency-svc-rgr7q
Sep  6 21:06:51.847: INFO: Got endpoints: latency-svc-zfmc2 [749.949318ms]
Sep  6 21:06:51.854: INFO: Created: latency-svc-j2nc7
Sep  6 21:06:51.897: INFO: Got endpoints: latency-svc-4qv4w [749.394887ms]
Sep  6 21:06:51.903: INFO: Created: latency-svc-4fxkj
Sep  6 21:06:51.948: INFO: Got endpoints: latency-svc-ftcc6 [750.255371ms]
Sep  6 21:06:51.953: INFO: Created: latency-svc-5hfml
Sep  6 21:06:51.998: INFO: Got endpoints: latency-svc-hrc7j [744.126391ms]
Sep  6 21:06:52.004: INFO: Created: latency-svc-88rcm
Sep  6 21:06:52.048: INFO: Got endpoints: latency-svc-gs2rh [750.070993ms]
Sep  6 21:06:52.055: INFO: Created: latency-svc-x2gst
Sep  6 21:06:52.098: INFO: Got endpoints: latency-svc-lhbjc [749.874941ms]
Sep  6 21:06:52.104: INFO: Created: latency-svc-nj52r
Sep  6 21:06:52.148: INFO: Got endpoints: latency-svc-kd875 [749.89443ms]
Sep  6 21:06:52.154: INFO: Created: latency-svc-bb9xf
Sep  6 21:06:52.198: INFO: Got endpoints: latency-svc-9cmm7 [750.354927ms]
Sep  6 21:06:52.205: INFO: Created: latency-svc-gll9v
Sep  6 21:06:52.248: INFO: Got endpoints: latency-svc-l7msb [749.95087ms]
Sep  6 21:06:52.255: INFO: Created: latency-svc-7lbsr
Sep  6 21:06:52.298: INFO: Got endpoints: latency-svc-zbtj8 [750.435704ms]
Sep  6 21:06:52.305: INFO: Created: latency-svc-x25jd
Sep  6 21:06:52.348: INFO: Got endpoints: latency-svc-wwcnk [750.35921ms]
Sep  6 21:06:52.354: INFO: Created: latency-svc-vm5xr
Sep  6 21:06:52.398: INFO: Got endpoints: latency-svc-9xks6 [750.682278ms]
Sep  6 21:06:52.404: INFO: Created: latency-svc-8854d
Sep  6 21:06:52.447: INFO: Got endpoints: latency-svc-plfzc [750.515808ms]
Sep  6 21:06:52.454: INFO: Created: latency-svc-dsq8r
Sep  6 21:06:52.498: INFO: Got endpoints: latency-svc-4scdc [750.273261ms]
Sep  6 21:06:52.504: INFO: Created: latency-svc-5tq8w
Sep  6 21:06:52.547: INFO: Got endpoints: latency-svc-rgr7q [750.13881ms]
Sep  6 21:06:52.553: INFO: Created: latency-svc-44wf4
Sep  6 21:06:52.598: INFO: Got endpoints: latency-svc-j2nc7 [750.489139ms]
Sep  6 21:06:52.604: INFO: Created: latency-svc-x2jr8
Sep  6 21:06:52.647: INFO: Got endpoints: latency-svc-4fxkj [750.244435ms]
Sep  6 21:06:52.654: INFO: Created: latency-svc-qd2qd
Sep  6 21:06:52.698: INFO: Got endpoints: latency-svc-5hfml [750.393935ms]
Sep  6 21:06:52.704: INFO: Created: latency-svc-27tch
Sep  6 21:06:52.748: INFO: Got endpoints: latency-svc-88rcm [750.07505ms]
Sep  6 21:06:52.754: INFO: Created: latency-svc-59hsl
Sep  6 21:06:52.798: INFO: Got endpoints: latency-svc-x2gst [749.704947ms]
Sep  6 21:06:52.804: INFO: Created: latency-svc-qzlnv
Sep  6 21:06:52.847: INFO: Got endpoints: latency-svc-nj52r [749.931671ms]
Sep  6 21:06:52.853: INFO: Created: latency-svc-gl8bb
Sep  6 21:06:52.898: INFO: Got endpoints: latency-svc-bb9xf [750.11006ms]
Sep  6 21:06:52.904: INFO: Created: latency-svc-f46nw
Sep  6 21:06:52.947: INFO: Got endpoints: latency-svc-gll9v [749.647687ms]
Sep  6 21:06:52.954: INFO: Created: latency-svc-8mzxt
Sep  6 21:06:52.998: INFO: Got endpoints: latency-svc-7lbsr [749.840987ms]
Sep  6 21:06:53.004: INFO: Created: latency-svc-nttq6
Sep  6 21:06:53.048: INFO: Got endpoints: latency-svc-x25jd [749.847933ms]
Sep  6 21:06:53.054: INFO: Created: latency-svc-hhr2n
Sep  6 21:06:53.097: INFO: Got endpoints: latency-svc-vm5xr [749.5904ms]
Sep  6 21:06:53.104: INFO: Created: latency-svc-gbvxk
Sep  6 21:06:53.148: INFO: Got endpoints: latency-svc-8854d [749.957789ms]
Sep  6 21:06:53.155: INFO: Created: latency-svc-ptv4k
Sep  6 21:06:53.198: INFO: Got endpoints: latency-svc-dsq8r [750.809746ms]
Sep  6 21:06:53.205: INFO: Created: latency-svc-tkmbn
Sep  6 21:06:53.248: INFO: Got endpoints: latency-svc-5tq8w [750.174885ms]
Sep  6 21:06:53.254: INFO: Created: latency-svc-75qf5
Sep  6 21:06:53.298: INFO: Got endpoints: latency-svc-44wf4 [750.257424ms]
Sep  6 21:06:53.304: INFO: Created: latency-svc-jrfx7
Sep  6 21:06:53.348: INFO: Got endpoints: latency-svc-x2jr8 [750.338707ms]
Sep  6 21:06:53.355: INFO: Created: latency-svc-c68l5
Sep  6 21:06:53.398: INFO: Got endpoints: latency-svc-qd2qd [750.204682ms]
Sep  6 21:06:53.404: INFO: Created: latency-svc-r2cfv
Sep  6 21:06:53.448: INFO: Got endpoints: latency-svc-27tch [749.475788ms]
Sep  6 21:06:53.454: INFO: Created: latency-svc-2flgt
Sep  6 21:06:53.498: INFO: Got endpoints: latency-svc-59hsl [749.962832ms]
Sep  6 21:06:53.505: INFO: Created: latency-svc-cjdht
Sep  6 21:06:53.548: INFO: Got endpoints: latency-svc-qzlnv [750.764543ms]
Sep  6 21:06:53.555: INFO: Created: latency-svc-md6v4
Sep  6 21:06:53.598: INFO: Got endpoints: latency-svc-gl8bb [750.378167ms]
Sep  6 21:06:53.605: INFO: Created: latency-svc-6nthb
Sep  6 21:06:53.648: INFO: Got endpoints: latency-svc-f46nw [750.092896ms]
Sep  6 21:06:53.654: INFO: Created: latency-svc-6h54q
Sep  6 21:06:53.698: INFO: Got endpoints: latency-svc-8mzxt [750.284843ms]
Sep  6 21:06:53.704: INFO: Created: latency-svc-jrnrc
Sep  6 21:06:53.748: INFO: Got endpoints: latency-svc-nttq6 [750.187325ms]
Sep  6 21:06:53.756: INFO: Created: latency-svc-kzmj6
Sep  6 21:06:53.798: INFO: Got endpoints: latency-svc-hhr2n [750.428745ms]
Sep  6 21:06:53.805: INFO: Created: latency-svc-64f56
Sep  6 21:06:53.848: INFO: Got endpoints: latency-svc-gbvxk [750.444419ms]
Sep  6 21:06:53.855: INFO: Created: latency-svc-hqjjz
Sep  6 21:06:53.898: INFO: Got endpoints: latency-svc-ptv4k [749.895463ms]
Sep  6 21:06:53.905: INFO: Created: latency-svc-685dg
Sep  6 21:06:53.948: INFO: Got endpoints: latency-svc-tkmbn [749.481549ms]
Sep  6 21:06:53.955: INFO: Created: latency-svc-47bz7
Sep  6 21:06:54.002: INFO: Got endpoints: latency-svc-75qf5 [754.312061ms]
Sep  6 21:06:54.009: INFO: Created: latency-svc-gctb2
Sep  6 21:06:54.048: INFO: Got endpoints: latency-svc-jrfx7 [750.255241ms]
Sep  6 21:06:54.055: INFO: Created: latency-svc-wv665
Sep  6 21:06:54.098: INFO: Got endpoints: latency-svc-c68l5 [749.768175ms]
Sep  6 21:06:54.105: INFO: Created: latency-svc-zqs6s
Sep  6 21:06:54.148: INFO: Got endpoints: latency-svc-r2cfv [750.702276ms]
Sep  6 21:06:54.155: INFO: Created: latency-svc-thz8q
Sep  6 21:06:54.198: INFO: Got endpoints: latency-svc-2flgt [750.197396ms]
Sep  6 21:06:54.205: INFO: Created: latency-svc-rtt5x
Sep  6 21:06:54.248: INFO: Got endpoints: latency-svc-cjdht [750.016669ms]
Sep  6 21:06:54.255: INFO: Created: latency-svc-grdcc
Sep  6 21:06:54.297: INFO: Got endpoints: latency-svc-md6v4 [749.101194ms]
Sep  6 21:06:54.303: INFO: Created: latency-svc-pkfmp
Sep  6 21:06:54.347: INFO: Got endpoints: latency-svc-6nthb [749.575366ms]
Sep  6 21:06:54.353: INFO: Created: latency-svc-wbvng
Sep  6 21:06:54.397: INFO: Got endpoints: latency-svc-6h54q [749.089753ms]
Sep  6 21:06:54.403: INFO: Created: latency-svc-br4gm
Sep  6 21:06:54.448: INFO: Got endpoints: latency-svc-jrnrc [749.731642ms]
Sep  6 21:06:54.453: INFO: Created: latency-svc-t4f6j
Sep  6 21:06:54.497: INFO: Got endpoints: latency-svc-kzmj6 [749.546768ms]
Sep  6 21:06:54.503: INFO: Created: latency-svc-4fgqk
Sep  6 21:06:54.548: INFO: Got endpoints: latency-svc-64f56 [749.659291ms]
Sep  6 21:06:54.554: INFO: Created: latency-svc-v6v7w
Sep  6 21:06:54.598: INFO: Got endpoints: latency-svc-hqjjz [749.719405ms]
Sep  6 21:06:54.604: INFO: Created: latency-svc-ch95x
Sep  6 21:06:54.648: INFO: Got endpoints: latency-svc-685dg [750.116784ms]
Sep  6 21:06:54.655: INFO: Created: latency-svc-pm9cz
Sep  6 21:06:54.698: INFO: Got endpoints: latency-svc-47bz7 [750.397172ms]
Sep  6 21:06:54.705: INFO: Created: latency-svc-q8mbd
Sep  6 21:06:54.748: INFO: Got endpoints: latency-svc-gctb2 [745.758535ms]
Sep  6 21:06:54.756: INFO: Created: latency-svc-s7nvg
Sep  6 21:06:54.798: INFO: Got endpoints: latency-svc-wv665 [749.67476ms]
Sep  6 21:06:54.804: INFO: Created: latency-svc-ls9x8
Sep  6 21:06:54.848: INFO: Got endpoints: latency-svc-zqs6s [749.6529ms]
Sep  6 21:06:54.854: INFO: Created: latency-svc-7rmzx
Sep  6 21:06:54.898: INFO: Got endpoints: latency-svc-thz8q [749.231587ms]
Sep  6 21:06:54.904: INFO: Created: latency-svc-rzzzb
Sep  6 21:06:54.948: INFO: Got endpoints: latency-svc-rtt5x [749.713562ms]
Sep  6 21:06:54.954: INFO: Created: latency-svc-s9g72
Sep  6 21:06:54.998: INFO: Got endpoints: latency-svc-grdcc [749.829235ms]
Sep  6 21:06:55.003: INFO: Created: latency-svc-wwbr8
Sep  6 21:06:55.047: INFO: Got endpoints: latency-svc-pkfmp [749.539056ms]
Sep  6 21:06:55.052: INFO: Created: latency-svc-vv9g2
Sep  6 21:06:55.097: INFO: Got endpoints: latency-svc-wbvng [749.682146ms]
Sep  6 21:06:55.103: INFO: Created: latency-svc-ds7vg
Sep  6 21:06:55.151: INFO: Got endpoints: latency-svc-br4gm [754.069131ms]
Sep  6 21:06:55.156: INFO: Created: latency-svc-ndf7s
Sep  6 21:06:55.198: INFO: Got endpoints: latency-svc-t4f6j [749.985149ms]
Sep  6 21:06:55.203: INFO: Created: latency-svc-m6rp6
Sep  6 21:06:55.255: INFO: Got endpoints: latency-svc-4fgqk [757.057721ms]
Sep  6 21:06:55.260: INFO: Created: latency-svc-snm7z
Sep  6 21:06:55.297: INFO: Got endpoints: latency-svc-v6v7w [749.473685ms]
Sep  6 21:06:55.303: INFO: Created: latency-svc-gffw6
Sep  6 21:06:55.347: INFO: Got endpoints: latency-svc-ch95x [749.460544ms]
Sep  6 21:06:55.353: INFO: Created: latency-svc-kqgzv
Sep  6 21:06:55.398: INFO: Got endpoints: latency-svc-pm9cz [750.224908ms]
Sep  6 21:06:55.404: INFO: Created: latency-svc-p2jcm
Sep  6 21:06:55.447: INFO: Got endpoints: latency-svc-q8mbd [748.802794ms]
Sep  6 21:06:55.453: INFO: Created: latency-svc-6b5nf
Sep  6 21:06:55.498: INFO: Got endpoints: latency-svc-s7nvg [749.699343ms]
Sep  6 21:06:55.504: INFO: Created: latency-svc-8rcl7
Sep  6 21:06:55.547: INFO: Got endpoints: latency-svc-ls9x8 [749.131101ms]
Sep  6 21:06:55.553: INFO: Created: latency-svc-qcmvs
Sep  6 21:06:55.598: INFO: Got endpoints: latency-svc-7rmzx [750.008656ms]
Sep  6 21:06:55.603: INFO: Created: latency-svc-5j98l
Sep  6 21:06:55.647: INFO: Got endpoints: latency-svc-rzzzb [749.753167ms]
Sep  6 21:06:55.654: INFO: Created: latency-svc-7hrrg
Sep  6 21:06:55.697: INFO: Got endpoints: latency-svc-s9g72 [749.865371ms]
Sep  6 21:06:55.704: INFO: Created: latency-svc-zfsc6
Sep  6 21:06:55.748: INFO: Got endpoints: latency-svc-wwbr8 [749.851489ms]
Sep  6 21:06:55.753: INFO: Created: latency-svc-rwtxh
Sep  6 21:06:55.797: INFO: Got endpoints: latency-svc-vv9g2 [750.397091ms]
Sep  6 21:06:55.803: INFO: Created: latency-svc-kkr66
Sep  6 21:06:55.847: INFO: Got endpoints: latency-svc-ds7vg [750.218369ms]
Sep  6 21:06:55.853: INFO: Created: latency-svc-57w7m
Sep  6 21:06:55.897: INFO: Got endpoints: latency-svc-ndf7s [746.294002ms]
Sep  6 21:06:55.904: INFO: Created: latency-svc-9dtgd
Sep  6 21:06:55.948: INFO: Got endpoints: latency-svc-m6rp6 [749.907462ms]
Sep  6 21:06:55.954: INFO: Created: latency-svc-pknx5
Sep  6 21:06:55.998: INFO: Got endpoints: latency-svc-snm7z [743.174637ms]
Sep  6 21:06:56.004: INFO: Created: latency-svc-6vrtv
Sep  6 21:06:56.047: INFO: Got endpoints: latency-svc-gffw6 [750.032042ms]
Sep  6 21:06:56.053: INFO: Created: latency-svc-qqzdl
Sep  6 21:06:56.097: INFO: Got endpoints: latency-svc-kqgzv [750.016977ms]
Sep  6 21:06:56.102: INFO: Created: latency-svc-9454m
Sep  6 21:06:56.147: INFO: Got endpoints: latency-svc-p2jcm [749.267806ms]
Sep  6 21:06:56.154: INFO: Created: latency-svc-42lzk
Sep  6 21:06:56.198: INFO: Got endpoints: latency-svc-6b5nf [750.417622ms]
Sep  6 21:06:56.204: INFO: Created: latency-svc-8ksh2
Sep  6 21:06:56.248: INFO: Got endpoints: latency-svc-8rcl7 [750.029135ms]
Sep  6 21:06:56.254: INFO: Created: latency-svc-4drjk
Sep  6 21:06:56.298: INFO: Got endpoints: latency-svc-qcmvs [750.528427ms]
Sep  6 21:06:56.304: INFO: Created: latency-svc-7wrvm
Sep  6 21:06:56.348: INFO: Got endpoints: latency-svc-5j98l [750.050874ms]
Sep  6 21:06:56.354: INFO: Created: latency-svc-fcd56
Sep  6 21:06:56.397: INFO: Got endpoints: latency-svc-7hrrg [750.10152ms]
Sep  6 21:06:56.404: INFO: Created: latency-svc-clxth
Sep  6 21:06:56.448: INFO: Got endpoints: latency-svc-zfsc6 [750.061882ms]
Sep  6 21:06:56.454: INFO: Created: latency-svc-xt2lm
Sep  6 21:06:56.498: INFO: Got endpoints: latency-svc-rwtxh [750.219868ms]
Sep  6 21:06:56.504: INFO: Created: latency-svc-mw6jr
Sep  6 21:06:56.547: INFO: Got endpoints: latency-svc-kkr66 [749.99495ms]
Sep  6 21:06:56.554: INFO: Created: latency-svc-mrkmf
Sep  6 21:06:56.598: INFO: Got endpoints: latency-svc-57w7m [750.380742ms]
Sep  6 21:06:56.605: INFO: Created: latency-svc-f7crc
Sep  6 21:06:56.648: INFO: Got endpoints: latency-svc-9dtgd [750.211012ms]
Sep  6 21:06:56.654: INFO: Created: latency-svc-8c6h8
Sep  6 21:06:56.698: INFO: Got endpoints: latency-svc-pknx5 [750.226728ms]
Sep  6 21:06:56.704: INFO: Created: latency-svc-9v97f
Sep  6 21:06:56.748: INFO: Got endpoints: latency-svc-6vrtv [749.793207ms]
Sep  6 21:06:56.754: INFO: Created: latency-svc-mmf5z
Sep  6 21:06:56.798: INFO: Got endpoints: latency-svc-qqzdl [750.056525ms]
Sep  6 21:06:56.803: INFO: Created: latency-svc-rtpjh
Sep  6 21:06:56.847: INFO: Got endpoints: latency-svc-9454m [749.819068ms]
Sep  6 21:06:56.853: INFO: Created: latency-svc-dx5s5
Sep  6 21:06:56.897: INFO: Got endpoints: latency-svc-42lzk [749.935804ms]
Sep  6 21:06:56.903: INFO: Created: latency-svc-wl5zn
Sep  6 21:06:56.947: INFO: Got endpoints: latency-svc-8ksh2 [749.771379ms]
Sep  6 21:06:56.953: INFO: Created: latency-svc-pw67r
Sep  6 21:06:56.997: INFO: Got endpoints: latency-svc-4drjk [749.515417ms]
Sep  6 21:06:57.003: INFO: Created: latency-svc-l2hqr
Sep  6 21:06:57.047: INFO: Got endpoints: latency-svc-7wrvm [749.704784ms]
Sep  6 21:06:57.053: INFO: Created: latency-svc-lpjbb
Sep  6 21:06:57.097: INFO: Got endpoints: latency-svc-fcd56 [749.374808ms]
Sep  6 21:06:57.103: INFO: Created: latency-svc-x6m8p
Sep  6 21:06:57.147: INFO: Got endpoints: latency-svc-clxth [749.317358ms]
Sep  6 21:06:57.152: INFO: Created: latency-svc-zkqmt
Sep  6 21:06:57.197: INFO: Got endpoints: latency-svc-xt2lm [749.874024ms]
Sep  6 21:06:57.203: INFO: Created: latency-svc-l4c7p
Sep  6 21:06:57.247: INFO: Got endpoints: latency-svc-mw6jr [749.597974ms]
Sep  6 21:06:57.254: INFO: Created: latency-svc-68csx
Sep  6 21:06:57.297: INFO: Got endpoints: latency-svc-mrkmf [749.856963ms]
Sep  6 21:06:57.304: INFO: Created: latency-svc-57ls4
Sep  6 21:06:57.348: INFO: Got endpoints: latency-svc-f7crc [749.789065ms]
Sep  6 21:06:57.354: INFO: Created: latency-svc-jbkng
Sep  6 21:06:57.398: INFO: Got endpoints: latency-svc-8c6h8 [750.149135ms]
Sep  6 21:06:57.405: INFO: Created: latency-svc-crl24
Sep  6 21:06:57.448: INFO: Got endpoints: latency-svc-9v97f [750.024828ms]
Sep  6 21:06:57.455: INFO: Created: latency-svc-8x5v2
Sep  6 21:06:57.498: INFO: Got endpoints: latency-svc-mmf5z [750.34689ms]
Sep  6 21:06:57.504: INFO: Created: latency-svc-6jdld
Sep  6 21:06:57.548: INFO: Got endpoints: latency-svc-rtpjh [750.197307ms]
Sep  6 21:06:57.554: INFO: Created: latency-svc-vklk2
Sep  6 21:06:57.598: INFO: Got endpoints: latency-svc-dx5s5 [751.043759ms]
Sep  6 21:06:57.605: INFO: Created: latency-svc-wlg54
Sep  6 21:06:57.647: INFO: Got endpoints: latency-svc-wl5zn [749.920735ms]
Sep  6 21:06:57.654: INFO: Created: latency-svc-wx7df
Sep  6 21:06:57.698: INFO: Got endpoints: latency-svc-pw67r [750.325842ms]
Sep  6 21:06:57.705: INFO: Created: latency-svc-vqc8l
Sep  6 21:06:57.748: INFO: Got endpoints: latency-svc-l2hqr [750.494582ms]
Sep  6 21:06:57.755: INFO: Created: latency-svc-jgl5x
Sep  6 21:06:57.798: INFO: Got endpoints: latency-svc-lpjbb [750.415122ms]
Sep  6 21:06:57.804: INFO: Created: latency-svc-c5p45
Sep  6 21:06:57.848: INFO: Got endpoints: latency-svc-x6m8p [750.503432ms]
Sep  6 21:06:57.854: INFO: Created: latency-svc-8m9zp
Sep  6 21:06:57.898: INFO: Got endpoints: latency-svc-zkqmt [751.019559ms]
Sep  6 21:06:57.905: INFO: Created: latency-svc-wzd92
Sep  6 21:06:57.948: INFO: Got endpoints: latency-svc-l4c7p [750.056051ms]
Sep  6 21:06:57.954: INFO: Created: latency-svc-v55xp
Sep  6 21:06:57.998: INFO: Got endpoints: latency-svc-68csx [750.10737ms]
Sep  6 21:06:58.004: INFO: Created: latency-svc-xc9km
Sep  6 21:06:58.048: INFO: Got endpoints: latency-svc-57ls4 [750.173594ms]
Sep  6 21:06:58.054: INFO: Created: latency-svc-9s96q
Sep  6 21:06:58.098: INFO: Got endpoints: latency-svc-jbkng [750.113143ms]
Sep  6 21:06:58.104: INFO: Created: latency-svc-47rcd
Sep  6 21:06:58.148: INFO: Got endpoints: latency-svc-crl24 [749.958623ms]
Sep  6 21:06:58.154: INFO: Created: latency-svc-dgp5g
Sep  6 21:06:58.198: INFO: Got endpoints: latency-svc-8x5v2 [750.035646ms]
Sep  6 21:06:58.205: INFO: Created: latency-svc-6gtqh
Sep  6 21:06:58.248: INFO: Got endpoints: latency-svc-6jdld [749.550192ms]
Sep  6 21:06:58.254: INFO: Created: latency-svc-wqvkr
Sep  6 21:06:58.297: INFO: Got endpoints: latency-svc-vklk2 [749.731841ms]
Sep  6 21:06:58.304: INFO: Created: latency-svc-lm74g
Sep  6 21:06:58.348: INFO: Got endpoints: latency-svc-wlg54 [749.492936ms]
Sep  6 21:06:58.354: INFO: Created: latency-svc-477ft
Sep  6 21:06:58.397: INFO: Got endpoints: latency-svc-wx7df [750.073177ms]
Sep  6 21:06:58.404: INFO: Created: latency-svc-l6mc9
Sep  6 21:06:58.448: INFO: Got endpoints: latency-svc-vqc8l [750.080568ms]
Sep  6 21:06:58.455: INFO: Created: latency-svc-wwp6t
Sep  6 21:06:58.498: INFO: Got endpoints: latency-svc-jgl5x [749.815002ms]
Sep  6 21:06:58.504: INFO: Created: latency-svc-mrsvq
Sep  6 21:06:58.548: INFO: Got endpoints: latency-svc-c5p45 [750.107301ms]
Sep  6 21:06:58.554: INFO: Created: latency-svc-4w89l
Sep  6 21:06:58.598: INFO: Got endpoints: latency-svc-8m9zp [750.207984ms]
Sep  6 21:06:58.604: INFO: Created: latency-svc-pwd88
Sep  6 21:06:58.648: INFO: Got endpoints: latency-svc-wzd92 [749.681917ms]
Sep  6 21:06:58.655: INFO: Created: latency-svc-f5m7x
Sep  6 21:06:58.698: INFO: Got endpoints: latency-svc-v55xp [750.417823ms]
Sep  6 21:06:58.704: INFO: Created: latency-svc-ck89v
Sep  6 21:06:58.748: INFO: Got endpoints: latency-svc-xc9km [750.047446ms]
Sep  6 21:06:58.754: INFO: Created: latency-svc-w7wbr
Sep  6 21:06:58.798: INFO: Got endpoints: latency-svc-9s96q [750.19747ms]
Sep  6 21:06:58.848: INFO: Got endpoints: latency-svc-47rcd [749.817594ms]
Sep  6 21:06:58.898: INFO: Got endpoints: latency-svc-dgp5g [749.891869ms]
Sep  6 21:06:58.948: INFO: Got endpoints: latency-svc-6gtqh [749.939212ms]
Sep  6 21:06:58.998: INFO: Got endpoints: latency-svc-wqvkr [750.124451ms]
Sep  6 21:06:59.048: INFO: Got endpoints: latency-svc-lm74g [750.431139ms]
Sep  6 21:06:59.098: INFO: Got endpoints: latency-svc-477ft [750.237979ms]
Sep  6 21:06:59.148: INFO: Got endpoints: latency-svc-l6mc9 [750.465898ms]
Sep  6 21:06:59.198: INFO: Got endpoints: latency-svc-wwp6t [749.986813ms]
Sep  6 21:06:59.256: INFO: Got endpoints: latency-svc-mrsvq [757.93732ms]
Sep  6 21:06:59.298: INFO: Got endpoints: latency-svc-4w89l [749.946371ms]
Sep  6 21:06:59.348: INFO: Got endpoints: latency-svc-pwd88 [749.90671ms]
Sep  6 21:06:59.398: INFO: Got endpoints: latency-svc-f5m7x [750.264403ms]
Sep  6 21:06:59.448: INFO: Got endpoints: latency-svc-ck89v [749.876602ms]
Sep  6 21:06:59.498: INFO: Got endpoints: latency-svc-w7wbr [750.14429ms]
Sep  6 21:06:59.498: INFO: Latencies: [11.320512ms 14.564966ms 16.383015ms 20.567878ms 23.316763ms 26.531834ms 29.383406ms 32.736783ms 35.615561ms 39.018577ms 42.024499ms 43.477169ms 43.715229ms 43.938336ms 44.306652ms 44.817664ms 44.862725ms 45.069971ms 45.369497ms 45.566553ms 45.798452ms 46.365656ms 46.527057ms 46.711982ms 46.731585ms 46.886879ms 47.12177ms 47.767401ms 48.59285ms 51.889452ms 54.214443ms 86.850016ms 134.785546ms 181.313861ms 234.678542ms 276.086392ms 322.814413ms 370.387811ms 417.298852ms 464.577521ms 511.642639ms 558.555551ms 605.411716ms 652.473113ms 700.237546ms 743.174637ms 744.126391ms 745.758535ms 746.294002ms 746.919527ms 748.802794ms 749.089753ms 749.101194ms 749.131101ms 749.231587ms 749.267806ms 749.317358ms 749.374808ms 749.394887ms 749.460544ms 749.473685ms 749.475788ms 749.481549ms 749.492936ms 749.515417ms 749.539056ms 749.546768ms 749.550192ms 749.575366ms 749.5904ms 749.597974ms 749.647687ms 749.6529ms 749.659291ms 749.67476ms 749.681917ms 749.682146ms 749.699343ms 749.704784ms 749.704947ms 749.713562ms 749.719405ms 749.731642ms 749.731841ms 749.753167ms 749.768175ms 749.771379ms 749.789065ms 749.793207ms 749.815002ms 749.817594ms 749.819068ms 749.829235ms 749.840987ms 749.847933ms 749.851489ms 749.856963ms 749.865371ms 749.874024ms 749.874941ms 749.876602ms 749.891869ms 749.89443ms 749.895463ms 749.90671ms 749.907462ms 749.920735ms 749.931671ms 749.935804ms 749.939212ms 749.946371ms 749.949318ms 749.95087ms 749.957789ms 749.958623ms 749.962832ms 749.985149ms 749.986813ms 749.99495ms 750.008656ms 750.016669ms 750.016977ms 750.024828ms 750.029135ms 750.032042ms 750.035646ms 750.047446ms 750.050874ms 750.056051ms 750.056525ms 750.061882ms 750.070993ms 750.073177ms 750.07505ms 750.080568ms 750.092896ms 750.10152ms 750.107301ms 750.10737ms 750.11006ms 750.113143ms 750.116784ms 750.124451ms 750.13881ms 750.14429ms 750.149135ms 750.173594ms 750.174885ms 750.187325ms 750.197307ms 750.197396ms 750.19747ms 750.204682ms 750.207984ms 750.211012ms 750.218369ms 750.219868ms 750.224908ms 750.226728ms 750.237979ms 750.244435ms 750.255241ms 750.255371ms 750.257424ms 750.264403ms 750.273261ms 750.284843ms 750.325842ms 750.338707ms 750.34689ms 750.354927ms 750.35921ms 750.378167ms 750.380742ms 750.393935ms 750.397091ms 750.397172ms 750.415122ms 750.417622ms 750.417823ms 750.428745ms 750.431139ms 750.435704ms 750.444419ms 750.465898ms 750.489139ms 750.494582ms 750.503432ms 750.515808ms 750.528427ms 750.682278ms 750.702276ms 750.764543ms 750.809746ms 751.019559ms 751.043759ms 754.069131ms 754.312061ms 757.057721ms 757.93732ms]
Sep  6 21:06:59.498: INFO: 50 %ile: 749.876602ms
Sep  6 21:06:59.498: INFO: 90 %ile: 750.428745ms
Sep  6 21:06:59.498: INFO: 99 %ile: 757.057721ms
Sep  6 21:06:59.498: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:06:59.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9725" for this suite.
Sep  6 21:07:13.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:07:13.613: INFO: namespace svc-latency-9725 deletion completed in 14.110940268s

• [SLOW TEST:24.966 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:07:13.613: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3596
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-4db02853-57fc-4de6-976c-1d026b4bc9b6
STEP: Creating configMap with name cm-test-opt-upd-78e9c1c4-4b4e-4ad6-a6ce-771c37117538
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4db02853-57fc-4de6-976c-1d026b4bc9b6
STEP: Updating configmap cm-test-opt-upd-78e9c1c4-4b4e-4ad6-a6ce-771c37117538
STEP: Creating configMap with name cm-test-opt-create-2c43cb22-ca45-45c3-beae-8ff741c282a1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:07:17.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3596" for this suite.
Sep  6 21:07:39.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:07:39.967: INFO: namespace projected-3596 deletion completed in 22.107967929s

• [SLOW TEST:26.354 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:07:39.967: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep  6 21:07:50.128: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0906 21:07:50.128175      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:07:50.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4351" for this suite.
Sep  6 21:07:56.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:07:56.235: INFO: namespace gc-4351 deletion completed in 6.10324609s

• [SLOW TEST:16.268 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:07:56.236: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 21:07:56.375: INFO: Waiting up to 5m0s for pod "downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460" in namespace "downward-api-4252" to be "success or failure"
Sep  6 21:07:56.378: INFO: Pod "downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597978ms
Sep  6 21:07:58.382: INFO: Pod "downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006572992s
STEP: Saw pod success
Sep  6 21:07:58.382: INFO: Pod "downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460" satisfied condition "success or failure"
Sep  6 21:07:58.385: INFO: Trying to get logs from node appserv9 pod downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460 container dapi-container: <nil>
STEP: delete the pod
Sep  6 21:07:58.406: INFO: Waiting for pod downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460 to disappear
Sep  6 21:07:58.409: INFO: Pod downward-api-27be2e7c-8ae5-415b-b1a0-250c95d34460 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:07:58.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4252" for this suite.
Sep  6 21:08:04.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:08:04.509: INFO: namespace downward-api-4252 deletion completed in 6.096047933s

• [SLOW TEST:8.274 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:08:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9418
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-97282054-3a98-445f-bfb7-bcb8420faba9
STEP: Creating a pod to test consume configMaps
Sep  6 21:08:04.655: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab" in namespace "projected-9418" to be "success or failure"
Sep  6 21:08:04.658: INFO: Pod "pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.661049ms
Sep  6 21:08:06.661: INFO: Pod "pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005780123s
STEP: Saw pod success
Sep  6 21:08:06.661: INFO: Pod "pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab" satisfied condition "success or failure"
Sep  6 21:08:06.663: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 21:08:06.677: INFO: Waiting for pod pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab to disappear
Sep  6 21:08:06.679: INFO: Pod pod-projected-configmaps-7aef1479-3db0-4264-8a9b-d521b44ef8ab no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:08:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9418" for this suite.
Sep  6 21:08:12.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:08:12.787: INFO: namespace projected-9418 deletion completed in 6.1039067s

• [SLOW TEST:8.278 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:08:12.788: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:08:12.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1519" for this suite.
Sep  6 21:08:18.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:08:19.042: INFO: namespace services-1519 deletion completed in 6.111601795s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.254 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:08:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  6 21:08:21.703: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-324 pod-service-account-c0904c49-a666-4444-bd74-4f232cb6008d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  6 21:08:21.971: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-324 pod-service-account-c0904c49-a666-4444-bd74-4f232cb6008d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  6 21:08:22.174: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-324 pod-service-account-c0904c49-a666-4444-bd74-4f232cb6008d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:08:22.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-324" for this suite.
Sep  6 21:08:28.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:08:28.480: INFO: namespace svcaccounts-324 deletion completed in 6.109952595s

• [SLOW TEST:9.438 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:08:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8567
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 21:08:28.616: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 21:08:50.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.11:8080/dial?request=hostName&protocol=udp&host=172.16.141.12&port=8081&tries=1'] Namespace:pod-network-test-8567 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 21:08:50.691: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 21:08:50.802: INFO: Waiting for endpoints: map[]
Sep  6 21:08:50.805: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.11:8080/dial?request=hostName&protocol=udp&host=172.16.141.13&port=8081&tries=1'] Namespace:pod-network-test-8567 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 21:08:50.805: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 21:08:50.900: INFO: Waiting for endpoints: map[]
Sep  6 21:08:50.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.141.11:8080/dial?request=hostName&protocol=udp&host=172.16.141.14&port=8081&tries=1'] Namespace:pod-network-test-8567 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 21:08:50.903: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
Sep  6 21:08:51.003: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:08:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8567" for this suite.
Sep  6 21:09:13.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:09:13.117: INFO: namespace pod-network-test-8567 deletion completed in 22.109973694s

• [SLOW TEST:44.637 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:09:13.118: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 21:09:13.254: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:09:16.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8983" for this suite.
Sep  6 21:09:22.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:09:22.136: INFO: namespace init-container-8983 deletion completed in 6.1066147s

• [SLOW TEST:9.018 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:09:22.137: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 21:09:22.283: INFO: (0) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 7.315595ms)
Sep  6 21:09:22.287: INFO: (1) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.778159ms)
Sep  6 21:09:22.291: INFO: (2) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.920002ms)
Sep  6 21:09:22.295: INFO: (3) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.8078ms)
Sep  6 21:09:22.299: INFO: (4) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.720035ms)
Sep  6 21:09:22.302: INFO: (5) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.745588ms)
Sep  6 21:09:22.306: INFO: (6) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.535378ms)
Sep  6 21:09:22.310: INFO: (7) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.732439ms)
Sep  6 21:09:22.313: INFO: (8) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.760501ms)
Sep  6 21:09:22.317: INFO: (9) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.516147ms)
Sep  6 21:09:22.320: INFO: (10) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.482ms)
Sep  6 21:09:22.324: INFO: (11) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.760142ms)
Sep  6 21:09:22.328: INFO: (12) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.707932ms)
Sep  6 21:09:22.332: INFO: (13) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 4.015337ms)
Sep  6 21:09:22.336: INFO: (14) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.687851ms)
Sep  6 21:09:22.339: INFO: (15) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.655719ms)
Sep  6 21:09:22.343: INFO: (16) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.577483ms)
Sep  6 21:09:22.347: INFO: (17) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.590633ms)
Sep  6 21:09:22.350: INFO: (18) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.361884ms)
Sep  6 21:09:22.354: INFO: (19) /api/v1/nodes/appserv10/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 3.520919ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:09:22.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3124" for this suite.
Sep  6 21:09:28.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:09:28.463: INFO: namespace proxy-3124 deletion completed in 6.10540596s

• [SLOW TEST:6.326 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:09:28.463: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 21:09:28.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b" in namespace "projected-572" to be "success or failure"
Sep  6 21:09:28.610: INFO: Pod "downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.107773ms
Sep  6 21:09:30.613: INFO: Pod "downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006499278s
STEP: Saw pod success
Sep  6 21:09:30.613: INFO: Pod "downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b" satisfied condition "success or failure"
Sep  6 21:09:30.616: INFO: Trying to get logs from node appserv10 pod downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b container client-container: <nil>
STEP: delete the pod
Sep  6 21:09:30.634: INFO: Waiting for pod downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b to disappear
Sep  6 21:09:30.636: INFO: Pod downwardapi-volume-93c96f8b-79ec-4b3c-8ead-ad1e5810f74b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:09:30.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-572" for this suite.
Sep  6 21:09:36.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:09:36.741: INFO: namespace projected-572 deletion completed in 6.101078s

• [SLOW TEST:8.278 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:09:36.741: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Sep  6 21:09:36.880: INFO: Waiting up to 5m0s for pod "var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6" in namespace "var-expansion-7536" to be "success or failure"
Sep  6 21:09:36.883: INFO: Pod "var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825289ms
Sep  6 21:09:38.886: INFO: Pod "var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006440029s
STEP: Saw pod success
Sep  6 21:09:38.887: INFO: Pod "var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6" satisfied condition "success or failure"
Sep  6 21:09:38.889: INFO: Trying to get logs from node appserv9 pod var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6 container dapi-container: <nil>
STEP: delete the pod
Sep  6 21:09:38.905: INFO: Waiting for pod var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6 to disappear
Sep  6 21:09:38.908: INFO: Pod var-expansion-894e67f5-ff46-491e-a6f1-5ca291691ce6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:09:38.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7536" for this suite.
Sep  6 21:09:44.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:09:45.013: INFO: namespace var-expansion-7536 deletion completed in 6.100219511s

• [SLOW TEST:8.272 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:09:45.013: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 21:09:45.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8773'
Sep  6 21:09:45.285: INFO: stderr: ""
Sep  6 21:09:45.285: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Sep  6 21:09:45.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete pods e2e-test-nginx-pod --namespace=kubectl-8773'
Sep  6 21:09:58.601: INFO: stderr: ""
Sep  6 21:09:58.601: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:09:58.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8773" for this suite.
Sep  6 21:10:04.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:10:04.709: INFO: namespace kubectl-8773 deletion completed in 6.103218411s

• [SLOW TEST:19.696 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:10:04.710: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-0ace338e-9812-4fd6-9355-f41226e9b671
STEP: Creating a pod to test consume secrets
Sep  6 21:10:04.855: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d" in namespace "projected-4847" to be "success or failure"
Sep  6 21:10:04.858: INFO: Pod "pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.768079ms
Sep  6 21:10:06.862: INFO: Pod "pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006233122s
STEP: Saw pod success
Sep  6 21:10:06.862: INFO: Pod "pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d" satisfied condition "success or failure"
Sep  6 21:10:06.864: INFO: Trying to get logs from node appserv11 pod pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 21:10:06.883: INFO: Waiting for pod pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d to disappear
Sep  6 21:10:06.885: INFO: Pod pod-projected-secrets-749ee895-6489-4935-a007-a10b4b903d4d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:10:06.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4847" for this suite.
Sep  6 21:10:12.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:10:12.995: INFO: namespace projected-4847 deletion completed in 6.105662561s

• [SLOW TEST:8.285 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:10:12.995: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 21:10:13.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 version'
Sep  6 21:10:13.239: INFO: stderr: ""
Sep  6 21:10:13.239: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:10:13.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7847" for this suite.
Sep  6 21:10:19.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:10:19.354: INFO: namespace kubectl-7847 deletion completed in 6.110803188s

• [SLOW TEST:6.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:10:19.354: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4afd2e4b-e4c1-4215-8a52-1a5d10be0fb4
STEP: Creating a pod to test consume configMaps
Sep  6 21:10:19.501: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec" in namespace "projected-9073" to be "success or failure"
Sep  6 21:10:19.504: INFO: Pod "pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.919361ms
Sep  6 21:10:21.508: INFO: Pod "pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006491894s
STEP: Saw pod success
Sep  6 21:10:21.508: INFO: Pod "pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec" satisfied condition "success or failure"
Sep  6 21:10:21.511: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 21:10:21.529: INFO: Waiting for pod pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec to disappear
Sep  6 21:10:21.532: INFO: Pod pod-projected-configmaps-f6d5d0a7-d821-4da7-830e-947aa73df5ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:10:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9073" for this suite.
Sep  6 21:10:27.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:10:27.647: INFO: namespace projected-9073 deletion completed in 6.111724591s

• [SLOW TEST:8.293 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:10:27.648: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Sep  6 21:10:27.785: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-256622946 proxy --unix-socket=/tmp/kubectl-proxy-unix185171801/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:10:27.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5388" for this suite.
Sep  6 21:10:33.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:10:34.004: INFO: namespace kubectl-5388 deletion completed in 6.118668174s

• [SLOW TEST:6.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:10:34.004: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 21:10:34.145: INFO: Waiting up to 5m0s for pod "pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5" in namespace "emptydir-2102" to be "success or failure"
Sep  6 21:10:34.148: INFO: Pod "pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.719801ms
Sep  6 21:10:36.151: INFO: Pod "pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006126567s
STEP: Saw pod success
Sep  6 21:10:36.151: INFO: Pod "pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5" satisfied condition "success or failure"
Sep  6 21:10:36.154: INFO: Trying to get logs from node appserv9 pod pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5 container test-container: <nil>
STEP: delete the pod
Sep  6 21:10:36.170: INFO: Waiting for pod pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5 to disappear
Sep  6 21:10:36.173: INFO: Pod pod-fd741190-6b7f-475d-b1c2-1956f14bd4d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:10:36.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2102" for this suite.
Sep  6 21:10:42.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:10:42.281: INFO: namespace emptydir-2102 deletion completed in 6.104523739s

• [SLOW TEST:8.277 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:10:42.281: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-47tf
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 21:10:42.431: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-47tf" in namespace "subpath-7819" to be "success or failure"
Sep  6 21:10:42.434: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970025ms
Sep  6 21:10:44.438: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006676655s
Sep  6 21:10:46.441: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 4.010117631s
Sep  6 21:10:48.445: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 6.013648034s
Sep  6 21:10:50.448: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 8.017254202s
Sep  6 21:10:52.452: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 10.020685495s
Sep  6 21:10:54.455: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 12.023851872s
Sep  6 21:10:56.458: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 14.027317131s
Sep  6 21:10:58.462: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 16.030949766s
Sep  6 21:11:00.465: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 18.034329607s
Sep  6 21:11:02.469: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Running", Reason="", readiness=true. Elapsed: 20.037660202s
Sep  6 21:11:04.473: INFO: Pod "pod-subpath-test-downwardapi-47tf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041491079s
STEP: Saw pod success
Sep  6 21:11:04.473: INFO: Pod "pod-subpath-test-downwardapi-47tf" satisfied condition "success or failure"
Sep  6 21:11:04.476: INFO: Trying to get logs from node appserv10 pod pod-subpath-test-downwardapi-47tf container test-container-subpath-downwardapi-47tf: <nil>
STEP: delete the pod
Sep  6 21:11:04.496: INFO: Waiting for pod pod-subpath-test-downwardapi-47tf to disappear
Sep  6 21:11:04.498: INFO: Pod pod-subpath-test-downwardapi-47tf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-47tf
Sep  6 21:11:04.498: INFO: Deleting pod "pod-subpath-test-downwardapi-47tf" in namespace "subpath-7819"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:11:04.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7819" for this suite.
Sep  6 21:11:10.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:11:10.604: INFO: namespace subpath-7819 deletion completed in 6.099934845s

• [SLOW TEST:28.323 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:11:10.605: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 21:11:10.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd" in namespace "projected-6475" to be "success or failure"
Sep  6 21:11:10.750: INFO: Pod "downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657663ms
Sep  6 21:11:12.753: INFO: Pod "downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006227362s
Sep  6 21:11:14.760: INFO: Pod "downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01234956s
STEP: Saw pod success
Sep  6 21:11:14.760: INFO: Pod "downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd" satisfied condition "success or failure"
Sep  6 21:11:14.762: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd container client-container: <nil>
STEP: delete the pod
Sep  6 21:11:14.780: INFO: Waiting for pod downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd to disappear
Sep  6 21:11:14.782: INFO: Pod downwardapi-volume-b5ecc1c3-7422-4c0b-9252-28b739f17acd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:11:14.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6475" for this suite.
Sep  6 21:11:20.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:11:20.894: INFO: namespace projected-6475 deletion completed in 6.108418691s

• [SLOW TEST:10.289 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:11:20.895: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 21:11:21.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-6599'
Sep  6 21:11:21.149: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 21:11:21.149: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Sep  6 21:11:23.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-256622946 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6599'
Sep  6 21:11:23.281: INFO: stderr: ""
Sep  6 21:11:23.281: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:11:23.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6599" for this suite.
Sep  6 21:11:45.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:11:45.393: INFO: namespace kubectl-6599 deletion completed in 22.107658915s

• [SLOW TEST:24.498 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:11:45.393: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9b8072c5-292f-4787-8dcd-c06651105eb3
STEP: Creating a pod to test consume configMaps
Sep  6 21:11:45.536: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662" in namespace "projected-1706" to be "success or failure"
Sep  6 21:11:45.539: INFO: Pod "pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.616878ms
Sep  6 21:11:47.542: INFO: Pod "pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662": Phase="Running", Reason="", readiness=true. Elapsed: 2.006045574s
Sep  6 21:11:49.546: INFO: Pod "pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009811357s
STEP: Saw pod success
Sep  6 21:11:49.546: INFO: Pod "pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662" satisfied condition "success or failure"
Sep  6 21:11:49.548: INFO: Trying to get logs from node appserv9 pod pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 21:11:49.567: INFO: Waiting for pod pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662 to disappear
Sep  6 21:11:49.570: INFO: Pod pod-projected-configmaps-d6e56fa0-cc7d-44f7-bc9d-24a90dc93662 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:11:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1706" for this suite.
Sep  6 21:11:55.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:11:55.668: INFO: namespace projected-1706 deletion completed in 6.094477461s

• [SLOW TEST:10.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:11:55.668: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  6 21:12:05.864: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0906 21:12:05.864133      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:12:05.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6433" for this suite.
Sep  6 21:12:11.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:12:11.970: INFO: namespace gc-6433 deletion completed in 6.103473094s

• [SLOW TEST:16.302 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:12:11.971: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 21:12:12.112: INFO: Waiting up to 5m0s for pod "pod-4c526eb8-a334-49c3-bbae-316fff685b33" in namespace "emptydir-8477" to be "success or failure"
Sep  6 21:12:12.115: INFO: Pod "pod-4c526eb8-a334-49c3-bbae-316fff685b33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.653726ms
Sep  6 21:12:14.119: INFO: Pod "pod-4c526eb8-a334-49c3-bbae-316fff685b33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00676804s
STEP: Saw pod success
Sep  6 21:12:14.119: INFO: Pod "pod-4c526eb8-a334-49c3-bbae-316fff685b33" satisfied condition "success or failure"
Sep  6 21:12:14.122: INFO: Trying to get logs from node appserv11 pod pod-4c526eb8-a334-49c3-bbae-316fff685b33 container test-container: <nil>
STEP: delete the pod
Sep  6 21:12:14.141: INFO: Waiting for pod pod-4c526eb8-a334-49c3-bbae-316fff685b33 to disappear
Sep  6 21:12:14.144: INFO: Pod pod-4c526eb8-a334-49c3-bbae-316fff685b33 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:12:14.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8477" for this suite.
Sep  6 21:12:20.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:12:20.260: INFO: namespace emptydir-8477 deletion completed in 6.111317022s

• [SLOW TEST:8.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:12:20.261: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  6 21:12:20.622: INFO: Pod name wrapped-volume-race-d543e836-aaed-458e-8cda-228a384b9e01: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d543e836-aaed-458e-8cda-228a384b9e01 in namespace emptydir-wrapper-4978, will wait for the garbage collector to delete the pods
Sep  6 21:12:36.746: INFO: Deleting ReplicationController wrapped-volume-race-d543e836-aaed-458e-8cda-228a384b9e01 took: 7.014181ms
Sep  6 21:12:37.247: INFO: Terminating ReplicationController wrapped-volume-race-d543e836-aaed-458e-8cda-228a384b9e01 pods took: 500.194308ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 21:13:28.664: INFO: Pod name wrapped-volume-race-a3130880-3d40-47c6-b4e0-49e9da99bbc2: Found 0 pods out of 5
Sep  6 21:13:33.671: INFO: Pod name wrapped-volume-race-a3130880-3d40-47c6-b4e0-49e9da99bbc2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a3130880-3d40-47c6-b4e0-49e9da99bbc2 in namespace emptydir-wrapper-4978, will wait for the garbage collector to delete the pods
Sep  6 21:13:43.754: INFO: Deleting ReplicationController wrapped-volume-race-a3130880-3d40-47c6-b4e0-49e9da99bbc2 took: 7.565222ms
Sep  6 21:13:44.254: INFO: Terminating ReplicationController wrapped-volume-race-a3130880-3d40-47c6-b4e0-49e9da99bbc2 pods took: 500.19956ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 21:14:38.670: INFO: Pod name wrapped-volume-race-6a0d5634-d6a6-46db-93e1-819d0a7266e3: Found 0 pods out of 5
Sep  6 21:14:43.678: INFO: Pod name wrapped-volume-race-6a0d5634-d6a6-46db-93e1-819d0a7266e3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6a0d5634-d6a6-46db-93e1-819d0a7266e3 in namespace emptydir-wrapper-4978, will wait for the garbage collector to delete the pods
Sep  6 21:14:53.761: INFO: Deleting ReplicationController wrapped-volume-race-6a0d5634-d6a6-46db-93e1-819d0a7266e3 took: 7.400413ms
Sep  6 21:14:54.261: INFO: Terminating ReplicationController wrapped-volume-race-6a0d5634-d6a6-46db-93e1-819d0a7266e3 pods took: 500.243175ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:15:40.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4978" for this suite.
Sep  6 21:15:46.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:15:46.927: INFO: namespace emptydir-wrapper-4978 deletion completed in 6.099813555s

• [SLOW TEST:206.666 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:15:46.928: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5993
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-26e637ab-4d16-4d21-b40a-824272f0b663
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:15:49.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5993" for this suite.
Sep  6 21:16:11.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:16:11.210: INFO: namespace configmap-5993 deletion completed in 22.103408189s

• [SLOW TEST:24.282 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:16:11.211: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594
Sep  6 21:16:11.354: INFO: Pod name my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594: Found 0 pods out of 1
Sep  6 21:16:16.358: INFO: Pod name my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594: Found 1 pods out of 1
Sep  6 21:16:16.358: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594" are running
Sep  6 21:16:16.361: INFO: Pod "my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594-hxpdp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:16:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:16:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:16:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:16:11 +0000 UTC Reason: Message:}])
Sep  6 21:16:16.361: INFO: Trying to dial the pod
Sep  6 21:16:21.374: INFO: Controller my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594: Got expected result from replica 1 [my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594-hxpdp]: "my-hostname-basic-907cf628-b368-4fd8-a985-ff0bff40a594-hxpdp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:16:21.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3800" for this suite.
Sep  6 21:16:27.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:16:27.506: INFO: namespace replication-controller-3800 deletion completed in 6.128569251s

• [SLOW TEST:16.296 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:16:27.507: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-85d59201-2240-4649-92ae-5593e08607cf
STEP: Creating a pod to test consume secrets
Sep  6 21:16:27.652: INFO: Waiting up to 5m0s for pod "pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786" in namespace "secrets-6412" to be "success or failure"
Sep  6 21:16:27.655: INFO: Pod "pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912467ms
Sep  6 21:16:29.658: INFO: Pod "pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006354247s
Sep  6 21:16:31.661: INFO: Pod "pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009797399s
STEP: Saw pod success
Sep  6 21:16:31.661: INFO: Pod "pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786" satisfied condition "success or failure"
Sep  6 21:16:31.664: INFO: Trying to get logs from node appserv9 pod pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 21:16:31.683: INFO: Waiting for pod pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786 to disappear
Sep  6 21:16:31.686: INFO: Pod pod-secrets-424f9d48-e5d5-4cc1-af3d-1cc8f8e87786 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:16:31.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6412" for this suite.
Sep  6 21:16:37.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:16:37.802: INFO: namespace secrets-6412 deletion completed in 6.112780626s

• [SLOW TEST:10.295 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:16:37.803: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-4238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4238
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4238
STEP: Deleting pre-stop pod
Sep  6 21:16:48.976: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:16:48.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4238" for this suite.
Sep  6 21:17:26.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:17:27.090: INFO: namespace prestop-4238 deletion completed in 38.102892388s

• [SLOW TEST:49.288 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:17:27.091: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-50f9723b-98b6-44a1-a12c-69c876bed8b1
STEP: Creating a pod to test consume secrets
Sep  6 21:17:27.242: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f" in namespace "projected-1481" to be "success or failure"
Sep  6 21:17:27.245: INFO: Pod "pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365752ms
Sep  6 21:17:29.248: INFO: Pod "pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006024438s
Sep  6 21:17:31.255: INFO: Pod "pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012180322s
STEP: Saw pod success
Sep  6 21:17:31.255: INFO: Pod "pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f" satisfied condition "success or failure"
Sep  6 21:17:31.257: INFO: Trying to get logs from node appserv10 pod pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 21:17:31.286: INFO: Waiting for pod pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f to disappear
Sep  6 21:17:31.289: INFO: Pod pod-projected-secrets-5aa1eb47-921f-4a71-b430-b1c5cb26611f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:17:31.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1481" for this suite.
Sep  6 21:17:37.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:17:37.387: INFO: namespace projected-1481 deletion completed in 6.094045918s

• [SLOW TEST:10.296 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:17:37.387: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 21:17:37.526: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 21:17:42.530: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:17:43.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7826" for this suite.
Sep  6 21:17:49.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:17:49.659: INFO: namespace replication-controller-7826 deletion completed in 6.110813582s

• [SLOW TEST:12.272 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:17:49.659: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Sep  6 21:17:50.315: INFO: created pod pod-service-account-defaultsa
Sep  6 21:17:50.315: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  6 21:17:50.320: INFO: created pod pod-service-account-mountsa
Sep  6 21:17:50.320: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  6 21:17:50.324: INFO: created pod pod-service-account-nomountsa
Sep  6 21:17:50.324: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  6 21:17:50.329: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  6 21:17:50.329: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  6 21:17:50.333: INFO: created pod pod-service-account-mountsa-mountspec
Sep  6 21:17:50.333: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  6 21:17:50.339: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  6 21:17:50.339: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  6 21:17:50.344: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  6 21:17:50.344: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  6 21:17:50.348: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  6 21:17:50.348: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  6 21:17:50.352: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  6 21:17:50.352: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:17:50.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4356" for this suite.
Sep  6 21:18:12.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:18:12.462: INFO: namespace svcaccounts-4356 deletion completed in 22.107404974s

• [SLOW TEST:22.803 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:18:12.463: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 21:18:16.636: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:16.639: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:18.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:18.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:20.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:20.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:22.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:22.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:24.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:24.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:26.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:26.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:28.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:28.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:30.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:30.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:32.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:32.643: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 21:18:34.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 21:18:34.643: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:18:34.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8803" for this suite.
Sep  6 21:18:56.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:18:56.758: INFO: namespace container-lifecycle-hook-8803 deletion completed in 22.101152072s

• [SLOW TEST:44.295 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:18:56.759: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 21:18:58.909: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:18:58.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3352" for this suite.
Sep  6 21:19:04.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:19:05.022: INFO: namespace container-runtime-3352 deletion completed in 6.09661089s

• [SLOW TEST:8.264 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:19:05.023: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1753.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1753.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1753.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1753.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 21:19:07.213: INFO: DNS probes using dns-test-73496ade-456f-4818-b28c-0b74cde76c1c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1753.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1753.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1753.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1753.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 21:19:11.268: INFO: DNS probes using dns-test-d5343fed-c25a-47d2-8448-af1f7b216f03 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1753.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1753.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1753.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1753.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 21:19:15.313: INFO: DNS probes using dns-test-2d5d0414-b912-4385-a3db-1325bf2fc75c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:19:15.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1753" for this suite.
Sep  6 21:19:21.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:19:21.449: INFO: namespace dns-1753 deletion completed in 6.109223013s

• [SLOW TEST:16.426 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:19:21.449: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 21:19:21.591: INFO: Waiting up to 5m0s for pod "pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b" in namespace "emptydir-5322" to be "success or failure"
Sep  6 21:19:21.594: INFO: Pod "pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583393ms
Sep  6 21:19:23.597: INFO: Pod "pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006123706s
STEP: Saw pod success
Sep  6 21:19:23.597: INFO: Pod "pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b" satisfied condition "success or failure"
Sep  6 21:19:23.600: INFO: Trying to get logs from node appserv10 pod pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b container test-container: <nil>
STEP: delete the pod
Sep  6 21:19:23.619: INFO: Waiting for pod pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b to disappear
Sep  6 21:19:23.622: INFO: Pod pod-f50b0fdb-38fa-45aa-a9e8-ac68f8c0414b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:19:23.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5322" for this suite.
Sep  6 21:19:29.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:19:29.736: INFO: namespace emptydir-5322 deletion completed in 6.109458822s

• [SLOW TEST:8.287 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:19:29.736: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 21:19:29.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed" in namespace "projected-7120" to be "success or failure"
Sep  6 21:19:29.883: INFO: Pod "downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853727ms
Sep  6 21:19:31.887: INFO: Pod "downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007129046s
Sep  6 21:19:33.891: INFO: Pod "downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011315296s
STEP: Saw pod success
Sep  6 21:19:33.892: INFO: Pod "downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed" satisfied condition "success or failure"
Sep  6 21:19:33.895: INFO: Trying to get logs from node appserv9 pod downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed container client-container: <nil>
STEP: delete the pod
Sep  6 21:19:33.914: INFO: Waiting for pod downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed to disappear
Sep  6 21:19:33.916: INFO: Pod downwardapi-volume-fcde7aaf-b54c-4b1f-9605-834aef1135ed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:19:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7120" for this suite.
Sep  6 21:19:39.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:19:40.028: INFO: namespace projected-7120 deletion completed in 6.107682216s

• [SLOW TEST:10.292 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:19:40.028: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-178f6180-e084-4909-9823-13706522b640
STEP: Creating a pod to test consume configMaps
Sep  6 21:19:40.174: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f" in namespace "configmap-2993" to be "success or failure"
Sep  6 21:19:40.177: INFO: Pod "pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.704092ms
Sep  6 21:19:42.181: INFO: Pod "pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006554694s
Sep  6 21:19:44.184: INFO: Pod "pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010176122s
STEP: Saw pod success
Sep  6 21:19:44.184: INFO: Pod "pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f" satisfied condition "success or failure"
Sep  6 21:19:44.187: INFO: Trying to get logs from node appserv10 pod pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 21:19:44.207: INFO: Waiting for pod pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f to disappear
Sep  6 21:19:44.209: INFO: Pod pod-configmaps-0b24e05c-0839-423a-a0e3-48126064105f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:19:44.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2993" for this suite.
Sep  6 21:19:50.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:19:50.317: INFO: namespace configmap-2993 deletion completed in 6.103272211s

• [SLOW TEST:10.289 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:19:50.317: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Sep  6 21:19:54.473: INFO: Pod pod-hostip-5a657d07-8ec3-4e57-ba00-ddc079119bcf has hostIP: 172.16.6.109
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:19:54.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8737" for this suite.
Sep  6 21:20:16.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:20:16.578: INFO: namespace pods-8737 deletion completed in 22.101492702s

• [SLOW TEST:26.261 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:20:16.578: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 21:20:16.739: INFO: Number of nodes with available pods: 0
Sep  6 21:20:16.739: INFO: Node appserv10 is running more than one daemon pod
Sep  6 21:20:17.747: INFO: Number of nodes with available pods: 0
Sep  6 21:20:17.748: INFO: Node appserv10 is running more than one daemon pod
Sep  6 21:20:18.748: INFO: Number of nodes with available pods: 2
Sep  6 21:20:18.748: INFO: Node appserv10 is running more than one daemon pod
Sep  6 21:20:19.748: INFO: Number of nodes with available pods: 3
Sep  6 21:20:19.748: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  6 21:20:19.767: INFO: Number of nodes with available pods: 2
Sep  6 21:20:19.767: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:20.774: INFO: Number of nodes with available pods: 2
Sep  6 21:20:20.774: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:21.775: INFO: Number of nodes with available pods: 2
Sep  6 21:20:21.775: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:22.775: INFO: Number of nodes with available pods: 2
Sep  6 21:20:22.775: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:23.775: INFO: Number of nodes with available pods: 2
Sep  6 21:20:23.775: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:24.775: INFO: Number of nodes with available pods: 2
Sep  6 21:20:24.775: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:25.778: INFO: Number of nodes with available pods: 2
Sep  6 21:20:25.778: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:26.775: INFO: Number of nodes with available pods: 2
Sep  6 21:20:26.775: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:27.779: INFO: Number of nodes with available pods: 2
Sep  6 21:20:27.779: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:28.775: INFO: Number of nodes with available pods: 2
Sep  6 21:20:28.775: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:29.776: INFO: Number of nodes with available pods: 2
Sep  6 21:20:29.776: INFO: Node appserv11 is running more than one daemon pod
Sep  6 21:20:30.775: INFO: Number of nodes with available pods: 3
Sep  6 21:20:30.775: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7487, will wait for the garbage collector to delete the pods
Sep  6 21:20:30.837: INFO: Deleting DaemonSet.extensions daemon-set took: 6.536287ms
Sep  6 21:20:31.337: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.21016ms
Sep  6 21:20:42.740: INFO: Number of nodes with available pods: 0
Sep  6 21:20:42.740: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 21:20:42.743: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7487/daemonsets","resourceVersion":"25930"},"items":null}

Sep  6 21:20:42.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7487/pods","resourceVersion":"25930"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:20:42.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7487" for this suite.
Sep  6 21:20:48.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:20:48.867: INFO: namespace daemonsets-7487 deletion completed in 6.104782208s

• [SLOW TEST:32.288 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:20:48.867: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-p877
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 21:20:49.019: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p877" in namespace "subpath-8921" to be "success or failure"
Sep  6 21:20:49.022: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612263ms
Sep  6 21:20:51.025: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 2.00597127s
Sep  6 21:20:53.029: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 4.009482114s
Sep  6 21:20:55.032: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 6.012700809s
Sep  6 21:20:57.035: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 8.016266479s
Sep  6 21:20:59.039: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 10.019903415s
Sep  6 21:21:01.043: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 12.023344389s
Sep  6 21:21:03.046: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 14.027209022s
Sep  6 21:21:05.050: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 16.030753359s
Sep  6 21:21:07.053: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 18.034148896s
Sep  6 21:21:09.057: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Running", Reason="", readiness=true. Elapsed: 20.038024179s
Sep  6 21:21:11.061: INFO: Pod "pod-subpath-test-configmap-p877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.041768168s
STEP: Saw pod success
Sep  6 21:21:11.061: INFO: Pod "pod-subpath-test-configmap-p877" satisfied condition "success or failure"
Sep  6 21:21:11.064: INFO: Trying to get logs from node appserv10 pod pod-subpath-test-configmap-p877 container test-container-subpath-configmap-p877: <nil>
STEP: delete the pod
Sep  6 21:21:11.082: INFO: Waiting for pod pod-subpath-test-configmap-p877 to disappear
Sep  6 21:21:11.084: INFO: Pod pod-subpath-test-configmap-p877 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p877
Sep  6 21:21:11.085: INFO: Deleting pod "pod-subpath-test-configmap-p877" in namespace "subpath-8921"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:21:11.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8921" for this suite.
Sep  6 21:21:17.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:21:17.192: INFO: namespace subpath-8921 deletion completed in 6.100742739s

• [SLOW TEST:28.325 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:21:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-knwm
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 21:21:17.341: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-knwm" in namespace "subpath-8108" to be "success or failure"
Sep  6 21:21:17.344: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.703433ms
Sep  6 21:21:19.347: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006150991s
Sep  6 21:21:21.350: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 4.009484364s
Sep  6 21:21:23.354: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 6.012906989s
Sep  6 21:21:25.357: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 8.016401606s
Sep  6 21:21:27.361: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 10.019929624s
Sep  6 21:21:29.364: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 12.023435499s
Sep  6 21:21:31.368: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 14.026779446s
Sep  6 21:21:33.372: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 16.030911333s
Sep  6 21:21:35.376: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 18.034668238s
Sep  6 21:21:37.380: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 20.038834966s
Sep  6 21:21:39.384: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Running", Reason="", readiness=true. Elapsed: 22.042638175s
Sep  6 21:21:41.387: INFO: Pod "pod-subpath-test-configmap-knwm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046475689s
STEP: Saw pod success
Sep  6 21:21:41.388: INFO: Pod "pod-subpath-test-configmap-knwm" satisfied condition "success or failure"
Sep  6 21:21:41.390: INFO: Trying to get logs from node appserv10 pod pod-subpath-test-configmap-knwm container test-container-subpath-configmap-knwm: <nil>
STEP: delete the pod
Sep  6 21:21:41.410: INFO: Waiting for pod pod-subpath-test-configmap-knwm to disappear
Sep  6 21:21:41.413: INFO: Pod pod-subpath-test-configmap-knwm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-knwm
Sep  6 21:21:41.413: INFO: Deleting pod "pod-subpath-test-configmap-knwm" in namespace "subpath-8108"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:21:41.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8108" for this suite.
Sep  6 21:21:47.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:21:47.526: INFO: namespace subpath-8108 deletion completed in 6.10665088s

• [SLOW TEST:30.334 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:21:47.526: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-3877
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3877 to expose endpoints map[]
Sep  6 21:21:47.670: INFO: Get endpoints failed (2.577486ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep  6 21:21:48.673: INFO: successfully validated that service endpoint-test2 in namespace services-3877 exposes endpoints map[] (1.006112306s elapsed)
STEP: Creating pod pod1 in namespace services-3877
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3877 to expose endpoints map[pod1:[80]]
Sep  6 21:21:50.699: INFO: successfully validated that service endpoint-test2 in namespace services-3877 exposes endpoints map[pod1:[80]] (2.017727482s elapsed)
STEP: Creating pod pod2 in namespace services-3877
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3877 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  6 21:21:52.729: INFO: successfully validated that service endpoint-test2 in namespace services-3877 exposes endpoints map[pod1:[80] pod2:[80]] (2.024312159s elapsed)
STEP: Deleting pod pod1 in namespace services-3877
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3877 to expose endpoints map[pod2:[80]]
Sep  6 21:21:53.745: INFO: successfully validated that service endpoint-test2 in namespace services-3877 exposes endpoints map[pod2:[80]] (1.011434951s elapsed)
STEP: Deleting pod pod2 in namespace services-3877
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3877 to expose endpoints map[]
Sep  6 21:21:54.756: INFO: successfully validated that service endpoint-test2 in namespace services-3877 exposes endpoints map[] (1.005700264s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:21:54.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3877" for this suite.
Sep  6 21:22:16.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:22:16.874: INFO: namespace services-3877 deletion completed in 22.10125857s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.347 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:22:16.874: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  6 21:22:17.017: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3513,SelfLink:/api/v1/namespaces/watch-3513/configmaps/e2e-watch-test-watch-closed,UID:dcbd2fd4-f804-413f-9e61-e7403df330b1,ResourceVersion:26287,Generation:0,CreationTimestamp:2019-09-06 21:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 21:22:17.017: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3513,SelfLink:/api/v1/namespaces/watch-3513/configmaps/e2e-watch-test-watch-closed,UID:dcbd2fd4-f804-413f-9e61-e7403df330b1,ResourceVersion:26288,Generation:0,CreationTimestamp:2019-09-06 21:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  6 21:22:17.030: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3513,SelfLink:/api/v1/namespaces/watch-3513/configmaps/e2e-watch-test-watch-closed,UID:dcbd2fd4-f804-413f-9e61-e7403df330b1,ResourceVersion:26289,Generation:0,CreationTimestamp:2019-09-06 21:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 21:22:17.030: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3513,SelfLink:/api/v1/namespaces/watch-3513/configmaps/e2e-watch-test-watch-closed,UID:dcbd2fd4-f804-413f-9e61-e7403df330b1,ResourceVersion:26290,Generation:0,CreationTimestamp:2019-09-06 21:22:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:22:17.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3513" for this suite.
Sep  6 21:22:23.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:22:23.155: INFO: namespace watch-3513 deletion completed in 6.120931763s

• [SLOW TEST:6.281 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:22:23.155: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1756
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e496124b-6359-41dc-af85-89a4ab4ff41e
STEP: Creating a pod to test consume secrets
Sep  6 21:22:23.303: INFO: Waiting up to 5m0s for pod "pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692" in namespace "secrets-1756" to be "success or failure"
Sep  6 21:22:23.306: INFO: Pod "pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360457ms
Sep  6 21:22:25.310: INFO: Pod "pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007289819s
STEP: Saw pod success
Sep  6 21:22:25.310: INFO: Pod "pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692" satisfied condition "success or failure"
Sep  6 21:22:25.313: INFO: Trying to get logs from node appserv10 pod pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 21:22:25.331: INFO: Waiting for pod pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692 to disappear
Sep  6 21:22:25.334: INFO: Pod pod-secrets-d152605d-67b5-47ef-947f-6468b2a2c692 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:22:25.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1756" for this suite.
Sep  6 21:22:31.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:22:31.441: INFO: namespace secrets-1756 deletion completed in 6.103561206s

• [SLOW TEST:8.286 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:22:31.441: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:22:37.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5871" for this suite.
Sep  6 21:22:43.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:22:43.304: INFO: namespace watch-5871 deletion completed in 6.206328944s

• [SLOW TEST:11.863 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:22:43.306: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  6 21:22:43.443: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Sep  6 21:22:44.503: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  6 21:22:48.265: INFO: Waited 1.726511569s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:22:49.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3809" for this suite.
Sep  6 21:22:55.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:22:55.394: INFO: namespace aggregator-3809 deletion completed in 6.187609518s

• [SLOW TEST:12.089 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:22:55.395: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 21:22:55.528: INFO: Creating ReplicaSet my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df
Sep  6 21:22:55.535: INFO: Pod name my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df: Found 0 pods out of 1
Sep  6 21:23:00.539: INFO: Pod name my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df: Found 1 pods out of 1
Sep  6 21:23:00.539: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df" is running
Sep  6 21:23:00.542: INFO: Pod "my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df-bn8mz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:22:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:22:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:22:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 21:22:55 +0000 UTC Reason: Message:}])
Sep  6 21:23:00.542: INFO: Trying to dial the pod
Sep  6 21:23:05.555: INFO: Controller my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df: Got expected result from replica 1 [my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df-bn8mz]: "my-hostname-basic-f3e4ba0b-8d8d-488e-92ab-65031a40b6df-bn8mz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:23:05.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4360" for this suite.
Sep  6 21:23:11.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:23:11.664: INFO: namespace replicaset-4360 deletion completed in 6.104962321s

• [SLOW TEST:16.270 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:23:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 21:23:11.798: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:23:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8000" for this suite.
Sep  6 21:23:37.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:23:37.656: INFO: namespace init-container-8000 deletion completed in 22.105208822s

• [SLOW TEST:25.991 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:23:37.656: INFO: >>> kubeConfig: /tmp/kubeconfig-256622946
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:23:37.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9412" for this suite.
Sep  6 21:23:43.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:23:43.923: INFO: namespace kubelet-test-9412 deletion completed in 6.109621865s

• [SLOW TEST:6.267 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSep  6 21:23:43.923: INFO: Running AfterSuite actions on all nodes
Sep  6 21:23:43.924: INFO: Running AfterSuite actions on node 1
Sep  6 21:23:43.924: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5898.950 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h38m20.951596323s
Test Suite Passed
