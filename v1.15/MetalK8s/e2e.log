I0906 21:54:02.905998      17 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-201627442
I0906 21:54:02.906120      17 e2e.go:241] Starting e2e run "09c4b461-9c01-4c36-a40a-11c849b0d1f3" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1567806841 - Will randomize all specs
Will run 215 of 4413 specs

Sep  6 21:54:03.100: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 21:54:03.102: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  6 21:54:03.122: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  6 21:54:03.150: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  6 21:54:03.150: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep  6 21:54:03.150: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  6 21:54:03.166: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  6 21:54:03.166: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  6 21:54:03.166: INFO: e2e test version: v1.15.3
Sep  6 21:54:03.168: INFO: kube-apiserver version: v1.15.3
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:54:03.168: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
Sep  6 21:54:03.218: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-37d77f86-bc89-4afa-9761-1e48c93cce63
STEP: Creating a pod to test consume configMaps
Sep  6 21:54:03.240: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6" in namespace "configmap-5309" to be "success or failure"
Sep  6 21:54:03.254: INFO: Pod "pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.589938ms
Sep  6 21:54:05.259: INFO: Pod "pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018741801s
Sep  6 21:54:07.263: INFO: Pod "pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022738187s
STEP: Saw pod success
Sep  6 21:54:07.263: INFO: Pod "pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6" satisfied condition "success or failure"
Sep  6 21:54:07.266: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 21:54:07.295: INFO: Waiting for pod pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6 to disappear
Sep  6 21:54:07.297: INFO: Pod pod-configmaps-e9f8b060-feea-4545-952b-d9207b3b7ef6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:54:07.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5309" for this suite.
Sep  6 21:54:13.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:54:13.456: INFO: namespace configmap-5309 deletion completed in 6.154403621s

• [SLOW TEST:10.288 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:54:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-2377fd74-9b79-4274-919c-3ea405a16191 in namespace container-probe-7702
Sep  6 21:54:17.525: INFO: Started pod liveness-2377fd74-9b79-4274-919c-3ea405a16191 in namespace container-probe-7702
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 21:54:17.528: INFO: Initial restart count of pod liveness-2377fd74-9b79-4274-919c-3ea405a16191 is 0
Sep  6 21:54:27.554: INFO: Restart count of pod container-probe-7702/liveness-2377fd74-9b79-4274-919c-3ea405a16191 is now 1 (10.026009392s elapsed)
Sep  6 21:54:47.611: INFO: Restart count of pod container-probe-7702/liveness-2377fd74-9b79-4274-919c-3ea405a16191 is now 2 (30.083661669s elapsed)
Sep  6 21:55:07.675: INFO: Restart count of pod container-probe-7702/liveness-2377fd74-9b79-4274-919c-3ea405a16191 is now 3 (50.147689902s elapsed)
Sep  6 21:55:27.728: INFO: Restart count of pod container-probe-7702/liveness-2377fd74-9b79-4274-919c-3ea405a16191 is now 4 (1m10.200462945s elapsed)
Sep  6 21:56:39.963: INFO: Restart count of pod container-probe-7702/liveness-2377fd74-9b79-4274-919c-3ea405a16191 is now 5 (2m22.435040968s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:56:39.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7702" for this suite.
Sep  6 21:56:46.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:56:46.222: INFO: namespace container-probe-7702 deletion completed in 6.231816206s

• [SLOW TEST:152.766 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:56:46.223: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 21:56:46.427: INFO: Waiting up to 5m0s for pod "pod-99abf466-aee0-47db-ab53-db95fbb4fa10" in namespace "emptydir-6085" to be "success or failure"
Sep  6 21:56:46.431: INFO: Pod "pod-99abf466-aee0-47db-ab53-db95fbb4fa10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583409ms
Sep  6 21:56:48.436: INFO: Pod "pod-99abf466-aee0-47db-ab53-db95fbb4fa10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008665s
STEP: Saw pod success
Sep  6 21:56:48.436: INFO: Pod "pod-99abf466-aee0-47db-ab53-db95fbb4fa10" satisfied condition "success or failure"
Sep  6 21:56:48.439: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-99abf466-aee0-47db-ab53-db95fbb4fa10 container test-container: <nil>
STEP: delete the pod
Sep  6 21:56:48.496: INFO: Waiting for pod pod-99abf466-aee0-47db-ab53-db95fbb4fa10 to disappear
Sep  6 21:56:48.499: INFO: Pod pod-99abf466-aee0-47db-ab53-db95fbb4fa10 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:56:48.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6085" for this suite.
Sep  6 21:56:54.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:56:54.860: INFO: namespace emptydir-6085 deletion completed in 6.353202446s

• [SLOW TEST:8.637 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:56:54.860: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep  6 21:56:54.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3184'
Sep  6 21:56:55.344: INFO: stderr: ""
Sep  6 21:56:55.344: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 21:56:55.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3184'
Sep  6 21:56:55.434: INFO: stderr: ""
Sep  6 21:56:55.434: INFO: stdout: "update-demo-nautilus-f8592 update-demo-nautilus-llqlj "
Sep  6 21:56:55.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-f8592 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3184'
Sep  6 21:56:55.541: INFO: stderr: ""
Sep  6 21:56:55.541: INFO: stdout: ""
Sep  6 21:56:55.541: INFO: update-demo-nautilus-f8592 is created but not running
Sep  6 21:57:00.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3184'
Sep  6 21:57:00.626: INFO: stderr: ""
Sep  6 21:57:00.627: INFO: stdout: "update-demo-nautilus-f8592 update-demo-nautilus-llqlj "
Sep  6 21:57:00.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-f8592 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3184'
Sep  6 21:57:00.721: INFO: stderr: ""
Sep  6 21:57:00.721: INFO: stdout: "true"
Sep  6 21:57:00.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-f8592 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3184'
Sep  6 21:57:00.809: INFO: stderr: ""
Sep  6 21:57:00.809: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 21:57:00.810: INFO: validating pod update-demo-nautilus-f8592
Sep  6 21:57:00.834: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 21:57:00.834: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 21:57:00.834: INFO: update-demo-nautilus-f8592 is verified up and running
Sep  6 21:57:00.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-llqlj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3184'
Sep  6 21:57:00.918: INFO: stderr: ""
Sep  6 21:57:00.918: INFO: stdout: "true"
Sep  6 21:57:00.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-llqlj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3184'
Sep  6 21:57:01.017: INFO: stderr: ""
Sep  6 21:57:01.017: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 21:57:01.017: INFO: validating pod update-demo-nautilus-llqlj
Sep  6 21:57:01.038: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 21:57:01.038: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 21:57:01.038: INFO: update-demo-nautilus-llqlj is verified up and running
STEP: using delete to clean up resources
Sep  6 21:57:01.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3184'
Sep  6 21:57:01.161: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 21:57:01.161: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 21:57:01.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3184'
Sep  6 21:57:01.258: INFO: stderr: "No resources found.\n"
Sep  6 21:57:01.258: INFO: stdout: ""
Sep  6 21:57:01.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -l name=update-demo --namespace=kubectl-3184 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 21:57:01.348: INFO: stderr: ""
Sep  6 21:57:01.348: INFO: stdout: "update-demo-nautilus-f8592\nupdate-demo-nautilus-llqlj\n"
Sep  6 21:57:01.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3184'
Sep  6 21:57:01.954: INFO: stderr: "No resources found.\n"
Sep  6 21:57:01.954: INFO: stdout: ""
Sep  6 21:57:01.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -l name=update-demo --namespace=kubectl-3184 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 21:57:02.060: INFO: stderr: ""
Sep  6 21:57:02.060: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:57:02.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3184" for this suite.
Sep  6 21:57:24.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:57:24.389: INFO: namespace kubectl-3184 deletion completed in 22.319031589s

• [SLOW TEST:29.530 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:57:24.390: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 21:57:24.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7" in namespace "projected-8729" to be "success or failure"
Sep  6 21:57:24.584: INFO: Pod "downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7": Phase="Pending", Reason="", readiness=false. Elapsed: 39.188616ms
Sep  6 21:57:26.588: INFO: Pod "downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04281146s
STEP: Saw pod success
Sep  6 21:57:26.588: INFO: Pod "downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7" satisfied condition "success or failure"
Sep  6 21:57:26.591: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7 container client-container: <nil>
STEP: delete the pod
Sep  6 21:57:26.607: INFO: Waiting for pod downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7 to disappear
Sep  6 21:57:26.610: INFO: Pod downwardapi-volume-685d6c50-8e0a-4dc5-8982-81abff5ea8d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:57:26.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8729" for this suite.
Sep  6 21:57:32.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:57:32.896: INFO: namespace projected-8729 deletion completed in 6.282032183s

• [SLOW TEST:8.506 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:57:32.896: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 21:57:32.979: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:57:37.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9357" for this suite.
Sep  6 21:58:25.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:58:25.445: INFO: namespace pods-9357 deletion completed in 48.265162453s

• [SLOW TEST:52.549 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:58:25.446: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:58:25.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3899" for this suite.
Sep  6 21:58:31.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:58:31.791: INFO: namespace kubelet-test-3899 deletion completed in 6.154521282s

• [SLOW TEST:6.345 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:58:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 21:58:34.396: INFO: Successfully updated pod "annotationupdate0f566c45-8400-453a-93b5-75cdb23619c6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:58:38.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8517" for this suite.
Sep  6 21:59:00.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:59:00.673: INFO: namespace projected-8517 deletion completed in 22.237262495s

• [SLOW TEST:28.883 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:59:00.675: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 21:59:06.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9978" for this suite.
Sep  6 21:59:12.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:59:13.285: INFO: namespace namespaces-9978 deletion completed in 6.384030569s
STEP: Destroying namespace "nsdeletetest-4078" for this suite.
Sep  6 21:59:13.288: INFO: Namespace nsdeletetest-4078 was already deleted
STEP: Destroying namespace "nsdeletetest-1038" for this suite.
Sep  6 21:59:19.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 21:59:19.421: INFO: namespace nsdeletetest-1038 deletion completed in 6.132557518s

• [SLOW TEST:18.746 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 21:59:19.421: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 21:59:19.505: INFO: Create a RollingUpdate DaemonSet
Sep  6 21:59:19.513: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 21:59:19.531: INFO: Number of nodes with available pods: 0
Sep  6 21:59:19.531: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 21:59:20.551: INFO: Number of nodes with available pods: 0
Sep  6 21:59:20.551: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 21:59:21.539: INFO: Number of nodes with available pods: 0
Sep  6 21:59:21.539: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 21:59:22.553: INFO: Number of nodes with available pods: 1
Sep  6 21:59:22.553: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 21:59:23.540: INFO: Number of nodes with available pods: 2
Sep  6 21:59:23.540: INFO: Number of running nodes: 2, number of available pods: 2
Sep  6 21:59:23.540: INFO: Update the DaemonSet to trigger a rollout
Sep  6 21:59:23.548: INFO: Updating DaemonSet daemon-set
Sep  6 21:59:27.563: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 21:59:27.572: INFO: Updating DaemonSet daemon-set
Sep  6 21:59:27.572: INFO: Make sure DaemonSet rollback is complete
Sep  6 21:59:27.582: INFO: Wrong image for pod: daemon-set-rz4wz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 21:59:27.582: INFO: Pod daemon-set-rz4wz is not available
Sep  6 21:59:28.591: INFO: Wrong image for pod: daemon-set-rz4wz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 21:59:28.591: INFO: Pod daemon-set-rz4wz is not available
Sep  6 21:59:29.591: INFO: Wrong image for pod: daemon-set-rz4wz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep  6 21:59:29.592: INFO: Pod daemon-set-rz4wz is not available
Sep  6 21:59:30.592: INFO: Pod daemon-set-dczp8 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9701, will wait for the garbage collector to delete the pods
Sep  6 21:59:30.691: INFO: Deleting DaemonSet.extensions daemon-set took: 8.957456ms
Sep  6 21:59:31.191: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.22396ms
Sep  6 22:00:40.703: INFO: Number of nodes with available pods: 0
Sep  6 22:00:40.703: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 22:00:40.710: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9701/daemonsets","resourceVersion":"51013"},"items":null}

Sep  6 22:00:40.714: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9701/pods","resourceVersion":"51013"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:00:40.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9701" for this suite.
Sep  6 22:00:46.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:00:46.886: INFO: namespace daemonsets-9701 deletion completed in 6.157159211s

• [SLOW TEST:87.465 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:00:46.887: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 22:00:54.987: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:00:54.991: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:00:56.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:00:56.994: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:00:58.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:00:59.003: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:00.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:00.995: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:02.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:02.995: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:04.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:05.001: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:06.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:06.995: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:08.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:08.996: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:10.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:10.997: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:12.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:12.995: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:14.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:15.177: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:16.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:16.995: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:18.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:18.994: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:20.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:20.996: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:22.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:22.994: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 22:01:24.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 22:01:24.995: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:01:24.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8504" for this suite.
Sep  6 22:01:47.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:01:47.170: INFO: namespace container-lifecycle-hook-8504 deletion completed in 22.170888473s

• [SLOW TEST:60.283 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:01:47.171: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7893.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7893.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 22:01:59.281: INFO: DNS probes using dns-7893/dns-test-090f0fa3-8edb-4d5b-99df-00d232940026 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:01:59.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7893" for this suite.
Sep  6 22:02:05.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:02:05.612: INFO: namespace dns-7893 deletion completed in 6.295134087s

• [SLOW TEST:18.441 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:02:05.612: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:02:05.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78" in namespace "projected-7422" to be "success or failure"
Sep  6 22:02:05.704: INFO: Pod "downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78": Phase="Pending", Reason="", readiness=false. Elapsed: 7.77719ms
Sep  6 22:02:07.709: INFO: Pod "downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012928318s
STEP: Saw pod success
Sep  6 22:02:07.709: INFO: Pod "downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78" satisfied condition "success or failure"
Sep  6 22:02:07.716: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78 container client-container: <nil>
STEP: delete the pod
Sep  6 22:02:07.744: INFO: Waiting for pod downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78 to disappear
Sep  6 22:02:07.748: INFO: Pod downwardapi-volume-9dbc2459-0d2b-4346-98f3-1b2fe4225f78 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:02:07.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7422" for this suite.
Sep  6 22:02:13.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:02:13.932: INFO: namespace projected-7422 deletion completed in 6.179508947s

• [SLOW TEST:8.319 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:02:13.933: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 22:02:13.992: INFO: Waiting up to 5m0s for pod "downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba" in namespace "downward-api-4246" to be "success or failure"
Sep  6 22:02:13.997: INFO: Pod "downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.631489ms
Sep  6 22:02:16.011: INFO: Pod "downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019489186s
STEP: Saw pod success
Sep  6 22:02:16.011: INFO: Pod "downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba" satisfied condition "success or failure"
Sep  6 22:02:16.016: INFO: Trying to get logs from node metalk8s-24-node1 pod downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba container dapi-container: <nil>
STEP: delete the pod
Sep  6 22:02:16.042: INFO: Waiting for pod downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba to disappear
Sep  6 22:02:16.046: INFO: Pod downward-api-3d3a5609-3be6-48d6-8105-1a816447b5ba no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:02:16.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4246" for this suite.
Sep  6 22:02:22.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:02:22.203: INFO: namespace downward-api-4246 deletion completed in 6.152581497s

• [SLOW TEST:8.270 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:02:22.203: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  6 22:02:22.231: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Sep  6 22:02:22.819: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  6 22:02:24.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:02:26.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:02:28.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:02:30.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703404142, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:02:34.031: INFO: Waited 1.127273487s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:02:34.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7402" for this suite.
Sep  6 22:02:40.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:02:41.218: INFO: namespace aggregator-7402 deletion completed in 6.468558972s

• [SLOW TEST:19.015 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:02:41.218: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-b1d2d23b-8f32-49f5-8a31-14b144d427d9
STEP: Creating a pod to test consume secrets
Sep  6 22:02:41.385: INFO: Waiting up to 5m0s for pod "pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da" in namespace "secrets-5849" to be "success or failure"
Sep  6 22:02:41.424: INFO: Pod "pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da": Phase="Pending", Reason="", readiness=false. Elapsed: 39.428059ms
Sep  6 22:02:43.429: INFO: Pod "pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044484791s
STEP: Saw pod success
Sep  6 22:02:43.429: INFO: Pod "pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da" satisfied condition "success or failure"
Sep  6 22:02:43.433: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:02:43.470: INFO: Waiting for pod pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da to disappear
Sep  6 22:02:43.476: INFO: Pod pod-secrets-2e2913c7-2c15-41d7-a5c9-060dfc04e9da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:02:43.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5849" for this suite.
Sep  6 22:02:49.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:02:49.642: INFO: namespace secrets-5849 deletion completed in 6.160186246s

• [SLOW TEST:8.423 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:02:49.642: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 22:02:49.683: INFO: Waiting up to 5m0s for pod "pod-c2667af7-6e54-44e7-acbe-67f0edd939c6" in namespace "emptydir-24" to be "success or failure"
Sep  6 22:02:49.692: INFO: Pod "pod-c2667af7-6e54-44e7-acbe-67f0edd939c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727432ms
Sep  6 22:02:51.697: INFO: Pod "pod-c2667af7-6e54-44e7-acbe-67f0edd939c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013825694s
STEP: Saw pod success
Sep  6 22:02:51.697: INFO: Pod "pod-c2667af7-6e54-44e7-acbe-67f0edd939c6" satisfied condition "success or failure"
Sep  6 22:02:51.701: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-c2667af7-6e54-44e7-acbe-67f0edd939c6 container test-container: <nil>
STEP: delete the pod
Sep  6 22:02:51.731: INFO: Waiting for pod pod-c2667af7-6e54-44e7-acbe-67f0edd939c6 to disappear
Sep  6 22:02:51.738: INFO: Pod pod-c2667af7-6e54-44e7-acbe-67f0edd939c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:02:51.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-24" for this suite.
Sep  6 22:02:57.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:02:57.930: INFO: namespace emptydir-24 deletion completed in 6.170745897s

• [SLOW TEST:8.288 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:02:57.930: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:02:58.022: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922" in namespace "projected-1973" to be "success or failure"
Sep  6 22:02:58.035: INFO: Pod "downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922": Phase="Pending", Reason="", readiness=false. Elapsed: 12.448906ms
Sep  6 22:03:00.038: INFO: Pod "downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016172408s
STEP: Saw pod success
Sep  6 22:03:00.039: INFO: Pod "downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922" satisfied condition "success or failure"
Sep  6 22:03:00.041: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922 container client-container: <nil>
STEP: delete the pod
Sep  6 22:03:00.056: INFO: Waiting for pod downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922 to disappear
Sep  6 22:03:00.061: INFO: Pod downwardapi-volume-2e3d7cdb-f0d7-4466-9512-c37518b8e922 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:03:00.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1973" for this suite.
Sep  6 22:03:06.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:03:06.384: INFO: namespace projected-1973 deletion completed in 6.316158263s

• [SLOW TEST:8.455 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:03:06.385: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-119f7a13-d8f6-4050-a5c0-b16ae93f6d7b
STEP: Creating a pod to test consume configMaps
Sep  6 22:03:06.481: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5" in namespace "projected-7927" to be "success or failure"
Sep  6 22:03:06.495: INFO: Pod "pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.625172ms
Sep  6 22:03:08.500: INFO: Pod "pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0190894s
STEP: Saw pod success
Sep  6 22:03:08.500: INFO: Pod "pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5" satisfied condition "success or failure"
Sep  6 22:03:08.503: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:03:08.522: INFO: Waiting for pod pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5 to disappear
Sep  6 22:03:08.527: INFO: Pod pod-projected-configmaps-120867f8-a5da-4953-9ff9-8c12f54728b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:03:08.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7927" for this suite.
Sep  6 22:03:14.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:03:14.709: INFO: namespace projected-7927 deletion completed in 6.176708129s

• [SLOW TEST:8.324 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:03:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-784c441b-76c9-4d89-8bbb-d1ea12920549
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-784c441b-76c9-4d89-8bbb-d1ea12920549
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:03:18.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8479" for this suite.
Sep  6 22:03:40.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:03:41.255: INFO: namespace configmap-8479 deletion completed in 22.40699489s

• [SLOW TEST:26.546 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:03:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-128b476b-8c0a-4458-90e2-929ceb603620
STEP: Creating a pod to test consume secrets
Sep  6 22:03:41.569: INFO: Waiting up to 5m0s for pod "pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d" in namespace "secrets-492" to be "success or failure"
Sep  6 22:03:41.576: INFO: Pod "pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.692284ms
Sep  6 22:03:43.581: INFO: Pod "pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011966703s
Sep  6 22:03:45.597: INFO: Pod "pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027542783s
STEP: Saw pod success
Sep  6 22:03:45.597: INFO: Pod "pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d" satisfied condition "success or failure"
Sep  6 22:03:45.602: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:03:45.636: INFO: Waiting for pod pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d to disappear
Sep  6 22:03:45.648: INFO: Pod pod-secrets-eb9a3d19-7f39-4497-8a05-2aae45c3873d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:03:45.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-492" for this suite.
Sep  6 22:03:51.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:03:51.983: INFO: namespace secrets-492 deletion completed in 6.315493924s
STEP: Destroying namespace "secret-namespace-618" for this suite.
Sep  6 22:03:57.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:03:58.224: INFO: namespace secret-namespace-618 deletion completed in 6.240525468s

• [SLOW TEST:16.968 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:03:58.224: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5466
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5466
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5466
Sep  6 22:03:58.341: INFO: Found 0 stateful pods, waiting for 1
Sep  6 22:04:08.349: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  6 22:04:08.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 22:04:08.609: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 22:04:08.609: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 22:04:08.609: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 22:04:08.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 22:04:18.625: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 22:04:18.625: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 22:04:18.648: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Sep  6 22:04:18.648: INFO: ss-0  metalk8s-24-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  }]
Sep  6 22:04:18.648: INFO: 
Sep  6 22:04:18.648: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  6 22:04:19.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992319724s
Sep  6 22:04:20.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983164003s
Sep  6 22:04:21.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975226125s
Sep  6 22:04:22.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968977299s
Sep  6 22:04:23.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958823735s
Sep  6 22:04:24.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95339688s
Sep  6 22:04:25.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.937835559s
Sep  6 22:04:26.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929239465s
Sep  6 22:04:27.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.960743ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5466
Sep  6 22:04:28.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 22:04:28.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 22:04:28.956: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 22:04:28.956: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 22:04:28.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 22:04:29.341: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 22:04:29.341: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 22:04:29.341: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 22:04:29.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 22:04:29.543: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  6 22:04:29.543: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 22:04:29.543: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 22:04:29.550: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep  6 22:04:39.557: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:04:39.558: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:04:39.558: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  6 22:04:39.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 22:04:39.767: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 22:04:39.767: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 22:04:39.767: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 22:04:39.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 22:04:40.357: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 22:04:40.357: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 22:04:40.357: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 22:04:40.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-5466 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 22:04:40.612: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 22:04:40.613: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 22:04:40.613: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 22:04:40.613: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 22:04:40.626: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  6 22:04:50.650: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 22:04:50.650: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 22:04:50.650: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 22:04:50.693: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Sep  6 22:04:50.693: INFO: ss-0  metalk8s-24-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  }]
Sep  6 22:04:50.693: INFO: ss-1  metalk8s-24        Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  }]
Sep  6 22:04:50.693: INFO: ss-2  metalk8s-24-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  }]
Sep  6 22:04:50.693: INFO: 
Sep  6 22:04:50.693: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 22:04:51.710: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Sep  6 22:04:51.710: INFO: ss-0  metalk8s-24-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  }]
Sep  6 22:04:51.710: INFO: ss-1  metalk8s-24        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  }]
Sep  6 22:04:51.710: INFO: ss-2  metalk8s-24-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  }]
Sep  6 22:04:51.710: INFO: 
Sep  6 22:04:51.710: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 22:04:52.750: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Sep  6 22:04:52.750: INFO: ss-0  metalk8s-24-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:03:58 +0000 UTC  }]
Sep  6 22:04:52.750: INFO: ss-1  metalk8s-24        Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  }]
Sep  6 22:04:52.750: INFO: ss-2  metalk8s-24-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:04:18 +0000 UTC  }]
Sep  6 22:04:52.750: INFO: 
Sep  6 22:04:52.750: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  6 22:04:53.754: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.918623062s
Sep  6 22:04:54.758: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.914918878s
Sep  6 22:04:55.763: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.910736517s
Sep  6 22:04:56.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.905808264s
Sep  6 22:04:57.771: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.901680877s
Sep  6 22:04:58.790: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.897960914s
Sep  6 22:04:59.794: INFO: Verifying statefulset ss doesn't scale past 0 for another 878.705027ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5466
Sep  6 22:05:00.799: INFO: Scaling statefulset ss to 0
Sep  6 22:05:00.813: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 22:05:00.817: INFO: Deleting all statefulset in ns statefulset-5466
Sep  6 22:05:00.821: INFO: Scaling statefulset ss to 0
Sep  6 22:05:00.837: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 22:05:00.841: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:05:00.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5466" for this suite.
Sep  6 22:05:06.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:05:07.048: INFO: namespace statefulset-5466 deletion completed in 6.166409739s

• [SLOW TEST:68.824 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:05:07.048: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-pnpc
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 22:05:07.125: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pnpc" in namespace "subpath-5105" to be "success or failure"
Sep  6 22:05:07.130: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289726ms
Sep  6 22:05:09.138: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 2.012830691s
Sep  6 22:05:11.143: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 4.017315533s
Sep  6 22:05:13.147: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 6.021960603s
Sep  6 22:05:15.278: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 8.152301573s
Sep  6 22:05:17.290: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 10.16410529s
Sep  6 22:05:19.294: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 12.168518127s
Sep  6 22:05:21.835: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 14.709207814s
Sep  6 22:05:23.839: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 16.713366049s
Sep  6 22:05:25.843: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 18.717036901s
Sep  6 22:05:27.846: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Running", Reason="", readiness=true. Elapsed: 20.7207438s
Sep  6 22:05:29.850: INFO: Pod "pod-subpath-test-projected-pnpc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.724933163s
STEP: Saw pod success
Sep  6 22:05:29.850: INFO: Pod "pod-subpath-test-projected-pnpc" satisfied condition "success or failure"
Sep  6 22:05:29.854: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-subpath-test-projected-pnpc container test-container-subpath-projected-pnpc: <nil>
STEP: delete the pod
Sep  6 22:05:29.870: INFO: Waiting for pod pod-subpath-test-projected-pnpc to disappear
Sep  6 22:05:29.873: INFO: Pod pod-subpath-test-projected-pnpc no longer exists
STEP: Deleting pod pod-subpath-test-projected-pnpc
Sep  6 22:05:29.873: INFO: Deleting pod "pod-subpath-test-projected-pnpc" in namespace "subpath-5105"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:05:29.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5105" for this suite.
Sep  6 22:05:35.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:05:36.060: INFO: namespace subpath-5105 deletion completed in 6.179724418s

• [SLOW TEST:29.012 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:05:36.060: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 22:05:36.174: INFO: Number of nodes with available pods: 0
Sep  6 22:05:36.174: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:05:37.188: INFO: Number of nodes with available pods: 0
Sep  6 22:05:37.188: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:05:38.183: INFO: Number of nodes with available pods: 1
Sep  6 22:05:38.183: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:05:39.186: INFO: Number of nodes with available pods: 2
Sep  6 22:05:39.186: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  6 22:05:39.213: INFO: Number of nodes with available pods: 1
Sep  6 22:05:39.213: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:05:40.237: INFO: Number of nodes with available pods: 1
Sep  6 22:05:40.237: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:05:41.225: INFO: Number of nodes with available pods: 1
Sep  6 22:05:41.225: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:05:42.246: INFO: Number of nodes with available pods: 1
Sep  6 22:05:42.246: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:05:43.239: INFO: Number of nodes with available pods: 1
Sep  6 22:05:43.239: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:05:44.221: INFO: Number of nodes with available pods: 1
Sep  6 22:05:44.221: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:05:45.244: INFO: Number of nodes with available pods: 2
Sep  6 22:05:45.244: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2677, will wait for the garbage collector to delete the pods
Sep  6 22:05:45.337: INFO: Deleting DaemonSet.extensions daemon-set took: 29.05355ms
Sep  6 22:05:45.837: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.402302ms
Sep  6 22:05:54.051: INFO: Number of nodes with available pods: 0
Sep  6 22:05:54.051: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 22:05:54.055: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2677/daemonsets","resourceVersion":"52389"},"items":null}

Sep  6 22:05:54.061: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2677/pods","resourceVersion":"52389"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:05:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2677" for this suite.
Sep  6 22:06:00.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:06:00.487: INFO: namespace daemonsets-2677 deletion completed in 6.400678928s

• [SLOW TEST:24.427 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:06:00.487: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 22:06:00.652: INFO: Waiting up to 5m0s for pod "pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8" in namespace "emptydir-4073" to be "success or failure"
Sep  6 22:06:00.657: INFO: Pod "pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239295ms
Sep  6 22:06:02.661: INFO: Pod "pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009193794s
Sep  6 22:06:04.665: INFO: Pod "pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013056953s
STEP: Saw pod success
Sep  6 22:06:04.665: INFO: Pod "pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8" satisfied condition "success or failure"
Sep  6 22:06:04.669: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8 container test-container: <nil>
STEP: delete the pod
Sep  6 22:06:04.693: INFO: Waiting for pod pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8 to disappear
Sep  6 22:06:04.696: INFO: Pod pod-3eb8f098-fc6a-42fb-bf4c-7f8f85aaedd8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:06:04.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4073" for this suite.
Sep  6 22:06:10.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:06:10.863: INFO: namespace emptydir-4073 deletion completed in 6.163088971s

• [SLOW TEST:10.376 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:06:10.863: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep  6 22:06:41.587: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:06:41.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0906 22:06:41.587102      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2052" for this suite.
Sep  6 22:06:49.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:06:49.791: INFO: namespace gc-2052 deletion completed in 8.197720192s

• [SLOW TEST:38.928 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:06:49.791: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1743
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1743
STEP: Creating statefulset with conflicting port in namespace statefulset-1743
STEP: Waiting until pod test-pod will start running in namespace statefulset-1743
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1743
Sep  6 22:06:54.031: INFO: Observed stateful pod in namespace: statefulset-1743, name: ss-0, uid: 87f442ad-dbe0-4b4e-b4b6-8d7f3b802ee9, status phase: Pending. Waiting for statefulset controller to delete.
Sep  6 22:06:54.158: INFO: Observed stateful pod in namespace: statefulset-1743, name: ss-0, uid: 87f442ad-dbe0-4b4e-b4b6-8d7f3b802ee9, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 22:06:54.199: INFO: Observed stateful pod in namespace: statefulset-1743, name: ss-0, uid: 87f442ad-dbe0-4b4e-b4b6-8d7f3b802ee9, status phase: Failed. Waiting for statefulset controller to delete.
Sep  6 22:06:54.217: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1743
STEP: Removing pod with conflicting port in namespace statefulset-1743
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1743 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 22:06:58.280: INFO: Deleting all statefulset in ns statefulset-1743
Sep  6 22:06:58.284: INFO: Scaling statefulset ss to 0
Sep  6 22:07:18.302: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 22:07:18.307: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:07:18.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1743" for this suite.
Sep  6 22:07:24.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:07:24.741: INFO: namespace statefulset-1743 deletion completed in 6.402079685s

• [SLOW TEST:34.950 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:07:24.741: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 22:07:24.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8450'
Sep  6 22:07:24.988: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 22:07:24.988: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Sep  6 22:07:24.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete jobs e2e-test-nginx-job --namespace=kubectl-8450'
Sep  6 22:07:25.089: INFO: stderr: ""
Sep  6 22:07:25.089: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:07:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8450" for this suite.
Sep  6 22:07:47.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:07:47.639: INFO: namespace kubectl-8450 deletion completed in 22.531204609s

• [SLOW TEST:22.897 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:07:47.639: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:07:47.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827" in namespace "downward-api-9670" to be "success or failure"
Sep  6 22:07:47.780: INFO: Pod "downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827": Phase="Pending", Reason="", readiness=false. Elapsed: 50.187758ms
Sep  6 22:07:49.785: INFO: Pod "downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054339524s
STEP: Saw pod success
Sep  6 22:07:49.785: INFO: Pod "downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827" satisfied condition "success or failure"
Sep  6 22:07:49.797: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827 container client-container: <nil>
STEP: delete the pod
Sep  6 22:07:49.832: INFO: Waiting for pod downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827 to disappear
Sep  6 22:07:49.836: INFO: Pod downwardapi-volume-1afea2cb-001c-425d-a588-e80d67518827 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:07:49.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9670" for this suite.
Sep  6 22:07:55.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:07:55.976: INFO: namespace downward-api-9670 deletion completed in 6.131060762s

• [SLOW TEST:8.337 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:07:55.976: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Sep  6 22:07:58.083: INFO: Pod pod-hostip-562bbed6-ba2f-4cbb-b554-032e528faa8a has hostIP: 10.10.0.45
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:07:58.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9889" for this suite.
Sep  6 22:08:20.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:08:20.426: INFO: namespace pods-9889 deletion completed in 22.336876634s

• [SLOW TEST:24.450 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:08:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-5aef4804-3fde-4422-aa2b-b2bc236e53ef
STEP: Creating secret with name s-test-opt-upd-51715e7e-e4be-4cb0-8867-6e153543527b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5aef4804-3fde-4422-aa2b-b2bc236e53ef
STEP: Updating secret s-test-opt-upd-51715e7e-e4be-4cb0-8867-6e153543527b
STEP: Creating secret with name s-test-opt-create-f2b763d7-e338-4cef-99c9-a7f8be4728f5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:08:24.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4284" for this suite.
Sep  6 22:08:48.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:08:49.032: INFO: namespace projected-4284 deletion completed in 24.191033107s

• [SLOW TEST:28.606 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:08:49.033: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  6 22:08:49.120: INFO: Waiting up to 5m0s for pod "pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395" in namespace "emptydir-4953" to be "success or failure"
Sep  6 22:08:49.136: INFO: Pod "pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395": Phase="Pending", Reason="", readiness=false. Elapsed: 15.46375ms
Sep  6 22:08:51.140: INFO: Pod "pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01915513s
STEP: Saw pod success
Sep  6 22:08:51.140: INFO: Pod "pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395" satisfied condition "success or failure"
Sep  6 22:08:51.143: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395 container test-container: <nil>
STEP: delete the pod
Sep  6 22:08:51.177: INFO: Waiting for pod pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395 to disappear
Sep  6 22:08:51.182: INFO: Pod pod-8bd7c5ed-0cbd-46b9-acbf-8a24566e9395 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:08:51.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4953" for this suite.
Sep  6 22:08:57.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:08:57.369: INFO: namespace emptydir-4953 deletion completed in 6.180343596s

• [SLOW TEST:8.337 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:08:57.370: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-aaf5fd8f-32c5-4e74-924d-aeb756cc4d55 in namespace container-probe-1205
Sep  6 22:08:59.493: INFO: Started pod busybox-aaf5fd8f-32c5-4e74-924d-aeb756cc4d55 in namespace container-probe-1205
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 22:08:59.496: INFO: Initial restart count of pod busybox-aaf5fd8f-32c5-4e74-924d-aeb756cc4d55 is 0
Sep  6 22:09:49.640: INFO: Restart count of pod container-probe-1205/busybox-aaf5fd8f-32c5-4e74-924d-aeb756cc4d55 is now 1 (50.143321862s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:09:49.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1205" for this suite.
Sep  6 22:09:55.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:09:55.843: INFO: namespace container-probe-1205 deletion completed in 6.182039406s

• [SLOW TEST:58.473 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:09:55.843: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-91521dae-9cca-4e75-8fee-de9e2217f822
STEP: Creating a pod to test consume secrets
Sep  6 22:09:55.914: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb" in namespace "projected-2111" to be "success or failure"
Sep  6 22:09:55.925: INFO: Pod "pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.460921ms
Sep  6 22:09:57.928: INFO: Pod "pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013987256s
STEP: Saw pod success
Sep  6 22:09:57.928: INFO: Pod "pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb" satisfied condition "success or failure"
Sep  6 22:09:57.931: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:09:57.947: INFO: Waiting for pod pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb to disappear
Sep  6 22:09:57.950: INFO: Pod pod-projected-secrets-d3c48d2c-faa4-4edb-87ad-9167d1cd42cb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:09:57.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2111" for this suite.
Sep  6 22:10:03.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:10:04.171: INFO: namespace projected-2111 deletion completed in 6.217139867s

• [SLOW TEST:8.328 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:10:04.171: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:10:04.252: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:10:05.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6183" for this suite.
Sep  6 22:10:11.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:10:11.636: INFO: namespace custom-resource-definition-6183 deletion completed in 6.264777902s

• [SLOW TEST:7.465 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:10:11.636: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:10:11.762: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 22:10:11.794: INFO: Number of nodes with available pods: 0
Sep  6 22:10:11.794: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:10:12.809: INFO: Number of nodes with available pods: 0
Sep  6 22:10:12.809: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:10:13.826: INFO: Number of nodes with available pods: 1
Sep  6 22:10:13.826: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:10:14.806: INFO: Number of nodes with available pods: 2
Sep  6 22:10:14.806: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  6 22:10:14.862: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:14.862: INFO: Wrong image for pod: daemon-set-7q8vs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:15.887: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:15.887: INFO: Wrong image for pod: daemon-set-7q8vs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:16.897: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:16.897: INFO: Wrong image for pod: daemon-set-7q8vs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:17.889: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:17.889: INFO: Wrong image for pod: daemon-set-7q8vs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:18.884: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:18.884: INFO: Wrong image for pod: daemon-set-7q8vs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:18.884: INFO: Pod daemon-set-7q8vs is not available
Sep  6 22:10:19.887: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:19.887: INFO: Pod daemon-set-cxgxg is not available
Sep  6 22:10:20.894: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:20.894: INFO: Pod daemon-set-cxgxg is not available
Sep  6 22:10:21.885: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:22.885: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:22.885: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:23.884: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:23.884: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:25.139: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:25.139: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:25.884: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:25.884: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:26.883: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:26.883: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:27.884: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:27.884: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:28.883: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:28.883: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:29.883: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:29.883: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:30.890: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:30.890: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:31.885: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:31.885: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:32.895: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:32.895: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:33.890: INFO: Wrong image for pod: daemon-set-4b868. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep  6 22:10:33.890: INFO: Pod daemon-set-4b868 is not available
Sep  6 22:10:34.883: INFO: Pod daemon-set-5zrsm is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  6 22:10:34.895: INFO: Number of nodes with available pods: 1
Sep  6 22:10:34.895: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:10:35.904: INFO: Number of nodes with available pods: 1
Sep  6 22:10:35.904: INFO: Node metalk8s-24-node1 is running more than one daemon pod
Sep  6 22:10:36.906: INFO: Number of nodes with available pods: 2
Sep  6 22:10:36.906: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8881, will wait for the garbage collector to delete the pods
Sep  6 22:10:36.999: INFO: Deleting DaemonSet.extensions daemon-set took: 23.039677ms
Sep  6 22:10:37.500: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.331869ms
Sep  6 22:10:40.707: INFO: Number of nodes with available pods: 0
Sep  6 22:10:40.707: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 22:10:40.709: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8881/daemonsets","resourceVersion":"53591"},"items":null}

Sep  6 22:10:40.716: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8881/pods","resourceVersion":"53591"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:10:40.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8881" for this suite.
Sep  6 22:10:46.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:10:46.977: INFO: namespace daemonsets-8881 deletion completed in 6.238048607s

• [SLOW TEST:35.340 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:10:46.977: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3736/configmap-test-b8c26629-0b89-464a-9ee5-0f24a02f101d
STEP: Creating a pod to test consume configMaps
Sep  6 22:10:47.271: INFO: Waiting up to 5m0s for pod "pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c" in namespace "configmap-3736" to be "success or failure"
Sep  6 22:10:47.283: INFO: Pod "pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.391838ms
Sep  6 22:10:49.289: INFO: Pod "pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017645821s
Sep  6 22:10:51.293: INFO: Pod "pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022319865s
STEP: Saw pod success
Sep  6 22:10:51.293: INFO: Pod "pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c" satisfied condition "success or failure"
Sep  6 22:10:51.296: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c container env-test: <nil>
STEP: delete the pod
Sep  6 22:10:51.322: INFO: Waiting for pod pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c to disappear
Sep  6 22:10:51.328: INFO: Pod pod-configmaps-21655048-e4dc-4dcd-a8d7-c1682482bd3c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:10:51.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3736" for this suite.
Sep  6 22:10:57.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:10:57.510: INFO: namespace configmap-3736 deletion completed in 6.178194928s

• [SLOW TEST:10.533 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:10:57.510: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-pbzm
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 22:10:57.577: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pbzm" in namespace "subpath-5791" to be "success or failure"
Sep  6 22:10:57.585: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.839812ms
Sep  6 22:10:59.589: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01175111s
Sep  6 22:11:01.604: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 4.026719566s
Sep  6 22:11:03.613: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 6.035699916s
Sep  6 22:11:05.627: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 8.050017654s
Sep  6 22:11:07.635: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 10.058257789s
Sep  6 22:11:09.640: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 12.06272868s
Sep  6 22:11:11.655: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 14.077959194s
Sep  6 22:11:13.666: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 16.088610546s
Sep  6 22:11:15.677: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 18.100204448s
Sep  6 22:11:17.701: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Running", Reason="", readiness=true. Elapsed: 20.124442477s
Sep  6 22:11:19.706: INFO: Pod "pod-subpath-test-configmap-pbzm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.128709169s
STEP: Saw pod success
Sep  6 22:11:19.706: INFO: Pod "pod-subpath-test-configmap-pbzm" satisfied condition "success or failure"
Sep  6 22:11:19.711: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-subpath-test-configmap-pbzm container test-container-subpath-configmap-pbzm: <nil>
STEP: delete the pod
Sep  6 22:11:19.755: INFO: Waiting for pod pod-subpath-test-configmap-pbzm to disappear
Sep  6 22:11:19.765: INFO: Pod pod-subpath-test-configmap-pbzm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pbzm
Sep  6 22:11:19.765: INFO: Deleting pod "pod-subpath-test-configmap-pbzm" in namespace "subpath-5791"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:11:19.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5791" for this suite.
Sep  6 22:11:25.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:11:25.995: INFO: namespace subpath-5791 deletion completed in 6.221407611s

• [SLOW TEST:28.484 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:11:25.995: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Sep  6 22:11:26.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-8290'
Sep  6 22:11:26.367: INFO: stderr: ""
Sep  6 22:11:26.367: INFO: stdout: "pod/pause created\n"
Sep  6 22:11:26.367: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 22:11:26.367: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8290" to be "running and ready"
Sep  6 22:11:26.373: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284487ms
Sep  6 22:11:28.378: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011374715s
Sep  6 22:11:28.378: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 22:11:28.378: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  6 22:11:28.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 label pods pause testing-label=testing-label-value --namespace=kubectl-8290'
Sep  6 22:11:28.457: INFO: stderr: ""
Sep  6 22:11:28.457: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  6 22:11:28.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pod pause -L testing-label --namespace=kubectl-8290'
Sep  6 22:11:28.539: INFO: stderr: ""
Sep  6 22:11:28.539: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  6 22:11:28.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 label pods pause testing-label- --namespace=kubectl-8290'
Sep  6 22:11:28.611: INFO: stderr: ""
Sep  6 22:11:28.611: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  6 22:11:28.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pod pause -L testing-label --namespace=kubectl-8290'
Sep  6 22:11:28.685: INFO: stderr: ""
Sep  6 22:11:28.685: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Sep  6 22:11:28.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-8290'
Sep  6 22:11:28.754: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:11:28.754: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 22:11:28.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get rc,svc -l name=pause --no-headers --namespace=kubectl-8290'
Sep  6 22:11:28.837: INFO: stderr: "No resources found.\n"
Sep  6 22:11:28.837: INFO: stdout: ""
Sep  6 22:11:28.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -l name=pause --namespace=kubectl-8290 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 22:11:28.902: INFO: stderr: ""
Sep  6 22:11:28.902: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:11:28.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8290" for this suite.
Sep  6 22:11:34.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:11:35.079: INFO: namespace kubectl-8290 deletion completed in 6.171640588s

• [SLOW TEST:9.084 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:11:35.079: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3875
I0906 22:11:35.147500      17 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3875, replica count: 1
I0906 22:11:36.197954      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 22:11:37.198200      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 22:11:37.311: INFO: Created: latency-svc-9kh8r
Sep  6 22:11:37.316: INFO: Got endpoints: latency-svc-9kh8r [18.038939ms]
Sep  6 22:11:37.335: INFO: Created: latency-svc-kvtzx
Sep  6 22:11:37.347: INFO: Created: latency-svc-zpnfv
Sep  6 22:11:37.347: INFO: Got endpoints: latency-svc-kvtzx [30.021873ms]
Sep  6 22:11:37.353: INFO: Got endpoints: latency-svc-zpnfv [36.135399ms]
Sep  6 22:11:37.360: INFO: Created: latency-svc-8h5wq
Sep  6 22:11:37.369: INFO: Got endpoints: latency-svc-8h5wq [51.376165ms]
Sep  6 22:11:37.380: INFO: Created: latency-svc-27bz4
Sep  6 22:11:37.384: INFO: Got endpoints: latency-svc-27bz4 [66.834117ms]
Sep  6 22:11:37.402: INFO: Created: latency-svc-twcbz
Sep  6 22:11:37.418: INFO: Got endpoints: latency-svc-twcbz [100.186815ms]
Sep  6 22:11:37.422: INFO: Created: latency-svc-xntjq
Sep  6 22:11:37.434: INFO: Got endpoints: latency-svc-xntjq [116.042333ms]
Sep  6 22:11:37.439: INFO: Created: latency-svc-dr6wb
Sep  6 22:11:37.442: INFO: Got endpoints: latency-svc-dr6wb [124.282239ms]
Sep  6 22:11:37.451: INFO: Created: latency-svc-jb2lh
Sep  6 22:11:37.454: INFO: Got endpoints: latency-svc-jb2lh [136.803539ms]
Sep  6 22:11:37.458: INFO: Created: latency-svc-h4g74
Sep  6 22:11:37.461: INFO: Got endpoints: latency-svc-h4g74 [143.779424ms]
Sep  6 22:11:37.466: INFO: Created: latency-svc-77b22
Sep  6 22:11:37.477: INFO: Got endpoints: latency-svc-77b22 [159.189991ms]
Sep  6 22:11:37.479: INFO: Created: latency-svc-pnzqp
Sep  6 22:11:37.482: INFO: Got endpoints: latency-svc-pnzqp [163.70361ms]
Sep  6 22:11:37.497: INFO: Created: latency-svc-cc7bj
Sep  6 22:11:37.510: INFO: Got endpoints: latency-svc-cc7bj [191.969744ms]
Sep  6 22:11:37.525: INFO: Created: latency-svc-pznjw
Sep  6 22:11:37.541: INFO: Got endpoints: latency-svc-pznjw [222.447576ms]
Sep  6 22:11:37.541: INFO: Created: latency-svc-x7q5v
Sep  6 22:11:37.545: INFO: Got endpoints: latency-svc-x7q5v [227.091886ms]
Sep  6 22:11:37.557: INFO: Created: latency-svc-5krhq
Sep  6 22:11:37.560: INFO: Got endpoints: latency-svc-5krhq [242.201451ms]
Sep  6 22:11:37.581: INFO: Created: latency-svc-lj66b
Sep  6 22:11:37.584: INFO: Got endpoints: latency-svc-lj66b [236.425932ms]
Sep  6 22:11:37.591: INFO: Created: latency-svc-vsd9j
Sep  6 22:11:37.597: INFO: Got endpoints: latency-svc-vsd9j [243.908904ms]
Sep  6 22:11:37.610: INFO: Created: latency-svc-rrsvb
Sep  6 22:11:37.624: INFO: Got endpoints: latency-svc-rrsvb [254.924905ms]
Sep  6 22:11:37.631: INFO: Created: latency-svc-t2wst
Sep  6 22:11:37.635: INFO: Got endpoints: latency-svc-t2wst [250.923857ms]
Sep  6 22:11:37.644: INFO: Created: latency-svc-msgfw
Sep  6 22:11:37.647: INFO: Got endpoints: latency-svc-msgfw [229.041475ms]
Sep  6 22:11:37.657: INFO: Created: latency-svc-vwmvv
Sep  6 22:11:37.666: INFO: Got endpoints: latency-svc-vwmvv [232.160508ms]
Sep  6 22:11:37.668: INFO: Created: latency-svc-jnjvk
Sep  6 22:11:37.674: INFO: Got endpoints: latency-svc-jnjvk [231.998389ms]
Sep  6 22:11:37.698: INFO: Created: latency-svc-86kwl
Sep  6 22:11:37.760: INFO: Got endpoints: latency-svc-86kwl [305.510034ms]
Sep  6 22:11:37.761: INFO: Created: latency-svc-nfd6w
Sep  6 22:11:37.782: INFO: Created: latency-svc-bxpk6
Sep  6 22:11:37.782: INFO: Got endpoints: latency-svc-nfd6w [320.626584ms]
Sep  6 22:11:37.800: INFO: Got endpoints: latency-svc-bxpk6 [322.575664ms]
Sep  6 22:11:37.801: INFO: Created: latency-svc-g7bhq
Sep  6 22:11:37.819: INFO: Got endpoints: latency-svc-g7bhq [336.835956ms]
Sep  6 22:11:37.838: INFO: Created: latency-svc-mdtlb
Sep  6 22:11:37.856: INFO: Got endpoints: latency-svc-mdtlb [345.878587ms]
Sep  6 22:11:37.885: INFO: Created: latency-svc-5x85m
Sep  6 22:11:37.904: INFO: Got endpoints: latency-svc-5x85m [362.952155ms]
Sep  6 22:11:37.905: INFO: Created: latency-svc-7k6k5
Sep  6 22:11:37.919: INFO: Got endpoints: latency-svc-7k6k5 [373.533317ms]
Sep  6 22:11:37.940: INFO: Created: latency-svc-7kvsd
Sep  6 22:11:37.954: INFO: Got endpoints: latency-svc-7kvsd [394.030664ms]
Sep  6 22:11:37.955: INFO: Created: latency-svc-jwnt2
Sep  6 22:11:37.968: INFO: Got endpoints: latency-svc-jwnt2 [384.197235ms]
Sep  6 22:11:37.969: INFO: Created: latency-svc-rnn8s
Sep  6 22:11:37.982: INFO: Got endpoints: latency-svc-rnn8s [384.696327ms]
Sep  6 22:11:37.992: INFO: Created: latency-svc-4xn9w
Sep  6 22:11:37.997: INFO: Got endpoints: latency-svc-4xn9w [373.335484ms]
Sep  6 22:11:38.007: INFO: Created: latency-svc-xt2cz
Sep  6 22:11:38.029: INFO: Created: latency-svc-6p47v
Sep  6 22:11:38.031: INFO: Got endpoints: latency-svc-xt2cz [395.512627ms]
Sep  6 22:11:38.039: INFO: Got endpoints: latency-svc-6p47v [391.85399ms]
Sep  6 22:11:38.040: INFO: Created: latency-svc-d8v4b
Sep  6 22:11:38.064: INFO: Got endpoints: latency-svc-d8v4b [398.403883ms]
Sep  6 22:11:38.079: INFO: Created: latency-svc-gpzbj
Sep  6 22:11:38.107: INFO: Created: latency-svc-96d5l
Sep  6 22:11:38.108: INFO: Got endpoints: latency-svc-gpzbj [434.086145ms]
Sep  6 22:11:38.116: INFO: Got endpoints: latency-svc-96d5l [356.1161ms]
Sep  6 22:11:38.127: INFO: Created: latency-svc-vkbmx
Sep  6 22:11:38.130: INFO: Got endpoints: latency-svc-vkbmx [347.456197ms]
Sep  6 22:11:38.152: INFO: Created: latency-svc-bnpt4
Sep  6 22:11:38.159: INFO: Got endpoints: latency-svc-bnpt4 [358.967086ms]
Sep  6 22:11:38.203: INFO: Created: latency-svc-q2crc
Sep  6 22:11:38.223: INFO: Got endpoints: latency-svc-q2crc [404.612962ms]
Sep  6 22:11:38.231: INFO: Created: latency-svc-wfr6k
Sep  6 22:11:38.240: INFO: Got endpoints: latency-svc-wfr6k [383.665429ms]
Sep  6 22:11:38.252: INFO: Created: latency-svc-x6r7s
Sep  6 22:11:38.255: INFO: Got endpoints: latency-svc-x6r7s [351.237045ms]
Sep  6 22:11:38.265: INFO: Created: latency-svc-b6qpc
Sep  6 22:11:38.286: INFO: Got endpoints: latency-svc-b6qpc [367.355535ms]
Sep  6 22:11:38.326: INFO: Created: latency-svc-5kcgf
Sep  6 22:11:38.407: INFO: Got endpoints: latency-svc-5kcgf [452.625104ms]
Sep  6 22:11:38.426: INFO: Created: latency-svc-jbrsl
Sep  6 22:11:38.433: INFO: Got endpoints: latency-svc-jbrsl [465.165187ms]
Sep  6 22:11:38.441: INFO: Created: latency-svc-v5v79
Sep  6 22:11:38.450: INFO: Got endpoints: latency-svc-v5v79 [468.424555ms]
Sep  6 22:11:38.452: INFO: Created: latency-svc-56sn6
Sep  6 22:11:38.456: INFO: Got endpoints: latency-svc-56sn6 [458.673848ms]
Sep  6 22:11:38.463: INFO: Created: latency-svc-nrdr6
Sep  6 22:11:38.467: INFO: Got endpoints: latency-svc-nrdr6 [435.703234ms]
Sep  6 22:11:38.469: INFO: Created: latency-svc-7fhfs
Sep  6 22:11:38.476: INFO: Got endpoints: latency-svc-7fhfs [437.302928ms]
Sep  6 22:11:38.476: INFO: Created: latency-svc-xdf4s
Sep  6 22:11:38.487: INFO: Got endpoints: latency-svc-xdf4s [421.999336ms]
Sep  6 22:11:38.491: INFO: Created: latency-svc-c8xvn
Sep  6 22:11:38.499: INFO: Got endpoints: latency-svc-c8xvn [391.49481ms]
Sep  6 22:11:38.501: INFO: Created: latency-svc-rj44b
Sep  6 22:11:38.512: INFO: Created: latency-svc-fzwhp
Sep  6 22:11:38.516: INFO: Got endpoints: latency-svc-rj44b [399.523017ms]
Sep  6 22:11:38.522: INFO: Created: latency-svc-hkhnb
Sep  6 22:11:38.530: INFO: Created: latency-svc-2njb2
Sep  6 22:11:38.540: INFO: Created: latency-svc-hj69d
Sep  6 22:11:38.547: INFO: Created: latency-svc-wqhgs
Sep  6 22:11:38.556: INFO: Created: latency-svc-5jbk5
Sep  6 22:11:38.563: INFO: Created: latency-svc-m2mgh
Sep  6 22:11:38.565: INFO: Got endpoints: latency-svc-fzwhp [435.408304ms]
Sep  6 22:11:38.570: INFO: Created: latency-svc-rxw5c
Sep  6 22:11:38.577: INFO: Created: latency-svc-l9qpd
Sep  6 22:11:38.585: INFO: Created: latency-svc-dskjr
Sep  6 22:11:38.598: INFO: Created: latency-svc-2cb9z
Sep  6 22:11:38.606: INFO: Created: latency-svc-gfj4w
Sep  6 22:11:38.632: INFO: Created: latency-svc-xz98s
Sep  6 22:11:38.632: INFO: Got endpoints: latency-svc-hkhnb [473.58091ms]
Sep  6 22:11:38.640: INFO: Created: latency-svc-cnfrm
Sep  6 22:11:38.651: INFO: Created: latency-svc-hn5cf
Sep  6 22:11:38.658: INFO: Created: latency-svc-qlzj7
Sep  6 22:11:38.664: INFO: Created: latency-svc-d5qlc
Sep  6 22:11:38.666: INFO: Got endpoints: latency-svc-2njb2 [442.439092ms]
Sep  6 22:11:38.681: INFO: Created: latency-svc-9psvw
Sep  6 22:11:38.720: INFO: Got endpoints: latency-svc-hj69d [480.669507ms]
Sep  6 22:11:38.741: INFO: Created: latency-svc-jv859
Sep  6 22:11:38.768: INFO: Got endpoints: latency-svc-wqhgs [513.4842ms]
Sep  6 22:11:38.786: INFO: Created: latency-svc-wxl68
Sep  6 22:11:38.820: INFO: Got endpoints: latency-svc-5jbk5 [533.989622ms]
Sep  6 22:11:38.835: INFO: Created: latency-svc-mhr26
Sep  6 22:11:38.867: INFO: Got endpoints: latency-svc-m2mgh [459.669161ms]
Sep  6 22:11:38.888: INFO: Created: latency-svc-mxzss
Sep  6 22:11:38.921: INFO: Got endpoints: latency-svc-rxw5c [488.405337ms]
Sep  6 22:11:38.947: INFO: Created: latency-svc-hlpdq
Sep  6 22:11:38.970: INFO: Got endpoints: latency-svc-l9qpd [519.647588ms]
Sep  6 22:11:38.990: INFO: Created: latency-svc-9sm5p
Sep  6 22:11:39.021: INFO: Got endpoints: latency-svc-dskjr [565.28019ms]
Sep  6 22:11:39.033: INFO: Created: latency-svc-p5qn2
Sep  6 22:11:39.069: INFO: Got endpoints: latency-svc-2cb9z [602.316386ms]
Sep  6 22:11:39.084: INFO: Created: latency-svc-gw4bk
Sep  6 22:11:39.119: INFO: Got endpoints: latency-svc-gfj4w [643.05019ms]
Sep  6 22:11:39.140: INFO: Created: latency-svc-chtp6
Sep  6 22:11:39.169: INFO: Got endpoints: latency-svc-xz98s [682.178945ms]
Sep  6 22:11:39.214: INFO: Created: latency-svc-jbgnw
Sep  6 22:11:39.223: INFO: Got endpoints: latency-svc-cnfrm [723.443813ms]
Sep  6 22:11:39.262: INFO: Created: latency-svc-rs2d8
Sep  6 22:11:39.274: INFO: Got endpoints: latency-svc-hn5cf [758.547314ms]
Sep  6 22:11:39.308: INFO: Created: latency-svc-54qnm
Sep  6 22:11:39.316: INFO: Got endpoints: latency-svc-qlzj7 [750.872556ms]
Sep  6 22:11:39.360: INFO: Created: latency-svc-f8jkd
Sep  6 22:11:39.366: INFO: Got endpoints: latency-svc-d5qlc [734.166111ms]
Sep  6 22:11:39.389: INFO: Created: latency-svc-lgb6h
Sep  6 22:11:39.420: INFO: Got endpoints: latency-svc-9psvw [754.765199ms]
Sep  6 22:11:39.437: INFO: Created: latency-svc-gtx9p
Sep  6 22:11:39.472: INFO: Got endpoints: latency-svc-jv859 [751.827394ms]
Sep  6 22:11:39.490: INFO: Created: latency-svc-7v6xw
Sep  6 22:11:39.517: INFO: Got endpoints: latency-svc-wxl68 [748.571295ms]
Sep  6 22:11:39.548: INFO: Created: latency-svc-z5m2s
Sep  6 22:11:39.607: INFO: Got endpoints: latency-svc-mhr26 [786.344721ms]
Sep  6 22:11:39.622: INFO: Got endpoints: latency-svc-mxzss [755.035409ms]
Sep  6 22:11:39.628: INFO: Created: latency-svc-mrq72
Sep  6 22:11:39.637: INFO: Created: latency-svc-bwr58
Sep  6 22:11:39.668: INFO: Got endpoints: latency-svc-hlpdq [746.177324ms]
Sep  6 22:11:39.678: INFO: Created: latency-svc-ht5tt
Sep  6 22:11:39.718: INFO: Got endpoints: latency-svc-9sm5p [748.472388ms]
Sep  6 22:11:39.739: INFO: Created: latency-svc-hzzz7
Sep  6 22:11:39.766: INFO: Got endpoints: latency-svc-p5qn2 [744.892224ms]
Sep  6 22:11:39.785: INFO: Created: latency-svc-6pqvr
Sep  6 22:11:39.819: INFO: Got endpoints: latency-svc-gw4bk [749.610115ms]
Sep  6 22:11:39.833: INFO: Created: latency-svc-kjrg5
Sep  6 22:11:39.867: INFO: Got endpoints: latency-svc-chtp6 [747.786ms]
Sep  6 22:11:39.892: INFO: Created: latency-svc-9qz65
Sep  6 22:11:39.921: INFO: Got endpoints: latency-svc-jbgnw [752.086475ms]
Sep  6 22:11:39.942: INFO: Created: latency-svc-mmbjv
Sep  6 22:11:39.968: INFO: Got endpoints: latency-svc-rs2d8 [745.562276ms]
Sep  6 22:11:39.995: INFO: Created: latency-svc-bjnt5
Sep  6 22:11:40.019: INFO: Got endpoints: latency-svc-54qnm [744.232444ms]
Sep  6 22:11:40.056: INFO: Created: latency-svc-jngrh
Sep  6 22:11:40.068: INFO: Got endpoints: latency-svc-f8jkd [752.335125ms]
Sep  6 22:11:40.127: INFO: Created: latency-svc-4wrs8
Sep  6 22:11:40.135: INFO: Got endpoints: latency-svc-lgb6h [768.774416ms]
Sep  6 22:11:40.176: INFO: Created: latency-svc-cc8bb
Sep  6 22:11:40.178: INFO: Got endpoints: latency-svc-gtx9p [757.657539ms]
Sep  6 22:11:40.249: INFO: Created: latency-svc-w747r
Sep  6 22:11:40.249: INFO: Got endpoints: latency-svc-7v6xw [776.645839ms]
Sep  6 22:11:40.289: INFO: Got endpoints: latency-svc-z5m2s [771.973781ms]
Sep  6 22:11:40.344: INFO: Got endpoints: latency-svc-mrq72 [736.830848ms]
Sep  6 22:11:40.344: INFO: Created: latency-svc-wwsmk
Sep  6 22:11:40.354: INFO: Created: latency-svc-jl7z2
Sep  6 22:11:40.370: INFO: Got endpoints: latency-svc-bwr58 [748.606156ms]
Sep  6 22:11:40.378: INFO: Created: latency-svc-tvqwf
Sep  6 22:11:40.423: INFO: Created: latency-svc-hnq5s
Sep  6 22:11:40.423: INFO: Got endpoints: latency-svc-ht5tt [754.974064ms]
Sep  6 22:11:40.451: INFO: Created: latency-svc-cgzp8
Sep  6 22:11:40.466: INFO: Got endpoints: latency-svc-hzzz7 [747.77265ms]
Sep  6 22:11:40.481: INFO: Created: latency-svc-82tgr
Sep  6 22:11:40.523: INFO: Got endpoints: latency-svc-6pqvr [757.160574ms]
Sep  6 22:11:40.542: INFO: Created: latency-svc-zg48j
Sep  6 22:11:40.587: INFO: Got endpoints: latency-svc-kjrg5 [768.855736ms]
Sep  6 22:11:40.637: INFO: Created: latency-svc-f4l2x
Sep  6 22:11:40.637: INFO: Got endpoints: latency-svc-9qz65 [770.520591ms]
Sep  6 22:11:40.650: INFO: Created: latency-svc-qvp8c
Sep  6 22:11:40.671: INFO: Got endpoints: latency-svc-mmbjv [749.945527ms]
Sep  6 22:11:40.685: INFO: Created: latency-svc-jnwkw
Sep  6 22:11:40.718: INFO: Got endpoints: latency-svc-bjnt5 [749.393996ms]
Sep  6 22:11:40.736: INFO: Created: latency-svc-dc9ls
Sep  6 22:11:40.770: INFO: Got endpoints: latency-svc-jngrh [750.989199ms]
Sep  6 22:11:40.791: INFO: Created: latency-svc-rfmtx
Sep  6 22:11:40.823: INFO: Got endpoints: latency-svc-4wrs8 [754.534301ms]
Sep  6 22:11:40.844: INFO: Created: latency-svc-8m7bg
Sep  6 22:11:40.867: INFO: Got endpoints: latency-svc-cc8bb [731.359503ms]
Sep  6 22:11:40.886: INFO: Created: latency-svc-82c2l
Sep  6 22:11:40.919: INFO: Got endpoints: latency-svc-w747r [740.342518ms]
Sep  6 22:11:40.953: INFO: Created: latency-svc-7bnmb
Sep  6 22:11:40.970: INFO: Got endpoints: latency-svc-wwsmk [721.550498ms]
Sep  6 22:11:40.997: INFO: Created: latency-svc-ztxzj
Sep  6 22:11:41.020: INFO: Got endpoints: latency-svc-jl7z2 [730.893647ms]
Sep  6 22:11:41.047: INFO: Created: latency-svc-ng9vl
Sep  6 22:11:41.076: INFO: Got endpoints: latency-svc-tvqwf [732.678032ms]
Sep  6 22:11:41.110: INFO: Created: latency-svc-d9q2t
Sep  6 22:11:41.122: INFO: Got endpoints: latency-svc-hnq5s [751.186342ms]
Sep  6 22:11:41.174: INFO: Created: latency-svc-rjp98
Sep  6 22:11:41.181: INFO: Got endpoints: latency-svc-cgzp8 [757.817755ms]
Sep  6 22:11:41.215: INFO: Created: latency-svc-dbnj7
Sep  6 22:11:41.228: INFO: Got endpoints: latency-svc-82tgr [762.324912ms]
Sep  6 22:11:41.283: INFO: Got endpoints: latency-svc-zg48j [759.927003ms]
Sep  6 22:11:41.329: INFO: Got endpoints: latency-svc-f4l2x [741.169639ms]
Sep  6 22:11:41.358: INFO: Created: latency-svc-xc6xz
Sep  6 22:11:41.373: INFO: Got endpoints: latency-svc-qvp8c [735.798117ms]
Sep  6 22:11:41.397: INFO: Created: latency-svc-rpzd5
Sep  6 22:11:41.416: INFO: Created: latency-svc-8226k
Sep  6 22:11:41.457: INFO: Created: latency-svc-59k59
Sep  6 22:11:41.457: INFO: Got endpoints: latency-svc-jnwkw [786.035031ms]
Sep  6 22:11:41.476: INFO: Created: latency-svc-p5gbz
Sep  6 22:11:41.476: INFO: Got endpoints: latency-svc-dc9ls [758.126249ms]
Sep  6 22:11:41.508: INFO: Created: latency-svc-zdnmn
Sep  6 22:11:41.535: INFO: Got endpoints: latency-svc-rfmtx [765.026672ms]
Sep  6 22:11:41.556: INFO: Created: latency-svc-klr4j
Sep  6 22:11:41.569: INFO: Got endpoints: latency-svc-8m7bg [746.11518ms]
Sep  6 22:11:41.614: INFO: Created: latency-svc-95dkp
Sep  6 22:11:41.624: INFO: Got endpoints: latency-svc-82c2l [756.970574ms]
Sep  6 22:11:41.642: INFO: Created: latency-svc-6vmxt
Sep  6 22:11:41.666: INFO: Got endpoints: latency-svc-7bnmb [747.443964ms]
Sep  6 22:11:41.706: INFO: Created: latency-svc-t29r4
Sep  6 22:11:41.719: INFO: Got endpoints: latency-svc-ztxzj [748.040272ms]
Sep  6 22:11:41.731: INFO: Created: latency-svc-kcr42
Sep  6 22:11:41.770: INFO: Got endpoints: latency-svc-ng9vl [750.096191ms]
Sep  6 22:11:41.843: INFO: Got endpoints: latency-svc-d9q2t [766.693305ms]
Sep  6 22:11:41.845: INFO: Created: latency-svc-9l9bq
Sep  6 22:11:41.861: INFO: Created: latency-svc-trdsf
Sep  6 22:11:41.868: INFO: Got endpoints: latency-svc-rjp98 [745.954928ms]
Sep  6 22:11:41.898: INFO: Created: latency-svc-mwwxj
Sep  6 22:11:41.917: INFO: Got endpoints: latency-svc-dbnj7 [736.314137ms]
Sep  6 22:11:41.974: INFO: Created: latency-svc-qlmr4
Sep  6 22:11:41.981: INFO: Got endpoints: latency-svc-xc6xz [752.114399ms]
Sep  6 22:11:42.017: INFO: Created: latency-svc-fftzm
Sep  6 22:11:42.021: INFO: Got endpoints: latency-svc-rpzd5 [737.8044ms]
Sep  6 22:11:42.036: INFO: Created: latency-svc-szpzj
Sep  6 22:11:42.071: INFO: Got endpoints: latency-svc-8226k [742.290171ms]
Sep  6 22:11:42.117: INFO: Got endpoints: latency-svc-59k59 [743.96486ms]
Sep  6 22:11:42.120: INFO: Created: latency-svc-zcm7d
Sep  6 22:11:42.167: INFO: Got endpoints: latency-svc-p5gbz [710.462437ms]
Sep  6 22:11:42.180: INFO: Created: latency-svc-bd2bp
Sep  6 22:11:42.212: INFO: Created: latency-svc-crkpq
Sep  6 22:11:42.220: INFO: Got endpoints: latency-svc-zdnmn [744.131727ms]
Sep  6 22:11:42.237: INFO: Created: latency-svc-kzdd5
Sep  6 22:11:42.267: INFO: Got endpoints: latency-svc-klr4j [732.07971ms]
Sep  6 22:11:42.277: INFO: Created: latency-svc-qvtdh
Sep  6 22:11:42.316: INFO: Got endpoints: latency-svc-95dkp [746.614061ms]
Sep  6 22:11:42.325: INFO: Created: latency-svc-jtdbh
Sep  6 22:11:42.365: INFO: Got endpoints: latency-svc-6vmxt [741.680552ms]
Sep  6 22:11:42.380: INFO: Created: latency-svc-wfv9s
Sep  6 22:11:42.419: INFO: Got endpoints: latency-svc-t29r4 [753.109773ms]
Sep  6 22:11:42.461: INFO: Created: latency-svc-qmzdv
Sep  6 22:11:42.473: INFO: Got endpoints: latency-svc-kcr42 [754.698476ms]
Sep  6 22:11:42.503: INFO: Created: latency-svc-87k9x
Sep  6 22:11:42.518: INFO: Got endpoints: latency-svc-9l9bq [747.919591ms]
Sep  6 22:11:42.545: INFO: Created: latency-svc-6l779
Sep  6 22:11:42.572: INFO: Got endpoints: latency-svc-trdsf [728.740904ms]
Sep  6 22:11:42.626: INFO: Created: latency-svc-wzrvc
Sep  6 22:11:42.626: INFO: Got endpoints: latency-svc-mwwxj [758.862542ms]
Sep  6 22:11:42.637: INFO: Created: latency-svc-9ng5t
Sep  6 22:11:42.665: INFO: Got endpoints: latency-svc-qlmr4 [748.314625ms]
Sep  6 22:11:42.681: INFO: Created: latency-svc-9t7vs
Sep  6 22:11:42.720: INFO: Got endpoints: latency-svc-fftzm [739.36458ms]
Sep  6 22:11:42.733: INFO: Created: latency-svc-hr2xs
Sep  6 22:11:42.770: INFO: Got endpoints: latency-svc-szpzj [749.23505ms]
Sep  6 22:11:42.788: INFO: Created: latency-svc-w2knh
Sep  6 22:11:42.816: INFO: Got endpoints: latency-svc-zcm7d [744.869965ms]
Sep  6 22:11:42.842: INFO: Created: latency-svc-tl9jm
Sep  6 22:11:42.866: INFO: Got endpoints: latency-svc-bd2bp [749.186257ms]
Sep  6 22:11:42.894: INFO: Created: latency-svc-vqvpw
Sep  6 22:11:42.919: INFO: Got endpoints: latency-svc-crkpq [751.746577ms]
Sep  6 22:11:42.936: INFO: Created: latency-svc-9lmfd
Sep  6 22:11:42.971: INFO: Got endpoints: latency-svc-kzdd5 [751.198882ms]
Sep  6 22:11:42.998: INFO: Created: latency-svc-tr9qn
Sep  6 22:11:43.020: INFO: Got endpoints: latency-svc-qvtdh [753.15318ms]
Sep  6 22:11:43.048: INFO: Created: latency-svc-lhgqn
Sep  6 22:11:43.078: INFO: Got endpoints: latency-svc-jtdbh [762.57667ms]
Sep  6 22:11:43.112: INFO: Created: latency-svc-2cgdq
Sep  6 22:11:43.129: INFO: Got endpoints: latency-svc-wfv9s [763.55637ms]
Sep  6 22:11:43.191: INFO: Got endpoints: latency-svc-qmzdv [771.575519ms]
Sep  6 22:11:43.191: INFO: Created: latency-svc-qhp74
Sep  6 22:11:43.221: INFO: Got endpoints: latency-svc-87k9x [747.900811ms]
Sep  6 22:11:43.224: INFO: Created: latency-svc-qr9l4
Sep  6 22:11:43.242: INFO: Created: latency-svc-wjhq6
Sep  6 22:11:43.269: INFO: Got endpoints: latency-svc-6l779 [751.280262ms]
Sep  6 22:11:43.322: INFO: Got endpoints: latency-svc-wzrvc [749.646668ms]
Sep  6 22:11:43.322: INFO: Created: latency-svc-clw2d
Sep  6 22:11:43.337: INFO: Created: latency-svc-2m9b4
Sep  6 22:11:43.370: INFO: Got endpoints: latency-svc-9ng5t [743.718338ms]
Sep  6 22:11:43.384: INFO: Created: latency-svc-tcbmz
Sep  6 22:11:43.435: INFO: Got endpoints: latency-svc-9t7vs [769.682218ms]
Sep  6 22:11:43.457: INFO: Created: latency-svc-srv78
Sep  6 22:11:43.468: INFO: Got endpoints: latency-svc-hr2xs [747.998055ms]
Sep  6 22:11:43.486: INFO: Created: latency-svc-k8t49
Sep  6 22:11:43.519: INFO: Got endpoints: latency-svc-w2knh [748.706758ms]
Sep  6 22:11:43.547: INFO: Created: latency-svc-6gqnh
Sep  6 22:11:43.574: INFO: Got endpoints: latency-svc-tl9jm [757.974382ms]
Sep  6 22:11:43.594: INFO: Created: latency-svc-dgkpq
Sep  6 22:11:43.623: INFO: Got endpoints: latency-svc-vqvpw [756.430794ms]
Sep  6 22:11:43.637: INFO: Created: latency-svc-dkgdt
Sep  6 22:11:43.670: INFO: Got endpoints: latency-svc-9lmfd [750.249442ms]
Sep  6 22:11:43.681: INFO: Created: latency-svc-wkkd6
Sep  6 22:11:43.729: INFO: Got endpoints: latency-svc-tr9qn [757.553867ms]
Sep  6 22:11:43.749: INFO: Created: latency-svc-rmzdw
Sep  6 22:11:43.766: INFO: Got endpoints: latency-svc-lhgqn [746.025653ms]
Sep  6 22:11:43.782: INFO: Created: latency-svc-bvpdv
Sep  6 22:11:43.817: INFO: Got endpoints: latency-svc-2cgdq [738.323687ms]
Sep  6 22:11:43.834: INFO: Created: latency-svc-8xf7v
Sep  6 22:11:43.896: INFO: Got endpoints: latency-svc-qhp74 [766.566145ms]
Sep  6 22:11:43.926: INFO: Got endpoints: latency-svc-qr9l4 [734.854382ms]
Sep  6 22:11:43.928: INFO: Created: latency-svc-lps8x
Sep  6 22:11:43.947: INFO: Created: latency-svc-9fcs8
Sep  6 22:11:43.967: INFO: Got endpoints: latency-svc-wjhq6 [745.53429ms]
Sep  6 22:11:43.982: INFO: Created: latency-svc-rjsq9
Sep  6 22:11:44.018: INFO: Got endpoints: latency-svc-clw2d [748.970226ms]
Sep  6 22:11:44.065: INFO: Created: latency-svc-h97zt
Sep  6 22:11:44.068: INFO: Got endpoints: latency-svc-2m9b4 [746.03143ms]
Sep  6 22:11:44.133: INFO: Created: latency-svc-mzn49
Sep  6 22:11:44.157: INFO: Got endpoints: latency-svc-tcbmz [786.825211ms]
Sep  6 22:11:44.196: INFO: Got endpoints: latency-svc-srv78 [761.02328ms]
Sep  6 22:11:44.204: INFO: Created: latency-svc-cgrfs
Sep  6 22:11:44.216: INFO: Created: latency-svc-p7jt9
Sep  6 22:11:44.216: INFO: Got endpoints: latency-svc-k8t49 [748.354684ms]
Sep  6 22:11:44.243: INFO: Created: latency-svc-6c72r
Sep  6 22:11:44.269: INFO: Got endpoints: latency-svc-6gqnh [750.056313ms]
Sep  6 22:11:44.301: INFO: Created: latency-svc-q5vw8
Sep  6 22:11:44.317: INFO: Got endpoints: latency-svc-dgkpq [742.884993ms]
Sep  6 22:11:44.334: INFO: Created: latency-svc-xqhmk
Sep  6 22:11:44.367: INFO: Got endpoints: latency-svc-dkgdt [744.093857ms]
Sep  6 22:11:44.411: INFO: Created: latency-svc-cqjw6
Sep  6 22:11:44.420: INFO: Got endpoints: latency-svc-wkkd6 [750.575278ms]
Sep  6 22:11:44.474: INFO: Created: latency-svc-z4m5z
Sep  6 22:11:44.479: INFO: Got endpoints: latency-svc-rmzdw [750.061187ms]
Sep  6 22:11:44.505: INFO: Created: latency-svc-pcsrw
Sep  6 22:11:44.519: INFO: Got endpoints: latency-svc-bvpdv [752.612687ms]
Sep  6 22:11:44.551: INFO: Created: latency-svc-cbgrb
Sep  6 22:11:44.566: INFO: Got endpoints: latency-svc-8xf7v [749.727082ms]
Sep  6 22:11:44.632: INFO: Got endpoints: latency-svc-lps8x [736.628071ms]
Sep  6 22:11:44.693: INFO: Got endpoints: latency-svc-9fcs8 [766.908316ms]
Sep  6 22:11:44.697: INFO: Created: latency-svc-sr225
Sep  6 22:11:44.706: INFO: Created: latency-svc-s5qlk
Sep  6 22:11:44.761: INFO: Created: latency-svc-hdbm4
Sep  6 22:11:44.762: INFO: Got endpoints: latency-svc-rjsq9 [795.209272ms]
Sep  6 22:11:44.778: INFO: Got endpoints: latency-svc-h97zt [759.719882ms]
Sep  6 22:11:44.840: INFO: Got endpoints: latency-svc-mzn49 [772.757371ms]
Sep  6 22:11:44.873: INFO: Created: latency-svc-ql7fn
Sep  6 22:11:44.890: INFO: Got endpoints: latency-svc-cgrfs [733.01495ms]
Sep  6 22:11:44.916: INFO: Got endpoints: latency-svc-p7jt9 [720.396176ms]
Sep  6 22:11:44.977: INFO: Got endpoints: latency-svc-6c72r [760.402707ms]
Sep  6 22:11:45.013: INFO: Created: latency-svc-xnddh
Sep  6 22:11:45.027: INFO: Created: latency-svc-hdvjs
Sep  6 22:11:45.027: INFO: Got endpoints: latency-svc-q5vw8 [758.146931ms]
Sep  6 22:11:45.060: INFO: Created: latency-svc-cvvqv
Sep  6 22:11:45.068: INFO: Got endpoints: latency-svc-xqhmk [750.871353ms]
Sep  6 22:11:45.090: INFO: Created: latency-svc-nm7k2
Sep  6 22:11:45.132: INFO: Created: latency-svc-nqlmd
Sep  6 22:11:45.132: INFO: Got endpoints: latency-svc-cqjw6 [765.389038ms]
Sep  6 22:11:45.153: INFO: Created: latency-svc-jjg5z
Sep  6 22:11:45.175: INFO: Created: latency-svc-52jvr
Sep  6 22:11:45.185: INFO: Got endpoints: latency-svc-z4m5z [765.158123ms]
Sep  6 22:11:45.207: INFO: Created: latency-svc-8fkcf
Sep  6 22:11:45.219: INFO: Got endpoints: latency-svc-pcsrw [740.257701ms]
Sep  6 22:11:45.334: INFO: Got endpoints: latency-svc-cbgrb [814.834572ms]
Sep  6 22:11:45.394: INFO: Got endpoints: latency-svc-s5qlk [762.026934ms]
Sep  6 22:11:45.394: INFO: Got endpoints: latency-svc-sr225 [827.886548ms]
Sep  6 22:11:45.417: INFO: Got endpoints: latency-svc-hdbm4 [724.404907ms]
Sep  6 22:11:45.467: INFO: Got endpoints: latency-svc-ql7fn [704.980312ms]
Sep  6 22:11:45.533: INFO: Got endpoints: latency-svc-xnddh [754.361498ms]
Sep  6 22:11:45.584: INFO: Got endpoints: latency-svc-hdvjs [743.278857ms]
Sep  6 22:11:45.620: INFO: Got endpoints: latency-svc-cvvqv [730.31942ms]
Sep  6 22:11:45.667: INFO: Got endpoints: latency-svc-nm7k2 [750.243289ms]
Sep  6 22:11:45.724: INFO: Got endpoints: latency-svc-nqlmd [747.539399ms]
Sep  6 22:11:45.767: INFO: Got endpoints: latency-svc-jjg5z [739.897571ms]
Sep  6 22:11:45.817: INFO: Got endpoints: latency-svc-52jvr [748.893975ms]
Sep  6 22:11:45.867: INFO: Got endpoints: latency-svc-8fkcf [734.810782ms]
Sep  6 22:11:45.867: INFO: Latencies: [30.021873ms 36.135399ms 51.376165ms 66.834117ms 100.186815ms 116.042333ms 124.282239ms 136.803539ms 143.779424ms 159.189991ms 163.70361ms 191.969744ms 222.447576ms 227.091886ms 229.041475ms 231.998389ms 232.160508ms 236.425932ms 242.201451ms 243.908904ms 250.923857ms 254.924905ms 305.510034ms 320.626584ms 322.575664ms 336.835956ms 345.878587ms 347.456197ms 351.237045ms 356.1161ms 358.967086ms 362.952155ms 367.355535ms 373.335484ms 373.533317ms 383.665429ms 384.197235ms 384.696327ms 391.49481ms 391.85399ms 394.030664ms 395.512627ms 398.403883ms 399.523017ms 404.612962ms 421.999336ms 434.086145ms 435.408304ms 435.703234ms 437.302928ms 442.439092ms 452.625104ms 458.673848ms 459.669161ms 465.165187ms 468.424555ms 473.58091ms 480.669507ms 488.405337ms 513.4842ms 519.647588ms 533.989622ms 565.28019ms 602.316386ms 643.05019ms 682.178945ms 704.980312ms 710.462437ms 720.396176ms 721.550498ms 723.443813ms 724.404907ms 728.740904ms 730.31942ms 730.893647ms 731.359503ms 732.07971ms 732.678032ms 733.01495ms 734.166111ms 734.810782ms 734.854382ms 735.798117ms 736.314137ms 736.628071ms 736.830848ms 737.8044ms 738.323687ms 739.36458ms 739.897571ms 740.257701ms 740.342518ms 741.169639ms 741.680552ms 742.290171ms 742.884993ms 743.278857ms 743.718338ms 743.96486ms 744.093857ms 744.131727ms 744.232444ms 744.869965ms 744.892224ms 745.53429ms 745.562276ms 745.954928ms 746.025653ms 746.03143ms 746.11518ms 746.177324ms 746.614061ms 747.443964ms 747.539399ms 747.77265ms 747.786ms 747.900811ms 747.919591ms 747.998055ms 748.040272ms 748.314625ms 748.354684ms 748.472388ms 748.571295ms 748.606156ms 748.706758ms 748.893975ms 748.970226ms 749.186257ms 749.23505ms 749.393996ms 749.610115ms 749.646668ms 749.727082ms 749.945527ms 750.056313ms 750.061187ms 750.096191ms 750.243289ms 750.249442ms 750.575278ms 750.871353ms 750.872556ms 750.989199ms 751.186342ms 751.198882ms 751.280262ms 751.746577ms 751.827394ms 752.086475ms 752.114399ms 752.335125ms 752.612687ms 753.109773ms 753.15318ms 754.361498ms 754.534301ms 754.698476ms 754.765199ms 754.974064ms 755.035409ms 756.430794ms 756.970574ms 757.160574ms 757.553867ms 757.657539ms 757.817755ms 757.974382ms 758.126249ms 758.146931ms 758.547314ms 758.862542ms 759.719882ms 759.927003ms 760.402707ms 761.02328ms 762.026934ms 762.324912ms 762.57667ms 763.55637ms 765.026672ms 765.158123ms 765.389038ms 766.566145ms 766.693305ms 766.908316ms 768.774416ms 768.855736ms 769.682218ms 770.520591ms 771.575519ms 771.973781ms 772.757371ms 776.645839ms 786.035031ms 786.344721ms 786.825211ms 795.209272ms 814.834572ms 827.886548ms]
Sep  6 22:11:45.867: INFO: 50 %ile: 744.131727ms
Sep  6 22:11:45.867: INFO: 90 %ile: 765.026672ms
Sep  6 22:11:45.867: INFO: 99 %ile: 814.834572ms
Sep  6 22:11:45.867: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:11:45.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3875" for this suite.
Sep  6 22:12:05.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:12:06.173: INFO: namespace svc-latency-3875 deletion completed in 20.297624378s

• [SLOW TEST:31.094 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:12:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-935df72f-4cb5-431f-848a-24cdfe4e49bb
STEP: Creating a pod to test consume configMaps
Sep  6 22:12:06.248: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391" in namespace "projected-6432" to be "success or failure"
Sep  6 22:12:06.253: INFO: Pod "pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461026ms
Sep  6 22:12:08.264: INFO: Pod "pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01599702s
STEP: Saw pod success
Sep  6 22:12:08.265: INFO: Pod "pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391" satisfied condition "success or failure"
Sep  6 22:12:08.270: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:12:08.301: INFO: Waiting for pod pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391 to disappear
Sep  6 22:12:08.304: INFO: Pod pod-projected-configmaps-1883d600-2368-4519-a00a-7b29124b2391 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:12:08.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6432" for this suite.
Sep  6 22:12:14.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:12:14.461: INFO: namespace projected-6432 deletion completed in 6.150976938s

• [SLOW TEST:8.288 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:12:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  6 22:12:14.518: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55315,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 22:12:14.518: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55315,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  6 22:12:24.557: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55336,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 22:12:24.558: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55336,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  6 22:12:34.569: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55357,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 22:12:34.569: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55357,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  6 22:12:44.630: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55377,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 22:12:44.630: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-a,UID:b58be337-9a02-45b0-bf09-162fa0f68156,ResourceVersion:55377,Generation:0,CreationTimestamp:2019-09-06 22:12:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  6 22:12:54.640: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-b,UID:ed931ec1-24bc-4a7d-b73e-1e53f2e1d1a0,ResourceVersion:55397,Generation:0,CreationTimestamp:2019-09-06 22:12:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 22:12:54.641: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-b,UID:ed931ec1-24bc-4a7d-b73e-1e53f2e1d1a0,ResourceVersion:55397,Generation:0,CreationTimestamp:2019-09-06 22:12:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  6 22:13:04.649: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-b,UID:ed931ec1-24bc-4a7d-b73e-1e53f2e1d1a0,ResourceVersion:55419,Generation:0,CreationTimestamp:2019-09-06 22:12:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 22:13:04.649: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7870,SelfLink:/api/v1/namespaces/watch-7870/configmaps/e2e-watch-test-configmap-b,UID:ed931ec1-24bc-4a7d-b73e-1e53f2e1d1a0,ResourceVersion:55419,Generation:0,CreationTimestamp:2019-09-06 22:12:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:13:14.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7870" for this suite.
Sep  6 22:13:20.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:13:20.876: INFO: namespace watch-7870 deletion completed in 6.218317254s

• [SLOW TEST:66.414 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:13:20.876: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-90a82157-ac31-447f-b2d4-ff1b8b977fb8
STEP: Creating a pod to test consume secrets
Sep  6 22:13:20.969: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-061b2447-700c-498e-9267-739113751866" in namespace "projected-4907" to be "success or failure"
Sep  6 22:13:20.989: INFO: Pod "pod-projected-secrets-061b2447-700c-498e-9267-739113751866": Phase="Pending", Reason="", readiness=false. Elapsed: 19.855766ms
Sep  6 22:13:23.025: INFO: Pod "pod-projected-secrets-061b2447-700c-498e-9267-739113751866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055811561s
STEP: Saw pod success
Sep  6 22:13:23.025: INFO: Pod "pod-projected-secrets-061b2447-700c-498e-9267-739113751866" satisfied condition "success or failure"
Sep  6 22:13:23.034: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-secrets-061b2447-700c-498e-9267-739113751866 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:13:23.114: INFO: Waiting for pod pod-projected-secrets-061b2447-700c-498e-9267-739113751866 to disappear
Sep  6 22:13:23.130: INFO: Pod pod-projected-secrets-061b2447-700c-498e-9267-739113751866 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:13:23.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4907" for this suite.
Sep  6 22:13:29.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:13:29.379: INFO: namespace projected-4907 deletion completed in 6.224312323s

• [SLOW TEST:8.503 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:13:29.379: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:13:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2822" for this suite.
Sep  6 22:13:51.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:13:51.755: INFO: namespace pods-2822 deletion completed in 22.241191364s

• [SLOW TEST:22.376 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:13:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:13:51.888: INFO: (0) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 31.674878ms)
Sep  6 22:13:51.903: INFO: (1) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 15.697118ms)
Sep  6 22:13:51.914: INFO: (2) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.944353ms)
Sep  6 22:13:51.923: INFO: (3) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.536542ms)
Sep  6 22:13:51.929: INFO: (4) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.193706ms)
Sep  6 22:13:51.937: INFO: (5) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.484745ms)
Sep  6 22:13:51.948: INFO: (6) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.501254ms)
Sep  6 22:13:51.956: INFO: (7) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.602973ms)
Sep  6 22:13:51.966: INFO: (8) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.394627ms)
Sep  6 22:13:51.972: INFO: (9) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.581677ms)
Sep  6 22:13:51.981: INFO: (10) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.71571ms)
Sep  6 22:13:51.993: INFO: (11) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 12.470806ms)
Sep  6 22:13:52.002: INFO: (12) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.506063ms)
Sep  6 22:13:52.017: INFO: (13) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 15.075775ms)
Sep  6 22:13:52.024: INFO: (14) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.109682ms)
Sep  6 22:13:52.031: INFO: (15) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.024424ms)
Sep  6 22:13:52.037: INFO: (16) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 6.286808ms)
Sep  6 22:13:52.045: INFO: (17) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.13324ms)
Sep  6 22:13:52.049: INFO: (18) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.79401ms)
Sep  6 22:13:52.055: INFO: (19) /api/v1/nodes/metalk8s-24:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.784952ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:13:52.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2845" for this suite.
Sep  6 22:13:58.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:13:58.260: INFO: namespace proxy-2845 deletion completed in 6.185433097s

• [SLOW TEST:6.502 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:13:58.261: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:14:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-239" for this suite.
Sep  6 22:14:46.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:14:46.651: INFO: namespace kubelet-test-239 deletion completed in 46.264705182s

• [SLOW TEST:48.391 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:14:46.652: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 22:14:46.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3747'
Sep  6 22:14:46.862: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 22:14:46.862: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep  6 22:14:46.885: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-2mjqt]
Sep  6 22:14:46.885: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-2mjqt" in namespace "kubectl-3747" to be "running and ready"
Sep  6 22:14:46.889: INFO: Pod "e2e-test-nginx-rc-2mjqt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076577ms
Sep  6 22:14:48.894: INFO: Pod "e2e-test-nginx-rc-2mjqt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008761555s
Sep  6 22:14:48.894: INFO: Pod "e2e-test-nginx-rc-2mjqt" satisfied condition "running and ready"
Sep  6 22:14:48.894: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-2mjqt]
Sep  6 22:14:48.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 logs rc/e2e-test-nginx-rc --namespace=kubectl-3747'
Sep  6 22:14:49.008: INFO: stderr: ""
Sep  6 22:14:49.008: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Sep  6 22:14:49.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete rc e2e-test-nginx-rc --namespace=kubectl-3747'
Sep  6 22:14:49.093: INFO: stderr: ""
Sep  6 22:14:49.093: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:14:49.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3747" for this suite.
Sep  6 22:14:55.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:14:55.224: INFO: namespace kubectl-3747 deletion completed in 6.126815918s

• [SLOW TEST:8.572 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:14:55.224: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Sep  6 22:14:55.260: INFO: Waiting up to 5m0s for pod "var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612" in namespace "var-expansion-7187" to be "success or failure"
Sep  6 22:14:55.264: INFO: Pod "var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774117ms
Sep  6 22:14:57.268: INFO: Pod "var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007689823s
STEP: Saw pod success
Sep  6 22:14:57.268: INFO: Pod "var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612" satisfied condition "success or failure"
Sep  6 22:14:57.270: INFO: Trying to get logs from node metalk8s-24-node1 pod var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612 container dapi-container: <nil>
STEP: delete the pod
Sep  6 22:14:57.290: INFO: Waiting for pod var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612 to disappear
Sep  6 22:14:57.295: INFO: Pod var-expansion-4319cf80-6b3f-459f-b7c0-128668a80612 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:14:57.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7187" for this suite.
Sep  6 22:15:03.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:15:03.542: INFO: namespace var-expansion-7187 deletion completed in 6.240958709s

• [SLOW TEST:8.317 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:15:03.542: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep  6 22:15:05.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec pod-sharedvolume-b03ff5d4-9368-4b13-8eaa-102aca3e0b66 -c busybox-main-container --namespace=emptydir-8043 -- cat /usr/share/volumeshare/shareddata.txt'
Sep  6 22:15:05.896: INFO: stderr: ""
Sep  6 22:15:05.896: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:15:05.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8043" for this suite.
Sep  6 22:15:11.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:15:12.186: INFO: namespace emptydir-8043 deletion completed in 6.282587072s

• [SLOW TEST:8.644 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:15:12.186: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:15:16.444: INFO: Waiting up to 5m0s for pod "client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251" in namespace "pods-8842" to be "success or failure"
Sep  6 22:15:16.448: INFO: Pod "client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800075ms
Sep  6 22:15:18.454: INFO: Pod "client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010490277s
STEP: Saw pod success
Sep  6 22:15:18.454: INFO: Pod "client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251" satisfied condition "success or failure"
Sep  6 22:15:18.457: INFO: Trying to get logs from node metalk8s-24-node1 pod client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251 container env3cont: <nil>
STEP: delete the pod
Sep  6 22:15:18.473: INFO: Waiting for pod client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251 to disappear
Sep  6 22:15:18.477: INFO: Pod client-envvars-b1bb2fe2-e337-413c-8505-d07634c5b251 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:15:18.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8842" for this suite.
Sep  6 22:15:58.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:15:58.627: INFO: namespace pods-8842 deletion completed in 40.145288253s

• [SLOW TEST:46.440 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:15:58.627: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b6ec228d-f49f-4896-86c4-c1a2fd59f5f9
STEP: Creating configMap with name cm-test-opt-upd-24c977a5-268c-44d9-af2c-11cdd6c271da
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b6ec228d-f49f-4896-86c4-c1a2fd59f5f9
STEP: Updating configmap cm-test-opt-upd-24c977a5-268c-44d9-af2c-11cdd6c271da
STEP: Creating configMap with name cm-test-opt-create-dcf6c42a-272a-41f5-8ad1-42c6d1709b49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:16:02.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4371" for this suite.
Sep  6 22:16:26.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:16:27.171: INFO: namespace configmap-4371 deletion completed in 24.22363418s

• [SLOW TEST:28.544 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:16:27.171: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Sep  6 22:16:27.243: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-201627442 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:16:27.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5345" for this suite.
Sep  6 22:16:33.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:16:33.668: INFO: namespace kubectl-5345 deletion completed in 6.35425079s

• [SLOW TEST:6.497 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:16:33.669: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-c2xf
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 22:16:33.913: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c2xf" in namespace "subpath-8359" to be "success or failure"
Sep  6 22:16:33.949: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Pending", Reason="", readiness=false. Elapsed: 35.646919ms
Sep  6 22:16:35.953: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040302826s
Sep  6 22:16:37.960: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 4.046745458s
Sep  6 22:16:39.981: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 6.067855901s
Sep  6 22:16:41.989: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 8.07630148s
Sep  6 22:16:45.152: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 11.239316009s
Sep  6 22:16:47.159: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 13.246171837s
Sep  6 22:16:49.165: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 15.251446642s
Sep  6 22:16:51.177: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 17.26415105s
Sep  6 22:16:53.183: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Running", Reason="", readiness=true. Elapsed: 19.270219024s
Sep  6 22:16:55.189: INFO: Pod "pod-subpath-test-configmap-c2xf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.275630316s
STEP: Saw pod success
Sep  6 22:16:55.189: INFO: Pod "pod-subpath-test-configmap-c2xf" satisfied condition "success or failure"
Sep  6 22:16:55.196: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-subpath-test-configmap-c2xf container test-container-subpath-configmap-c2xf: <nil>
STEP: delete the pod
Sep  6 22:16:55.259: INFO: Waiting for pod pod-subpath-test-configmap-c2xf to disappear
Sep  6 22:16:55.271: INFO: Pod pod-subpath-test-configmap-c2xf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c2xf
Sep  6 22:16:55.271: INFO: Deleting pod "pod-subpath-test-configmap-c2xf" in namespace "subpath-8359"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:16:55.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8359" for this suite.
Sep  6 22:17:01.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:17:01.631: INFO: namespace subpath-8359 deletion completed in 6.350924962s

• [SLOW TEST:27.963 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:17:01.632: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-aaf4e395-f94f-4417-8ed7-bfa56cc48a45
STEP: Creating a pod to test consume configMaps
Sep  6 22:17:01.799: INFO: Waiting up to 5m0s for pod "pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854" in namespace "configmap-6229" to be "success or failure"
Sep  6 22:17:01.814: INFO: Pod "pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854": Phase="Pending", Reason="", readiness=false. Elapsed: 15.298462ms
Sep  6 22:17:03.818: INFO: Pod "pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01913026s
STEP: Saw pod success
Sep  6 22:17:03.818: INFO: Pod "pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854" satisfied condition "success or failure"
Sep  6 22:17:03.820: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:17:03.852: INFO: Waiting for pod pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854 to disappear
Sep  6 22:17:03.857: INFO: Pod pod-configmaps-65571a02-4e45-4e2a-8418-060a1468e854 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:17:03.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6229" for this suite.
Sep  6 22:17:11.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:17:12.176: INFO: namespace configmap-6229 deletion completed in 8.310735819s

• [SLOW TEST:10.544 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:17:12.176: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 22:17:16.380: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 22:17:16.390: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 22:17:18.390: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 22:17:18.398: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 22:17:20.390: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 22:17:20.395: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:17:20.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9480" for this suite.
Sep  6 22:17:44.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:17:44.601: INFO: namespace container-lifecycle-hook-9480 deletion completed in 24.182209038s

• [SLOW TEST:32.425 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:17:44.602: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Sep  6 22:17:44.711: INFO: Waiting up to 5m0s for pod "client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902" in namespace "containers-2471" to be "success or failure"
Sep  6 22:17:44.731: INFO: Pod "client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902": Phase="Pending", Reason="", readiness=false. Elapsed: 19.577932ms
Sep  6 22:17:46.735: INFO: Pod "client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023491569s
Sep  6 22:17:48.739: INFO: Pod "client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027782749s
STEP: Saw pod success
Sep  6 22:17:48.739: INFO: Pod "client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902" satisfied condition "success or failure"
Sep  6 22:17:48.743: INFO: Trying to get logs from node metalk8s-24-node1 pod client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902 container test-container: <nil>
STEP: delete the pod
Sep  6 22:17:48.771: INFO: Waiting for pod client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902 to disappear
Sep  6 22:17:48.775: INFO: Pod client-containers-b53f5e1b-f57d-4826-8d0c-811f9c669902 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:17:48.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2471" for this suite.
Sep  6 22:17:54.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:17:54.988: INFO: namespace containers-2471 deletion completed in 6.207981937s

• [SLOW TEST:10.387 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:17:54.989: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-92066197-a35f-489d-8bc2-5225f80aac93 in namespace container-probe-8983
Sep  6 22:17:57.121: INFO: Started pod test-webserver-92066197-a35f-489d-8bc2-5225f80aac93 in namespace container-probe-8983
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 22:17:57.124: INFO: Initial restart count of pod test-webserver-92066197-a35f-489d-8bc2-5225f80aac93 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:21:57.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8983" for this suite.
Sep  6 22:22:03.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:22:04.314: INFO: namespace container-probe-8983 deletion completed in 6.373835s

• [SLOW TEST:249.326 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:22:04.315: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Sep  6 22:22:04.378: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep  6 22:22:04.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3562'
Sep  6 22:22:04.650: INFO: stderr: ""
Sep  6 22:22:04.650: INFO: stdout: "service/redis-slave created\n"
Sep  6 22:22:04.650: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep  6 22:22:04.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3562'
Sep  6 22:22:04.816: INFO: stderr: ""
Sep  6 22:22:04.816: INFO: stdout: "service/redis-master created\n"
Sep  6 22:22:04.816: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  6 22:22:04.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3562'
Sep  6 22:22:04.994: INFO: stderr: ""
Sep  6 22:22:04.994: INFO: stdout: "service/frontend created\n"
Sep  6 22:22:04.995: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep  6 22:22:04.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3562'
Sep  6 22:22:05.141: INFO: stderr: ""
Sep  6 22:22:05.141: INFO: stdout: "deployment.apps/frontend created\n"
Sep  6 22:22:05.141: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  6 22:22:05.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3562'
Sep  6 22:22:05.286: INFO: stderr: ""
Sep  6 22:22:05.286: INFO: stdout: "deployment.apps/redis-master created\n"
Sep  6 22:22:05.287: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep  6 22:22:05.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-3562'
Sep  6 22:22:05.446: INFO: stderr: ""
Sep  6 22:22:05.446: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep  6 22:22:05.447: INFO: Waiting for all frontend pods to be Running.
Sep  6 22:22:20.498: INFO: Waiting for frontend to serve content.
Sep  6 22:22:20.520: INFO: Trying to add a new entry to the guestbook.
Sep  6 22:22:20.541: INFO: Verifying that added entry can be retrieved.
Sep  6 22:22:20.560: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Sep  6 22:22:25.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3562'
Sep  6 22:22:25.748: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:22:25.749: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 22:22:25.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3562'
Sep  6 22:22:25.956: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:22:25.956: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 22:22:25.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3562'
Sep  6 22:22:26.308: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:22:26.308: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 22:22:26.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3562'
Sep  6 22:22:26.440: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:22:26.440: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 22:22:26.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3562'
Sep  6 22:22:26.539: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:22:26.539: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep  6 22:22:26.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-3562'
Sep  6 22:22:26.620: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:22:26.620: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:22:26.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3562" for this suite.
Sep  6 22:23:14.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:23:14.999: INFO: namespace kubectl-3562 deletion completed in 48.373147247s

• [SLOW TEST:70.684 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:23:14.999: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5twp5 in namespace proxy-884
I0906 22:23:15.175979      17 runners.go:180] Created replication controller with name: proxy-service-5twp5, namespace: proxy-884, replica count: 1
I0906 22:23:16.226434      17 runners.go:180] proxy-service-5twp5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 22:23:17.226657      17 runners.go:180] proxy-service-5twp5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0906 22:23:18.226989      17 runners.go:180] proxy-service-5twp5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 22:23:19.227231      17 runners.go:180] proxy-service-5twp5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 22:23:20.227493      17 runners.go:180] proxy-service-5twp5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0906 22:23:21.227711      17 runners.go:180] proxy-service-5twp5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  6 22:23:21.232: INFO: setup took 6.085987255s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 22:23:21.296: INFO: (0) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 63.661946ms)
Sep  6 22:23:21.296: INFO: (0) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 63.671703ms)
Sep  6 22:23:21.297: INFO: (0) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 65.046206ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 65.326129ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 65.523961ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 65.446037ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 65.460924ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 65.590895ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 65.532599ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 65.583853ms)
Sep  6 22:23:21.298: INFO: (0) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 66.025285ms)
Sep  6 22:23:21.306: INFO: (0) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 73.862153ms)
Sep  6 22:23:21.306: INFO: (0) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 73.750814ms)
Sep  6 22:23:21.306: INFO: (0) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 73.998347ms)
Sep  6 22:23:21.306: INFO: (0) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 73.979923ms)
Sep  6 22:23:21.308: INFO: (0) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 75.266772ms)
Sep  6 22:23:21.321: INFO: (1) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 12.55822ms)
Sep  6 22:23:21.321: INFO: (1) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 12.908863ms)
Sep  6 22:23:21.321: INFO: (1) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 12.37248ms)
Sep  6 22:23:21.321: INFO: (1) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 12.486217ms)
Sep  6 22:23:21.321: INFO: (1) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 12.894773ms)
Sep  6 22:23:21.322: INFO: (1) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 13.305211ms)
Sep  6 22:23:21.323: INFO: (1) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 14.829332ms)
Sep  6 22:23:21.324: INFO: (1) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 15.272117ms)
Sep  6 22:23:21.332: INFO: (1) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 23.684417ms)
Sep  6 22:23:21.332: INFO: (1) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 23.87408ms)
Sep  6 22:23:21.332: INFO: (1) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 23.620946ms)
Sep  6 22:23:21.332: INFO: (1) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 23.410208ms)
Sep  6 22:23:21.332: INFO: (1) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 23.524642ms)
Sep  6 22:23:21.332: INFO: (1) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 23.248431ms)
Sep  6 22:23:21.333: INFO: (1) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 24.733408ms)
Sep  6 22:23:21.336: INFO: (1) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 28.124958ms)
Sep  6 22:23:21.352: INFO: (2) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 15.64793ms)
Sep  6 22:23:21.352: INFO: (2) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 15.892382ms)
Sep  6 22:23:21.352: INFO: (2) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 15.798998ms)
Sep  6 22:23:21.352: INFO: (2) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 16.316098ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 17.353333ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 17.641814ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 17.375301ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 17.552879ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 17.99326ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 18.34553ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 18.209929ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 18.210444ms)
Sep  6 22:23:21.354: INFO: (2) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 18.155892ms)
Sep  6 22:23:21.355: INFO: (2) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 18.939993ms)
Sep  6 22:23:21.355: INFO: (2) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 19.00951ms)
Sep  6 22:23:21.356: INFO: (2) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 20.001018ms)
Sep  6 22:23:21.371: INFO: (3) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 14.793834ms)
Sep  6 22:23:21.371: INFO: (3) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 14.799062ms)
Sep  6 22:23:21.371: INFO: (3) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 15.18113ms)
Sep  6 22:23:21.371: INFO: (3) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 14.769099ms)
Sep  6 22:23:21.372: INFO: (3) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 15.081115ms)
Sep  6 22:23:21.372: INFO: (3) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 15.023832ms)
Sep  6 22:23:21.372: INFO: (3) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 15.200269ms)
Sep  6 22:23:21.382: INFO: (3) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 25.693513ms)
Sep  6 22:23:21.382: INFO: (3) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 25.841822ms)
Sep  6 22:23:21.387: INFO: (3) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 30.838838ms)
Sep  6 22:23:21.387: INFO: (3) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 30.627803ms)
Sep  6 22:23:21.389: INFO: (3) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 32.60245ms)
Sep  6 22:23:21.389: INFO: (3) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 32.642123ms)
Sep  6 22:23:21.395: INFO: (3) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 38.681361ms)
Sep  6 22:23:21.395: INFO: (3) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 38.944046ms)
Sep  6 22:23:21.395: INFO: (3) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 38.136967ms)
Sep  6 22:23:21.440: INFO: (4) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 44.973473ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 46.124757ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 46.40105ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 46.197909ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 45.936194ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 45.968046ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 46.437991ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 46.019002ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 46.633541ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 46.309389ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 45.983528ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 46.511628ms)
Sep  6 22:23:21.442: INFO: (4) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 46.656564ms)
Sep  6 22:23:21.444: INFO: (4) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 48.17167ms)
Sep  6 22:23:21.445: INFO: (4) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 49.019432ms)
Sep  6 22:23:21.446: INFO: (4) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 50.315054ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 18.089762ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 18.620878ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 18.083452ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 18.029477ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 18.501368ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 18.225038ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 17.932592ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 18.13588ms)
Sep  6 22:23:21.464: INFO: (5) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 18.496282ms)
Sep  6 22:23:21.465: INFO: (5) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 19.104926ms)
Sep  6 22:23:21.466: INFO: (5) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 19.873119ms)
Sep  6 22:23:21.466: INFO: (5) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 19.978911ms)
Sep  6 22:23:21.466: INFO: (5) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 19.431301ms)
Sep  6 22:23:21.466: INFO: (5) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 19.68144ms)
Sep  6 22:23:21.466: INFO: (5) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 19.649355ms)
Sep  6 22:23:21.467: INFO: (5) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 20.351341ms)
Sep  6 22:23:21.479: INFO: (6) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 11.768723ms)
Sep  6 22:23:21.486: INFO: (6) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 18.521651ms)
Sep  6 22:23:21.486: INFO: (6) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 19.160107ms)
Sep  6 22:23:21.488: INFO: (6) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 21.256281ms)
Sep  6 22:23:21.488: INFO: (6) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 21.511106ms)
Sep  6 22:23:21.488: INFO: (6) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 21.137954ms)
Sep  6 22:23:21.488: INFO: (6) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.844882ms)
Sep  6 22:23:21.489: INFO: (6) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 20.823753ms)
Sep  6 22:23:21.489: INFO: (6) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 20.986853ms)
Sep  6 22:23:21.489: INFO: (6) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.926322ms)
Sep  6 22:23:21.489: INFO: (6) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 21.410969ms)
Sep  6 22:23:21.489: INFO: (6) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 21.603092ms)
Sep  6 22:23:21.493: INFO: (6) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 25.119553ms)
Sep  6 22:23:21.493: INFO: (6) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 26.300854ms)
Sep  6 22:23:21.494: INFO: (6) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 26.169085ms)
Sep  6 22:23:21.494: INFO: (6) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 26.94572ms)
Sep  6 22:23:21.505: INFO: (7) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 9.827004ms)
Sep  6 22:23:21.505: INFO: (7) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 10.114538ms)
Sep  6 22:23:21.507: INFO: (7) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 12.420354ms)
Sep  6 22:23:21.508: INFO: (7) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 12.310562ms)
Sep  6 22:23:21.508: INFO: (7) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 12.300612ms)
Sep  6 22:23:21.508: INFO: (7) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 13.522901ms)
Sep  6 22:23:21.518: INFO: (7) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 22.375702ms)
Sep  6 22:23:21.519: INFO: (7) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 24.219201ms)
Sep  6 22:23:21.519: INFO: (7) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 24.121266ms)
Sep  6 22:23:21.519: INFO: (7) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 24.708634ms)
Sep  6 22:23:21.530: INFO: (7) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 35.198698ms)
Sep  6 22:23:21.530: INFO: (7) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 35.280009ms)
Sep  6 22:23:21.530: INFO: (7) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 35.742018ms)
Sep  6 22:23:21.530: INFO: (7) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 35.56995ms)
Sep  6 22:23:21.531: INFO: (7) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 35.729367ms)
Sep  6 22:23:21.531: INFO: (7) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 36.237561ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 17.030493ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 16.70347ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 16.84805ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 16.749015ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 16.792673ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 17.312274ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 17.623022ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 16.844641ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 16.971881ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 17.276718ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 17.048942ms)
Sep  6 22:23:21.549: INFO: (8) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 16.981817ms)
Sep  6 22:23:21.551: INFO: (8) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 18.892852ms)
Sep  6 22:23:21.551: INFO: (8) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 18.847446ms)
Sep  6 22:23:21.551: INFO: (8) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 18.833149ms)
Sep  6 22:23:21.551: INFO: (8) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 19.035911ms)
Sep  6 22:23:21.570: INFO: (9) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 19.0029ms)
Sep  6 22:23:21.571: INFO: (9) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 19.494371ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 20.707684ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 20.959195ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 20.497482ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 20.715022ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 20.480429ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 20.467631ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.752777ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 20.487705ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 21.175629ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 20.687636ms)
Sep  6 22:23:21.572: INFO: (9) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 20.756288ms)
Sep  6 22:23:21.573: INFO: (9) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.815946ms)
Sep  6 22:23:21.573: INFO: (9) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 20.537975ms)
Sep  6 22:23:21.573: INFO: (9) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 21.071686ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 16.018491ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 16.197138ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 16.554574ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 16.378285ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 16.329238ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 16.3879ms)
Sep  6 22:23:21.589: INFO: (10) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 16.370604ms)
Sep  6 22:23:21.590: INFO: (10) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 16.73429ms)
Sep  6 22:23:21.591: INFO: (10) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 17.708062ms)
Sep  6 22:23:21.591: INFO: (10) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 17.69942ms)
Sep  6 22:23:21.591: INFO: (10) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 17.582627ms)
Sep  6 22:23:21.591: INFO: (10) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 17.82444ms)
Sep  6 22:23:21.591: INFO: (10) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 18.510693ms)
Sep  6 22:23:21.591: INFO: (10) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 18.278276ms)
Sep  6 22:23:21.592: INFO: (10) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 18.699875ms)
Sep  6 22:23:21.593: INFO: (10) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 19.238799ms)
Sep  6 22:23:21.602: INFO: (11) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 9.033627ms)
Sep  6 22:23:21.602: INFO: (11) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 9.235899ms)
Sep  6 22:23:21.603: INFO: (11) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 10.104057ms)
Sep  6 22:23:21.603: INFO: (11) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 9.898838ms)
Sep  6 22:23:21.603: INFO: (11) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 10.399469ms)
Sep  6 22:23:21.603: INFO: (11) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 10.099224ms)
Sep  6 22:23:21.604: INFO: (11) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 10.293351ms)
Sep  6 22:23:21.604: INFO: (11) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 10.473078ms)
Sep  6 22:23:21.604: INFO: (11) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 10.458897ms)
Sep  6 22:23:21.604: INFO: (11) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 10.535012ms)
Sep  6 22:23:21.604: INFO: (11) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 10.665415ms)
Sep  6 22:23:21.605: INFO: (11) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 11.528962ms)
Sep  6 22:23:21.605: INFO: (11) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 12.125147ms)
Sep  6 22:23:21.605: INFO: (11) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 11.887324ms)
Sep  6 22:23:21.606: INFO: (11) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 12.593591ms)
Sep  6 22:23:21.608: INFO: (11) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 14.570438ms)
Sep  6 22:23:21.614: INFO: (12) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 6.846793ms)
Sep  6 22:23:21.624: INFO: (12) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 15.321902ms)
Sep  6 22:23:21.624: INFO: (12) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 15.450373ms)
Sep  6 22:23:21.624: INFO: (12) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 14.936604ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 21.235576ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 21.328604ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 20.95036ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 21.057296ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 21.33522ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 21.88395ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 21.308956ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 22.01037ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 21.330654ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 21.333083ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 21.99688ms)
Sep  6 22:23:21.630: INFO: (12) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 21.257542ms)
Sep  6 22:23:21.654: INFO: (13) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 23.165024ms)
Sep  6 22:23:21.666: INFO: (13) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 35.317491ms)
Sep  6 22:23:21.666: INFO: (13) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 35.182726ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 46.167696ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 46.586018ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 46.026792ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 46.126533ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 46.509863ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 46.257137ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 46.163444ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 46.476326ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 46.07657ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 46.179352ms)
Sep  6 22:23:21.677: INFO: (13) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 46.386727ms)
Sep  6 22:23:21.679: INFO: (13) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 48.65827ms)
Sep  6 22:23:21.679: INFO: (13) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 48.578302ms)
Sep  6 22:23:21.692: INFO: (14) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 12.587206ms)
Sep  6 22:23:21.699: INFO: (14) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 19.564859ms)
Sep  6 22:23:21.700: INFO: (14) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 20.445077ms)
Sep  6 22:23:21.700: INFO: (14) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 20.243142ms)
Sep  6 22:23:21.700: INFO: (14) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.387093ms)
Sep  6 22:23:21.700: INFO: (14) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 20.293618ms)
Sep  6 22:23:21.700: INFO: (14) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 20.284315ms)
Sep  6 22:23:21.700: INFO: (14) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 20.29748ms)
Sep  6 22:23:21.701: INFO: (14) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 21.197441ms)
Sep  6 22:23:21.704: INFO: (14) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 24.907022ms)
Sep  6 22:23:21.705: INFO: (14) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 25.03649ms)
Sep  6 22:23:21.709: INFO: (14) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 29.878079ms)
Sep  6 22:23:21.709: INFO: (14) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 30.013889ms)
Sep  6 22:23:21.709: INFO: (14) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 29.97315ms)
Sep  6 22:23:21.709: INFO: (14) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 30.160348ms)
Sep  6 22:23:21.710: INFO: (14) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 29.989148ms)
Sep  6 22:23:21.725: INFO: (15) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 15.411386ms)
Sep  6 22:23:21.726: INFO: (15) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 16.298924ms)
Sep  6 22:23:21.726: INFO: (15) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 16.681989ms)
Sep  6 22:23:21.732: INFO: (15) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 22.051065ms)
Sep  6 22:23:21.739: INFO: (15) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 28.309999ms)
Sep  6 22:23:21.742: INFO: (15) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 31.855826ms)
Sep  6 22:23:21.742: INFO: (15) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 31.948307ms)
Sep  6 22:23:21.743: INFO: (15) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 32.241723ms)
Sep  6 22:23:21.743: INFO: (15) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 32.681693ms)
Sep  6 22:23:21.743: INFO: (15) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 32.580664ms)
Sep  6 22:23:21.743: INFO: (15) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 32.812047ms)
Sep  6 22:23:21.743: INFO: (15) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 32.949803ms)
Sep  6 22:23:21.744: INFO: (15) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 33.562604ms)
Sep  6 22:23:21.746: INFO: (15) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 36.493193ms)
Sep  6 22:23:21.746: INFO: (15) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 36.036387ms)
Sep  6 22:23:21.747: INFO: (15) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 36.334168ms)
Sep  6 22:23:21.756: INFO: (16) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 9.298997ms)
Sep  6 22:23:21.756: INFO: (16) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 9.095017ms)
Sep  6 22:23:21.756: INFO: (16) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 9.069067ms)
Sep  6 22:23:21.760: INFO: (16) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 12.576414ms)
Sep  6 22:23:21.760: INFO: (16) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 12.442315ms)
Sep  6 22:23:21.764: INFO: (16) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 16.854125ms)
Sep  6 22:23:21.764: INFO: (16) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 17.341544ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 21.174978ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.601881ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 20.687301ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 20.963953ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 21.296122ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 20.8475ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 21.057375ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 20.801608ms)
Sep  6 22:23:21.768: INFO: (16) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 20.988537ms)
Sep  6 22:23:21.808: INFO: (17) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 39.15481ms)
Sep  6 22:23:21.808: INFO: (17) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 39.248047ms)
Sep  6 22:23:21.817: INFO: (17) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 48.344773ms)
Sep  6 22:23:21.817: INFO: (17) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 48.31112ms)
Sep  6 22:23:21.843: INFO: (17) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 74.61825ms)
Sep  6 22:23:21.843: INFO: (17) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 74.563726ms)
Sep  6 22:23:21.844: INFO: (17) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 75.752284ms)
Sep  6 22:23:21.844: INFO: (17) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 75.622875ms)
Sep  6 22:23:21.849: INFO: (17) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 80.999566ms)
Sep  6 22:23:21.849: INFO: (17) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 80.625458ms)
Sep  6 22:23:21.849: INFO: (17) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 81.019698ms)
Sep  6 22:23:21.854: INFO: (17) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 85.328051ms)
Sep  6 22:23:21.854: INFO: (17) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 85.398905ms)
Sep  6 22:23:21.928: INFO: (17) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 159.237116ms)
Sep  6 22:23:21.928: INFO: (17) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 159.910026ms)
Sep  6 22:23:21.929: INFO: (17) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 160.235044ms)
Sep  6 22:23:21.947: INFO: (18) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 18.075643ms)
Sep  6 22:23:21.954: INFO: (18) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 25.001813ms)
Sep  6 22:23:21.955: INFO: (18) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 25.951835ms)
Sep  6 22:23:21.959: INFO: (18) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 30.515755ms)
Sep  6 22:23:21.959: INFO: (18) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 30.220383ms)
Sep  6 22:23:21.959: INFO: (18) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 30.692852ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 30.215674ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 30.117118ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 30.183687ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 30.112001ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 30.034993ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 29.971618ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 30.372827ms)
Sep  6 22:23:21.960: INFO: (18) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 30.961665ms)
Sep  6 22:23:21.967: INFO: (18) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 37.914868ms)
Sep  6 22:23:21.970: INFO: (18) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 40.666938ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:160/proxy/: foo (200; 14.095425ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:162/proxy/: bar (200; 14.154314ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:460/proxy/: tls baz (200; 14.208903ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:1080/proxy/rewriteme">test</... (200; 14.258742ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/http:proxy-service-5twp5-smdvl:1080/proxy/rewriteme">t... (200; 14.201751ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:443/proxy/tlsrewriteme... (200; 14.212206ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/: <a href="/api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl/proxy/rewriteme">test</a> (200; 14.355215ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:160/proxy/: foo (200; 14.619119ms)
Sep  6 22:23:21.985: INFO: (19) /api/v1/namespaces/proxy-884/pods/proxy-service-5twp5-smdvl:162/proxy/: bar (200; 14.753125ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname1/proxy/: foo (200; 29.312048ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname2/proxy/: tls qux (200; 29.493056ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/services/proxy-service-5twp5:portname2/proxy/: bar (200; 29.382853ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/services/https:proxy-service-5twp5:tlsportname1/proxy/: tls baz (200; 29.368215ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname1/proxy/: foo (200; 29.548324ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/pods/https:proxy-service-5twp5-smdvl:462/proxy/: tls qux (200; 29.702153ms)
Sep  6 22:23:22.000: INFO: (19) /api/v1/namespaces/proxy-884/services/http:proxy-service-5twp5:portname2/proxy/: bar (200; 29.917327ms)
STEP: deleting ReplicationController proxy-service-5twp5 in namespace proxy-884, will wait for the garbage collector to delete the pods
Sep  6 22:23:22.070: INFO: Deleting ReplicationController proxy-service-5twp5 took: 10.541018ms
Sep  6 22:23:22.570: INFO: Terminating ReplicationController proxy-service-5twp5 pods took: 500.175639ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:23:33.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-884" for this suite.
Sep  6 22:23:40.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:23:40.400: INFO: namespace proxy-884 deletion completed in 6.423024825s

• [SLOW TEST:25.401 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:23:40.400: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Sep  6 22:23:40.499: INFO: Waiting up to 5m0s for pod "var-expansion-c6ac240c-9952-4c17-9586-508299b7da47" in namespace "var-expansion-2890" to be "success or failure"
Sep  6 22:23:40.512: INFO: Pod "var-expansion-c6ac240c-9952-4c17-9586-508299b7da47": Phase="Pending", Reason="", readiness=false. Elapsed: 12.925046ms
Sep  6 22:23:42.516: INFO: Pod "var-expansion-c6ac240c-9952-4c17-9586-508299b7da47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01714199s
STEP: Saw pod success
Sep  6 22:23:42.516: INFO: Pod "var-expansion-c6ac240c-9952-4c17-9586-508299b7da47" satisfied condition "success or failure"
Sep  6 22:23:42.521: INFO: Trying to get logs from node metalk8s-24-node1 pod var-expansion-c6ac240c-9952-4c17-9586-508299b7da47 container dapi-container: <nil>
STEP: delete the pod
Sep  6 22:23:42.552: INFO: Waiting for pod var-expansion-c6ac240c-9952-4c17-9586-508299b7da47 to disappear
Sep  6 22:23:42.565: INFO: Pod var-expansion-c6ac240c-9952-4c17-9586-508299b7da47 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:23:42.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2890" for this suite.
Sep  6 22:23:48.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:23:48.780: INFO: namespace var-expansion-2890 deletion completed in 6.204534871s

• [SLOW TEST:8.379 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:23:48.780: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 22:23:48.892: INFO: Waiting up to 5m0s for pod "pod-f54f4e7d-098f-491e-8065-309e56ce0053" in namespace "emptydir-821" to be "success or failure"
Sep  6 22:23:48.896: INFO: Pod "pod-f54f4e7d-098f-491e-8065-309e56ce0053": Phase="Pending", Reason="", readiness=false. Elapsed: 4.182802ms
Sep  6 22:23:50.900: INFO: Pod "pod-f54f4e7d-098f-491e-8065-309e56ce0053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008127937s
STEP: Saw pod success
Sep  6 22:23:50.900: INFO: Pod "pod-f54f4e7d-098f-491e-8065-309e56ce0053" satisfied condition "success or failure"
Sep  6 22:23:50.902: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-f54f4e7d-098f-491e-8065-309e56ce0053 container test-container: <nil>
STEP: delete the pod
Sep  6 22:23:50.948: INFO: Waiting for pod pod-f54f4e7d-098f-491e-8065-309e56ce0053 to disappear
Sep  6 22:23:50.951: INFO: Pod pod-f54f4e7d-098f-491e-8065-309e56ce0053 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:23:50.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-821" for this suite.
Sep  6 22:23:56.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:23:57.129: INFO: namespace emptydir-821 deletion completed in 6.172851735s

• [SLOW TEST:8.349 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:23:57.130: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:23:57.171: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  6 22:23:59.214: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:24:00.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-957" for this suite.
Sep  6 22:24:08.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:24:08.503: INFO: namespace replication-controller-957 deletion completed in 8.241423117s

• [SLOW TEST:11.374 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:24:08.504: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae
Sep  6 22:24:08.561: INFO: Pod name my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae: Found 0 pods out of 1
Sep  6 22:24:13.566: INFO: Pod name my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae: Found 1 pods out of 1
Sep  6 22:24:13.566: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae" are running
Sep  6 22:24:13.570: INFO: Pod "my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae-dgzrz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:24:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:24:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:24:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:24:08 +0000 UTC Reason: Message:}])
Sep  6 22:24:13.570: INFO: Trying to dial the pod
Sep  6 22:24:18.587: INFO: Controller my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae: Got expected result from replica 1 [my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae-dgzrz]: "my-hostname-basic-7d82708d-7120-40ad-bf57-7fc4c7b43dae-dgzrz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:24:18.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8995" for this suite.
Sep  6 22:24:24.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:24:24.944: INFO: namespace replication-controller-8995 deletion completed in 6.352008165s

• [SLOW TEST:16.440 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:24:24.944: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:24:25.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 version'
Sep  6 22:24:25.129: INFO: stderr: ""
Sep  6 22:24:25.129: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:24:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4372" for this suite.
Sep  6 22:24:31.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:24:31.340: INFO: namespace kubectl-4372 deletion completed in 6.194536692s

• [SLOW TEST:6.396 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:24:31.340: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:24:33.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-532" for this suite.
Sep  6 22:24:39.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:24:39.742: INFO: namespace emptydir-wrapper-532 deletion completed in 6.246533703s

• [SLOW TEST:8.402 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:24:39.742: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  6 22:24:39.840: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2149,SelfLink:/api/v1/namespaces/watch-2149/configmaps/e2e-watch-test-label-changed,UID:35094855-a6b2-493e-88cf-1d8c50f5be6f,ResourceVersion:57804,Generation:0,CreationTimestamp:2019-09-06 22:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 22:24:39.840: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2149,SelfLink:/api/v1/namespaces/watch-2149/configmaps/e2e-watch-test-label-changed,UID:35094855-a6b2-493e-88cf-1d8c50f5be6f,ResourceVersion:57805,Generation:0,CreationTimestamp:2019-09-06 22:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep  6 22:24:39.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2149,SelfLink:/api/v1/namespaces/watch-2149/configmaps/e2e-watch-test-label-changed,UID:35094855-a6b2-493e-88cf-1d8c50f5be6f,ResourceVersion:57806,Generation:0,CreationTimestamp:2019-09-06 22:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  6 22:24:49.895: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2149,SelfLink:/api/v1/namespaces/watch-2149/configmaps/e2e-watch-test-label-changed,UID:35094855-a6b2-493e-88cf-1d8c50f5be6f,ResourceVersion:57828,Generation:0,CreationTimestamp:2019-09-06 22:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 22:24:49.895: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2149,SelfLink:/api/v1/namespaces/watch-2149/configmaps/e2e-watch-test-label-changed,UID:35094855-a6b2-493e-88cf-1d8c50f5be6f,ResourceVersion:57829,Generation:0,CreationTimestamp:2019-09-06 22:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep  6 22:24:49.895: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2149,SelfLink:/api/v1/namespaces/watch-2149/configmaps/e2e-watch-test-label-changed,UID:35094855-a6b2-493e-88cf-1d8c50f5be6f,ResourceVersion:57830,Generation:0,CreationTimestamp:2019-09-06 22:24:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:24:49.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2149" for this suite.
Sep  6 22:24:55.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:24:56.098: INFO: namespace watch-2149 deletion completed in 6.180153403s

• [SLOW TEST:16.356 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:24:56.099: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:24:56.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf" in namespace "projected-3014" to be "success or failure"
Sep  6 22:24:56.213: INFO: Pod "downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.823841ms
Sep  6 22:24:58.217: INFO: Pod "downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015275525s
STEP: Saw pod success
Sep  6 22:24:58.218: INFO: Pod "downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf" satisfied condition "success or failure"
Sep  6 22:24:58.220: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf container client-container: <nil>
STEP: delete the pod
Sep  6 22:24:58.248: INFO: Waiting for pod downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf to disappear
Sep  6 22:24:58.252: INFO: Pod downwardapi-volume-f32fd836-f6f6-4ad6-a7b8-8b129c150baf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:24:58.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3014" for this suite.
Sep  6 22:25:04.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:25:04.527: INFO: namespace projected-3014 deletion completed in 6.262066312s

• [SLOW TEST:8.428 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:25:04.528: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3970.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3970.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3970.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3970.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3970.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 220.126.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.126.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.126.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.126.220_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3970.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3970.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3970.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3970.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3970.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3970.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 220.126.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.126.220_udp@PTR;check="$$(dig +tcp +noall +answer +search 220.126.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.126.220_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 22:25:06.734: INFO: Unable to read wheezy_udp@dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.740: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.746: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.752: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.804: INFO: Unable to read jessie_udp@dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.818: INFO: Unable to read jessie_tcp@dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.825: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local from pod dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d: the server could not find the requested resource (get pods dns-test-d219801e-3897-4eb9-ab49-2230940b153d)
Sep  6 22:25:06.866: INFO: Lookups using dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d failed for: [wheezy_udp@dns-test-service.dns-3970.svc.cluster.local wheezy_tcp@dns-test-service.dns-3970.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local jessie_udp@dns-test-service.dns-3970.svc.cluster.local jessie_tcp@dns-test-service.dns-3970.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3970.svc.cluster.local]

Sep  6 22:25:12.127: INFO: DNS probes using dns-3970/dns-test-d219801e-3897-4eb9-ab49-2230940b153d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:25:12.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3970" for this suite.
Sep  6 22:25:18.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:25:18.435: INFO: namespace dns-3970 deletion completed in 6.160292708s

• [SLOW TEST:13.907 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:25:18.436: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-1ad1c1ea-1df5-4e52-a5ab-d865a819b901
STEP: Creating a pod to test consume secrets
Sep  6 22:25:18.510: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226" in namespace "projected-2666" to be "success or failure"
Sep  6 22:25:18.516: INFO: Pod "pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226": Phase="Pending", Reason="", readiness=false. Elapsed: 6.631511ms
Sep  6 22:25:20.521: INFO: Pod "pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011164564s
STEP: Saw pod success
Sep  6 22:25:20.521: INFO: Pod "pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226" satisfied condition "success or failure"
Sep  6 22:25:20.525: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:25:20.555: INFO: Waiting for pod pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226 to disappear
Sep  6 22:25:20.559: INFO: Pod pod-projected-secrets-802eb026-7cf3-428b-89a2-4d2a816fe226 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:25:20.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2666" for this suite.
Sep  6 22:25:26.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:25:26.790: INFO: namespace projected-2666 deletion completed in 6.224977696s

• [SLOW TEST:8.354 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:25:26.790: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-382dfc2e-1160-41aa-b16a-c50f8cf15567
STEP: Creating a pod to test consume secrets
Sep  6 22:25:26.852: INFO: Waiting up to 5m0s for pod "pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec" in namespace "secrets-2693" to be "success or failure"
Sep  6 22:25:26.868: INFO: Pod "pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec": Phase="Pending", Reason="", readiness=false. Elapsed: 16.210022ms
Sep  6 22:25:28.874: INFO: Pod "pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022710704s
STEP: Saw pod success
Sep  6 22:25:28.874: INFO: Pod "pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec" satisfied condition "success or failure"
Sep  6 22:25:28.883: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:25:28.907: INFO: Waiting for pod pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec to disappear
Sep  6 22:25:28.911: INFO: Pod pod-secrets-caa0b542-8cb8-4166-a271-acc1b02576ec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:25:28.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2693" for this suite.
Sep  6 22:25:34.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:25:35.179: INFO: namespace secrets-2693 deletion completed in 6.262218546s

• [SLOW TEST:8.389 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:25:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  6 22:25:35.370: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1811,SelfLink:/api/v1/namespaces/watch-1811/configmaps/e2e-watch-test-resource-version,UID:e065af43-c1e1-4b77-b6b7-a3e516643fb0,ResourceVersion:58083,Generation:0,CreationTimestamp:2019-09-06 22:25:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 22:25:35.370: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1811,SelfLink:/api/v1/namespaces/watch-1811/configmaps/e2e-watch-test-resource-version,UID:e065af43-c1e1-4b77-b6b7-a3e516643fb0,ResourceVersion:58084,Generation:0,CreationTimestamp:2019-09-06 22:25:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:25:35.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1811" for this suite.
Sep  6 22:25:41.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:25:41.673: INFO: namespace watch-1811 deletion completed in 6.297074747s

• [SLOW TEST:6.494 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:25:41.673: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 22:25:41.793: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 22:25:46.798: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:25:46.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1113" for this suite.
Sep  6 22:25:52.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:25:53.073: INFO: namespace replication-controller-1113 deletion completed in 6.237385762s

• [SLOW TEST:11.399 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:25:53.073: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 22:25:53.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-760'
Sep  6 22:25:53.215: INFO: stderr: ""
Sep  6 22:25:53.215: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Sep  6 22:25:53.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete pods e2e-test-nginx-pod --namespace=kubectl-760'
Sep  6 22:26:03.921: INFO: stderr: ""
Sep  6 22:26:03.921: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:26:03.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-760" for this suite.
Sep  6 22:26:10.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:26:10.373: INFO: namespace kubectl-760 deletion completed in 6.443066847s

• [SLOW TEST:17.300 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:26:10.373: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:26:10.460: INFO: Creating deployment "nginx-deployment"
Sep  6 22:26:10.468: INFO: Waiting for observed generation 1
Sep  6 22:26:12.507: INFO: Waiting for all required pods to come up
Sep  6 22:26:12.539: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 22:26:14.560: INFO: Waiting for deployment "nginx-deployment" to complete
Sep  6 22:26:14.569: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep  6 22:26:14.587: INFO: Updating deployment nginx-deployment
Sep  6 22:26:14.587: INFO: Waiting for observed generation 2
Sep  6 22:26:16.665: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 22:26:16.726: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 22:26:16.754: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 22:26:16.796: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 22:26:16.796: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 22:26:16.812: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep  6 22:26:16.858: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep  6 22:26:16.858: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep  6 22:26:16.902: INFO: Updating deployment nginx-deployment
Sep  6 22:26:16.902: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep  6 22:26:16.988: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 22:26:17.023: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 22:26:17.115: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7497,SelfLink:/apis/apps/v1/namespaces/deployment-7497/deployments/nginx-deployment,UID:1a7435d7-f788-4a12-9170-a9d4bcf0611e,ResourceVersion:58497,Generation:3,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-09-06 22:26:14 +0000 UTC 2019-09-06 22:26:10 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-09-06 22:26:16 +0000 UTC 2019-09-06 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 22:26:17.177: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-7497,SelfLink:/apis/apps/v1/namespaces/deployment-7497/replicasets/nginx-deployment-55fb7cb77f,UID:34e33dd4-6c26-4e32-8fc6-0116cc4a22e1,ResourceVersion:58491,Generation:3,CreationTimestamp:2019-09-06 22:26:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1a7435d7-f788-4a12-9170-a9d4bcf0611e 0xc000a0b277 0xc000a0b278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 22:26:17.178: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep  6 22:26:17.178: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-7497,SelfLink:/apis/apps/v1/namespaces/deployment-7497/replicasets/nginx-deployment-7b8c6f4498,UID:9d4068a5-cc4e-43c2-9668-04fd1911af93,ResourceVersion:58488,Generation:3,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1a7435d7-f788-4a12-9170-a9d4bcf0611e 0xc000a0b347 0xc000a0b348}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep  6 22:26:17.218: INFO: Pod "nginx-deployment-55fb7cb77f-2tm5s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2tm5s,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-2tm5s,UID:5c4fbdd5-bb13-4a24-92c0-f13d917c325b,ResourceVersion:58506,Generation:0,CreationTimestamp:2019-09-06 22:26:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.0.51/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc002013b87 0xc002013b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002013c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002013c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.10,PodIP:,StartTime:2019-09-06 22:26:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.218: INFO: Pod "nginx-deployment-55fb7cb77f-6bmqw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6bmqw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-6bmqw,UID:ecf32f44-4b8a-43c8-ad05-3f282505b99e,ResourceVersion:58507,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc002013cf0 0xc002013cf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002013d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002013d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.218: INFO: Pod "nginx-deployment-55fb7cb77f-bwdxv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bwdxv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-bwdxv,UID:43101bcb-8cc9-400c-89ae-3c8adca6d5fe,ResourceVersion:58509,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc002013de7 0xc002013de8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002013e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002013e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.218: INFO: Pod "nginx-deployment-55fb7cb77f-kxtmt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kxtmt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-kxtmt,UID:598224e0-dfaf-4690-a2fc-40e7ea454049,ResourceVersion:58510,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc002013ed7 0xc002013ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002013f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002013f70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.219: INFO: Pod "nginx-deployment-55fb7cb77f-mbhf7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mbhf7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-mbhf7,UID:a1991537-6fae-4d18-9671-f850e1751abf,ResourceVersion:58468,Generation:0,CreationTimestamp:2019-09-06 22:26:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.215/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc002013ff0 0xc002013ff1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008580a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008580c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:,StartTime:2019-09-06 22:26:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.219: INFO: Pod "nginx-deployment-55fb7cb77f-p22gx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p22gx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-p22gx,UID:22205f3a-fa68-469d-be96-f5a850954297,ResourceVersion:58480,Generation:0,CreationTimestamp:2019-09-06 22:26:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.0.43/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc0008581d0 0xc0008581d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008582c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008582e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.10,PodIP:,StartTime:2019-09-06 22:26:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.219: INFO: Pod "nginx-deployment-55fb7cb77f-t9s78" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-t9s78,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-t9s78,UID:de1229e0-51dd-4ab3-bd5f-661ae300cd19,ResourceVersion:58458,Generation:0,CreationTimestamp:2019-09-06 22:26:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.214/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc0008583b0 0xc0008583b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000858450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:,StartTime:2019-09-06 22:26:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.219: INFO: Pod "nginx-deployment-55fb7cb77f-txvgv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-txvgv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-55fb7cb77f-txvgv,UID:cab73b16-e15d-4284-92c3-92d819f86bb0,ResourceVersion:58473,Generation:0,CreationTimestamp:2019-09-06 22:26:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.217/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 34e33dd4-6c26-4e32-8fc6-0116cc4a22e1 0xc0008585e0 0xc0008585e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008586a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:14 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:,StartTime:2019-09-06 22:26:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-9dd6x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9dd6x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-9dd6x,UID:55785219-f672-406a-98cc-123cd269b661,ResourceVersion:58354,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.213/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858830 0xc000858831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008588a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008588c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.213,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://aae7415c6abad84b1ea0301ffb2260cd84a1f79d0377bfeaef11b6ee80c2dcc2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-bwzns" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bwzns,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-bwzns,UID:e517f57c-1207-486c-b7bd-04116ff3550a,ResourceVersion:58504,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858990 0xc000858991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000858a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-ch9z9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ch9z9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-ch9z9,UID:e432239d-226c-490a-a990-e0733a113bdc,ResourceVersion:58383,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.208/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858a87 0xc000858a88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000858b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.208,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://fb6872b8139bb39f655295b1a098401191bb19e3688665d34bcc3350b7368222}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-f98zn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f98zn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-f98zn,UID:46a84adf-f36a-465b-a04a-059a787cdf4a,ResourceVersion:58379,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.209/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858be0 0xc000858be1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858c40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000858c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.209,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://f6a0e92d68f1eb706af87f6f9afab34c957c36ed56da12c2a1ebdeb542a2462b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-kw79d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kw79d,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-kw79d,UID:ee7d7122-5613-4cdd-b28c-af62de37dd4a,ResourceVersion:58493,Generation:0,CreationTimestamp:2019-09-06 22:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858d40 0xc000858d41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000858dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-kzdt8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kzdt8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-kzdt8,UID:e366d530-9db9-4e6a-b41f-49785e9e7725,ResourceVersion:58351,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.210/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858e50 0xc000858e51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000858ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000858ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.210,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://b259188dc6996fb40a93e07445890413c787d86e215cb609da0d99fd6d47c22f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.220: INFO: Pod "nginx-deployment-7b8c6f4498-m4krj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m4krj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-m4krj,UID:bf008e64-e57f-4cec-b792-caa44e9f4590,ResourceVersion:58501,Generation:0,CreationTimestamp:2019-09-06 22:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000858fb0 0xc000858fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000859010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000859030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.221: INFO: Pod "nginx-deployment-7b8c6f4498-mdncm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mdncm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-mdncm,UID:f5a86860-ac34-4e02-bbee-cbbc1ec41c18,ResourceVersion:58389,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.0.33/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc0008590c0 0xc0008590c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000859130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000859150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.10,PodIP:10.233.0.33,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://73780a826290b781e0de81daee6ab4ddae5e4b6888ebdc675a99231df17917b4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.221: INFO: Pod "nginx-deployment-7b8c6f4498-pgt7p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pgt7p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-pgt7p,UID:72800693-65d9-4123-b55d-fb6f3e339a16,ResourceVersion:58508,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859220 0xc000859221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000859280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008592b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.221: INFO: Pod "nginx-deployment-7b8c6f4498-tk4kf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tk4kf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-tk4kf,UID:312097d1-6a4e-4136-9629-3a928d7f23e0,ResourceVersion:58498,Generation:0,CreationTimestamp:2019-09-06 22:26:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859317 0xc000859318}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008593a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008593c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.221: INFO: Pod "nginx-deployment-7b8c6f4498-txgw8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-txgw8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-txgw8,UID:015f173a-ac9a-41bb-8e8b-64a46a1fb727,ResourceVersion:58359,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.211/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859440 0xc000859441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008594a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008594c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.211,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://2a688bd44e55264edceab0093cac771240d38f7e7814ad3aef557d77eb8ebeea}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.221: INFO: Pod "nginx-deployment-7b8c6f4498-vlgjp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vlgjp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-vlgjp,UID:b99900b8-0572-468d-ad9c-e511cd8fa979,ResourceVersion:58505,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859590 0xc000859591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008595f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000859610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.221: INFO: Pod "nginx-deployment-7b8c6f4498-vqkhg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vqkhg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-vqkhg,UID:e9f1ba6f-2e1b-43cb-93cf-fcf7dcda9c2b,ResourceVersion:58392,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.0.28/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859687 0xc000859688}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000859720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000859740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.10,PodIP:10.233.0.28,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://8bd076bec2e27e0f844c83d397e0d6d7f0f63391e3f64c22b4b7e6a34810da0e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.222: INFO: Pod "nginx-deployment-7b8c6f4498-whlps" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-whlps,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-whlps,UID:cd88ce69-7415-47a5-9bc9-7fb4bcf361f8,ResourceVersion:58371,Generation:0,CreationTimestamp:2019-09-06 22:26:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.212/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859810 0xc000859811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000859870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000859890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:10 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.212,StartTime:2019-09-06 22:26:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-06 22:26:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://910637bd86cf920da0f677b1caeab14df396a5be349928b9068e8135a80a9fe9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep  6 22:26:17.222: INFO: Pod "nginx-deployment-7b8c6f4498-xkpl5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xkpl5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-7497,SelfLink:/api/v1/namespaces/deployment-7497/pods/nginx-deployment-7b8c6f4498-xkpl5,UID:a8e10c4a-b0d7-44b9-a9dc-b106cf95e05d,ResourceVersion:58513,Generation:0,CreationTimestamp:2019-09-06 22:26:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 9d4068a5-cc4e-43c2-9668-04fd1911af93 0xc000859960 0xc000859961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rmrhv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rmrhv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-rmrhv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0008599d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0008599f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:26:17 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:26:17.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7497" for this suite.
Sep  6 22:26:25.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:26:25.769: INFO: namespace deployment-7497 deletion completed in 8.468883903s

• [SLOW TEST:15.396 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:26:25.769: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:26:25.951: INFO: (0) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 21.517332ms)
Sep  6 22:26:25.984: INFO: (1) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 33.189201ms)
Sep  6 22:26:26.023: INFO: (2) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 39.011791ms)
Sep  6 22:26:26.031: INFO: (3) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 7.190706ms)
Sep  6 22:26:26.041: INFO: (4) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.268393ms)
Sep  6 22:26:26.061: INFO: (5) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 19.907826ms)
Sep  6 22:26:26.085: INFO: (6) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 24.349745ms)
Sep  6 22:26:26.116: INFO: (7) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 30.575215ms)
Sep  6 22:26:26.172: INFO: (8) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 56.334679ms)
Sep  6 22:26:26.184: INFO: (9) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 11.906989ms)
Sep  6 22:26:26.198: INFO: (10) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 13.258833ms)
Sep  6 22:26:26.206: INFO: (11) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.084691ms)
Sep  6 22:26:26.243: INFO: (12) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 37.053296ms)
Sep  6 22:26:26.255: INFO: (13) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 12.098298ms)
Sep  6 22:26:26.271: INFO: (14) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 16.059214ms)
Sep  6 22:26:26.297: INFO: (15) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 26.328246ms)
Sep  6 22:26:26.338: INFO: (16) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 41.060508ms)
Sep  6 22:26:26.361: INFO: (17) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 22.246787ms)
Sep  6 22:26:26.372: INFO: (18) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.935534ms)
Sep  6 22:26:26.381: INFO: (19) /api/v1/nodes/metalk8s-24/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.802896ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:26:26.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1210" for this suite.
Sep  6 22:26:32.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:26:32.720: INFO: namespace proxy-1210 deletion completed in 6.300650867s

• [SLOW TEST:6.951 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:26:32.721: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  6 22:26:38.871: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:26:38.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0906 22:26:38.871304      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7948" for this suite.
Sep  6 22:26:44.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:26:45.422: INFO: namespace gc-7948 deletion completed in 6.539792153s

• [SLOW TEST:12.701 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:26:45.424: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 22:26:48.109: INFO: Successfully updated pod "labelsupdateacf1720c-dbb5-4430-8087-6ff199a8ed30"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:26:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9466" for this suite.
Sep  6 22:27:14.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:27:14.351: INFO: namespace projected-9466 deletion completed in 22.200026599s

• [SLOW TEST:28.926 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:27:14.351: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 22:27:14.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5179'
Sep  6 22:27:14.529: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 22:27:14.529: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep  6 22:27:14.537: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep  6 22:27:14.544: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep  6 22:27:14.564: INFO: scanned /root for discovery docs: <nil>
Sep  6 22:27:14.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5179'
Sep  6 22:27:30.479: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 22:27:30.479: INFO: stdout: "Created e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d\nScaling up e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep  6 22:27:30.479: INFO: stdout: "Created e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d\nScaling up e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep  6 22:27:30.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5179'
Sep  6 22:27:30.565: INFO: stderr: ""
Sep  6 22:27:30.565: INFO: stdout: "e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d-sxmxx "
Sep  6 22:27:30.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d-sxmxx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5179'
Sep  6 22:27:30.652: INFO: stderr: ""
Sep  6 22:27:30.652: INFO: stdout: "true"
Sep  6 22:27:30.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d-sxmxx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5179'
Sep  6 22:27:30.743: INFO: stderr: ""
Sep  6 22:27:30.743: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep  6 22:27:30.743: INFO: e2e-test-nginx-rc-8506c37cc0f9c14e6425354daabda56d-sxmxx is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Sep  6 22:27:30.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete rc e2e-test-nginx-rc --namespace=kubectl-5179'
Sep  6 22:27:30.840: INFO: stderr: ""
Sep  6 22:27:30.840: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:27:30.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5179" for this suite.
Sep  6 22:27:48.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:27:49.066: INFO: namespace kubectl-5179 deletion completed in 18.188632634s

• [SLOW TEST:34.715 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:27:49.067: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  6 22:27:49.121: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 22:27:49.146: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 22:27:49.150: INFO: 
Logging pods the kubelet thinks is on node metalk8s-24 before test
Sep  6 22:27:49.174: INFO: kube-proxy-ftwr7 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 22:27:49.174: INFO: prometheus-k8s-1 from metalk8s-monitoring started at 2019-09-06 16:49:49 +0000 UTC (3 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 22:27:49.174: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: kube-state-metrics-8d6c9b57-mgpd6 from metalk8s-monitoring started at 2019-09-06 21:23:05 +0000 UTC (4 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  6 22:27:49.174: INFO: coredns-7df84d55f8-gsmw8 from kube-system started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container coredns ready: true, restart count 1
Sep  6 22:27:49.174: INFO: alertmanager-main-0 from metalk8s-monitoring started at 2019-09-06 16:49:42 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: etcd-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container etcd ready: true, restart count 0
Sep  6 22:27:49.174: INFO: prometheus-adapter-764ffd477c-f57gq from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep  6 22:27:49.174: INFO: storage-operator-6fc69d48bd-kvwr9 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container storage-operator ready: true, restart count 0
Sep  6 22:27:49.174: INFO: sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-k452p from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 22:27:49.174: INFO: kube-scheduler-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  6 22:27:49.174: INFO: nginx-ingress-controller-v6vmq from metalk8s-ingress started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 22:27:49.174: INFO: nginx-ingress-default-backend-657d8c587c-wqzq8 from metalk8s-ingress started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep  6 22:27:49.174: INFO: grafana-5ff47b469d-w68rc from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container grafana ready: true, restart count 0
Sep  6 22:27:49.174: INFO: calico-node-wpmfl from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 22:27:49.174: INFO: coredns-7df84d55f8-2gxk7 from kube-system started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container coredns ready: true, restart count 1
Sep  6 22:27:49.174: INFO: kube-apiserver-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  6 22:27:49.174: INFO: prometheus-operator-86bbccc5c5-bpgr7 from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  6 22:27:49.174: INFO: prometheus-k8s-0 from metalk8s-monitoring started at 2019-09-06 16:49:49 +0000 UTC (3 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 22:27:49.174: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: repositories-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container repositories ready: true, restart count 0
Sep  6 22:27:49.174: INFO: alertmanager-main-1 from metalk8s-monitoring started at 2019-09-06 16:49:51 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: alertmanager-main-2 from metalk8s-monitoring started at 2019-09-06 16:49:59 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 22:27:49.174: INFO: metalk8s-ui-94545c497-vnq2q from metalk8s-ui started at 2019-09-06 16:55:16 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container metalk8s-ui ready: true, restart count 0
Sep  6 22:27:49.174: INFO: salt-master-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container salt-api ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container salt-master ready: true, restart count 0
Sep  6 22:27:49.174: INFO: calico-kube-controllers-696f846f6b-lv8ss from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 22:27:49.174: INFO: kube-controller-manager-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  6 22:27:49.174: INFO: node-exporter-qsfqz from metalk8s-monitoring started at 2019-09-06 16:45:02 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.174: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 22:27:49.174: INFO: 
Logging pods the kubelet thinks is on node metalk8s-24-node1 before test
Sep  6 22:27:49.184: INFO: nginx-ingress-controller-c8rsj from metalk8s-ingress started at 2019-09-06 21:50:56 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 22:27:49.184: INFO: sonobuoy-e2e-job-c91c5586720441aa from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container e2e ready: true, restart count 0
Sep  6 22:27:49.184: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 22:27:49.184: INFO: node-exporter-5rd8w from metalk8s-monitoring started at 2019-09-06 21:50:54 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 22:27:49.184: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 22:27:49.184: INFO: calico-node-d62tz from kube-system started at 2019-09-06 21:50:54 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 22:27:49.184: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 21:53:31 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 22:27:49.184: INFO: sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-2vx68 from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 22:27:49.184: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 22:27:49.184: INFO: kube-proxy-w5xhn from kube-system started at 2019-09-06 21:50:56 +0000 UTC (1 container statuses recorded)
Sep  6 22:27:49.184: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c1f9dc2788bc54], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:27:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1707" for this suite.
Sep  6 22:27:56.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:27:56.429: INFO: namespace sched-pred-1707 deletion completed in 6.154944609s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.363 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:27:56.430: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-7244
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7244 to expose endpoints map[]
Sep  6 22:27:56.534: INFO: Get endpoints failed (5.462249ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep  6 22:27:57.539: INFO: successfully validated that service endpoint-test2 in namespace services-7244 exposes endpoints map[] (1.010352415s elapsed)
STEP: Creating pod pod1 in namespace services-7244
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7244 to expose endpoints map[pod1:[80]]
Sep  6 22:27:58.587: INFO: successfully validated that service endpoint-test2 in namespace services-7244 exposes endpoints map[pod1:[80]] (1.03721675s elapsed)
STEP: Creating pod pod2 in namespace services-7244
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7244 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  6 22:28:02.691: INFO: successfully validated that service endpoint-test2 in namespace services-7244 exposes endpoints map[pod1:[80] pod2:[80]] (4.098685217s elapsed)
STEP: Deleting pod pod1 in namespace services-7244
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7244 to expose endpoints map[pod2:[80]]
Sep  6 22:28:02.740: INFO: successfully validated that service endpoint-test2 in namespace services-7244 exposes endpoints map[pod2:[80]] (36.318348ms elapsed)
STEP: Deleting pod pod2 in namespace services-7244
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7244 to expose endpoints map[]
Sep  6 22:28:02.808: INFO: successfully validated that service endpoint-test2 in namespace services-7244 exposes endpoints map[] (54.547548ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:28:02.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7244" for this suite.
Sep  6 22:28:08.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:28:09.378: INFO: namespace services-7244 deletion completed in 6.494814399s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:12.948 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:28:09.378: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  6 22:28:11.536: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-59204415-11b8-440e-a0b9-a81558a09c7f,GenerateName:,Namespace:events-1199,SelfLink:/api/v1/namespaces/events-1199/pods/send-events-59204415-11b8-440e-a0b9-a81558a09c7f,UID:214897b0-622a-4d66-a507-475365593e98,ResourceVersion:59607,Generation:0,CreationTimestamp:2019-09-06 22:28:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 452158225,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.231/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rd259 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rd259,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-rd259 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023922a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023922c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:28:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:28:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:28:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:28:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.231,StartTime:2019-09-06 22:28:09 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-06 22:28:10 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://e523e1cafdfaf87eef78973b0ee5a1a96c578dc03677f6d836e25882b8d89acc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep  6 22:28:13.543: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  6 22:28:15.549: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:28:15.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1199" for this suite.
Sep  6 22:28:55.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:28:55.848: INFO: namespace events-1199 deletion completed in 40.265863421s

• [SLOW TEST:46.470 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:28:55.848: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:28:55.914: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 22:29:00.918: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 22:29:00.918: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 22:29:02.923: INFO: Creating deployment "test-rollover-deployment"
Sep  6 22:29:02.933: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 22:29:04.941: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 22:29:04.947: INFO: Ensure that both replica sets have 1 created replica
Sep  6 22:29:04.957: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 22:29:04.978: INFO: Updating deployment test-rollover-deployment
Sep  6 22:29:04.978: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 22:29:06.988: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 22:29:07.008: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 22:29:07.015: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 22:29:07.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405746, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:29:09.029: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 22:29:09.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405746, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:29:11.032: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 22:29:11.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405746, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:29:13.043: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 22:29:13.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405746, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:29:15.138: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 22:29:15.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405746, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703405742, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 22:29:17.030: INFO: 
Sep  6 22:29:17.030: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 22:29:17.044: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-6625,SelfLink:/apis/apps/v1/namespaces/deployment-6625/deployments/test-rollover-deployment,UID:93980587-210b-4286-886b-05a8d8eca3c7,ResourceVersion:59843,Generation:2,CreationTimestamp:2019-09-06 22:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 22:29:02 +0000 UTC 2019-09-06 22:29:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 22:29:16 +0000 UTC 2019-09-06 22:29:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 22:29:17.049: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-6625,SelfLink:/apis/apps/v1/namespaces/deployment-6625/replicasets/test-rollover-deployment-854595fc44,UID:2ee3928b-b886-400b-b122-51f736ccbbdf,ResourceVersion:59832,Generation:2,CreationTimestamp:2019-09-06 22:29:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 93980587-210b-4286-886b-05a8d8eca3c7 0xc00124a547 0xc00124a548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 22:29:17.049: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 22:29:17.050: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-6625,SelfLink:/apis/apps/v1/namespaces/deployment-6625/replicasets/test-rollover-controller,UID:ec5a9bb1-aca5-4bef-8637-9c42f1f640b2,ResourceVersion:59842,Generation:2,CreationTimestamp:2019-09-06 22:28:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 93980587-210b-4286-886b-05a8d8eca3c7 0xc00124a477 0xc00124a478}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 22:29:17.050: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-6625,SelfLink:/apis/apps/v1/namespaces/deployment-6625/replicasets/test-rollover-deployment-9b8b997cf,UID:eaee945d-4731-4b53-bb24-47ea7ca124bc,ResourceVersion:59789,Generation:2,CreationTimestamp:2019-09-06 22:29:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 93980587-210b-4286-886b-05a8d8eca3c7 0xc00124a610 0xc00124a611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 22:29:17.055: INFO: Pod "test-rollover-deployment-854595fc44-2dxnv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-2dxnv,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-6625,SelfLink:/api/v1/namespaces/deployment-6625/pods/test-rollover-deployment-854595fc44-2dxnv,UID:964e6050-afa1-4b0f-841f-95de55f39919,ResourceVersion:59810,Generation:0,CreationTimestamp:2019-09-06 22:29:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.234/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 2ee3928b-b886-400b-b122-51f736ccbbdf 0xc00360f587 0xc00360f588}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-str64 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-str64,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-str64 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00360f5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00360f610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:29:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:29:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:29:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:29:05 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.234,StartTime:2019-09-06 22:29:05 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 22:29:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://9b8bfe6b642f784386dc4778e35c09bafaa66f1231b2d7fd052e3489fa5dd9ad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:29:17.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6625" for this suite.
Sep  6 22:29:23.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:29:23.280: INFO: namespace deployment-6625 deletion completed in 6.219981398s

• [SLOW TEST:27.432 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:29:23.281: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-cb7b122c-1eab-4039-b8c0-2f88f81de976
STEP: Creating a pod to test consume configMaps
Sep  6 22:29:23.376: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc" in namespace "configmap-320" to be "success or failure"
Sep  6 22:29:23.382: INFO: Pod "pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.898476ms
Sep  6 22:29:25.392: INFO: Pod "pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016067782s
STEP: Saw pod success
Sep  6 22:29:25.392: INFO: Pod "pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc" satisfied condition "success or failure"
Sep  6 22:29:25.407: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:29:25.484: INFO: Waiting for pod pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc to disappear
Sep  6 22:29:25.505: INFO: Pod pod-configmaps-7c6bba8b-e925-4b61-863e-b32e8fd251cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:29:25.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-320" for this suite.
Sep  6 22:29:31.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:29:31.755: INFO: namespace configmap-320 deletion completed in 6.241448251s

• [SLOW TEST:8.474 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:29:31.755: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 22:29:31.818: INFO: Waiting up to 5m0s for pod "downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20" in namespace "downward-api-706" to be "success or failure"
Sep  6 22:29:31.831: INFO: Pod "downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.891009ms
Sep  6 22:29:33.836: INFO: Pod "downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017215426s
STEP: Saw pod success
Sep  6 22:29:33.836: INFO: Pod "downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20" satisfied condition "success or failure"
Sep  6 22:29:33.840: INFO: Trying to get logs from node metalk8s-24-node1 pod downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20 container dapi-container: <nil>
STEP: delete the pod
Sep  6 22:29:33.863: INFO: Waiting for pod downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20 to disappear
Sep  6 22:29:33.868: INFO: Pod downward-api-c69ab796-ec8a-4f4a-9a58-cf3699da0c20 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:29:33.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-706" for this suite.
Sep  6 22:29:39.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:29:40.142: INFO: namespace downward-api-706 deletion completed in 6.26842496s

• [SLOW TEST:8.387 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:29:40.142: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-1713d3d0-24c1-4a1c-b66f-36d79df0d671
STEP: Creating a pod to test consume secrets
Sep  6 22:29:40.274: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436" in namespace "projected-122" to be "success or failure"
Sep  6 22:29:40.331: INFO: Pod "pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436": Phase="Pending", Reason="", readiness=false. Elapsed: 57.086702ms
Sep  6 22:29:42.351: INFO: Pod "pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076850362s
STEP: Saw pod success
Sep  6 22:29:42.351: INFO: Pod "pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436" satisfied condition "success or failure"
Sep  6 22:29:42.370: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:29:42.412: INFO: Waiting for pod pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436 to disappear
Sep  6 22:29:42.415: INFO: Pod pod-projected-secrets-88a9bdf3-c0d5-4fe4-8217-43fdedb85436 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:29:42.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-122" for this suite.
Sep  6 22:29:48.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:29:48.601: INFO: namespace projected-122 deletion completed in 6.177765997s

• [SLOW TEST:8.459 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:29:48.601: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f8a6e526-c0ab-4ad4-a9ef-79a5c3b5e59b
STEP: Creating a pod to test consume configMaps
Sep  6 22:29:48.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86" in namespace "projected-448" to be "success or failure"
Sep  6 22:29:48.668: INFO: Pod "pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.565847ms
Sep  6 22:29:50.673: INFO: Pod "pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009937138s
STEP: Saw pod success
Sep  6 22:29:50.673: INFO: Pod "pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86" satisfied condition "success or failure"
Sep  6 22:29:50.677: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:29:50.705: INFO: Waiting for pod pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86 to disappear
Sep  6 22:29:50.710: INFO: Pod pod-projected-configmaps-d090b832-a9bf-471b-af5e-70f609085e86 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:29:50.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-448" for this suite.
Sep  6 22:29:56.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:29:56.880: INFO: namespace projected-448 deletion completed in 6.163024466s

• [SLOW TEST:8.280 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:29:56.881: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:29:56.969: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 22:29:56.983: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 22:30:01.995: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 22:30:01.995: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 22:30:02.006: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 22:30:02.026: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 22:30:04.050: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 22:30:04.065: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 22:30:04.088: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9526,SelfLink:/apis/apps/v1/namespaces/deployment-9526/deployments/test-rolling-update-deployment,UID:d69ab538-3f5c-4469-93ce-478e999599c2,ResourceVersion:60151,Generation:1,CreationTimestamp:2019-09-06 22:30:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 22:30:02 +0000 UTC 2019-09-06 22:30:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 22:30:03 +0000 UTC 2019-09-06 22:30:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 22:30:04.093: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-9526,SelfLink:/apis/apps/v1/namespaces/deployment-9526/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:bda53f45-fc39-4d2f-883f-25e8b32f84ce,ResourceVersion:60140,Generation:1,CreationTimestamp:2019-09-06 22:30:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d69ab538-3f5c-4469-93ce-478e999599c2 0xc003745b47 0xc003745b48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 22:30:04.093: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 22:30:04.093: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9526,SelfLink:/apis/apps/v1/namespaces/deployment-9526/replicasets/test-rolling-update-controller,UID:cd0221dd-0e51-44c3-9693-6c4d23b1e8c7,ResourceVersion:60150,Generation:2,CreationTimestamp:2019-09-06 22:29:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d69ab538-3f5c-4469-93ce-478e999599c2 0xc003745a77 0xc003745a78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 22:30:04.114: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-mm6qk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-mm6qk,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-9526,SelfLink:/api/v1/namespaces/deployment-9526/pods/test-rolling-update-deployment-79f6b9d75c-mm6qk,UID:9710a5a8-36f7-4164-bf65-adaba99d1d51,ResourceVersion:60139,Generation:0,CreationTimestamp:2019-09-06 22:30:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.240/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c bda53f45-fc39-4d2f-883f-25e8b32f84ce 0xc000a0ac37 0xc000a0ac38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ncqr9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ncqr9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ncqr9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000a0adc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000a0ae00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:30:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:30:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:30:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 22:30:02 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.240,StartTime:2019-09-06 22:30:02 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 22:30:03 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://ae158024af89c12ca3d100f3d784751302ec0dcda0ad3b6401652c48efc944c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:30:04.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9526" for this suite.
Sep  6 22:30:10.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:30:10.458: INFO: namespace deployment-9526 deletion completed in 6.316702996s

• [SLOW TEST:13.578 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:30:10.458: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:30:28.543: INFO: Container started at 2019-09-06 22:30:11 +0000 UTC, pod became ready at 2019-09-06 22:30:27 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:30:28.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4266" for this suite.
Sep  6 22:30:50.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:30:50.711: INFO: namespace container-probe-4266 deletion completed in 22.163592367s

• [SLOW TEST:40.253 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:30:50.712: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:30:50.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4230" for this suite.
Sep  6 22:30:56.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:30:56.941: INFO: namespace services-4230 deletion completed in 6.151403526s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.230 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:30:56.941: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:30:57.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297" in namespace "projected-1948" to be "success or failure"
Sep  6 22:30:57.006: INFO: Pod "downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297": Phase="Pending", Reason="", readiness=false. Elapsed: 3.290486ms
Sep  6 22:30:59.011: INFO: Pod "downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008594387s
STEP: Saw pod success
Sep  6 22:30:59.011: INFO: Pod "downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297" satisfied condition "success or failure"
Sep  6 22:30:59.020: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297 container client-container: <nil>
STEP: delete the pod
Sep  6 22:30:59.040: INFO: Waiting for pod downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297 to disappear
Sep  6 22:30:59.046: INFO: Pod downwardapi-volume-29ee2e92-9a82-4044-a214-827f5092e297 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:30:59.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1948" for this suite.
Sep  6 22:31:05.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:31:05.405: INFO: namespace projected-1948 deletion completed in 6.352527893s

• [SLOW TEST:8.464 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:31:05.406: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:31:05.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27" in namespace "downward-api-8417" to be "success or failure"
Sep  6 22:31:05.491: INFO: Pod "downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012628ms
Sep  6 22:31:07.495: INFO: Pod "downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014188658s
STEP: Saw pod success
Sep  6 22:31:07.495: INFO: Pod "downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27" satisfied condition "success or failure"
Sep  6 22:31:07.499: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27 container client-container: <nil>
STEP: delete the pod
Sep  6 22:31:07.517: INFO: Waiting for pod downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27 to disappear
Sep  6 22:31:07.521: INFO: Pod downwardapi-volume-9650de08-2dd9-4c3f-8907-7f1446b34c27 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:31:07.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8417" for this suite.
Sep  6 22:31:13.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:31:13.669: INFO: namespace downward-api-8417 deletion completed in 6.142512351s

• [SLOW TEST:8.264 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:31:13.669: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Sep  6 22:31:13.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-4273'
Sep  6 22:31:13.892: INFO: stderr: ""
Sep  6 22:31:13.892: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Sep  6 22:31:14.901: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 22:31:14.901: INFO: Found 0 / 1
Sep  6 22:31:15.901: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 22:31:15.901: INFO: Found 1 / 1
Sep  6 22:31:15.901: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 22:31:15.905: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 22:31:15.905: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep  6 22:31:15.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 logs redis-master-926hn redis-master --namespace=kubectl-4273'
Sep  6 22:31:16.003: INFO: stderr: ""
Sep  6 22:31:16.003: INFO: stdout: "1:M 06 Sep 22:31:14.714 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 06 Sep 22:31:14.714 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 06 Sep 22:31:14.714 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 22:31:14.715 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 22:31:14.715 # Server started, Redis version 3.2.12\n1:M 06 Sep 22:31:14.715 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 22:31:14.715 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep  6 22:31:16.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 log redis-master-926hn redis-master --namespace=kubectl-4273 --tail=1'
Sep  6 22:31:16.091: INFO: stderr: ""
Sep  6 22:31:16.091: INFO: stdout: "1:M 06 Sep 22:31:14.715 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep  6 22:31:16.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 log redis-master-926hn redis-master --namespace=kubectl-4273 --limit-bytes=1'
Sep  6 22:31:16.180: INFO: stderr: ""
Sep  6 22:31:16.180: INFO: stdout: "1"
STEP: exposing timestamps
Sep  6 22:31:16.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 log redis-master-926hn redis-master --namespace=kubectl-4273 --tail=1 --timestamps'
Sep  6 22:31:16.281: INFO: stderr: ""
Sep  6 22:31:16.281: INFO: stdout: "2019-09-06T22:31:14.716150222Z 1:M 06 Sep 22:31:14.715 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep  6 22:31:18.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 log redis-master-926hn redis-master --namespace=kubectl-4273 --since=1s'
Sep  6 22:31:18.867: INFO: stderr: ""
Sep  6 22:31:18.867: INFO: stdout: ""
Sep  6 22:31:18.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 log redis-master-926hn redis-master --namespace=kubectl-4273 --since=24h'
Sep  6 22:31:18.958: INFO: stderr: ""
Sep  6 22:31:18.958: INFO: stdout: "1:M 06 Sep 22:31:14.714 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 06 Sep 22:31:14.714 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 06 Sep 22:31:14.714 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 22:31:14.715 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 22:31:14.715 # Server started, Redis version 3.2.12\n1:M 06 Sep 22:31:14.715 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 22:31:14.715 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Sep  6 22:31:18.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-4273'
Sep  6 22:31:19.038: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 22:31:19.038: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep  6 22:31:19.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4273'
Sep  6 22:31:19.123: INFO: stderr: "No resources found.\n"
Sep  6 22:31:19.123: INFO: stdout: ""
Sep  6 22:31:19.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -l name=nginx --namespace=kubectl-4273 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 22:31:19.211: INFO: stderr: ""
Sep  6 22:31:19.211: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:31:19.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4273" for this suite.
Sep  6 22:31:25.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:31:25.551: INFO: namespace kubectl-4273 deletion completed in 6.334505925s

• [SLOW TEST:11.881 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:31:25.551: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Sep  6 22:31:25.631: INFO: Waiting up to 5m0s for pod "var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e" in namespace "var-expansion-5145" to be "success or failure"
Sep  6 22:31:25.645: INFO: Pod "var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01494ms
Sep  6 22:31:27.650: INFO: Pod "var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018737366s
STEP: Saw pod success
Sep  6 22:31:27.650: INFO: Pod "var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e" satisfied condition "success or failure"
Sep  6 22:31:27.653: INFO: Trying to get logs from node metalk8s-24-node1 pod var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e container dapi-container: <nil>
STEP: delete the pod
Sep  6 22:31:27.673: INFO: Waiting for pod var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e to disappear
Sep  6 22:31:27.675: INFO: Pod var-expansion-2b0381e3-abba-43f6-be2c-6ebc2ebdc12e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:31:27.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5145" for this suite.
Sep  6 22:31:33.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:31:33.889: INFO: namespace var-expansion-5145 deletion completed in 6.21012309s

• [SLOW TEST:8.338 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:31:33.890: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:31:33.999: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823" in namespace "downward-api-2497" to be "success or failure"
Sep  6 22:31:34.008: INFO: Pod "downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823": Phase="Pending", Reason="", readiness=false. Elapsed: 8.845471ms
Sep  6 22:31:36.013: INFO: Pod "downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013674208s
STEP: Saw pod success
Sep  6 22:31:36.013: INFO: Pod "downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823" satisfied condition "success or failure"
Sep  6 22:31:36.016: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823 container client-container: <nil>
STEP: delete the pod
Sep  6 22:31:36.044: INFO: Waiting for pod downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823 to disappear
Sep  6 22:31:36.053: INFO: Pod downwardapi-volume-81feae06-3475-4f91-8270-589eebca8823 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:31:36.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2497" for this suite.
Sep  6 22:31:42.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:31:42.250: INFO: namespace downward-api-2497 deletion completed in 6.192829229s

• [SLOW TEST:8.360 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:31:42.250: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-1511cc7b-d1a6-4106-ad65-3069cea7b2af
STEP: Creating a pod to test consume configMaps
Sep  6 22:31:42.347: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a" in namespace "projected-6401" to be "success or failure"
Sep  6 22:31:42.352: INFO: Pod "pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.909037ms
Sep  6 22:31:44.364: INFO: Pod "pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017483692s
STEP: Saw pod success
Sep  6 22:31:44.364: INFO: Pod "pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a" satisfied condition "success or failure"
Sep  6 22:31:44.369: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:31:44.393: INFO: Waiting for pod pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a to disappear
Sep  6 22:31:44.397: INFO: Pod pod-projected-configmaps-58d27515-cdf3-4791-a4b7-0addf53d9f6a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:31:44.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6401" for this suite.
Sep  6 22:31:50.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:31:50.766: INFO: namespace projected-6401 deletion completed in 6.354872093s

• [SLOW TEST:8.515 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:31:50.766: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 22:31:50.909: INFO: Number of nodes with available pods: 0
Sep  6 22:31:50.909: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:31:51.925: INFO: Number of nodes with available pods: 0
Sep  6 22:31:51.925: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:31:52.949: INFO: Number of nodes with available pods: 1
Sep  6 22:31:52.949: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:31:53.926: INFO: Number of nodes with available pods: 2
Sep  6 22:31:53.926: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  6 22:31:53.972: INFO: Number of nodes with available pods: 1
Sep  6 22:31:53.972: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:31:54.991: INFO: Number of nodes with available pods: 1
Sep  6 22:31:54.991: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:31:55.987: INFO: Number of nodes with available pods: 1
Sep  6 22:31:55.987: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:31:56.982: INFO: Number of nodes with available pods: 2
Sep  6 22:31:56.982: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3576, will wait for the garbage collector to delete the pods
Sep  6 22:31:57.052: INFO: Deleting DaemonSet.extensions daemon-set took: 10.420268ms
Sep  6 22:31:58.853: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.800267195s
Sep  6 22:32:10.758: INFO: Number of nodes with available pods: 0
Sep  6 22:32:10.758: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 22:32:10.761: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3576/daemonsets","resourceVersion":"60730"},"items":null}

Sep  6 22:32:10.766: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3576/pods","resourceVersion":"60730"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:32:10.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3576" for this suite.
Sep  6 22:32:16.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:32:17.004: INFO: namespace daemonsets-3576 deletion completed in 6.210537996s

• [SLOW TEST:26.238 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:32:17.004: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-c85e6723-e6c6-4beb-bac3-0c2fa4da42af
STEP: Creating configMap with name cm-test-opt-upd-cfd4394c-0b64-41af-82b7-88e95791e91a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c85e6723-e6c6-4beb-bac3-0c2fa4da42af
STEP: Updating configmap cm-test-opt-upd-cfd4394c-0b64-41af-82b7-88e95791e91a
STEP: Creating configMap with name cm-test-opt-create-5ccd90ad-89f1-416a-9131-98d34b41602e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:32:23.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7544" for this suite.
Sep  6 22:32:45.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:32:45.840: INFO: namespace projected-7544 deletion completed in 22.253239046s

• [SLOW TEST:28.836 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:32:45.841: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  6 22:32:45.929: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4788,SelfLink:/api/v1/namespaces/watch-4788/configmaps/e2e-watch-test-watch-closed,UID:15f5382a-6a34-4ef3-89e6-946c405e6f17,ResourceVersion:60878,Generation:0,CreationTimestamp:2019-09-06 22:32:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep  6 22:32:45.930: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4788,SelfLink:/api/v1/namespaces/watch-4788/configmaps/e2e-watch-test-watch-closed,UID:15f5382a-6a34-4ef3-89e6-946c405e6f17,ResourceVersion:60879,Generation:0,CreationTimestamp:2019-09-06 22:32:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  6 22:32:45.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4788,SelfLink:/api/v1/namespaces/watch-4788/configmaps/e2e-watch-test-watch-closed,UID:15f5382a-6a34-4ef3-89e6-946c405e6f17,ResourceVersion:60880,Generation:0,CreationTimestamp:2019-09-06 22:32:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep  6 22:32:45.959: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4788,SelfLink:/api/v1/namespaces/watch-4788/configmaps/e2e-watch-test-watch-closed,UID:15f5382a-6a34-4ef3-89e6-946c405e6f17,ResourceVersion:60881,Generation:0,CreationTimestamp:2019-09-06 22:32:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:32:45.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4788" for this suite.
Sep  6 22:32:51.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:32:52.198: INFO: namespace watch-4788 deletion completed in 6.229789394s

• [SLOW TEST:6.357 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:32:52.198: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 22:32:53.297: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:32:53.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7612" for this suite.
Sep  6 22:32:59.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:32:59.593: INFO: namespace container-runtime-7612 deletion completed in 6.264054334s

• [SLOW TEST:7.395 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:32:59.594: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:32:59.718: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f5217c8b-3db9-45d5-a3bb-c2db06f608bc", Controller:(*bool)(0xc0030ff7b6), BlockOwnerDeletion:(*bool)(0xc0030ff7b7)}}
Sep  6 22:32:59.735: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"811c8832-beef-4d97-bf7b-a1f668acba96", Controller:(*bool)(0xc0039386de), BlockOwnerDeletion:(*bool)(0xc0039386df)}}
Sep  6 22:32:59.746: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"284cfeaa-d6d1-4df4-a8a4-c8d55c41270d", Controller:(*bool)(0xc002f4d5ee), BlockOwnerDeletion:(*bool)(0xc002f4d5ef)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:33:04.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8005" for this suite.
Sep  6 22:33:10.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:33:11.067: INFO: namespace gc-8005 deletion completed in 6.300864311s

• [SLOW TEST:11.473 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:33:11.067: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-72c7fcc8-2ce7-4dac-9d1b-e4f05d6f2e5f
STEP: Creating a pod to test consume secrets
Sep  6 22:33:11.348: INFO: Waiting up to 5m0s for pod "pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a" in namespace "secrets-4154" to be "success or failure"
Sep  6 22:33:11.353: INFO: Pod "pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.698207ms
Sep  6 22:33:13.359: INFO: Pod "pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01163158s
STEP: Saw pod success
Sep  6 22:33:13.359: INFO: Pod "pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a" satisfied condition "success or failure"
Sep  6 22:33:13.363: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:33:13.401: INFO: Waiting for pod pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a to disappear
Sep  6 22:33:13.408: INFO: Pod pod-secrets-2cfae50f-2628-4381-be4b-cbc05822c91a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:33:13.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4154" for this suite.
Sep  6 22:33:19.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:33:19.601: INFO: namespace secrets-4154 deletion completed in 6.176706168s

• [SLOW TEST:8.534 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:33:19.602: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep  6 22:33:29.685: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:33:29.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0906 22:33:29.685715      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5044" for this suite.
Sep  6 22:33:35.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:33:35.907: INFO: namespace gc-5044 deletion completed in 6.213839619s

• [SLOW TEST:16.305 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:33:35.907: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 22:33:36.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8447'
Sep  6 22:33:36.145: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 22:33:36.145: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Sep  6 22:33:38.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8447'
Sep  6 22:33:38.238: INFO: stderr: ""
Sep  6 22:33:38.238: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:33:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8447" for this suite.
Sep  6 22:34:00.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:34:00.498: INFO: namespace kubectl-8447 deletion completed in 22.252664889s

• [SLOW TEST:24.592 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:34:00.499: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:34:00.545: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:34:02.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3073" for this suite.
Sep  6 22:34:42.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:34:42.951: INFO: namespace pods-3073 deletion completed in 40.312836542s

• [SLOW TEST:42.452 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:34:42.951: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:34:48.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7162" for this suite.
Sep  6 22:34:54.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:34:55.032: INFO: namespace watch-7162 deletion completed in 6.474674445s

• [SLOW TEST:12.080 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:34:55.032: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-dfe6f8a1-60ab-43f6-b711-c0592aa82dfd
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:34:55.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8879" for this suite.
Sep  6 22:35:01.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:35:01.360: INFO: namespace configmap-8879 deletion completed in 6.234558504s

• [SLOW TEST:6.329 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:35:01.360: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:35:03.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8546" for this suite.
Sep  6 22:35:43.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:35:43.744: INFO: namespace kubelet-test-8546 deletion completed in 40.219782992s

• [SLOW TEST:42.384 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:35:43.744: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  6 22:35:43.792: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 22:35:43.804: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 22:35:43.809: INFO: 
Logging pods the kubelet thinks is on node metalk8s-24 before test
Sep  6 22:35:43.837: INFO: kube-proxy-ftwr7 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 22:35:43.837: INFO: prometheus-k8s-1 from metalk8s-monitoring started at 2019-09-06 16:49:49 +0000 UTC (3 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 22:35:43.837: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: kube-state-metrics-8d6c9b57-mgpd6 from metalk8s-monitoring started at 2019-09-06 21:23:05 +0000 UTC (4 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  6 22:35:43.837: INFO: coredns-7df84d55f8-gsmw8 from kube-system started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container coredns ready: true, restart count 1
Sep  6 22:35:43.837: INFO: alertmanager-main-0 from metalk8s-monitoring started at 2019-09-06 16:49:42 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: etcd-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container etcd ready: true, restart count 0
Sep  6 22:35:43.837: INFO: prometheus-adapter-764ffd477c-f57gq from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep  6 22:35:43.837: INFO: storage-operator-6fc69d48bd-kvwr9 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container storage-operator ready: true, restart count 0
Sep  6 22:35:43.837: INFO: sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-k452p from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  6 22:35:43.837: INFO: kube-scheduler-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  6 22:35:43.837: INFO: nginx-ingress-controller-v6vmq from metalk8s-ingress started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 22:35:43.837: INFO: nginx-ingress-default-backend-657d8c587c-wqzq8 from metalk8s-ingress started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep  6 22:35:43.837: INFO: grafana-5ff47b469d-w68rc from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container grafana ready: true, restart count 0
Sep  6 22:35:43.837: INFO: calico-node-wpmfl from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 22:35:43.837: INFO: coredns-7df84d55f8-2gxk7 from kube-system started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container coredns ready: true, restart count 1
Sep  6 22:35:43.837: INFO: kube-apiserver-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  6 22:35:43.837: INFO: prometheus-operator-86bbccc5c5-bpgr7 from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  6 22:35:43.837: INFO: prometheus-k8s-0 from metalk8s-monitoring started at 2019-09-06 16:49:49 +0000 UTC (3 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 22:35:43.837: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: repositories-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container repositories ready: true, restart count 0
Sep  6 22:35:43.837: INFO: alertmanager-main-1 from metalk8s-monitoring started at 2019-09-06 16:49:51 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: alertmanager-main-2 from metalk8s-monitoring started at 2019-09-06 16:49:59 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 22:35:43.837: INFO: metalk8s-ui-94545c497-vnq2q from metalk8s-ui started at 2019-09-06 16:55:16 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container metalk8s-ui ready: true, restart count 0
Sep  6 22:35:43.837: INFO: salt-master-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container salt-api ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container salt-master ready: true, restart count 0
Sep  6 22:35:43.837: INFO: calico-kube-controllers-696f846f6b-lv8ss from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 22:35:43.837: INFO: kube-controller-manager-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  6 22:35:43.837: INFO: node-exporter-qsfqz from metalk8s-monitoring started at 2019-09-06 16:45:02 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 22:35:43.837: INFO: 
Logging pods the kubelet thinks is on node metalk8s-24-node1 before test
Sep  6 22:35:43.853: INFO: kube-proxy-w5xhn from kube-system started at 2019-09-06 21:50:56 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 22:35:43.853: INFO: nginx-ingress-controller-c8rsj from metalk8s-ingress started at 2019-09-06 21:50:56 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 22:35:43.853: INFO: sonobuoy-e2e-job-c91c5586720441aa from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container e2e ready: true, restart count 0
Sep  6 22:35:43.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 22:35:43.853: INFO: node-exporter-5rd8w from metalk8s-monitoring started at 2019-09-06 21:50:54 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 22:35:43.853: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 22:35:43.853: INFO: calico-node-d62tz from kube-system started at 2019-09-06 21:50:54 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 22:35:43.853: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 21:53:31 +0000 UTC (1 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 22:35:43.853: INFO: sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-2vx68 from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 22:35:43.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 22:35:43.853: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node metalk8s-24
STEP: verifying the node has the label node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod sonobuoy requesting resource cpu=0m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod sonobuoy-e2e-job-c91c5586720441aa requesting resource cpu=0m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-2vx68 requesting resource cpu=0m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-k452p requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod calico-kube-controllers-696f846f6b-lv8ss requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod calico-node-d62tz requesting resource cpu=250m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod calico-node-wpmfl requesting resource cpu=250m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod coredns-7df84d55f8-2gxk7 requesting resource cpu=100m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod coredns-7df84d55f8-gsmw8 requesting resource cpu=100m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod etcd-metalk8s-24 requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod kube-apiserver-metalk8s-24 requesting resource cpu=250m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod kube-controller-manager-metalk8s-24 requesting resource cpu=200m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod kube-proxy-ftwr7 requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod kube-proxy-w5xhn requesting resource cpu=0m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod kube-scheduler-metalk8s-24 requesting resource cpu=100m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod repositories-metalk8s-24 requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod salt-master-metalk8s-24 requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod storage-operator-6fc69d48bd-kvwr9 requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod nginx-ingress-controller-c8rsj requesting resource cpu=0m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod nginx-ingress-controller-v6vmq requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod nginx-ingress-default-backend-657d8c587c-wqzq8 requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod alertmanager-main-0 requesting resource cpu=50m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod alertmanager-main-1 requesting resource cpu=50m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod alertmanager-main-2 requesting resource cpu=50m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod grafana-5ff47b469d-w68rc requesting resource cpu=100m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod kube-state-metrics-8d6c9b57-mgpd6 requesting resource cpu=132m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod node-exporter-5rd8w requesting resource cpu=112m on Node metalk8s-24-node1
Sep  6 22:35:43.921: INFO: Pod node-exporter-qsfqz requesting resource cpu=112m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod prometheus-adapter-764ffd477c-f57gq requesting resource cpu=0m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod prometheus-operator-86bbccc5c5-bpgr7 requesting resource cpu=100m on Node metalk8s-24
Sep  6 22:35:43.921: INFO: Pod metalk8s-ui-94545c497-vnq2q requesting resource cpu=100m on Node metalk8s-24
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-632d490f-fb13-4289-9192-95a05d496e6f.15c1fa4aae0e7fa0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5352/filler-pod-632d490f-fb13-4289-9192-95a05d496e6f to metalk8s-24-node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-632d490f-fb13-4289-9192-95a05d496e6f.15c1fa4ae527ebd0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-632d490f-fb13-4289-9192-95a05d496e6f.15c1fa4ae78bffd9], Reason = [Created], Message = [Created container filler-pod-632d490f-fb13-4289-9192-95a05d496e6f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-632d490f-fb13-4289-9192-95a05d496e6f.15c1fa4aedb2202f], Reason = [Started], Message = [Started container filler-pod-632d490f-fb13-4289-9192-95a05d496e6f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7.15c1fa4aad6b506e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5352/filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7 to metalk8s-24]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7.15c1fa4b0a7625b7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7.15c1fa4b15cf306d], Reason = [Created], Message = [Created container filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7.15c1fa4b2c7cfcd0], Reason = [Started], Message = [Started container filler-pod-adf1982c-edaf-4d44-a42c-cdf3fd7387c7]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c1fa4b9fd64727], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node metalk8s-24-node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node metalk8s-24
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:35:49.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5352" for this suite.
Sep  6 22:35:57.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:35:57.259: INFO: namespace sched-pred-5352 deletion completed in 8.187946885s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:13.515 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:35:57.260: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-e6acfe40-6f21-4003-9f47-1c23d9da9bc1
STEP: Creating a pod to test consume secrets
Sep  6 22:35:57.356: INFO: Waiting up to 5m0s for pod "pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c" in namespace "secrets-9681" to be "success or failure"
Sep  6 22:35:57.364: INFO: Pod "pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.744199ms
Sep  6 22:35:59.370: INFO: Pod "pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013430886s
STEP: Saw pod success
Sep  6 22:35:59.370: INFO: Pod "pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c" satisfied condition "success or failure"
Sep  6 22:35:59.375: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:35:59.431: INFO: Waiting for pod pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c to disappear
Sep  6 22:35:59.438: INFO: Pod pod-secrets-8959fd37-249b-4391-a89a-88f30d43f86c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:35:59.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9681" for this suite.
Sep  6 22:36:05.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:36:05.636: INFO: namespace secrets-9681 deletion completed in 6.191506479s

• [SLOW TEST:8.376 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:36:05.636: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:36:05.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984" in namespace "projected-4893" to be "success or failure"
Sep  6 22:36:05.760: INFO: Pod "downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984": Phase="Pending", Reason="", readiness=false. Elapsed: 10.934317ms
Sep  6 22:36:07.777: INFO: Pod "downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028005884s
STEP: Saw pod success
Sep  6 22:36:07.777: INFO: Pod "downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984" satisfied condition "success or failure"
Sep  6 22:36:07.797: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984 container client-container: <nil>
STEP: delete the pod
Sep  6 22:36:07.855: INFO: Waiting for pod downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984 to disappear
Sep  6 22:36:07.865: INFO: Pod downwardapi-volume-b75c4586-bdef-40c3-bf2d-fe580bf08984 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:36:07.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4893" for this suite.
Sep  6 22:36:15.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:36:16.181: INFO: namespace projected-4893 deletion completed in 8.295853377s

• [SLOW TEST:10.545 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:36:16.181: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 22:36:16.311: INFO: Waiting up to 5m0s for pod "pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab" in namespace "emptydir-866" to be "success or failure"
Sep  6 22:36:16.349: INFO: Pod "pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab": Phase="Pending", Reason="", readiness=false. Elapsed: 38.131848ms
Sep  6 22:36:18.353: INFO: Pod "pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042097062s
STEP: Saw pod success
Sep  6 22:36:18.353: INFO: Pod "pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab" satisfied condition "success or failure"
Sep  6 22:36:18.356: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab container test-container: <nil>
STEP: delete the pod
Sep  6 22:36:18.377: INFO: Waiting for pod pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab to disappear
Sep  6 22:36:18.383: INFO: Pod pod-0a10dbf7-5a0d-4063-8ba0-dd083c583aab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:36:18.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-866" for this suite.
Sep  6 22:36:24.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:36:24.731: INFO: namespace emptydir-866 deletion completed in 6.341849794s

• [SLOW TEST:8.550 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:36:24.731: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:36:46.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1885" for this suite.
Sep  6 22:36:52.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:36:52.334: INFO: namespace container-runtime-1885 deletion completed in 6.188751948s

• [SLOW TEST:27.604 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:36:52.335: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2b6f7844-32ce-45b5-b438-c3b214628914
STEP: Creating a pod to test consume secrets
Sep  6 22:36:52.420: INFO: Waiting up to 5m0s for pod "pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a" in namespace "secrets-3870" to be "success or failure"
Sep  6 22:36:52.436: INFO: Pod "pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.481361ms
Sep  6 22:36:54.460: INFO: Pod "pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04050384s
STEP: Saw pod success
Sep  6 22:36:54.460: INFO: Pod "pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a" satisfied condition "success or failure"
Sep  6 22:36:54.465: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a container secret-env-test: <nil>
STEP: delete the pod
Sep  6 22:36:54.548: INFO: Waiting for pod pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a to disappear
Sep  6 22:36:54.568: INFO: Pod pod-secrets-7958b3d3-ab51-4a4d-b576-ebc540aa684a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:36:54.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3870" for this suite.
Sep  6 22:37:00.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:37:00.902: INFO: namespace secrets-3870 deletion completed in 6.321674058s

• [SLOW TEST:8.567 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:37:00.902: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 22:37:00.975: INFO: Waiting up to 5m0s for pod "pod-e18966d8-c321-461a-b37c-5c9986ba4ab9" in namespace "emptydir-6132" to be "success or failure"
Sep  6 22:37:01.014: INFO: Pod "pod-e18966d8-c321-461a-b37c-5c9986ba4ab9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.745177ms
Sep  6 22:37:03.034: INFO: Pod "pod-e18966d8-c321-461a-b37c-5c9986ba4ab9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.058670145s
STEP: Saw pod success
Sep  6 22:37:03.034: INFO: Pod "pod-e18966d8-c321-461a-b37c-5c9986ba4ab9" satisfied condition "success or failure"
Sep  6 22:37:03.048: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-e18966d8-c321-461a-b37c-5c9986ba4ab9 container test-container: <nil>
STEP: delete the pod
Sep  6 22:37:03.080: INFO: Waiting for pod pod-e18966d8-c321-461a-b37c-5c9986ba4ab9 to disappear
Sep  6 22:37:03.089: INFO: Pod pod-e18966d8-c321-461a-b37c-5c9986ba4ab9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:37:03.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6132" for this suite.
Sep  6 22:37:09.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:37:09.448: INFO: namespace emptydir-6132 deletion completed in 6.350264856s

• [SLOW TEST:8.546 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:37:09.449: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 22:37:09.543: INFO: Waiting up to 5m0s for pod "pod-9d92cdb8-771a-422f-8f12-ea9aea190c48" in namespace "emptydir-5318" to be "success or failure"
Sep  6 22:37:09.561: INFO: Pod "pod-9d92cdb8-771a-422f-8f12-ea9aea190c48": Phase="Pending", Reason="", readiness=false. Elapsed: 18.460098ms
Sep  6 22:37:11.570: INFO: Pod "pod-9d92cdb8-771a-422f-8f12-ea9aea190c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027651132s
STEP: Saw pod success
Sep  6 22:37:11.570: INFO: Pod "pod-9d92cdb8-771a-422f-8f12-ea9aea190c48" satisfied condition "success or failure"
Sep  6 22:37:11.578: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-9d92cdb8-771a-422f-8f12-ea9aea190c48 container test-container: <nil>
STEP: delete the pod
Sep  6 22:37:11.609: INFO: Waiting for pod pod-9d92cdb8-771a-422f-8f12-ea9aea190c48 to disappear
Sep  6 22:37:11.613: INFO: Pod pod-9d92cdb8-771a-422f-8f12-ea9aea190c48 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:37:11.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5318" for this suite.
Sep  6 22:37:17.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:37:17.884: INFO: namespace emptydir-5318 deletion completed in 6.260098444s

• [SLOW TEST:8.435 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:37:17.885: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-723
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep  6 22:37:18.189: INFO: Found 0 stateful pods, waiting for 3
Sep  6 22:37:28.198: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:37:28.198: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:37:28.198: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 22:37:28.229: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  6 22:37:38.286: INFO: Updating stateful set ss2
Sep  6 22:37:38.321: INFO: Waiting for Pod statefulset-723/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Sep  6 22:37:48.398: INFO: Found 1 stateful pods, waiting for 3
Sep  6 22:37:58.403: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:37:58.403: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:37:58.403: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  6 22:37:58.430: INFO: Updating stateful set ss2
Sep  6 22:37:58.453: INFO: Waiting for Pod statefulset-723/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:38:08.464: INFO: Waiting for Pod statefulset-723/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:38:18.485: INFO: Updating stateful set ss2
Sep  6 22:38:18.502: INFO: Waiting for StatefulSet statefulset-723/ss2 to complete update
Sep  6 22:38:18.502: INFO: Waiting for Pod statefulset-723/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 22:38:28.513: INFO: Deleting all statefulset in ns statefulset-723
Sep  6 22:38:28.530: INFO: Scaling statefulset ss2 to 0
Sep  6 22:38:48.552: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 22:38:48.557: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:38:48.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-723" for this suite.
Sep  6 22:38:54.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:38:54.901: INFO: namespace statefulset-723 deletion completed in 6.304876783s

• [SLOW TEST:97.016 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:38:54.901: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-1303/secret-test-64971c75-ddf3-486b-8d93-e13daf4106a1
STEP: Creating a pod to test consume secrets
Sep  6 22:38:55.030: INFO: Waiting up to 5m0s for pod "pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995" in namespace "secrets-1303" to be "success or failure"
Sep  6 22:38:55.037: INFO: Pod "pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995": Phase="Pending", Reason="", readiness=false. Elapsed: 7.025999ms
Sep  6 22:38:57.042: INFO: Pod "pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01146069s
STEP: Saw pod success
Sep  6 22:38:57.042: INFO: Pod "pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995" satisfied condition "success or failure"
Sep  6 22:38:57.046: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995 container env-test: <nil>
STEP: delete the pod
Sep  6 22:38:57.069: INFO: Waiting for pod pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995 to disappear
Sep  6 22:38:57.081: INFO: Pod pod-configmaps-933e9adc-c73a-4e1c-80cf-78c0d120f995 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:38:57.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1303" for this suite.
Sep  6 22:39:03.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:39:03.316: INFO: namespace secrets-1303 deletion completed in 6.218985399s

• [SLOW TEST:8.415 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:39:03.316: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep  6 22:39:43.417: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0906 22:39:43.417793      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 22:39:43.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8975" for this suite.
Sep  6 22:39:51.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:39:51.713: INFO: namespace gc-8975 deletion completed in 8.291181538s

• [SLOW TEST:48.397 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:39:51.714: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Sep  6 22:39:51.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 --namespace=kubectl-3471 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep  6 22:39:53.206: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep  6 22:39:53.206: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:39:55.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3471" for this suite.
Sep  6 22:40:05.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:40:05.494: INFO: namespace kubectl-3471 deletion completed in 10.271268008s

• [SLOW TEST:13.781 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:40:05.495: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 22:40:07.663: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:40:07.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9526" for this suite.
Sep  6 22:40:13.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:40:14.057: INFO: namespace container-runtime-9526 deletion completed in 6.363275522s

• [SLOW TEST:8.563 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:40:14.058: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-b3c5a1c7-8cd8-4bea-bc89-72f4eb69a8f4
STEP: Creating a pod to test consume secrets
Sep  6 22:40:14.156: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a" in namespace "projected-9464" to be "success or failure"
Sep  6 22:40:14.166: INFO: Pod "pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.855975ms
Sep  6 22:40:16.182: INFO: Pod "pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026072553s
STEP: Saw pod success
Sep  6 22:40:16.182: INFO: Pod "pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a" satisfied condition "success or failure"
Sep  6 22:40:16.185: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:40:16.250: INFO: Waiting for pod pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a to disappear
Sep  6 22:40:16.254: INFO: Pod pod-projected-secrets-730a0e1b-4eb6-46c9-9895-80ea7713c30a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:40:16.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9464" for this suite.
Sep  6 22:40:22.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:40:24.121: INFO: namespace projected-9464 deletion completed in 7.860476288s

• [SLOW TEST:10.064 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:40:24.122: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 22:40:24.236: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 22:41:04.229: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-46aa2acc-41d5-4671-9685-95dbcdd3d1b2", GenerateName:"", Namespace:"init-container-3907", SelfLink:"/api/v1/namespaces/init-container-3907/pods/pod-init-46aa2acc-41d5-4671-9685-95dbcdd3d1b2", UID:"c8f9e15c-3880-4202-b350-a8b32fb3ec7d", ResourceVersion:"63361", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703406424, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"236505384"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.233.223.225/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bn6q5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002cc8a40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bn6q5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bn6q5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bn6q5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f32148), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"metalk8s-24-node1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0033fcf60), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f321c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f321e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f321e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f321ec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703406424, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703406424, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703406424, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703406424, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.0.45", PodIP:"10.233.223.225", StartTime:(*v1.Time)(0xc003360440), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b1fd50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b1fdc0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://14138e60081eb62df86db5713af529abd95a87ab3008a88aa9f9747cc5d779aa"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003360480), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003360460), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:41:04.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3907" for this suite.
Sep  6 22:41:28.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:41:28.408: INFO: namespace init-container-3907 deletion completed in 24.156863378s

• [SLOW TEST:64.286 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:41:28.408: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-451d5207-703c-475c-b346-8ac18c78d724
STEP: Creating a pod to test consume secrets
Sep  6 22:41:28.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6" in namespace "projected-328" to be "success or failure"
Sep  6 22:41:28.490: INFO: Pod "pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.594982ms
Sep  6 22:41:30.498: INFO: Pod "pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015798381s
STEP: Saw pod success
Sep  6 22:41:30.498: INFO: Pod "pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6" satisfied condition "success or failure"
Sep  6 22:41:30.506: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:41:30.539: INFO: Waiting for pod pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6 to disappear
Sep  6 22:41:30.542: INFO: Pod pod-projected-secrets-dff730fd-138e-4076-86f6-a69c625ca3a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:41:30.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-328" for this suite.
Sep  6 22:41:36.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:41:36.778: INFO: namespace projected-328 deletion completed in 6.227710784s

• [SLOW TEST:8.370 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:41:36.778: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-3cde0856-e25a-4de9-9ef7-c4a81a13f9ac in namespace container-probe-3134
Sep  6 22:41:38.872: INFO: Started pod busybox-3cde0856-e25a-4de9-9ef7-c4a81a13f9ac in namespace container-probe-3134
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 22:41:38.875: INFO: Initial restart count of pod busybox-3cde0856-e25a-4de9-9ef7-c4a81a13f9ac is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:45:39.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3134" for this suite.
Sep  6 22:45:45.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:45:46.069: INFO: namespace container-probe-3134 deletion completed in 6.301074602s

• [SLOW TEST:249.291 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:45:46.069: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-aa9f458a-6d66-4ea3-a681-3c923906e916
STEP: Creating secret with name secret-projected-all-test-volume-d065dba5-47ce-4b70-a92e-ab3d7b720d19
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  6 22:45:46.162: INFO: Waiting up to 5m0s for pod "projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf" in namespace "projected-3093" to be "success or failure"
Sep  6 22:45:46.180: INFO: Pod "projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01695ms
Sep  6 22:45:48.191: INFO: Pod "projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028081774s
STEP: Saw pod success
Sep  6 22:45:48.191: INFO: Pod "projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf" satisfied condition "success or failure"
Sep  6 22:45:48.198: INFO: Trying to get logs from node metalk8s-24-node1 pod projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  6 22:45:48.235: INFO: Waiting for pod projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf to disappear
Sep  6 22:45:48.238: INFO: Pod projected-volume-9ac94ba1-de5b-4f76-abc7-2560e82650cf no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:45:48.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3093" for this suite.
Sep  6 22:45:54.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:45:54.463: INFO: namespace projected-3093 deletion completed in 6.220135311s

• [SLOW TEST:8.394 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:45:54.463: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:45:54.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1" in namespace "downward-api-2926" to be "success or failure"
Sep  6 22:45:54.535: INFO: Pod "downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542437ms
Sep  6 22:45:56.540: INFO: Pod "downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00836686s
STEP: Saw pod success
Sep  6 22:45:56.540: INFO: Pod "downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1" satisfied condition "success or failure"
Sep  6 22:45:56.543: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1 container client-container: <nil>
STEP: delete the pod
Sep  6 22:45:56.570: INFO: Waiting for pod downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1 to disappear
Sep  6 22:45:56.574: INFO: Pod downwardapi-volume-21b67081-fcf5-4b5c-8494-8f3036a70ef1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:45:56.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2926" for this suite.
Sep  6 22:46:02.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:46:02.849: INFO: namespace downward-api-2926 deletion completed in 6.269600848s

• [SLOW TEST:8.386 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:46:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:46:03.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397" in namespace "projected-8924" to be "success or failure"
Sep  6 22:46:03.081: INFO: Pod "downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397": Phase="Pending", Reason="", readiness=false. Elapsed: 4.554353ms
Sep  6 22:46:05.102: INFO: Pod "downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025575027s
STEP: Saw pod success
Sep  6 22:46:05.102: INFO: Pod "downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397" satisfied condition "success or failure"
Sep  6 22:46:05.120: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397 container client-container: <nil>
STEP: delete the pod
Sep  6 22:46:05.195: INFO: Waiting for pod downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397 to disappear
Sep  6 22:46:05.217: INFO: Pod downwardapi-volume-68599c3f-0ced-4d67-b225-b134471c2397 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:46:05.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8924" for this suite.
Sep  6 22:46:11.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:46:11.639: INFO: namespace projected-8924 deletion completed in 6.37826828s

• [SLOW TEST:8.789 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:46:11.639: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Sep  6 22:46:11.723: INFO: Waiting up to 5m0s for pod "client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562" in namespace "containers-4951" to be "success or failure"
Sep  6 22:46:11.732: INFO: Pod "client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562": Phase="Pending", Reason="", readiness=false. Elapsed: 8.356932ms
Sep  6 22:46:13.736: INFO: Pod "client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012551631s
STEP: Saw pod success
Sep  6 22:46:13.736: INFO: Pod "client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562" satisfied condition "success or failure"
Sep  6 22:46:13.740: INFO: Trying to get logs from node metalk8s-24-node1 pod client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562 container test-container: <nil>
STEP: delete the pod
Sep  6 22:46:13.775: INFO: Waiting for pod client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562 to disappear
Sep  6 22:46:13.779: INFO: Pod client-containers-ad4d6c7b-2f28-4427-b305-2c98340b4562 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:46:13.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4951" for this suite.
Sep  6 22:46:19.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:46:20.089: INFO: namespace containers-4951 deletion completed in 6.301758067s

• [SLOW TEST:8.450 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:46:20.090: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-9lrr
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 22:46:20.214: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9lrr" in namespace "subpath-4338" to be "success or failure"
Sep  6 22:46:20.226: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Pending", Reason="", readiness=false. Elapsed: 12.502466ms
Sep  6 22:46:22.231: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.017215369s
Sep  6 22:46:24.236: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 4.022188951s
Sep  6 22:46:26.246: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 6.031997783s
Sep  6 22:46:28.250: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 8.036566814s
Sep  6 22:46:30.264: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 10.050456837s
Sep  6 22:46:32.270: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 12.055642481s
Sep  6 22:46:34.274: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 14.060419812s
Sep  6 22:46:36.280: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 16.065948714s
Sep  6 22:46:38.284: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 18.070453302s
Sep  6 22:46:40.290: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Running", Reason="", readiness=true. Elapsed: 20.075938485s
Sep  6 22:46:42.297: INFO: Pod "pod-subpath-test-secret-9lrr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0833238s
STEP: Saw pod success
Sep  6 22:46:42.297: INFO: Pod "pod-subpath-test-secret-9lrr" satisfied condition "success or failure"
Sep  6 22:46:42.325: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-subpath-test-secret-9lrr container test-container-subpath-secret-9lrr: <nil>
STEP: delete the pod
Sep  6 22:46:42.422: INFO: Waiting for pod pod-subpath-test-secret-9lrr to disappear
Sep  6 22:46:42.450: INFO: Pod pod-subpath-test-secret-9lrr no longer exists
STEP: Deleting pod pod-subpath-test-secret-9lrr
Sep  6 22:46:42.450: INFO: Deleting pod "pod-subpath-test-secret-9lrr" in namespace "subpath-4338"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:46:42.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4338" for this suite.
Sep  6 22:46:48.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:46:48.800: INFO: namespace subpath-4338 deletion completed in 6.321096291s

• [SLOW TEST:28.711 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:46:48.800: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8717
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Sep  6 22:46:48.922: INFO: Found 0 stateful pods, waiting for 3
Sep  6 22:46:58.929: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:46:58.929: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:46:58.929: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 22:46:58.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-8717 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 22:46:59.614: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 22:46:59.614: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 22:46:59.614: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep  6 22:47:09.669: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  6 22:47:19.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-8717 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 22:47:20.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 22:47:20.164: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 22:47:20.164: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 22:47:30.254: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:47:30.254: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:47:30.254: INFO: Waiting for Pod statefulset-8717/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:47:30.254: INFO: Waiting for Pod statefulset-8717/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:47:40.283: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:47:40.283: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:47:40.283: INFO: Waiting for Pod statefulset-8717/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Sep  6 22:47:50.263: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:47:50.263: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Sep  6 22:48:00.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-8717 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 22:48:00.755: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 22:48:00.755: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 22:48:00.755: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 22:48:10.821: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  6 22:48:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-8717 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 22:48:21.265: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 22:48:21.265: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 22:48:21.265: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 22:48:21.290: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:48:21.290: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:21.290: INFO: Waiting for Pod statefulset-8717/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:21.290: INFO: Waiting for Pod statefulset-8717/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:31.301: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:48:31.301: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:31.301: INFO: Waiting for Pod statefulset-8717/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:31.301: INFO: Waiting for Pod statefulset-8717/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:41.313: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:48:41.313: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Sep  6 22:48:51.303: INFO: Waiting for StatefulSet statefulset-8717/ss2 to complete update
Sep  6 22:48:51.303: INFO: Waiting for Pod statefulset-8717/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 22:49:01.300: INFO: Deleting all statefulset in ns statefulset-8717
Sep  6 22:49:01.306: INFO: Scaling statefulset ss2 to 0
Sep  6 22:49:31.373: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 22:49:31.397: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:49:31.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8717" for this suite.
Sep  6 22:49:37.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:49:37.866: INFO: namespace statefulset-8717 deletion completed in 6.324837943s

• [SLOW TEST:169.065 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:49:37.866: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 22:49:39.962: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:49:40.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8143" for this suite.
Sep  6 22:49:46.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:49:46.477: INFO: namespace container-runtime-8143 deletion completed in 6.46444824s

• [SLOW TEST:8.612 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:49:46.478: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 22:49:46.570: INFO: Waiting up to 5m0s for pod "downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331" in namespace "downward-api-2025" to be "success or failure"
Sep  6 22:49:46.585: INFO: Pod "downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331": Phase="Pending", Reason="", readiness=false. Elapsed: 14.975115ms
Sep  6 22:49:48.606: INFO: Pod "downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036324688s
STEP: Saw pod success
Sep  6 22:49:48.606: INFO: Pod "downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331" satisfied condition "success or failure"
Sep  6 22:49:48.609: INFO: Trying to get logs from node metalk8s-24-node1 pod downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331 container dapi-container: <nil>
STEP: delete the pod
Sep  6 22:49:48.646: INFO: Waiting for pod downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331 to disappear
Sep  6 22:49:48.649: INFO: Pod downward-api-9bb34f88-68b6-4cb3-b9c6-720f4a119331 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:49:48.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2025" for this suite.
Sep  6 22:49:54.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:49:54.856: INFO: namespace downward-api-2025 deletion completed in 6.199883935s

• [SLOW TEST:8.378 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:49:54.856: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 22:49:54.900: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:49:58.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-234" for this suite.
Sep  6 22:50:04.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:50:04.367: INFO: namespace init-container-234 deletion completed in 6.295441106s

• [SLOW TEST:9.511 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:50:04.367: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-56sc
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 22:50:04.514: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-56sc" in namespace "subpath-9542" to be "success or failure"
Sep  6 22:50:04.519: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672413ms
Sep  6 22:50:06.522: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008016263s
Sep  6 22:50:08.526: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 4.012035892s
Sep  6 22:50:10.532: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 6.018114965s
Sep  6 22:50:12.536: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 8.022110937s
Sep  6 22:50:14.541: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 10.026185124s
Sep  6 22:50:16.544: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 12.029203227s
Sep  6 22:50:18.549: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 14.03452327s
Sep  6 22:50:20.558: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 16.043382877s
Sep  6 22:50:22.570: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 18.055441286s
Sep  6 22:50:25.228: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Running", Reason="", readiness=true. Elapsed: 20.713341842s
Sep  6 22:50:27.237: INFO: Pod "pod-subpath-test-downwardapi-56sc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.722444354s
STEP: Saw pod success
Sep  6 22:50:27.237: INFO: Pod "pod-subpath-test-downwardapi-56sc" satisfied condition "success or failure"
Sep  6 22:50:27.241: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-subpath-test-downwardapi-56sc container test-container-subpath-downwardapi-56sc: <nil>
STEP: delete the pod
Sep  6 22:50:27.301: INFO: Waiting for pod pod-subpath-test-downwardapi-56sc to disappear
Sep  6 22:50:27.306: INFO: Pod pod-subpath-test-downwardapi-56sc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-56sc
Sep  6 22:50:27.306: INFO: Deleting pod "pod-subpath-test-downwardapi-56sc" in namespace "subpath-9542"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:50:27.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9542" for this suite.
Sep  6 22:50:33.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:50:33.564: INFO: namespace subpath-9542 deletion completed in 6.242382384s

• [SLOW TEST:29.196 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:50:33.564: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:50:33.603: INFO: Creating ReplicaSet my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476
Sep  6 22:50:33.629: INFO: Pod name my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476: Found 0 pods out of 1
Sep  6 22:50:38.647: INFO: Pod name my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476: Found 1 pods out of 1
Sep  6 22:50:38.647: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476" is running
Sep  6 22:50:38.650: INFO: Pod "my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476-j7k7s" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:50:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:50:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:50:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-06 22:50:33 +0000 UTC Reason: Message:}])
Sep  6 22:50:38.650: INFO: Trying to dial the pod
Sep  6 22:50:43.671: INFO: Controller my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476: Got expected result from replica 1 [my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476-j7k7s]: "my-hostname-basic-df122c80-a183-4ed5-9ac4-bfd5c0f30476-j7k7s", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:50:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7229" for this suite.
Sep  6 22:50:49.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:50:49.856: INFO: namespace replicaset-7229 deletion completed in 6.17456592s

• [SLOW TEST:16.293 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:50:49.857: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2e5c4383-2ff7-453d-97b7-aeff47158d21
STEP: Creating a pod to test consume secrets
Sep  6 22:50:50.051: INFO: Waiting up to 5m0s for pod "pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7" in namespace "secrets-7494" to be "success or failure"
Sep  6 22:50:50.089: INFO: Pod "pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 37.209312ms
Sep  6 22:50:52.094: INFO: Pod "pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042713s
STEP: Saw pod success
Sep  6 22:50:52.094: INFO: Pod "pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7" satisfied condition "success or failure"
Sep  6 22:50:52.102: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 22:50:52.131: INFO: Waiting for pod pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7 to disappear
Sep  6 22:50:52.135: INFO: Pod pod-secrets-8b5dd712-3710-463d-af2a-0d921541a2c7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:50:52.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7494" for this suite.
Sep  6 22:50:58.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:50:58.282: INFO: namespace secrets-7494 deletion completed in 6.139237928s

• [SLOW TEST:8.426 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:50:58.282: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Sep  6 22:50:58.853: INFO: created pod pod-service-account-defaultsa
Sep  6 22:50:58.853: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  6 22:50:58.859: INFO: created pod pod-service-account-mountsa
Sep  6 22:50:58.859: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  6 22:50:58.865: INFO: created pod pod-service-account-nomountsa
Sep  6 22:50:58.865: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  6 22:50:58.872: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  6 22:50:58.872: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  6 22:50:58.877: INFO: created pod pod-service-account-mountsa-mountspec
Sep  6 22:50:58.877: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  6 22:50:58.885: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  6 22:50:58.885: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  6 22:50:58.890: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  6 22:50:58.890: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  6 22:50:58.899: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  6 22:50:58.899: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  6 22:50:58.904: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  6 22:50:58.904: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:50:58.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7529" for this suite.
Sep  6 22:51:22.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:51:23.203: INFO: namespace svcaccounts-7529 deletion completed in 24.286816986s

• [SLOW TEST:24.921 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:51:23.203: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3914.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3914.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3914.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3914.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3914.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 22:51:25.448: INFO: DNS probes using dns-3914/dns-test-a38a9b46-7f77-4e41-a5e1-a253228bc219 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:51:25.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3914" for this suite.
Sep  6 22:51:31.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:51:31.858: INFO: namespace dns-3914 deletion completed in 6.320736966s

• [SLOW TEST:8.655 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:51:31.859: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 22:51:31.985: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  6 22:51:31.995: INFO: Number of nodes with available pods: 0
Sep  6 22:51:31.995: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  6 22:51:32.043: INFO: Number of nodes with available pods: 0
Sep  6 22:51:32.044: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:33.049: INFO: Number of nodes with available pods: 0
Sep  6 22:51:33.049: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:34.048: INFO: Number of nodes with available pods: 1
Sep  6 22:51:34.048: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  6 22:51:34.080: INFO: Number of nodes with available pods: 1
Sep  6 22:51:34.080: INFO: Number of running nodes: 0, number of available pods: 1
Sep  6 22:51:35.085: INFO: Number of nodes with available pods: 0
Sep  6 22:51:35.085: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  6 22:51:35.106: INFO: Number of nodes with available pods: 0
Sep  6 22:51:35.107: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:36.113: INFO: Number of nodes with available pods: 0
Sep  6 22:51:36.113: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:37.111: INFO: Number of nodes with available pods: 0
Sep  6 22:51:37.111: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:38.112: INFO: Number of nodes with available pods: 0
Sep  6 22:51:38.112: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:39.116: INFO: Number of nodes with available pods: 0
Sep  6 22:51:39.116: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:40.114: INFO: Number of nodes with available pods: 0
Sep  6 22:51:40.114: INFO: Node metalk8s-24 is running more than one daemon pod
Sep  6 22:51:41.113: INFO: Number of nodes with available pods: 1
Sep  6 22:51:41.113: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2090, will wait for the garbage collector to delete the pods
Sep  6 22:51:41.217: INFO: Deleting DaemonSet.extensions daemon-set took: 21.172635ms
Sep  6 22:51:41.717: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.234647ms
Sep  6 22:51:50.726: INFO: Number of nodes with available pods: 0
Sep  6 22:51:50.726: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 22:51:50.741: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2090/daemonsets","resourceVersion":"65716"},"items":null}

Sep  6 22:51:50.746: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2090/pods","resourceVersion":"65716"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:51:50.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2090" for this suite.
Sep  6 22:51:56.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:51:56.998: INFO: namespace daemonsets-2090 deletion completed in 6.197606809s

• [SLOW TEST:25.139 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:51:56.999: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:51:57.071: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f" in namespace "downward-api-8523" to be "success or failure"
Sep  6 22:51:57.086: INFO: Pod "downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.799532ms
Sep  6 22:51:59.092: INFO: Pod "downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020198224s
STEP: Saw pod success
Sep  6 22:51:59.092: INFO: Pod "downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f" satisfied condition "success or failure"
Sep  6 22:51:59.095: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f container client-container: <nil>
STEP: delete the pod
Sep  6 22:51:59.113: INFO: Waiting for pod downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f to disappear
Sep  6 22:51:59.116: INFO: Pod downwardapi-volume-1c858e08-6ac5-4b09-9d27-d3e6bec6242f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:51:59.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8523" for this suite.
Sep  6 22:52:05.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:52:05.636: INFO: namespace downward-api-8523 deletion completed in 6.515375153s

• [SLOW TEST:8.638 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:52:05.637: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 22:52:05.787: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:52:09.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2585" for this suite.
Sep  6 22:52:33.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:52:33.693: INFO: namespace init-container-2585 deletion completed in 24.227567617s

• [SLOW TEST:28.056 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:52:33.693: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-42f08d94-6602-4c6f-b809-3399e8cf6e50
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:52:33.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9040" for this suite.
Sep  6 22:52:39.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:52:39.984: INFO: namespace secrets-9040 deletion completed in 6.202513794s

• [SLOW TEST:6.291 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:52:39.984: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-18504890-aade-4efb-aa48-afb416868455
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-18504890-aade-4efb-aa48-afb416868455
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:52:44.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6261" for this suite.
Sep  6 22:53:08.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:53:08.462: INFO: namespace projected-6261 deletion completed in 24.264797491s

• [SLOW TEST:28.478 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:53:08.463: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4865
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 22:53:08.561: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 22:53:30.688: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.223.199:8080/dial?request=hostName&protocol=udp&host=10.233.223.198&port=8081&tries=1'] Namespace:pod-network-test-4865 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 22:53:30.688: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 22:53:30.827: INFO: Waiting for endpoints: map[]
Sep  6 22:53:30.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.223.199:8080/dial?request=hostName&protocol=udp&host=10.233.0.19&port=8081&tries=1'] Namespace:pod-network-test-4865 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 22:53:30.831: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 22:53:31.001: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:53:31.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4865" for this suite.
Sep  6 22:53:55.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:53:55.371: INFO: namespace pod-network-test-4865 deletion completed in 24.341165359s

• [SLOW TEST:46.907 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:53:55.371: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:53:59.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5751" for this suite.
Sep  6 22:54:05.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:54:05.870: INFO: namespace kubelet-test-5751 deletion completed in 6.267237851s

• [SLOW TEST:10.499 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:54:05.870: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep  6 22:54:08.047: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-201627442 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep  6 22:54:18.188: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:54:18.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8293" for this suite.
Sep  6 22:54:24.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:54:24.894: INFO: namespace pods-8293 deletion completed in 6.622653353s

• [SLOW TEST:19.024 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:54:24.894: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-3b9b6c21-37d6-4d33-99f5-1f2023aef50a
STEP: Creating a pod to test consume configMaps
Sep  6 22:54:24.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f" in namespace "configmap-5556" to be "success or failure"
Sep  6 22:54:25.033: INFO: Pod "pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.909812ms
Sep  6 22:54:27.038: INFO: Pod "pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.053541718s
STEP: Saw pod success
Sep  6 22:54:27.038: INFO: Pod "pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f" satisfied condition "success or failure"
Sep  6 22:54:27.041: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 22:54:27.069: INFO: Waiting for pod pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f to disappear
Sep  6 22:54:27.073: INFO: Pod pod-configmaps-f7dfffb9-eecf-4099-b20c-b3c5d338ad2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:54:27.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5556" for this suite.
Sep  6 22:54:33.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:54:33.283: INFO: namespace configmap-5556 deletion completed in 6.203754186s

• [SLOW TEST:8.389 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:54:33.284: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Sep  6 22:54:33.355: INFO: Waiting up to 5m0s for pod "client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8" in namespace "containers-2714" to be "success or failure"
Sep  6 22:54:33.370: INFO: Pod "client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.606506ms
Sep  6 22:54:35.389: INFO: Pod "client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033958812s
STEP: Saw pod success
Sep  6 22:54:35.389: INFO: Pod "client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8" satisfied condition "success or failure"
Sep  6 22:54:35.396: INFO: Trying to get logs from node metalk8s-24-node1 pod client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8 container test-container: <nil>
STEP: delete the pod
Sep  6 22:54:35.433: INFO: Waiting for pod client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8 to disappear
Sep  6 22:54:35.450: INFO: Pod client-containers-206ae7f9-23ea-414a-81c4-b6da9fbd07c8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:54:35.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2714" for this suite.
Sep  6 22:54:41.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:54:41.867: INFO: namespace containers-2714 deletion completed in 6.40718114s

• [SLOW TEST:8.584 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:54:41.867: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 22:54:42.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc" in namespace "projected-5706" to be "success or failure"
Sep  6 22:54:42.016: INFO: Pod "downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.455137ms
Sep  6 22:54:44.019: INFO: Pod "downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013550376s
STEP: Saw pod success
Sep  6 22:54:44.019: INFO: Pod "downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc" satisfied condition "success or failure"
Sep  6 22:54:44.024: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc container client-container: <nil>
STEP: delete the pod
Sep  6 22:54:44.053: INFO: Waiting for pod downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc to disappear
Sep  6 22:54:44.058: INFO: Pod downwardapi-volume-d3b32041-dea8-42f3-a315-66bf90b125bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:54:44.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5706" for this suite.
Sep  6 22:54:50.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:54:50.287: INFO: namespace projected-5706 deletion completed in 6.225835804s

• [SLOW TEST:8.420 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:54:50.288: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 22:54:52.994: INFO: Successfully updated pod "annotationupdate060e4809-c47c-4b3b-930a-e89faf9577b5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:54:57.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3777" for this suite.
Sep  6 22:55:21.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:55:21.225: INFO: namespace downward-api-3777 deletion completed in 24.199898439s

• [SLOW TEST:30.937 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:55:21.225: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:56:21.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2652" for this suite.
Sep  6 22:56:43.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:56:43.613: INFO: namespace container-probe-2652 deletion completed in 22.204080752s

• [SLOW TEST:82.388 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:56:43.613: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 22:56:43.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-4689'
Sep  6 22:56:43.777: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep  6 22:56:43.777: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Sep  6 22:56:47.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4689'
Sep  6 22:56:47.895: INFO: stderr: ""
Sep  6 22:56:47.895: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:56:47.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4689" for this suite.
Sep  6 22:56:53.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:56:54.118: INFO: namespace kubectl-4689 deletion completed in 6.209806628s

• [SLOW TEST:10.506 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:56:54.119: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9773, will wait for the garbage collector to delete the pods
Sep  6 22:56:58.291: INFO: Deleting Job.batch foo took: 7.958895ms
Sep  6 22:56:58.391: INFO: Terminating Job.batch foo pods took: 100.22471ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 22:57:34.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9773" for this suite.
Sep  6 22:57:40.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 22:57:40.436: INFO: namespace job-9773 deletion completed in 6.421352843s

• [SLOW TEST:46.318 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 22:57:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  6 22:57:41.536: INFO: Pod name wrapped-volume-race-7a10d420-f0b8-4358-9cf3-dda8ac84f195: Found 0 pods out of 5
Sep  6 22:57:46.547: INFO: Pod name wrapped-volume-race-7a10d420-f0b8-4358-9cf3-dda8ac84f195: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7a10d420-f0b8-4358-9cf3-dda8ac84f195 in namespace emptydir-wrapper-6831, will wait for the garbage collector to delete the pods
Sep  6 22:57:58.866: INFO: Deleting ReplicationController wrapped-volume-race-7a10d420-f0b8-4358-9cf3-dda8ac84f195 took: 23.612838ms
Sep  6 22:57:59.566: INFO: Terminating ReplicationController wrapped-volume-race-7a10d420-f0b8-4358-9cf3-dda8ac84f195 pods took: 700.210378ms
STEP: Creating RC which spawns configmap-volume pods
Sep  6 22:58:41.722: INFO: Pod name wrapped-volume-race-6b51fe39-2bc8-4900-9ee6-2d59f882aeef: Found 0 pods out of 5
Sep  6 22:58:46.733: INFO: Pod name wrapped-volume-race-6b51fe39-2bc8-4900-9ee6-2d59f882aeef: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6b51fe39-2bc8-4900-9ee6-2d59f882aeef in namespace emptydir-wrapper-6831, will wait for the garbage collector to delete the pods
Sep  6 22:58:58.832: INFO: Deleting ReplicationController wrapped-volume-race-6b51fe39-2bc8-4900-9ee6-2d59f882aeef took: 22.076075ms
Sep  6 22:59:01.232: INFO: Terminating ReplicationController wrapped-volume-race-6b51fe39-2bc8-4900-9ee6-2d59f882aeef pods took: 2.400283681s
STEP: Creating RC which spawns configmap-volume pods
Sep  6 22:59:41.784: INFO: Pod name wrapped-volume-race-13ee406f-9348-409a-95f6-c52202b4c83f: Found 0 pods out of 5
Sep  6 22:59:46.795: INFO: Pod name wrapped-volume-race-13ee406f-9348-409a-95f6-c52202b4c83f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-13ee406f-9348-409a-95f6-c52202b4c83f in namespace emptydir-wrapper-6831, will wait for the garbage collector to delete the pods
Sep  6 22:59:58.938: INFO: Deleting ReplicationController wrapped-volume-race-13ee406f-9348-409a-95f6-c52202b4c83f took: 12.019301ms
Sep  6 23:00:01.538: INFO: Terminating ReplicationController wrapped-volume-race-13ee406f-9348-409a-95f6-c52202b4c83f pods took: 2.600319746s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:00:42.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6831" for this suite.
Sep  6 23:00:50.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:00:50.597: INFO: namespace emptydir-wrapper-6831 deletion completed in 8.235099288s

• [SLOW TEST:190.161 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:00:50.598: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4305
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4305
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4305
Sep  6 23:00:50.740: INFO: Found 0 stateful pods, waiting for 1
Sep  6 23:01:00.746: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  6 23:01:00.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 23:01:01.049: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 23:01:01.049: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 23:01:01.049: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 23:01:01.055: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  6 23:01:11.060: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 23:01:11.060: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 23:01:11.099: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999607s
Sep  6 23:01:12.142: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98837136s
Sep  6 23:01:13.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.945414602s
Sep  6 23:01:14.154: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.939502532s
Sep  6 23:01:15.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.933549538s
Sep  6 23:01:16.166: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.926509373s
Sep  6 23:01:17.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.921011278s
Sep  6 23:01:18.185: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.909323389s
Sep  6 23:01:19.193: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.902304883s
Sep  6 23:01:20.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 894.629777ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4305
Sep  6 23:01:21.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 23:01:21.409: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 23:01:21.409: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 23:01:21.409: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 23:01:21.413: INFO: Found 1 stateful pods, waiting for 3
Sep  6 23:01:31.420: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 23:01:31.420: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 23:01:31.420: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  6 23:01:31.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 23:01:31.665: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 23:01:31.665: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 23:01:31.665: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 23:01:31.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 23:01:32.446: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 23:01:32.446: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 23:01:32.446: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 23:01:32.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep  6 23:01:32.771: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep  6 23:01:32.771: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep  6 23:01:32.771: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep  6 23:01:32.771: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 23:01:32.781: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Sep  6 23:01:42.792: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 23:01:42.792: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 23:01:42.792: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  6 23:01:42.813: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999635s
Sep  6 23:01:43.823: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992259588s
Sep  6 23:01:44.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982813613s
Sep  6 23:01:45.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975063214s
Sep  6 23:01:46.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967800885s
Sep  6 23:01:47.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962907299s
Sep  6 23:01:48.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957864628s
Sep  6 23:01:49.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951818063s
Sep  6 23:01:50.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940584997s
Sep  6 23:01:51.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 929.182254ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4305
Sep  6 23:01:52.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 23:01:53.106: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 23:01:53.106: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 23:01:53.106: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 23:01:53.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 23:01:53.495: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 23:01:53.495: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 23:01:53.495: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 23:01:53.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 exec --namespace=statefulset-4305 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep  6 23:01:53.717: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep  6 23:01:53.717: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep  6 23:01:53.717: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep  6 23:01:53.717: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Sep  6 23:02:13.740: INFO: Deleting all statefulset in ns statefulset-4305
Sep  6 23:02:13.745: INFO: Scaling statefulset ss to 0
Sep  6 23:02:13.758: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 23:02:13.762: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:02:13.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4305" for this suite.
Sep  6 23:02:19.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:02:19.974: INFO: namespace statefulset-4305 deletion completed in 6.188628082s

• [SLOW TEST:89.376 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:02:19.974: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 23:02:24.116: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:24.120: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:26.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:26.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:28.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:28.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:30.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:30.134: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:32.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:32.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:34.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:34.125: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:36.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:36.125: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:38.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:38.125: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:40.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:40.127: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:42.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:42.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:44.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:44.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:46.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:46.125: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:48.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:48.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:50.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:50.125: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:52.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:52.124: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 23:02:54.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 23:02:54.124: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:02:54.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9618" for this suite.
Sep  6 23:03:16.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:03:16.318: INFO: namespace container-lifecycle-hook-9618 deletion completed in 22.179962838s

• [SLOW TEST:56.344 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:03:16.318: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep  6 23:03:17.425: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0906 23:03:17.425815      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 23:03:17.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5649" for this suite.
Sep  6 23:03:23.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:03:23.610: INFO: namespace gc-5649 deletion completed in 6.17704311s

• [SLOW TEST:7.292 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:03:23.610: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep  6 23:03:23.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-5228'
Sep  6 23:03:23.862: INFO: stderr: ""
Sep  6 23:03:23.862: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 23:03:24.866: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:03:24.866: INFO: Found 1 / 1
Sep  6 23:03:24.866: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  6 23:03:24.871: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:03:24.871: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 23:03:24.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 patch pod redis-master-lq7bb --namespace=kubectl-5228 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  6 23:03:24.955: INFO: stderr: ""
Sep  6 23:03:24.955: INFO: stdout: "pod/redis-master-lq7bb patched\n"
STEP: checking annotations
Sep  6 23:03:24.961: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:03:24.961: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:03:24.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5228" for this suite.
Sep  6 23:03:48.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:03:49.210: INFO: namespace kubectl-5228 deletion completed in 24.243943892s

• [SLOW TEST:25.600 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:03:49.211: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-1272
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1272 to expose endpoints map[]
Sep  6 23:03:49.371: INFO: Get endpoints failed (16.270775ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep  6 23:03:50.383: INFO: successfully validated that service multi-endpoint-test in namespace services-1272 exposes endpoints map[] (1.02797466s elapsed)
STEP: Creating pod pod1 in namespace services-1272
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1272 to expose endpoints map[pod1:[100]]
Sep  6 23:03:52.439: INFO: successfully validated that service multi-endpoint-test in namespace services-1272 exposes endpoints map[pod1:[100]] (2.036175482s elapsed)
STEP: Creating pod pod2 in namespace services-1272
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1272 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 23:03:55.563: INFO: successfully validated that service multi-endpoint-test in namespace services-1272 exposes endpoints map[pod1:[100] pod2:[101]] (3.113729199s elapsed)
STEP: Deleting pod pod1 in namespace services-1272
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1272 to expose endpoints map[pod2:[101]]
Sep  6 23:03:55.585: INFO: successfully validated that service multi-endpoint-test in namespace services-1272 exposes endpoints map[pod2:[101]] (12.423548ms elapsed)
STEP: Deleting pod pod2 in namespace services-1272
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1272 to expose endpoints map[]
Sep  6 23:03:56.597: INFO: successfully validated that service multi-endpoint-test in namespace services-1272 exposes endpoints map[] (1.007010723s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:03:56.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1272" for this suite.
Sep  6 23:04:02.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:04:02.909: INFO: namespace services-1272 deletion completed in 6.260881055s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:13.698 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:04:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:04:06.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3523" for this suite.
Sep  6 23:04:28.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:04:28.245: INFO: namespace replication-controller-3523 deletion completed in 22.177891129s

• [SLOW TEST:25.337 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:04:28.245: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 23:04:28.306: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7" in namespace "downward-api-3950" to be "success or failure"
Sep  6 23:04:28.313: INFO: Pod "downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.664255ms
Sep  6 23:04:30.325: INFO: Pod "downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019342721s
STEP: Saw pod success
Sep  6 23:04:30.325: INFO: Pod "downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7" satisfied condition "success or failure"
Sep  6 23:04:30.331: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7 container client-container: <nil>
STEP: delete the pod
Sep  6 23:04:30.371: INFO: Waiting for pod downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7 to disappear
Sep  6 23:04:30.396: INFO: Pod downwardapi-volume-8a14257c-2754-47ac-99f3-8677030c24c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:04:30.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3950" for this suite.
Sep  6 23:04:36.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:04:36.619: INFO: namespace downward-api-3950 deletion completed in 6.218215861s

• [SLOW TEST:8.374 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:04:36.619: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 23:04:39.222: INFO: Successfully updated pod "pod-update-05a9cded-959d-4aae-8e8c-e83df2900a84"
STEP: verifying the updated pod is in kubernetes
Sep  6 23:04:39.242: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:04:39.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7308" for this suite.
Sep  6 23:05:03.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:05:03.451: INFO: namespace pods-7308 deletion completed in 24.203179101s

• [SLOW TEST:26.832 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:05:03.451: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-a87b96ad-d2da-4469-bf00-d59d437e8704
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:05:05.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5398" for this suite.
Sep  6 23:05:29.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:05:29.823: INFO: namespace configmap-5398 deletion completed in 24.164297034s

• [SLOW TEST:26.372 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:05:29.823: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  6 23:05:29.883: INFO: Waiting up to 5m0s for pod "pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524" in namespace "emptydir-4420" to be "success or failure"
Sep  6 23:05:29.890: INFO: Pod "pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524": Phase="Pending", Reason="", readiness=false. Elapsed: 7.627405ms
Sep  6 23:05:31.901: INFO: Pod "pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018443992s
STEP: Saw pod success
Sep  6 23:05:31.901: INFO: Pod "pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524" satisfied condition "success or failure"
Sep  6 23:05:31.906: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524 container test-container: <nil>
STEP: delete the pod
Sep  6 23:05:31.935: INFO: Waiting for pod pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524 to disappear
Sep  6 23:05:31.938: INFO: Pod pod-5cfe68f9-b26e-470c-a01f-f5d9d7951524 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:05:31.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4420" for this suite.
Sep  6 23:05:37.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:05:38.138: INFO: namespace emptydir-4420 deletion completed in 6.195164329s

• [SLOW TEST:8.315 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:05:38.139: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 23:05:38.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254" in namespace "downward-api-3485" to be "success or failure"
Sep  6 23:05:38.243: INFO: Pod "downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254": Phase="Pending", Reason="", readiness=false. Elapsed: 9.898067ms
Sep  6 23:05:40.248: INFO: Pod "downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014422356s
STEP: Saw pod success
Sep  6 23:05:40.248: INFO: Pod "downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254" satisfied condition "success or failure"
Sep  6 23:05:40.253: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254 container client-container: <nil>
STEP: delete the pod
Sep  6 23:05:40.318: INFO: Waiting for pod downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254 to disappear
Sep  6 23:05:40.325: INFO: Pod downwardapi-volume-ddca9511-a5ae-46e6-b76c-f3ae8b277254 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:05:40.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3485" for this suite.
Sep  6 23:05:46.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:05:46.549: INFO: namespace downward-api-3485 deletion completed in 6.210322031s

• [SLOW TEST:8.410 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:05:46.549: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Sep  6 23:05:46.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 api-versions'
Sep  6 23:05:46.668: INFO: stderr: ""
Sep  6 23:05:46.668: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nstorage.metalk8s.scality.com/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:05:46.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3196" for this suite.
Sep  6 23:05:52.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:05:52.906: INFO: namespace kubectl-3196 deletion completed in 6.23168395s

• [SLOW TEST:6.357 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:05:52.906: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:05:55.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7873" for this suite.
Sep  6 23:06:35.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:06:35.312: INFO: namespace kubelet-test-7873 deletion completed in 40.214589736s

• [SLOW TEST:42.406 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:06:35.312: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Sep  6 23:06:35.384: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:06:37.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2689" for this suite.
Sep  6 23:06:43.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:06:43.522: INFO: namespace init-container-2689 deletion completed in 6.265559792s

• [SLOW TEST:8.210 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:06:43.523: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  6 23:06:43.630: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:06:53.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-221" for this suite.
Sep  6 23:06:59.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:07:00.185: INFO: namespace pods-221 deletion completed in 6.262156485s

• [SLOW TEST:16.663 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:07:00.186: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 23:07:04.367: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 23:07:04.376: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 23:07:06.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 23:07:06.380: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 23:07:08.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 23:07:08.380: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 23:07:10.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 23:07:10.384: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 23:07:12.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 23:07:12.381: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 23:07:14.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 23:07:14.381: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:07:14.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4722" for this suite.
Sep  6 23:07:36.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:07:36.679: INFO: namespace container-lifecycle-hook-4722 deletion completed in 22.292074299s

• [SLOW TEST:36.494 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:07:36.680: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3280
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 23:07:36.762: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 23:07:56.898: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.223.228:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3280 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:07:56.898: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:07:57.040: INFO: Found all expected endpoints: [netserver-0]
Sep  6 23:07:57.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.0.23:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3280 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:07:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:07:57.173: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:07:57.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3280" for this suite.
Sep  6 23:08:21.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:08:21.360: INFO: namespace pod-network-test-3280 deletion completed in 24.177985523s

• [SLOW TEST:44.680 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:08:21.360: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 23:08:21.433: INFO: Waiting up to 5m0s for pod "downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b" in namespace "downward-api-9026" to be "success or failure"
Sep  6 23:08:21.444: INFO: Pod "downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.340652ms
Sep  6 23:08:23.447: INFO: Pod "downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014173882s
STEP: Saw pod success
Sep  6 23:08:23.448: INFO: Pod "downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b" satisfied condition "success or failure"
Sep  6 23:08:23.450: INFO: Trying to get logs from node metalk8s-24-node1 pod downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b container dapi-container: <nil>
STEP: delete the pod
Sep  6 23:08:23.470: INFO: Waiting for pod downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b to disappear
Sep  6 23:08:23.474: INFO: Pod downward-api-0f792029-dbfe-407d-b465-7ee30ae0ac3b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:08:23.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9026" for this suite.
Sep  6 23:08:29.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:08:29.674: INFO: namespace downward-api-9026 deletion completed in 6.196277843s

• [SLOW TEST:8.314 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:08:29.674: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 23:08:29.758: INFO: Waiting up to 5m0s for pod "pod-a01de63e-0c64-4081-b955-aee796e7290a" in namespace "emptydir-2688" to be "success or failure"
Sep  6 23:08:29.783: INFO: Pod "pod-a01de63e-0c64-4081-b955-aee796e7290a": Phase="Pending", Reason="", readiness=false. Elapsed: 25.2532ms
Sep  6 23:08:31.788: INFO: Pod "pod-a01de63e-0c64-4081-b955-aee796e7290a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030500721s
STEP: Saw pod success
Sep  6 23:08:31.788: INFO: Pod "pod-a01de63e-0c64-4081-b955-aee796e7290a" satisfied condition "success or failure"
Sep  6 23:08:31.795: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-a01de63e-0c64-4081-b955-aee796e7290a container test-container: <nil>
STEP: delete the pod
Sep  6 23:08:31.817: INFO: Waiting for pod pod-a01de63e-0c64-4081-b955-aee796e7290a to disappear
Sep  6 23:08:31.822: INFO: Pod pod-a01de63e-0c64-4081-b955-aee796e7290a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:08:31.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2688" for this suite.
Sep  6 23:08:37.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:08:38.156: INFO: namespace emptydir-2688 deletion completed in 6.329071349s

• [SLOW TEST:8.482 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:08:38.156: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-6ced81a8-443b-4a87-9eb0-832649b49b3c
STEP: Creating a pod to test consume configMaps
Sep  6 23:08:38.249: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46" in namespace "projected-4355" to be "success or failure"
Sep  6 23:08:38.255: INFO: Pod "pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46": Phase="Pending", Reason="", readiness=false. Elapsed: 5.011524ms
Sep  6 23:08:40.264: INFO: Pod "pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014120026s
STEP: Saw pod success
Sep  6 23:08:40.264: INFO: Pod "pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46" satisfied condition "success or failure"
Sep  6 23:08:40.273: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 23:08:40.346: INFO: Waiting for pod pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46 to disappear
Sep  6 23:08:40.370: INFO: Pod pod-projected-configmaps-78f956f6-186b-434b-bb12-f2ea55a54d46 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:08:40.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4355" for this suite.
Sep  6 23:08:46.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:08:46.522: INFO: namespace projected-4355 deletion completed in 6.146661378s

• [SLOW TEST:8.365 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:08:46.522: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7892
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 23:08:46.559: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 23:09:10.650: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.0.43 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7892 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:09:10.650: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:09:11.762: INFO: Found all expected endpoints: [netserver-0]
Sep  6 23:09:11.767: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.223.234 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7892 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:09:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:09:12.901: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:09:12.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7892" for this suite.
Sep  6 23:09:36.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:09:37.029: INFO: namespace pod-network-test-7892 deletion completed in 24.122967011s

• [SLOW TEST:50.507 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:09:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Sep  6 23:09:37.067: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-201627442 proxy --unix-socket=/tmp/kubectl-proxy-unix805420589/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:09:37.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1006" for this suite.
Sep  6 23:09:43.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:09:43.458: INFO: namespace kubectl-1006 deletion completed in 6.325035694s

• [SLOW TEST:6.428 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:09:43.458: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Sep  6 23:09:43.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-285'
Sep  6 23:09:43.652: INFO: stderr: ""
Sep  6 23:09:43.652: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 23:09:43.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-285'
Sep  6 23:09:43.736: INFO: stderr: ""
Sep  6 23:09:43.736: INFO: stdout: "update-demo-nautilus-6l58t update-demo-nautilus-pc4xp "
Sep  6 23:09:43.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-6l58t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:09:43.808: INFO: stderr: ""
Sep  6 23:09:43.808: INFO: stdout: ""
Sep  6 23:09:43.808: INFO: update-demo-nautilus-6l58t is created but not running
Sep  6 23:09:48.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-285'
Sep  6 23:09:48.880: INFO: stderr: ""
Sep  6 23:09:48.880: INFO: stdout: "update-demo-nautilus-6l58t update-demo-nautilus-pc4xp "
Sep  6 23:09:48.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-6l58t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:09:48.953: INFO: stderr: ""
Sep  6 23:09:48.953: INFO: stdout: "true"
Sep  6 23:09:48.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-6l58t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:09:49.029: INFO: stderr: ""
Sep  6 23:09:49.029: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:09:49.029: INFO: validating pod update-demo-nautilus-6l58t
Sep  6 23:09:49.036: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:09:49.036: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:09:49.036: INFO: update-demo-nautilus-6l58t is verified up and running
Sep  6 23:09:49.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-pc4xp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:09:49.117: INFO: stderr: ""
Sep  6 23:09:49.117: INFO: stdout: "true"
Sep  6 23:09:49.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-pc4xp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:09:49.191: INFO: stderr: ""
Sep  6 23:09:49.191: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:09:49.191: INFO: validating pod update-demo-nautilus-pc4xp
Sep  6 23:09:49.199: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:09:49.199: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:09:49.199: INFO: update-demo-nautilus-pc4xp is verified up and running
STEP: rolling-update to new replication controller
Sep  6 23:09:49.201: INFO: scanned /root for discovery docs: <nil>
Sep  6 23:09:49.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-285'
Sep  6 23:10:11.715: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep  6 23:10:11.715: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 23:10:11.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-285'
Sep  6 23:10:11.791: INFO: stderr: ""
Sep  6 23:10:11.791: INFO: stdout: "update-demo-kitten-bc6wp update-demo-kitten-rqj9n update-demo-nautilus-6l58t "
STEP: Replicas for name=update-demo: expected=2 actual=3
Sep  6 23:10:16.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-285'
Sep  6 23:10:17.028: INFO: stderr: ""
Sep  6 23:10:17.028: INFO: stdout: "update-demo-kitten-bc6wp update-demo-kitten-rqj9n "
Sep  6 23:10:17.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-kitten-bc6wp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:10:17.114: INFO: stderr: ""
Sep  6 23:10:17.114: INFO: stdout: "true"
Sep  6 23:10:17.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-kitten-bc6wp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:10:17.195: INFO: stderr: ""
Sep  6 23:10:17.195: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 23:10:17.195: INFO: validating pod update-demo-kitten-bc6wp
Sep  6 23:10:17.202: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 23:10:17.202: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 23:10:17.202: INFO: update-demo-kitten-bc6wp is verified up and running
Sep  6 23:10:17.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-kitten-rqj9n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:10:17.281: INFO: stderr: ""
Sep  6 23:10:17.281: INFO: stdout: "true"
Sep  6 23:10:17.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-kitten-rqj9n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-285'
Sep  6 23:10:17.352: INFO: stderr: ""
Sep  6 23:10:17.353: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep  6 23:10:17.353: INFO: validating pod update-demo-kitten-rqj9n
Sep  6 23:10:17.358: INFO: got data: {
  "image": "kitten.jpg"
}

Sep  6 23:10:17.358: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep  6 23:10:17.358: INFO: update-demo-kitten-rqj9n is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:10:17.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-285" for this suite.
Sep  6 23:10:41.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:10:41.713: INFO: namespace kubectl-285 deletion completed in 24.349495917s

• [SLOW TEST:58.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:10:41.714: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 23:10:41.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7" in namespace "projected-5259" to be "success or failure"
Sep  6 23:10:41.829: INFO: Pod "downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.00413ms
Sep  6 23:10:43.832: INFO: Pod "downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01046122s
STEP: Saw pod success
Sep  6 23:10:43.832: INFO: Pod "downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7" satisfied condition "success or failure"
Sep  6 23:10:43.835: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7 container client-container: <nil>
STEP: delete the pod
Sep  6 23:10:43.863: INFO: Waiting for pod downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7 to disappear
Sep  6 23:10:43.865: INFO: Pod downwardapi-volume-fd474ecf-02e0-46be-b1fd-b874cff8cae7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:10:43.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5259" for this suite.
Sep  6 23:10:49.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:10:50.119: INFO: namespace projected-5259 deletion completed in 6.248059818s

• [SLOW TEST:8.405 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:10:50.119: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  6 23:10:52.721: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1152 pod-service-account-0dcd7716-a4d9-4195-a882-22712565229d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  6 23:10:52.979: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1152 pod-service-account-0dcd7716-a4d9-4195-a882-22712565229d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  6 23:10:53.186: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1152 pod-service-account-0dcd7716-a4d9-4195-a882-22712565229d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:10:53.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1152" for this suite.
Sep  6 23:10:59.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:10:59.607: INFO: namespace svcaccounts-1152 deletion completed in 6.201515109s

• [SLOW TEST:9.487 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:10:59.607: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  6 23:11:10.153: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0906 23:11:10.153314      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep  6 23:11:10.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7841" for this suite.
Sep  6 23:11:18.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:11:18.375: INFO: namespace gc-7841 deletion completed in 8.215319356s

• [SLOW TEST:18.769 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:11:18.376: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Sep  6 23:11:18.429: INFO: namespace kubectl-7738
Sep  6 23:11:18.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-7738'
Sep  6 23:11:18.641: INFO: stderr: ""
Sep  6 23:11:18.641: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 23:11:19.646: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:11:19.646: INFO: Found 0 / 1
Sep  6 23:11:20.646: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:11:20.646: INFO: Found 1 / 1
Sep  6 23:11:20.646: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 23:11:20.650: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:11:20.650: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 23:11:20.650: INFO: wait on redis-master startup in kubectl-7738 
Sep  6 23:11:20.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 logs redis-master-vdd9n redis-master --namespace=kubectl-7738'
Sep  6 23:11:20.741: INFO: stderr: ""
Sep  6 23:11:20.741: INFO: stdout: "1:M 06 Sep 23:11:19.377 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 06 Sep 23:11:19.378 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 06 Sep 23:11:19.378 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 06 Sep 23:11:19.378 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 06 Sep 23:11:19.378 # Server started, Redis version 3.2.12\n1:M 06 Sep 23:11:19.378 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 06 Sep 23:11:19.378 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep  6 23:11:20.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7738'
Sep  6 23:11:20.838: INFO: stderr: ""
Sep  6 23:11:20.838: INFO: stdout: "service/rm2 exposed\n"
Sep  6 23:11:20.842: INFO: Service rm2 in namespace kubectl-7738 found.
STEP: exposing service
Sep  6 23:11:22.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7738'
Sep  6 23:11:22.982: INFO: stderr: ""
Sep  6 23:11:22.982: INFO: stdout: "service/rm3 exposed\n"
Sep  6 23:11:22.993: INFO: Service rm3 in namespace kubectl-7738 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:11:25.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7738" for this suite.
Sep  6 23:11:49.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:11:49.213: INFO: namespace kubectl-7738 deletion completed in 24.198038831s

• [SLOW TEST:30.837 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:11:49.213: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep  6 23:11:49.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-920'
Sep  6 23:11:49.392: INFO: stderr: ""
Sep  6 23:11:49.392: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep  6 23:11:54.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pod e2e-test-nginx-pod --namespace=kubectl-920 -o json'
Sep  6 23:11:54.521: INFO: stderr: ""
Sep  6 23:11:54.521: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.233.223.249/32\"\n        },\n        \"creationTimestamp\": \"2019-09-06T23:11:49Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-920\",\n        \"resourceVersion\": \"71228\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-920/pods/e2e-test-nginx-pod\",\n        \"uid\": \"65984027-7e30-43c7-a229-2b2d323723f1\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-v5n2v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"metalk8s-24-node1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-v5n2v\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-v5n2v\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T23:11:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T23:11:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T23:11:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-06T23:11:49Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://3cf67905300b3d7b67126cf884b79c2199e1fa41740ff547e285be583f3249d0\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-06T23:11:50Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.0.45\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.223.249\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-06T23:11:49Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  6 23:11:54.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 replace -f - --namespace=kubectl-920'
Sep  6 23:11:54.688: INFO: stderr: ""
Sep  6 23:11:54.688: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Sep  6 23:11:54.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete pods e2e-test-nginx-pod --namespace=kubectl-920'
Sep  6 23:12:03.923: INFO: stderr: ""
Sep  6 23:12:03.923: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:12:03.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-920" for this suite.
Sep  6 23:12:09.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:12:10.238: INFO: namespace kubectl-920 deletion completed in 6.305562336s

• [SLOW TEST:21.025 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:12:10.239: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 23:12:12.989: INFO: Successfully updated pod "pod-update-activedeadlineseconds-0a93d9e5-83b1-4b98-a85c-3c3929484601"
Sep  6 23:12:12.989: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-0a93d9e5-83b1-4b98-a85c-3c3929484601" in namespace "pods-9454" to be "terminated due to deadline exceeded"
Sep  6 23:12:12.996: INFO: Pod "pod-update-activedeadlineseconds-0a93d9e5-83b1-4b98-a85c-3c3929484601": Phase="Running", Reason="", readiness=true. Elapsed: 7.475404ms
Sep  6 23:12:15.002: INFO: Pod "pod-update-activedeadlineseconds-0a93d9e5-83b1-4b98-a85c-3c3929484601": Phase="Running", Reason="", readiness=true. Elapsed: 2.012832756s
Sep  6 23:12:17.013: INFO: Pod "pod-update-activedeadlineseconds-0a93d9e5-83b1-4b98-a85c-3c3929484601": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.024059023s
Sep  6 23:12:17.013: INFO: Pod "pod-update-activedeadlineseconds-0a93d9e5-83b1-4b98-a85c-3c3929484601" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:12:17.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9454" for this suite.
Sep  6 23:12:23.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:12:23.216: INFO: namespace pods-9454 deletion completed in 6.197912784s

• [SLOW TEST:12.978 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:12:23.217: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 23:12:26.388: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:12:26.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1525" for this suite.
Sep  6 23:12:48.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:12:48.671: INFO: namespace replicaset-1525 deletion completed in 22.235621656s

• [SLOW TEST:25.455 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:12:48.672: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 23:12:48.781: INFO: Creating deployment "test-recreate-deployment"
Sep  6 23:12:48.792: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  6 23:12:48.828: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  6 23:12:50.837: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  6 23:12:50.840: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  6 23:12:50.849: INFO: Updating deployment test-recreate-deployment
Sep  6 23:12:50.849: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 23:12:50.921: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-782,SelfLink:/apis/apps/v1/namespaces/deployment-782/deployments/test-recreate-deployment,UID:5db65c5f-6762-4dfe-93be-4aff7d49f901,ResourceVersion:71509,Generation:2,CreationTimestamp:2019-09-06 23:12:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-06 23:12:50 +0000 UTC 2019-09-06 23:12:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-06 23:12:50 +0000 UTC 2019-09-06 23:12:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep  6 23:12:50.932: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-782,SelfLink:/apis/apps/v1/namespaces/deployment-782/replicasets/test-recreate-deployment-5c8c9cc69d,UID:48efdeed-3fd6-455d-abfb-d1c6a2164b87,ResourceVersion:71507,Generation:1,CreationTimestamp:2019-09-06 23:12:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5db65c5f-6762-4dfe-93be-4aff7d49f901 0xc002879e77 0xc002879e78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 23:12:50.932: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  6 23:12:50.932: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-782,SelfLink:/apis/apps/v1/namespaces/deployment-782/replicasets/test-recreate-deployment-6df85df6b9,UID:f419e74d-c45c-4baa-ad18-01b61ada8259,ResourceVersion:71498,Generation:2,CreationTimestamp:2019-09-06 23:12:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5db65c5f-6762-4dfe-93be-4aff7d49f901 0xc002879f47 0xc002879f48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep  6 23:12:50.936: INFO: Pod "test-recreate-deployment-5c8c9cc69d-kgslp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-kgslp,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-782,SelfLink:/api/v1/namespaces/deployment-782/pods/test-recreate-deployment-5c8c9cc69d-kgslp,UID:a9c6218f-7099-41dd-9195-8ab249470887,ResourceVersion:71508,Generation:0,CreationTimestamp:2019-09-06 23:12:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 48efdeed-3fd6-455d-abfb-d1c6a2164b87 0xc001f33237 0xc001f33238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qggbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qggbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qggbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001f332a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001f332c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:12:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:12:50 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:,StartTime:2019-09-06 23:12:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:12:50.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-782" for this suite.
Sep  6 23:12:56.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:12:57.140: INFO: namespace deployment-782 deletion completed in 6.198259928s

• [SLOW TEST:8.468 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:12:57.140: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 23:12:57.250: INFO: Waiting up to 5m0s for pod "pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff" in namespace "emptydir-4496" to be "success or failure"
Sep  6 23:12:57.268: INFO: Pod "pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff": Phase="Pending", Reason="", readiness=false. Elapsed: 18.927531ms
Sep  6 23:12:59.272: INFO: Pod "pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02281988s
STEP: Saw pod success
Sep  6 23:12:59.272: INFO: Pod "pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff" satisfied condition "success or failure"
Sep  6 23:12:59.276: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff container test-container: <nil>
STEP: delete the pod
Sep  6 23:12:59.316: INFO: Waiting for pod pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff to disappear
Sep  6 23:12:59.324: INFO: Pod pod-219f574e-5eb4-4415-b120-2a2fdd8ae4ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:12:59.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4496" for this suite.
Sep  6 23:13:05.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:13:05.706: INFO: namespace emptydir-4496 deletion completed in 6.376723484s

• [SLOW TEST:8.566 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:13:05.706: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Sep  6 23:13:08.317: INFO: Successfully updated pod "labelsupdate089ccbc7-2a0d-47ec-90d3-d4f782c1f46f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:13:10.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2486" for this suite.
Sep  6 23:13:32.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:13:32.620: INFO: namespace downward-api-2486 deletion completed in 22.247587443s

• [SLOW TEST:26.914 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:13:32.620: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2ba5b617-de53-4c22-8b05-472374b4819a
STEP: Creating a pod to test consume secrets
Sep  6 23:13:32.703: INFO: Waiting up to 5m0s for pod "pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004" in namespace "secrets-587" to be "success or failure"
Sep  6 23:13:32.715: INFO: Pod "pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004": Phase="Pending", Reason="", readiness=false. Elapsed: 12.59487ms
Sep  6 23:13:34.720: INFO: Pod "pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017411152s
STEP: Saw pod success
Sep  6 23:13:34.720: INFO: Pod "pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004" satisfied condition "success or failure"
Sep  6 23:13:34.725: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 23:13:34.757: INFO: Waiting for pod pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004 to disappear
Sep  6 23:13:34.761: INFO: Pod pod-secrets-ab633d5f-2451-4352-9f03-1fbd14309004 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:13:34.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-587" for this suite.
Sep  6 23:13:40.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:13:41.053: INFO: namespace secrets-587 deletion completed in 6.285809717s

• [SLOW TEST:8.433 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:13:41.053: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-366.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-366.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-366.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 23:13:43.193: INFO: File wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local from pod  dns-366/dns-test-42aca5ae-5601-41be-8b1a-281db77e0b31 contains '' instead of 'foo.example.com.'
Sep  6 23:13:43.229: INFO: Lookups using dns-366/dns-test-42aca5ae-5601-41be-8b1a-281db77e0b31 failed for: [wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local]

Sep  6 23:13:48.241: INFO: File wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local from pod  dns-366/dns-test-42aca5ae-5601-41be-8b1a-281db77e0b31 contains '' instead of 'foo.example.com.'
Sep  6 23:13:48.253: INFO: Lookups using dns-366/dns-test-42aca5ae-5601-41be-8b1a-281db77e0b31 failed for: [wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local]

Sep  6 23:13:53.238: INFO: DNS probes using dns-test-42aca5ae-5601-41be-8b1a-281db77e0b31 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-366.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-366.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-366.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 23:13:55.304: INFO: File wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local from pod  dns-366/dns-test-2f0074df-4c4e-4766-93ef-8e3dc61dda60 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  6 23:13:55.314: INFO: File jessie_udp@dns-test-service-3.dns-366.svc.cluster.local from pod  dns-366/dns-test-2f0074df-4c4e-4766-93ef-8e3dc61dda60 contains '' instead of 'bar.example.com.'
Sep  6 23:13:55.314: INFO: Lookups using dns-366/dns-test-2f0074df-4c4e-4766-93ef-8e3dc61dda60 failed for: [wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local jessie_udp@dns-test-service-3.dns-366.svc.cluster.local]

Sep  6 23:14:00.336: INFO: DNS probes using dns-test-2f0074df-4c4e-4766-93ef-8e3dc61dda60 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-366.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-366.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-366.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-366.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 23:14:02.461: INFO: DNS probes using dns-test-e82c6d11-9258-4c5e-b628-30545db43249 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:14:02.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-366" for this suite.
Sep  6 23:14:08.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:14:08.793: INFO: namespace dns-366 deletion completed in 6.245497631s

• [SLOW TEST:27.740 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:14:08.793: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4417
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4417
STEP: Deleting pre-stop pod
Sep  6 23:14:21.964: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:14:21.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4417" for this suite.
Sep  6 23:15:01.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:15:02.176: INFO: namespace prestop-4417 deletion completed in 40.193019782s

• [SLOW TEST:53.383 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:15:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8319/configmap-test-0e879877-9e8e-4968-bae7-941efd55cbae
STEP: Creating a pod to test consume configMaps
Sep  6 23:15:02.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35" in namespace "configmap-8319" to be "success or failure"
Sep  6 23:15:06.808: INFO: Pod "pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.538022085s
Sep  6 23:15:08.853: INFO: Pod "pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.582079178s
STEP: Saw pod success
Sep  6 23:15:08.853: INFO: Pod "pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35" satisfied condition "success or failure"
Sep  6 23:15:08.873: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35 container env-test: <nil>
STEP: delete the pod
Sep  6 23:15:08.963: INFO: Waiting for pod pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35 to disappear
Sep  6 23:15:08.970: INFO: Pod pod-configmaps-56ec09d8-aeb3-4a12-a024-2c37c301bb35 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:15:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8319" for this suite.
Sep  6 23:15:15.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:15:15.292: INFO: namespace configmap-8319 deletion completed in 6.314019588s

• [SLOW TEST:13.116 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:15:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-41110d42-adc8-427b-a130-e60576de9899
STEP: Creating secret with name s-test-opt-upd-82cc4950-3881-49b0-bf35-521bb2564919
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-41110d42-adc8-427b-a130-e60576de9899
STEP: Updating secret s-test-opt-upd-82cc4950-3881-49b0-bf35-521bb2564919
STEP: Creating secret with name s-test-opt-create-329cb658-aadd-45d6-b73c-b0e3ddd3c249
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:15:19.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1257" for this suite.
Sep  6 23:15:42.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:15:42.945: INFO: namespace secrets-1257 deletion completed in 23.408745698s

• [SLOW TEST:27.651 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:15:42.945: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Sep  6 23:15:43.058: INFO: Waiting up to 5m0s for pod "downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4" in namespace "downward-api-2771" to be "success or failure"
Sep  6 23:15:43.066: INFO: Pod "downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.211247ms
Sep  6 23:15:45.132: INFO: Pod "downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073354351s
STEP: Saw pod success
Sep  6 23:15:45.132: INFO: Pod "downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4" satisfied condition "success or failure"
Sep  6 23:15:45.170: INFO: Trying to get logs from node metalk8s-24-node1 pod downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4 container dapi-container: <nil>
STEP: delete the pod
Sep  6 23:15:45.214: INFO: Waiting for pod downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4 to disappear
Sep  6 23:15:45.258: INFO: Pod downward-api-354eef6a-362c-4d5f-af3f-2e10ff151fa4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:15:45.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2771" for this suite.
Sep  6 23:15:51.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:15:51.485: INFO: namespace downward-api-2771 deletion completed in 6.179416914s

• [SLOW TEST:8.540 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:15:51.485: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-92e6c4b5-af74-4ac4-bad0-d4e2390029cc in namespace container-probe-7096
Sep  6 23:15:53.590: INFO: Started pod liveness-92e6c4b5-af74-4ac4-bad0-d4e2390029cc in namespace container-probe-7096
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 23:15:53.593: INFO: Initial restart count of pod liveness-92e6c4b5-af74-4ac4-bad0-d4e2390029cc is 0
Sep  6 23:16:11.645: INFO: Restart count of pod container-probe-7096/liveness-92e6c4b5-af74-4ac4-bad0-d4e2390029cc is now 1 (18.051730919s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:16:11.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7096" for this suite.
Sep  6 23:16:17.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:16:17.873: INFO: namespace container-probe-7096 deletion completed in 6.189138996s

• [SLOW TEST:26.388 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:16:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-84cab715-484a-4c17-b9de-1115693f46de
STEP: Creating a pod to test consume configMaps
Sep  6 23:16:17.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2" in namespace "configmap-1478" to be "success or failure"
Sep  6 23:16:17.946: INFO: Pod "pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589223ms
Sep  6 23:16:19.950: INFO: Pod "pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00945541s
STEP: Saw pod success
Sep  6 23:16:19.950: INFO: Pod "pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2" satisfied condition "success or failure"
Sep  6 23:16:19.955: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 23:16:19.984: INFO: Waiting for pod pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2 to disappear
Sep  6 23:16:19.992: INFO: Pod pod-configmaps-82db0210-fc32-4cec-b36a-379426e1f5c2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:16:19.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1478" for this suite.
Sep  6 23:16:26.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:16:26.367: INFO: namespace configmap-1478 deletion completed in 6.367600184s

• [SLOW TEST:8.494 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:16:26.368: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8d90f986-9a28-4084-af9e-c07191e9f5a2
STEP: Creating a pod to test consume configMaps
Sep  6 23:16:26.500: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838" in namespace "projected-8270" to be "success or failure"
Sep  6 23:16:26.505: INFO: Pod "pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810742ms
Sep  6 23:16:28.513: INFO: Pod "pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01339764s
STEP: Saw pod success
Sep  6 23:16:28.513: INFO: Pod "pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838" satisfied condition "success or failure"
Sep  6 23:16:28.517: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 23:16:28.604: INFO: Waiting for pod pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838 to disappear
Sep  6 23:16:28.610: INFO: Pod pod-projected-configmaps-d0466b00-76c8-4aea-87ea-7a0ce3f2c838 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:16:28.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8270" for this suite.
Sep  6 23:16:34.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:16:34.777: INFO: namespace projected-8270 deletion completed in 6.151213123s

• [SLOW TEST:8.409 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:16:34.777: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-281b12bd-dfa6-4ed2-9e4b-800ef487256e
STEP: Creating a pod to test consume configMaps
Sep  6 23:16:34.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc" in namespace "configmap-881" to be "success or failure"
Sep  6 23:16:34.902: INFO: Pod "pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.000125ms
Sep  6 23:16:36.906: INFO: Pod "pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009699865s
STEP: Saw pod success
Sep  6 23:16:36.906: INFO: Pod "pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc" satisfied condition "success or failure"
Sep  6 23:16:36.909: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 23:16:36.927: INFO: Waiting for pod pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc to disappear
Sep  6 23:16:36.931: INFO: Pod pod-configmaps-af026bcb-db00-4da2-a71d-e5a88138cbcc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:16:36.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-881" for this suite.
Sep  6 23:16:42.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:16:43.157: INFO: namespace configmap-881 deletion completed in 6.22090371s

• [SLOW TEST:8.380 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:16:43.157: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 23:16:43.237: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  6 23:16:48.243: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 23:16:48.243: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Sep  6 23:16:51.981: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-176,SelfLink:/apis/apps/v1/namespaces/deployment-176/deployments/test-cleanup-deployment,UID:70103264-ea77-4652-ba15-b65bd1a03ab5,ResourceVersion:72591,Generation:1,CreationTimestamp:2019-09-06 23:16:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-06 23:16:48 +0000 UTC 2019-09-06 23:16:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-06 23:16:49 +0000 UTC 2019-09-06 23:16:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 23:16:51.989: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-176,SelfLink:/apis/apps/v1/namespaces/deployment-176/replicasets/test-cleanup-deployment-55bbcbc84c,UID:1cc2514c-e933-4232-bd7e-3f024d2c356f,ResourceVersion:72580,Generation:1,CreationTimestamp:2019-09-06 23:16:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 70103264-ea77-4652-ba15-b65bd1a03ab5 0xc003961817 0xc003961818}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep  6 23:16:51.995: INFO: Pod "test-cleanup-deployment-55bbcbc84c-24r9k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-24r9k,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-176,SelfLink:/api/v1/namespaces/deployment-176/pods/test-cleanup-deployment-55bbcbc84c-24r9k,UID:9eaa211f-a8ff-4294-b677-16e5fd0c48ed,ResourceVersion:72579,Generation:0,CreationTimestamp:2019-09-06 23:16:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.233.223.211/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 1cc2514c-e933-4232-bd7e-3f024d2c356f 0xc00329d577 0xc00329d578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-slvrc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-slvrc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-slvrc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:metalk8s-24-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00329d5e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00329d600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:16:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:16:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:16:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-06 23:16:48 +0000 UTC  }],Message:,Reason:,HostIP:10.10.0.45,PodIP:10.233.223.211,StartTime:2019-09-06 23:16:48 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-06 23:16:49 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://14965aa93c28019f077afae2d2875ad8825be074a38ce57c74ed5beb12861b09}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:16:51.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-176" for this suite.
Sep  6 23:16:58.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:16:58.161: INFO: namespace deployment-176 deletion completed in 6.159670143s

• [SLOW TEST:15.005 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:16:58.163: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 23:16:58.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c" in namespace "downward-api-710" to be "success or failure"
Sep  6 23:16:58.242: INFO: Pod "downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.305613ms
Sep  6 23:17:00.249: INFO: Pod "downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021492529s
STEP: Saw pod success
Sep  6 23:17:00.249: INFO: Pod "downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c" satisfied condition "success or failure"
Sep  6 23:17:00.256: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c container client-container: <nil>
STEP: delete the pod
Sep  6 23:17:00.288: INFO: Waiting for pod downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c to disappear
Sep  6 23:17:00.296: INFO: Pod downwardapi-volume-667e08bf-6e62-4af7-a769-1d68ece2fd4c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:17:00.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-710" for this suite.
Sep  6 23:17:06.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:17:06.704: INFO: namespace downward-api-710 deletion completed in 6.396298757s

• [SLOW TEST:8.541 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:17:06.704: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Sep  6 23:17:06.808: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-474" to be "success or failure"
Sep  6 23:17:06.816: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.99586ms
Sep  6 23:17:08.824: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015434211s
STEP: Saw pod success
Sep  6 23:17:08.824: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep  6 23:17:08.828: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep  6 23:17:08.879: INFO: Waiting for pod pod-host-path-test to disappear
Sep  6 23:17:08.902: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:17:08.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-474" for this suite.
Sep  6 23:17:14.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:17:15.160: INFO: namespace hostpath-474 deletion completed in 6.250576946s

• [SLOW TEST:8.456 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:17:15.160: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Sep  6 23:17:15.292: INFO: Waiting up to 5m0s for pod "client-containers-64875efe-2582-4232-abcb-d6df3edb65da" in namespace "containers-4116" to be "success or failure"
Sep  6 23:17:15.301: INFO: Pod "client-containers-64875efe-2582-4232-abcb-d6df3edb65da": Phase="Pending", Reason="", readiness=false. Elapsed: 8.769817ms
Sep  6 23:17:17.306: INFO: Pod "client-containers-64875efe-2582-4232-abcb-d6df3edb65da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013652761s
STEP: Saw pod success
Sep  6 23:17:17.306: INFO: Pod "client-containers-64875efe-2582-4232-abcb-d6df3edb65da" satisfied condition "success or failure"
Sep  6 23:17:17.310: INFO: Trying to get logs from node metalk8s-24-node1 pod client-containers-64875efe-2582-4232-abcb-d6df3edb65da container test-container: <nil>
STEP: delete the pod
Sep  6 23:17:17.356: INFO: Waiting for pod client-containers-64875efe-2582-4232-abcb-d6df3edb65da to disappear
Sep  6 23:17:17.371: INFO: Pod client-containers-64875efe-2582-4232-abcb-d6df3edb65da no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:17:17.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4116" for this suite.
Sep  6 23:17:23.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:17:23.584: INFO: namespace containers-4116 deletion completed in 6.193829528s

• [SLOW TEST:8.424 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:17:23.584: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Sep  6 23:17:23.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-9838'
Sep  6 23:17:23.842: INFO: stderr: ""
Sep  6 23:17:23.842: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep  6 23:17:23.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-9838'
Sep  6 23:17:23.987: INFO: stderr: ""
Sep  6 23:17:23.987: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep  6 23:17:24.994: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:17:24.994: INFO: Found 0 / 1
Sep  6 23:17:25.992: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:17:25.992: INFO: Found 1 / 1
Sep  6 23:17:25.992: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 23:17:25.996: INFO: Selector matched 1 pods for map[app:redis]
Sep  6 23:17:25.996: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 23:17:25.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 describe pod redis-master-5gfzz --namespace=kubectl-9838'
Sep  6 23:17:26.100: INFO: stderr: ""
Sep  6 23:17:26.100: INFO: stdout: "Name:           redis-master-5gfzz\nNamespace:      kubectl-9838\nPriority:       0\nNode:           metalk8s-24-node1/10.10.0.45\nStart Time:     Fri, 06 Sep 2019 23:17:23 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.233.223.208/32\nStatus:         Running\nIP:             10.233.223.208\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://cbe49547dc07ceeab58036461a0e1e90e62d19e78a861442b2e4a8c8b0fe3744\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 06 Sep 2019 23:17:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mxppm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mxppm:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mxppm\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                        Message\n  ----    ------     ----  ----                        -------\n  Normal  Scheduled  3s    default-scheduler           Successfully assigned kubectl-9838/redis-master-5gfzz to metalk8s-24-node1\n  Normal  Pulled     2s    kubelet, metalk8s-24-node1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, metalk8s-24-node1  Created container redis-master\n  Normal  Started    2s    kubelet, metalk8s-24-node1  Started container redis-master\n"
Sep  6 23:17:26.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 describe rc redis-master --namespace=kubectl-9838'
Sep  6 23:17:26.211: INFO: stderr: ""
Sep  6 23:17:26.211: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9838\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-5gfzz\n"
Sep  6 23:17:26.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 describe service redis-master --namespace=kubectl-9838'
Sep  6 23:17:26.308: INFO: stderr: ""
Sep  6 23:17:26.308: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9838\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.107.102.28\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.223.208:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  6 23:17:26.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 describe node metalk8s-24'
Sep  6 23:17:26.466: INFO: stderr: ""
Sep  6 23:17:26.466: INFO: stdout: "Name:               metalk8s-24\nRoles:              bootstrap,etc,infra,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=metalk8s-24\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/bootstrap=\n                    node-role.kubernetes.io/etc=\n                    node-role.kubernetes.io/infra=\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.20.0.5/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.233.0.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 06 Sep 2019 16:45:00 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 06 Sep 2019 16:45:14 +0000   Fri, 06 Sep 2019 16:45:14 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 06 Sep 2019 23:16:27 +0000   Fri, 06 Sep 2019 16:45:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 06 Sep 2019 23:16:27 +0000   Fri, 06 Sep 2019 16:45:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 06 Sep 2019 23:16:27 +0000   Fri, 06 Sep 2019 16:45:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 06 Sep 2019 23:16:27 +0000   Fri, 06 Sep 2019 16:45:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.0.10\n  Hostname:    metalk8s-24\nCapacity:\n cpu:                8\n ephemeral-storage:  41931756Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16266524Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  38644306266\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16164124Ki\n pods:               110\nSystem Info:\n Machine ID:                 b30d0f2110ac3807b210c19ede3ce88f\n System UUID:                9C53FEEB-D916-431D-96A0-4CFA766CD67E\n Boot ID:                    0c06ba4a-324d-4fce-90c6-d7627b67e51f\n Kernel Version:             3.10.0-862.3.2.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.4\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     10.233.0.0/24\nNon-terminated Pods:         (26 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-k452p    0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                calico-kube-controllers-696f846f6b-lv8ss                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h36m\n  kube-system                calico-node-wpmfl                                          250m (3%)     0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                coredns-7df84d55f8-2gxk7                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     6h36m\n  kube-system                coredns-7df84d55f8-gsmw8                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     6h36m\n  kube-system                etcd-metalk8s-24                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                kube-apiserver-metalk8s-24                                 250m (3%)     0 (0%)      0 (0%)           0 (0%)         6h23m\n  kube-system                kube-controller-manager-metalk8s-24                        200m (2%)     0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                kube-proxy-ftwr7                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                kube-scheduler-metalk8s-24                                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                repositories-metalk8s-24                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                salt-master-metalk8s-24                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h32m\n  kube-system                storage-operator-6fc69d48bd-kvwr9                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h35m\n  metalk8s-ingress           nginx-ingress-controller-v6vmq                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h32m\n  metalk8s-ingress           nginx-ingress-default-backend-657d8c587c-wqzq8             0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h35m\n  metalk8s-monitoring        alertmanager-main-0                                        50m (0%)      50m (0%)    210Mi (1%)       10Mi (0%)      6h27m\n  metalk8s-monitoring        alertmanager-main-1                                        50m (0%)      50m (0%)    210Mi (1%)       10Mi (0%)      6h27m\n  metalk8s-monitoring        alertmanager-main-2                                        50m (0%)      50m (0%)    210Mi (1%)       10Mi (0%)      6h27m\n  metalk8s-monitoring        grafana-5ff47b469d-w68rc                                   100m (1%)     200m (2%)   100Mi (0%)       200Mi (1%)     6h35m\n  metalk8s-monitoring        kube-state-metrics-8d6c9b57-mgpd6                          132m (1%)     192m (2%)   250Mi (1%)       290Mi (1%)     114m\n  metalk8s-monitoring        node-exporter-qsfqz                                        112m (1%)     270m (3%)   200Mi (1%)       220Mi (1%)     6h32m\n  metalk8s-monitoring        prometheus-adapter-764ffd477c-f57gq                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h35m\n  metalk8s-monitoring        prometheus-k8s-0                                           75m (0%)      75m (0%)    460Mi (2%)       60Mi (0%)      6h27m\n  metalk8s-monitoring        prometheus-k8s-1                                           75m (0%)      75m (0%)    460Mi (2%)       60Mi (0%)      6h27m\n  metalk8s-monitoring        prometheus-operator-86bbccc5c5-bpgr7                       100m (1%)     200m (2%)   100Mi (0%)       200Mi (1%)     6h36m\n  metalk8s-ui                metalk8s-ui-94545c497-vnq2q                                100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     6h22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1844m (23%)   1162m (14%)\n  memory             2410Mi (15%)  1570Mi (9%)\n  ephemeral-storage  0 (0%)        0 (0%)\nEvents:              <none>\n"
Sep  6 23:17:26.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 describe namespace kubectl-9838'
Sep  6 23:17:26.571: INFO: stderr: ""
Sep  6 23:17:26.571: INFO: stdout: "Name:         kubectl-9838\nLabels:       e2e-framework=kubectl\n              e2e-run=09c4b461-9c01-4c36-a40a-11c849b0d1f3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:17:26.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9838" for this suite.
Sep  6 23:17:48.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:17:48.765: INFO: namespace kubectl-9838 deletion completed in 22.187816899s

• [SLOW TEST:25.181 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:17:48.766: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-b1463343-061f-4c5f-92ed-68e6cf0d22b2
STEP: Creating a pod to test consume configMaps
Sep  6 23:17:48.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9" in namespace "configmap-6689" to be "success or failure"
Sep  6 23:17:48.926: INFO: Pod "pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223975ms
Sep  6 23:17:50.929: INFO: Pod "pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009422528s
STEP: Saw pod success
Sep  6 23:17:50.929: INFO: Pod "pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9" satisfied condition "success or failure"
Sep  6 23:17:50.932: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 23:17:50.947: INFO: Waiting for pod pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9 to disappear
Sep  6 23:17:50.950: INFO: Pod pod-configmaps-d8e9617b-1c3a-4d06-a60b-9b49f578f9c9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:17:50.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6689" for this suite.
Sep  6 23:17:56.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:17:57.119: INFO: namespace configmap-6689 deletion completed in 6.165997462s

• [SLOW TEST:8.354 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:17:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Sep  6 23:17:57.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 cluster-info'
Sep  6 23:17:57.255: INFO: stderr: ""
Sep  6 23:17:57.255: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:17:57.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5052" for this suite.
Sep  6 23:18:03.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:18:03.441: INFO: namespace kubectl-5052 deletion completed in 6.180901958s

• [SLOW TEST:6.322 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:18:03.441: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:18:28.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3784" for this suite.
Sep  6 23:18:34.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:18:34.905: INFO: namespace namespaces-3784 deletion completed in 6.222635395s
STEP: Destroying namespace "nsdeletetest-6569" for this suite.
Sep  6 23:18:34.908: INFO: Namespace nsdeletetest-6569 was already deleted
STEP: Destroying namespace "nsdeletetest-795" for this suite.
Sep  6 23:18:40.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:18:41.238: INFO: namespace nsdeletetest-795 deletion completed in 6.329778352s

• [SLOW TEST:37.796 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:18:41.238: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  6 23:18:41.399: INFO: Waiting up to 5m0s for pod "pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb" in namespace "emptydir-9635" to be "success or failure"
Sep  6 23:18:41.406: INFO: Pod "pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.482904ms
Sep  6 23:18:43.411: INFO: Pod "pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011893163s
STEP: Saw pod success
Sep  6 23:18:43.411: INFO: Pod "pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb" satisfied condition "success or failure"
Sep  6 23:18:43.416: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb container test-container: <nil>
STEP: delete the pod
Sep  6 23:18:43.461: INFO: Waiting for pod pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb to disappear
Sep  6 23:18:43.463: INFO: Pod pod-0a33b519-4cb8-40cf-9bb4-965d9b60f6fb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:18:43.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9635" for this suite.
Sep  6 23:18:49.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:18:49.663: INFO: namespace emptydir-9635 deletion completed in 6.189218144s

• [SLOW TEST:8.425 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:18:49.663: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  6 23:18:49.727: INFO: Waiting up to 5m0s for pod "pod-d87c7448-0a7c-495c-8b61-177de9def7e1" in namespace "emptydir-4795" to be "success or failure"
Sep  6 23:18:49.748: INFO: Pod "pod-d87c7448-0a7c-495c-8b61-177de9def7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.283502ms
Sep  6 23:18:51.751: INFO: Pod "pod-d87c7448-0a7c-495c-8b61-177de9def7e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024911147s
STEP: Saw pod success
Sep  6 23:18:51.751: INFO: Pod "pod-d87c7448-0a7c-495c-8b61-177de9def7e1" satisfied condition "success or failure"
Sep  6 23:18:51.755: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-d87c7448-0a7c-495c-8b61-177de9def7e1 container test-container: <nil>
STEP: delete the pod
Sep  6 23:18:51.775: INFO: Waiting for pod pod-d87c7448-0a7c-495c-8b61-177de9def7e1 to disappear
Sep  6 23:18:51.777: INFO: Pod pod-d87c7448-0a7c-495c-8b61-177de9def7e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:18:51.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4795" for this suite.
Sep  6 23:18:57.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:18:57.963: INFO: namespace emptydir-4795 deletion completed in 6.178161148s

• [SLOW TEST:8.300 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:18:57.963: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 23:19:00.106: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:19:00.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5312" for this suite.
Sep  6 23:19:06.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:19:06.353: INFO: namespace container-runtime-5312 deletion completed in 6.187309315s

• [SLOW TEST:8.390 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:19:06.354: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-44c21ca9-5606-4830-942b-ffeedccc63c8
STEP: Creating a pod to test consume configMaps
Sep  6 23:19:06.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74" in namespace "projected-8568" to be "success or failure"
Sep  6 23:19:06.456: INFO: Pod "pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74": Phase="Pending", Reason="", readiness=false. Elapsed: 12.105886ms
Sep  6 23:19:08.461: INFO: Pod "pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017001036s
STEP: Saw pod success
Sep  6 23:19:08.461: INFO: Pod "pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74" satisfied condition "success or failure"
Sep  6 23:19:08.464: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 23:19:08.481: INFO: Waiting for pod pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74 to disappear
Sep  6 23:19:08.484: INFO: Pod pod-projected-configmaps-dfea9de8-e4e0-4211-852c-735555926b74 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:19:08.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8568" for this suite.
Sep  6 23:19:14.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:19:14.709: INFO: namespace projected-8568 deletion completed in 6.221742283s

• [SLOW TEST:8.355 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:19:14.710: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Sep  6 23:19:14.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 create -f - --namespace=kubectl-4329'
Sep  6 23:19:14.944: INFO: stderr: ""
Sep  6 23:19:14.944: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 23:19:14.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:15.054: INFO: stderr: ""
Sep  6 23:19:15.054: INFO: stdout: "update-demo-nautilus-2p5jc update-demo-nautilus-txvdx "
Sep  6 23:19:15.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-2p5jc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:15.162: INFO: stderr: ""
Sep  6 23:19:15.162: INFO: stdout: ""
Sep  6 23:19:15.162: INFO: update-demo-nautilus-2p5jc is created but not running
Sep  6 23:19:20.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:20.285: INFO: stderr: ""
Sep  6 23:19:20.285: INFO: stdout: "update-demo-nautilus-2p5jc update-demo-nautilus-txvdx "
Sep  6 23:19:20.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-2p5jc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:20.400: INFO: stderr: ""
Sep  6 23:19:20.400: INFO: stdout: "true"
Sep  6 23:19:20.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-2p5jc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:20.474: INFO: stderr: ""
Sep  6 23:19:20.474: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:19:20.474: INFO: validating pod update-demo-nautilus-2p5jc
Sep  6 23:19:20.495: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:19:20.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:19:20.495: INFO: update-demo-nautilus-2p5jc is verified up and running
Sep  6 23:19:20.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-txvdx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:20.571: INFO: stderr: ""
Sep  6 23:19:20.571: INFO: stdout: "true"
Sep  6 23:19:20.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-txvdx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:20.645: INFO: stderr: ""
Sep  6 23:19:20.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:19:20.645: INFO: validating pod update-demo-nautilus-txvdx
Sep  6 23:19:20.665: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:19:20.665: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:19:20.665: INFO: update-demo-nautilus-txvdx is verified up and running
STEP: scaling down the replication controller
Sep  6 23:19:20.666: INFO: scanned /root for discovery docs: <nil>
Sep  6 23:19:20.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4329'
Sep  6 23:19:21.771: INFO: stderr: ""
Sep  6 23:19:21.771: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 23:19:21.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:21.852: INFO: stderr: ""
Sep  6 23:19:21.852: INFO: stdout: "update-demo-nautilus-2p5jc update-demo-nautilus-txvdx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 23:19:26.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:26.927: INFO: stderr: ""
Sep  6 23:19:26.927: INFO: stdout: "update-demo-nautilus-2p5jc update-demo-nautilus-txvdx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 23:19:31.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:32.012: INFO: stderr: ""
Sep  6 23:19:32.012: INFO: stdout: "update-demo-nautilus-txvdx "
Sep  6 23:19:32.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-txvdx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:32.083: INFO: stderr: ""
Sep  6 23:19:32.083: INFO: stdout: "true"
Sep  6 23:19:32.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-txvdx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:32.162: INFO: stderr: ""
Sep  6 23:19:32.162: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:19:32.162: INFO: validating pod update-demo-nautilus-txvdx
Sep  6 23:19:32.172: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:19:32.172: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:19:32.172: INFO: update-demo-nautilus-txvdx is verified up and running
STEP: scaling up the replication controller
Sep  6 23:19:32.174: INFO: scanned /root for discovery docs: <nil>
Sep  6 23:19:32.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4329'
Sep  6 23:19:33.281: INFO: stderr: ""
Sep  6 23:19:33.281: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 23:19:33.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:33.360: INFO: stderr: ""
Sep  6 23:19:33.360: INFO: stdout: "update-demo-nautilus-nb6fc update-demo-nautilus-txvdx "
Sep  6 23:19:33.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-nb6fc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:33.430: INFO: stderr: ""
Sep  6 23:19:33.430: INFO: stdout: ""
Sep  6 23:19:33.430: INFO: update-demo-nautilus-nb6fc is created but not running
Sep  6 23:19:38.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4329'
Sep  6 23:19:38.501: INFO: stderr: ""
Sep  6 23:19:38.501: INFO: stdout: "update-demo-nautilus-nb6fc update-demo-nautilus-txvdx "
Sep  6 23:19:38.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-nb6fc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:38.569: INFO: stderr: ""
Sep  6 23:19:38.569: INFO: stdout: "true"
Sep  6 23:19:38.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-nb6fc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:38.645: INFO: stderr: ""
Sep  6 23:19:38.645: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:19:38.645: INFO: validating pod update-demo-nautilus-nb6fc
Sep  6 23:19:38.658: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:19:38.658: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:19:38.658: INFO: update-demo-nautilus-nb6fc is verified up and running
Sep  6 23:19:38.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-txvdx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:38.737: INFO: stderr: ""
Sep  6 23:19:38.737: INFO: stdout: "true"
Sep  6 23:19:38.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods update-demo-nautilus-txvdx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4329'
Sep  6 23:19:38.822: INFO: stderr: ""
Sep  6 23:19:38.822: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 23:19:38.822: INFO: validating pod update-demo-nautilus-txvdx
Sep  6 23:19:38.829: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 23:19:38.829: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 23:19:38.829: INFO: update-demo-nautilus-txvdx is verified up and running
STEP: using delete to clean up resources
Sep  6 23:19:38.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 delete --grace-period=0 --force -f - --namespace=kubectl-4329'
Sep  6 23:19:38.915: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 23:19:38.915: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 23:19:38.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4329'
Sep  6 23:19:39.026: INFO: stderr: "No resources found.\n"
Sep  6 23:19:39.026: INFO: stdout: ""
Sep  6 23:19:39.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -l name=update-demo --namespace=kubectl-4329 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 23:19:39.113: INFO: stderr: ""
Sep  6 23:19:39.113: INFO: stdout: "update-demo-nautilus-nb6fc\nupdate-demo-nautilus-txvdx\n"
Sep  6 23:19:39.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4329'
Sep  6 23:19:39.726: INFO: stderr: "No resources found.\n"
Sep  6 23:19:39.726: INFO: stdout: ""
Sep  6 23:19:39.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-201627442 get pods -l name=update-demo --namespace=kubectl-4329 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 23:19:39.828: INFO: stderr: ""
Sep  6 23:19:39.828: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:19:39.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4329" for this suite.
Sep  6 23:19:45.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:19:46.075: INFO: namespace kubectl-4329 deletion completed in 6.230013076s

• [SLOW TEST:31.365 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:19:46.076: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Sep  6 23:19:46.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86" in namespace "downward-api-6117" to be "success or failure"
Sep  6 23:19:46.166: INFO: Pod "downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.419333ms
Sep  6 23:19:48.170: INFO: Pod "downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016556543s
STEP: Saw pod success
Sep  6 23:19:48.170: INFO: Pod "downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86" satisfied condition "success or failure"
Sep  6 23:19:48.173: INFO: Trying to get logs from node metalk8s-24-node1 pod downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86 container client-container: <nil>
STEP: delete the pod
Sep  6 23:19:48.196: INFO: Waiting for pod downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86 to disappear
Sep  6 23:19:48.199: INFO: Pod downwardapi-volume-be2c3b1a-4f49-480c-acac-2c7bf4fdaa86 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:19:48.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6117" for this suite.
Sep  6 23:19:54.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:19:54.352: INFO: namespace downward-api-6117 deletion completed in 6.149653745s

• [SLOW TEST:8.277 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:19:54.352: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Sep  6 23:19:54.409: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 23:19:54.439: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 23:19:54.455: INFO: 
Logging pods the kubelet thinks is on node metalk8s-24 before test
Sep  6 23:19:54.564: INFO: prometheus-operator-86bbccc5c5-bpgr7 from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.564: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep  6 23:19:54.564: INFO: prometheus-k8s-0 from metalk8s-monitoring started at 2019-09-06 16:49:49 +0000 UTC (3 container statuses recorded)
Sep  6 23:19:54.564: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 23:19:54.564: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 23:19:54.564: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 23:19:54.565: INFO: repositories-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container repositories ready: true, restart count 0
Sep  6 23:19:54.565: INFO: alertmanager-main-1 from metalk8s-monitoring started at 2019-09-06 16:49:51 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 23:19:54.565: INFO: alertmanager-main-2 from metalk8s-monitoring started at 2019-09-06 16:49:59 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 23:19:54.565: INFO: metalk8s-ui-94545c497-vnq2q from metalk8s-ui started at 2019-09-06 16:55:16 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container metalk8s-ui ready: true, restart count 0
Sep  6 23:19:54.565: INFO: salt-master-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container salt-api ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container salt-master ready: true, restart count 0
Sep  6 23:19:54.565: INFO: calico-kube-controllers-696f846f6b-lv8ss from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  6 23:19:54.565: INFO: kube-controller-manager-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  6 23:19:54.565: INFO: node-exporter-qsfqz from metalk8s-monitoring started at 2019-09-06 16:45:02 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 23:19:54.565: INFO: kube-proxy-ftwr7 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 23:19:54.565: INFO: prometheus-k8s-1 from metalk8s-monitoring started at 2019-09-06 16:49:49 +0000 UTC (3 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container prometheus ready: true, restart count 1
Sep  6 23:19:54.565: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep  6 23:19:54.565: INFO: kube-state-metrics-8d6c9b57-mgpd6 from metalk8s-monitoring started at 2019-09-06 21:23:05 +0000 UTC (4 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container addon-resizer ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  6 23:19:54.565: INFO: coredns-7df84d55f8-gsmw8 from kube-system started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container coredns ready: true, restart count 1
Sep  6 23:19:54.565: INFO: alertmanager-main-0 from metalk8s-monitoring started at 2019-09-06 16:49:42 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container alertmanager ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 	Container config-reloader ready: true, restart count 0
Sep  6 23:19:54.565: INFO: etcd-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container etcd ready: true, restart count 0
Sep  6 23:19:54.565: INFO: prometheus-adapter-764ffd477c-f57gq from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container prometheus-adapter ready: true, restart count 0
Sep  6 23:19:54.565: INFO: storage-operator-6fc69d48bd-kvwr9 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container storage-operator ready: true, restart count 0
Sep  6 23:19:54.565: INFO: sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-k452p from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 23:19:54.565: INFO: 	Container systemd-logs ready: true, restart count 1
Sep  6 23:19:54.565: INFO: kube-scheduler-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container kube-scheduler ready: true, restart count 0
Sep  6 23:19:54.565: INFO: nginx-ingress-controller-v6vmq from metalk8s-ingress started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 23:19:54.565: INFO: nginx-ingress-default-backend-657d8c587c-wqzq8 from metalk8s-ingress started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Sep  6 23:19:54.565: INFO: grafana-5ff47b469d-w68rc from metalk8s-monitoring started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container grafana ready: true, restart count 0
Sep  6 23:19:54.565: INFO: calico-node-wpmfl from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 23:19:54.565: INFO: coredns-7df84d55f8-2gxk7 from kube-system started at 2019-09-06 16:49:24 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container coredns ready: true, restart count 1
Sep  6 23:19:54.565: INFO: kube-apiserver-metalk8s-24 from kube-system started at 2019-09-06 16:45:02 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.565: INFO: 	Container kube-apiserver ready: true, restart count 0
Sep  6 23:19:54.565: INFO: 
Logging pods the kubelet thinks is on node metalk8s-24-node1 before test
Sep  6 23:19:54.596: INFO: kube-proxy-w5xhn from kube-system started at 2019-09-06 21:50:56 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 23:19:54.596: INFO: nginx-ingress-controller-c8rsj from metalk8s-ingress started at 2019-09-06 21:50:56 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Sep  6 23:19:54.596: INFO: sonobuoy-e2e-job-c91c5586720441aa from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container e2e ready: true, restart count 0
Sep  6 23:19:54.596: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  6 23:19:54.596: INFO: node-exporter-5rd8w from metalk8s-monitoring started at 2019-09-06 21:50:54 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 23:19:54.596: INFO: 	Container node-exporter ready: true, restart count 0
Sep  6 23:19:54.596: INFO: calico-node-d62tz from kube-system started at 2019-09-06 21:50:54 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container calico-node ready: true, restart count 0
Sep  6 23:19:54.596: INFO: sonobuoy from heptio-sonobuoy started at 2019-09-06 21:53:31 +0000 UTC (1 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  6 23:19:54.596: INFO: sonobuoy-systemd-logs-daemon-set-db98c9dc639d4ec7-2vx68 from heptio-sonobuoy started at 2019-09-06 21:53:36 +0000 UTC (2 container statuses recorded)
Sep  6 23:19:54.596: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep  6 23:19:54.596: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-096b0bc0-ab82-4f34-b6ba-2ea39ace2a2a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-096b0bc0-ab82-4f34-b6ba-2ea39ace2a2a off the node metalk8s-24-node1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-096b0bc0-ab82-4f34-b6ba-2ea39ace2a2a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:19:58.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4329" for this suite.
Sep  6 23:20:20.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:20:20.953: INFO: namespace sched-pred-4329 deletion completed in 22.251243476s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:26.601 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:20:20.953: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6206
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  6 23:20:21.006: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep  6 23:20:37.161: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.223.223:8080/dial?request=hostName&protocol=http&host=10.233.0.54&port=8080&tries=1'] Namespace:pod-network-test-6206 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:20:37.161: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:20:37.296: INFO: Waiting for endpoints: map[]
Sep  6 23:20:37.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.223.223:8080/dial?request=hostName&protocol=http&host=10.233.223.222&port=8080&tries=1'] Namespace:pod-network-test-6206 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:20:37.304: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:20:37.448: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:20:37.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6206" for this suite.
Sep  6 23:21:01.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:21:01.755: INFO: namespace pod-network-test-6206 deletion completed in 24.295587314s

• [SLOW TEST:40.801 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:21:01.755: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  6 23:21:01.864: INFO: Waiting up to 5m0s for pod "pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef" in namespace "emptydir-3733" to be "success or failure"
Sep  6 23:21:01.868: INFO: Pod "pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90844ms
Sep  6 23:21:03.872: INFO: Pod "pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008050406s
STEP: Saw pod success
Sep  6 23:21:03.872: INFO: Pod "pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef" satisfied condition "success or failure"
Sep  6 23:21:03.875: INFO: Trying to get logs from node metalk8s-24-node1 pod pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef container test-container: <nil>
STEP: delete the pod
Sep  6 23:21:03.905: INFO: Waiting for pod pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef to disappear
Sep  6 23:21:03.908: INFO: Pod pod-3653044d-ddd0-4354-b0e9-60dceeaa41ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:21:03.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3733" for this suite.
Sep  6 23:21:09.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:21:10.184: INFO: namespace emptydir-3733 deletion completed in 6.271661156s

• [SLOW TEST:8.429 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Sep  6 23:21:10.184: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep  6 23:21:14.338: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:14.338: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:14.479: INFO: Exec stderr: ""
Sep  6 23:21:14.479: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:14.479: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:14.601: INFO: Exec stderr: ""
Sep  6 23:21:14.601: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:14.714: INFO: Exec stderr: ""
Sep  6 23:21:14.714: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:14.714: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:14.863: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep  6 23:21:14.863: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:14.863: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:15.014: INFO: Exec stderr: ""
Sep  6 23:21:15.014: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:15.014: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:15.164: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep  6 23:21:15.164: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:15.164: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:15.288: INFO: Exec stderr: ""
Sep  6 23:21:15.288: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:15.288: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:15.434: INFO: Exec stderr: ""
Sep  6 23:21:15.434: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:15.434: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:15.558: INFO: Exec stderr: ""
Sep  6 23:21:15.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-649 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  6 23:21:15.558: INFO: >>> kubeConfig: /tmp/kubeconfig-201627442
Sep  6 23:21:15.677: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Sep  6 23:21:15.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-649" for this suite.
Sep  6 23:21:55.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep  6 23:21:55.830: INFO: namespace e2e-kubelet-etc-hosts-649 deletion completed in 40.148089523s

• [SLOW TEST:45.646 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSep  6 23:21:55.830: INFO: Running AfterSuite actions on all nodes
Sep  6 23:21:55.831: INFO: Running AfterSuite actions on node 1
Sep  6 23:21:55.831: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5272.736 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h27m54.094900488s
Test Suite Passed
